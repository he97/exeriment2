[2022-10-02 15:47:26 demo] (houston_program2.py 721): INFO Full config saved to outputs/demo/finetune_100/config.json
[2022-10-02 15:47:26 demo] (houston_program2.py 724): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 32
  CHANNEL_DIM: 48
  CLASS_NUM: 7
  DATASET: huston13-18
  DATA_PATH: ''
  DATA_SOURCE_PATH: dataset/houston13-18/Houston13.mat
  DATA_TARGET_PATH: dataset/houston13-18/Houston18.mat
  DIM: 48
  EPOCHS: 80
  GAMMA: 0.9
  HALFWIDTH: 2
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  LABEL_SOURCE_PATH: dataset/houston13-18/Houston13_7gt.mat
  LABEL_TARGET_PATH: dataset/houston13-18/Houston18_7gt.mat
  LEARNING_RATE: 2.5e-05
  MASK_PATCH_SIZE: 4
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  N_CLASS: 32
  PATCH_DIM: 512
  PIN_MEMORY: true
  SAMPLE_NUM: 180
  SEED: 0
EVAL_MODE: false
IS_DIST: false
IS_HSI: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  Dtransformer:
    APE: false
    DEPTH: 2
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 6
  LABEL_SMOOTHING: 0.0
  LEARNING_RATE: 2.5e-05
  NAME: demo
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: Dtransformer
  VIT:
    DEPTH: 12
    EMBED_DIM: 768
    INIT_VALUES: 0.1
    IN_CHANS: 3
    MLP_RATIO: 4
    NUM_HEADS: 12
    PATCH_SIZE: 16
    QKV_BIAS: true
    USE_APE: false
    USE_MEAN_POOLING: false
    USE_RPB: false
    USE_SHARED_RPB: true
OUTPUT: outputs/demo/finetune_100
PRETRAINED: ''
PRINT_FREQ: 100
SAVE_FREQ: 5
SEED: 0
TAG: finetune_100
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_LR: 5.0e-05
  CLIP_GRAD: 5.0
  EPOCHS: 100
  FINETUNE:
    BASE_LR: 0.005
    MIN_LR: 5.0e-07
    WARMUP_EPOCHS: 10
    WARMUP_LR: 5.0e-06
    WEIGHT_DECAY: 0.05
  LAYER_DECAY: 0.7
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
  MIN_LR: 1.5625e-08
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2022-10-02 15:47:36 demo] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 15:47:36 demo] (optimizer.py 27): INFO No weight decay: {'encoder.absolute_pos_embed'}
[2022-10-02 15:47:36 demo] (optimizer.py 30): INFO No weight decay keywords: {'encoder.relative_position_bias_table'}
[2022-10-02 15:47:36 demo] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed.proj.bias', 'encoder.patch_to_embedding.bias', 'encoder.attn_layers.layers.0.0.weight', 'encoder.attn_layers.layers.0.0.bias', 'encoder.attn_layers.layers.0.1.to_out.bias', 'encoder.attn_layers.layers.1.0.weight', 'encoder.attn_layers.layers.1.0.bias', 'encoder.attn_layers.layers.1.1.net.0.0.bias', 'encoder.attn_layers.layers.1.1.net.2.bias', 'encoder.attn_layers.layers.2.0.weight', 'encoder.attn_layers.layers.2.0.bias', 'encoder.attn_layers.layers.2.1.to_out.bias', 'encoder.attn_layers.layers.3.0.weight', 'encoder.attn_layers.layers.3.0.bias', 'encoder.attn_layers.layers.3.1.net.0.0.bias', 'encoder.attn_layers.layers.3.1.net.2.bias', 'encoder.norm.weight', 'encoder.norm.bias', 'decoder.0.bias']
[2022-10-02 15:47:36 demo] (optimizer.py 64): INFO Has decay params: ['encoder.pos_embedding', 'encoder.cls_token', 'encoder.mask_token', 'encoder.patch_embed.proj.weight', 'encoder.patch_to_embedding.weight', 'encoder.attn_layers.layers.0.1.to_q.weight', 'encoder.attn_layers.layers.0.1.to_k.weight', 'encoder.attn_layers.layers.0.1.to_v.weight', 'encoder.attn_layers.layers.0.1.to_out.weight', 'encoder.attn_layers.layers.1.1.net.0.0.weight', 'encoder.attn_layers.layers.1.1.net.2.weight', 'encoder.attn_layers.layers.2.1.to_q.weight', 'encoder.attn_layers.layers.2.1.to_k.weight', 'encoder.attn_layers.layers.2.1.to_v.weight', 'encoder.attn_layers.layers.2.1.to_out.weight', 'encoder.attn_layers.layers.3.1.net.0.0.weight', 'encoder.attn_layers.layers.3.1.net.2.weight', 'decoder.0.weight']
[2022-10-02 15:47:36 demo] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 15:47:36 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 15:47:36 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 15:47:36 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 15:47:36 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 15:47:36 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 15:47:36 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 15:47:36 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 15:47:36 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 15:47:36 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 15:47:36 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 15:47:36 demo] (houston_program2.py 109): INFO Start training
[2022-10-02 15:47:37 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0245 (0.0245)	loss 0.0751 (0.0751)	grad_norm 0.0324 (0.0324)	mem 177MB
[2022-10-02 15:47:37 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:37 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0228 (0.0228)	loss 0.0774 (0.0774)	grad_norm 0.0330 (0.0330)	mem 371MB
[2022-10-02 15:47:37 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:37 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0143 (0.0143)	loss 0.0832 (0.0832)	grad_norm 0.0323 (0.0323)	mem 371MB
[2022-10-02 15:47:37 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:37 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0194 (0.0194)	loss 0.0893 (0.0893)	grad_norm 0.0302 (0.0302)	mem 371MB
[2022-10-02 15:47:37 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:37 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0132 (0.0132)	loss 0.0840 (0.0840)	grad_norm 0.0298 (0.0298)	mem 371MB
[2022-10-02 15:47:37 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:38 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0215 (0.0215)	loss 0.0791 (0.0791)	grad_norm 0.0332 (0.0332)	mem 371MB
[2022-10-02 15:47:38 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:38 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0107 (0.0107)	loss 0.0983 (0.0983)	grad_norm 0.0326 (0.0326)	mem 371MB
[2022-10-02 15:47:38 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:38 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0202 (0.0202)	loss 0.0788 (0.0788)	grad_norm 0.0371 (0.0371)	mem 371MB
[2022-10-02 15:47:38 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:38 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0119 (0.0119)	loss 0.0711 (0.0711)	grad_norm 0.0361 (0.0361)	mem 371MB
[2022-10-02 15:47:38 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:38 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0216 (0.0216)	loss 0.0798 (0.0798)	grad_norm 0.0376 (0.0376)	mem 371MB
[2022-10-02 15:47:38 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:38 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0150 (0.0150)	loss 0.0842 (0.0842)	grad_norm 0.0334 (0.0334)	mem 371MB
[2022-10-02 15:47:38 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:39 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0210 (0.0210)	loss 0.0711 (0.0711)	grad_norm 0.0382 (0.0382)	mem 371MB
[2022-10-02 15:47:39 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:39 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0199 (0.0199)	loss 0.0799 (0.0799)	grad_norm 0.0308 (0.0308)	mem 371MB
[2022-10-02 15:47:39 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:39 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0162 (0.0162)	loss 0.0882 (0.0882)	grad_norm 0.0399 (0.0399)	mem 371MB
[2022-10-02 15:47:39 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:39 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0240 (0.0240)	loss 0.0744 (0.0744)	grad_norm 0.0359 (0.0359)	mem 371MB
[2022-10-02 15:47:39 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:39 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0206 (0.0206)	loss 0.0875 (0.0875)	grad_norm 0.0339 (0.0339)	mem 371MB
[2022-10-02 15:47:39 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:39 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0218 (0.0218)	loss 0.0839 (0.0839)	grad_norm 0.0310 (0.0310)	mem 371MB
[2022-10-02 15:47:39 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:40 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0173 (0.0173)	loss 0.0752 (0.0752)	grad_norm 0.0322 (0.0322)	mem 371MB
[2022-10-02 15:47:40 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:40 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0198 (0.0198)	loss 0.0700 (0.0700)	grad_norm 0.0362 (0.0362)	mem 371MB
[2022-10-02 15:47:40 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:40 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0163 (0.0163)	loss 0.0942 (0.0942)	grad_norm 0.0292 (0.0292)	mem 371MB
[2022-10-02 15:47:40 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:40 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0218 (0.0218)	loss 0.0753 (0.0753)	grad_norm 0.0347 (0.0347)	mem 371MB
[2022-10-02 15:47:40 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:40 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0144 (0.0144)	loss 0.0947 (0.0947)	grad_norm 0.0293 (0.0293)	mem 371MB
[2022-10-02 15:47:40 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:40 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0199 (0.0199)	loss 0.0623 (0.0623)	grad_norm 0.0355 (0.0355)	mem 371MB
[2022-10-02 15:47:41 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:41 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0113 (0.0113)	loss 0.0855 (0.0855)	grad_norm 0.0320 (0.0320)	mem 371MB
[2022-10-02 15:47:41 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:41 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0198 (0.0198)	loss 0.0792 (0.0792)	grad_norm 0.0301 (0.0301)	mem 371MB
[2022-10-02 15:47:41 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:41 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0131 (0.0131)	loss 0.0879 (0.0879)	grad_norm 0.0278 (0.0278)	mem 371MB
[2022-10-02 15:47:41 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:41 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0221 (0.0221)	loss 0.0724 (0.0724)	grad_norm 0.0332 (0.0332)	mem 371MB
[2022-10-02 15:47:41 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:41 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0118 (0.0118)	loss 0.0783 (0.0783)	grad_norm 0.0304 (0.0304)	mem 371MB
[2022-10-02 15:47:41 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:42 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0221 (0.0221)	loss 0.0786 (0.0786)	grad_norm 0.0297 (0.0297)	mem 371MB
[2022-10-02 15:47:42 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:42 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0123 (0.0123)	loss 0.0711 (0.0711)	grad_norm 0.0296 (0.0296)	mem 371MB
[2022-10-02 15:47:42 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:42 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0203 (0.0203)	loss 0.0621 (0.0621)	grad_norm 0.0333 (0.0333)	mem 371MB
[2022-10-02 15:47:42 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:42 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0146 (0.0146)	loss 0.0977 (0.0977)	grad_norm 0.0289 (0.0289)	mem 371MB
[2022-10-02 15:47:42 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:42 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0209 (0.0209)	loss 0.0834 (0.0834)	grad_norm 0.0309 (0.0309)	mem 371MB
[2022-10-02 15:47:42 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:42 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0123 (0.0123)	loss 0.0666 (0.0666)	grad_norm 0.0290 (0.0290)	mem 371MB
[2022-10-02 15:47:42 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:43 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0151 (0.0151)	loss 0.0758 (0.0758)	grad_norm 0.0280 (0.0280)	mem 371MB
[2022-10-02 15:47:43 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:43 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0155 (0.0155)	loss 0.0715 (0.0715)	grad_norm 0.0297 (0.0297)	mem 371MB
[2022-10-02 15:47:43 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:43 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0134 (0.0134)	loss 0.0986 (0.0986)	grad_norm 0.0273 (0.0273)	mem 371MB
[2022-10-02 15:47:43 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:43 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0205 (0.0205)	loss 0.0788 (0.0788)	grad_norm 0.0369 (0.0369)	mem 371MB
[2022-10-02 15:47:43 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:43 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0159 (0.0159)	loss 0.0779 (0.0779)	grad_norm 0.0350 (0.0350)	mem 371MB
[2022-10-02 15:47:43 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 15:47:44 demo] (houston_program2.py 333): INFO Train Ep: 0 	Loss1: 0.617298	Loss2: 0.647448	 Dis: 13.595806 Entropy: 4.372677 
[2022-10-02 15:47:44 demo] (houston_program2.py 335): INFO time_0_epoch:7.610705614089966
[2022-10-02 15:47:44 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 15:47:44 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 15:47:44 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 15:47:44 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 15:47:44 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 15:47:44 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 15:47:44 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 15:47:44 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 15:47:44 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 15:47:44 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 15:47:50 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.506128	Loss2: 0.563586	 Dis: 15.604969 Entropy: 4.709267 
[2022-10-02 15:47:50 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:47:57 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.273915	Loss2: 0.294403	 Dis: 17.043392 Entropy: 4.564573 
[2022-10-02 15:47:57 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:48:03 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.484168	Loss2: 0.502485	 Dis: 12.250050 Entropy: 4.095652 
[2022-10-02 15:48:03 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:48:10 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.374704	Loss2: 0.346228	 Dis: 10.629948 Entropy: 4.338943 
[2022-10-02 15:48:10 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:48:16 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.360536	Loss2: 0.385624	 Dis: 9.981983 Entropy: 5.094683 
[2022-10-02 15:48:16 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:48:22 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.333262	Loss2: 0.360971	 Dis: 11.114079 Entropy: 4.854876 
[2022-10-02 15:48:22 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:48:29 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.352258	Loss2: 0.331609	 Dis: 11.000095 Entropy: 5.149246 
[2022-10-02 15:48:29 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:48:35 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.253277	Loss2: 0.258841	 Dis: 7.147648 Entropy: 4.230400 
[2022-10-02 15:48:35 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:48:41 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.441504	Loss2: 0.372976	 Dis: 10.394970 Entropy: 5.151466 
[2022-10-02 15:48:41 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:48:48 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.211361	Loss2: 0.200756	 Dis: 9.331871 Entropy: 4.426788 
[2022-10-02 15:48:48 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:48:54 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.342783	Loss2: 0.322589	 Dis: 8.892921 Entropy: 4.556499 
[2022-10-02 15:48:54 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:49:00 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.335162	Loss2: 0.354823	 Dis: 8.388241 Entropy: 4.700541 
[2022-10-02 15:49:00 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:49:07 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.080840	Loss2: 0.075917	 Dis: 10.411682 Entropy: 4.399092 
[2022-10-02 15:49:07 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:49:13 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.210700	Loss2: 0.217072	 Dis: 7.838205 Entropy: 4.451155 
[2022-10-02 15:49:13 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:49:20 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.334138	Loss2: 0.369665	 Dis: 7.738554 Entropy: 5.027869 
[2022-10-02 15:49:20 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:49:26 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.173929	Loss2: 0.180226	 Dis: 6.747967 Entropy: 5.111948 
[2022-10-02 15:49:26 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:49:33 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.371461	Loss2: 0.375459	 Dis: 6.164463 Entropy: 5.248742 
[2022-10-02 15:49:33 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:49:39 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.169379	Loss2: 0.159147	 Dis: 7.170843 Entropy: 4.690690 
[2022-10-02 15:49:39 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:49:46 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.148198	Loss2: 0.132878	 Dis: 7.765957 Entropy: 4.990176 
[2022-10-02 15:49:46 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:49:52 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.244213	Loss2: 0.267719	 Dis: 9.010134 Entropy: 4.913832 
[2022-10-02 15:49:52 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:49:57 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.227852	Loss2: 0.180274	 Dis: 7.923437 Entropy: 4.359108 
[2022-10-02 15:49:57 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:50:03 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.341281	Loss2: 0.301774	 Dis: 7.467640 Entropy: 4.904906 
[2022-10-02 15:50:03 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:50:10 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.191863	Loss2: 0.222416	 Dis: 9.234108 Entropy: 4.353466 
[2022-10-02 15:50:10 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:50:16 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.176561	Loss2: 0.180338	 Dis: 8.324631 Entropy: 4.694196 
[2022-10-02 15:50:16 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:50:23 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.224978	Loss2: 0.213672	 Dis: 6.231678 Entropy: 4.479241 
[2022-10-02 15:50:23 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:50:29 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.160741	Loss2: 0.194593	 Dis: 8.446886 Entropy: 4.969556 
[2022-10-02 15:50:29 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:50:34 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.152714	Loss2: 0.140718	 Dis: 8.199894 Entropy: 4.546586 
[2022-10-02 15:50:34 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:50:41 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.178697	Loss2: 0.183034	 Dis: 5.918053 Entropy: 4.313620 
[2022-10-02 15:50:41 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:50:48 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.194666	Loss2: 0.197935	 Dis: 7.476105 Entropy: 5.424014 
[2022-10-02 15:50:48 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:50:54 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.213238	Loss2: 0.190376	 Dis: 6.911118 Entropy: 4.256251 
[2022-10-02 15:50:54 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:51:01 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.187622	Loss2: 0.189540	 Dis: 9.582779 Entropy: 4.558610 
[2022-10-02 15:51:01 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:51:07 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.203825	Loss2: 0.195070	 Dis: 10.143740 Entropy: 4.739956 
[2022-10-02 15:51:07 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:51:13 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.388519	Loss2: 0.375684	 Dis: 7.646944 Entropy: 4.277876 
[2022-10-02 15:51:13 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:51:19 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.273024	Loss2: 0.287352	 Dis: 9.349592 Entropy: 4.519094 
[2022-10-02 15:51:19 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:51:26 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.130148	Loss2: 0.144147	 Dis: 6.673401 Entropy: 5.279263 
[2022-10-02 15:51:26 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:51:32 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.203275	Loss2: 0.204857	 Dis: 7.262159 Entropy: 4.493731 
[2022-10-02 15:51:32 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:51:39 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.180318	Loss2: 0.202038	 Dis: 8.725323 Entropy: 4.795121 
[2022-10-02 15:51:39 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:51:45 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.288138	Loss2: 0.275012	 Dis: 11.309389 Entropy: 4.720235 
[2022-10-02 15:51:45 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:51:52 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.215037	Loss2: 0.247588	 Dis: 7.318213 Entropy: 4.565540 
[2022-10-02 15:51:52 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:51:58 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.070590	Loss2: 0.076037	 Dis: 7.063450 Entropy: 5.439449 
[2022-10-02 15:51:58 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:52:05 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.079330	Loss2: 0.104698	 Dis: 7.502302 Entropy: 5.028561 
[2022-10-02 15:52:05 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:52:11 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.124014	Loss2: 0.135310	 Dis: 8.241768 Entropy: 5.020988 
[2022-10-02 15:52:11 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:52:18 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.461546	Loss2: 0.510971	 Dis: 6.067978 Entropy: 4.696012 
[2022-10-02 15:52:18 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:52:24 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.126989	Loss2: 0.105737	 Dis: 7.304716 Entropy: 4.094332 
[2022-10-02 15:52:24 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:52:31 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 1.865578	Loss2: 1.954242	 Dis: 20.197123 Entropy: 4.233050 
[2022-10-02 15:52:31 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:52:37 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.292656	Loss2: 0.332113	 Dis: 12.115183 Entropy: 4.903618 
[2022-10-02 15:52:37 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:52:44 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.445393	Loss2: 0.441662	 Dis: 13.973082 Entropy: 4.632324 
[2022-10-02 15:52:44 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:52:50 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.221458	Loss2: 0.213882	 Dis: 9.599596 Entropy: 5.500161 
[2022-10-02 15:52:50 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:52:56 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.279211	Loss2: 0.243710	 Dis: 8.222420 Entropy: 4.517214 
[2022-10-02 15:52:56 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:53:02 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.256918	Loss2: 0.257133	 Dis: 7.312437 Entropy: 5.040261 
[2022-10-02 15:53:02 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:53:09 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.269203	Loss2: 0.256757	 Dis: 7.875208 Entropy: 4.326249 
[2022-10-02 15:53:09 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:53:15 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.209092	Loss2: 0.206529	 Dis: 8.542282 Entropy: 5.566554 
[2022-10-02 15:53:15 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:53:22 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.318436	Loss2: 0.302080	 Dis: 6.661047 Entropy: 5.225003 
[2022-10-02 15:53:22 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:53:29 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.150280	Loss2: 0.116962	 Dis: 8.447752 Entropy: 5.590103 
[2022-10-02 15:53:29 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:53:35 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.195588	Loss2: 0.150473	 Dis: 8.827768 Entropy: 5.706616 
[2022-10-02 15:53:35 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:53:42 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.176394	Loss2: 0.154970	 Dis: 8.693144 Entropy: 4.806613 
[2022-10-02 15:53:42 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:53:48 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.136852	Loss2: 0.128686	 Dis: 6.894880 Entropy: 5.581744 
[2022-10-02 15:53:48 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:53:54 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.232564	Loss2: 0.255335	 Dis: 7.475527 Entropy: 4.529971 
[2022-10-02 15:53:54 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:54:01 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.070211	Loss2: 0.095319	 Dis: 6.402222 Entropy: 5.804601 
[2022-10-02 15:54:01 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:54:07 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.171389	Loss2: 0.130669	 Dis: 7.934692 Entropy: 5.061093 
[2022-10-02 15:54:07 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:54:14 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.101454	Loss2: 0.083181	 Dis: 7.016434 Entropy: 4.790133 
[2022-10-02 15:54:14 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:54:20 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.185199	Loss2: 0.175864	 Dis: 5.752064 Entropy: 4.751989 
[2022-10-02 15:54:20 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:54:27 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.322122	Loss2: 0.239507	 Dis: 7.261169 Entropy: 5.157162 
[2022-10-02 15:54:27 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:54:33 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.143009	Loss2: 0.113157	 Dis: 6.683294 Entropy: 5.880236 
[2022-10-02 15:54:33 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:54:39 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.134409	Loss2: 0.129191	 Dis: 7.281111 Entropy: 4.959552 
[2022-10-02 15:54:39 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:54:45 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.038400	Loss2: 0.036678	 Dis: 5.705889 Entropy: 5.032866 
[2022-10-02 15:54:45 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:54:52 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.170220	Loss2: 0.175402	 Dis: 6.524538 Entropy: 5.695063 
[2022-10-02 15:54:52 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:54:58 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.054092	Loss2: 0.069732	 Dis: 6.520117 Entropy: 4.813729 
[2022-10-02 15:54:58 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:55:05 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.223377	Loss2: 0.189927	 Dis: 7.073545 Entropy: 4.865538 
[2022-10-02 15:55:05 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:55:12 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.213741	Loss2: 0.186922	 Dis: 8.476341 Entropy: 4.693202 
[2022-10-02 15:55:12 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:55:19 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.095914	Loss2: 0.075174	 Dis: 8.104412 Entropy: 5.204882 
[2022-10-02 15:55:19 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:55:25 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.143603	Loss2: 0.115072	 Dis: 4.029840 Entropy: 5.438034 
[2022-10-02 15:55:25 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:55:32 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.138628	Loss2: 0.171866	 Dis: 5.090437 Entropy: 5.296706 
[2022-10-02 15:55:32 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:55:38 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.146739	Loss2: 0.148225	 Dis: 5.716988 Entropy: 5.399792 
[2022-10-02 15:55:38 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:55:45 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.125493	Loss2: 0.143731	 Dis: 4.568089 Entropy: 4.829277 
[2022-10-02 15:55:45 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:55:51 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.183759	Loss2: 0.215131	 Dis: 7.286776 Entropy: 5.550663 
[2022-10-02 15:55:51 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:55:57 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.257414	Loss2: 0.243833	 Dis: 11.352720 Entropy: 4.690544 
[2022-10-02 15:55:57 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:56:04 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.167943	Loss2: 0.159989	 Dis: 6.442007 Entropy: 5.117547 
[2022-10-02 15:56:04 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:56:11 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.318900	Loss2: 0.306677	 Dis: 6.522734 Entropy: 5.549083 
[2022-10-02 15:56:11 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:56:17 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.100498	Loss2: 0.123843	 Dis: 8.161880 Entropy: 5.673842 
[2022-10-02 15:56:17 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:56:23 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.115071	Loss2: 0.144811	 Dis: 7.208200 Entropy: 5.043202 
[2022-10-02 15:56:23 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:56:30 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.137288	Loss2: 0.123755	 Dis: 5.405413 Entropy: 5.295778 
[2022-10-02 15:56:30 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:56:36 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.084191	Loss2: 0.081572	 Dis: 5.022337 Entropy: 4.736485 
[2022-10-02 15:56:36 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:56:43 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.254928	Loss2: 0.237764	 Dis: 7.402954 Entropy: 5.416495 
[2022-10-02 15:56:43 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:56:50 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.299071	Loss2: 0.272696	 Dis: 5.608620 Entropy: 5.684700 
[2022-10-02 15:56:50 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:56:56 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.144862	Loss2: 0.171067	 Dis: 6.729483 Entropy: 4.674904 
[2022-10-02 15:56:56 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:57:02 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.297546	Loss2: 0.277768	 Dis: 5.318644 Entropy: 5.954166 
[2022-10-02 15:57:02 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:57:09 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.135627	Loss2: 0.072188	 Dis: 3.806271 Entropy: 5.289743 
[2022-10-02 15:57:09 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:57:17 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.078334	Loss2: 0.104557	 Dis: 5.457436 Entropy: 5.264349 
[2022-10-02 15:57:17 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:57:23 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.139489	Loss2: 0.155823	 Dis: 4.950119 Entropy: 4.593763 
[2022-10-02 15:57:23 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:57:29 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.161479	Loss2: 0.234275	 Dis: 5.073160 Entropy: 5.033024 
[2022-10-02 15:57:29 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:57:36 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.123852	Loss2: 0.115078	 Dis: 3.934008 Entropy: 5.294047 
[2022-10-02 15:57:36 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:57:42 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.238186	Loss2: 0.240619	 Dis: 4.859024 Entropy: 5.197381 
[2022-10-02 15:57:42 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:57:49 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.081030	Loss2: 0.087419	 Dis: 4.870447 Entropy: 4.929270 
[2022-10-02 15:57:49 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:57:55 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.086944	Loss2: 0.087271	 Dis: 2.781384 Entropy: 5.184468 
[2022-10-02 15:57:55 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:58:02 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.105162	Loss2: 0.131773	 Dis: 5.755177 Entropy: 4.930595 
[2022-10-02 15:58:02 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:58:08 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.139549	Loss2: 0.156192	 Dis: 5.732920 Entropy: 5.327602 
[2022-10-02 15:58:08 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:58:14 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.052300	Loss2: 0.042031	 Dis: 6.606291 Entropy: 5.431843 
[2022-10-02 15:58:14 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:58:21 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.239214	Loss2: 0.228599	 Dis: 7.804752 Entropy: 5.001590 
[2022-10-02 15:58:21 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:58:27 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.110279	Loss2: 0.064584	 Dis: 6.929733 Entropy: 5.474597 
[2022-10-02 15:58:27 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:58:27 demo] (houston_program2.py 509): INFO time_0_epoch:643.6157009601593
[2022-10-02 15:58:36 demo] (houston_program2.py 667): INFO 	val_Accuracy: 32386/53200 (60.88%)	
[2022-10-02 15:58:36 demo] (utils.py 47): INFO outputs/demo/finetune_100/ckpt_epoch_0.pth saving......
[2022-10-02 15:58:36 demo] (utils.py 49): INFO outputs/demo/finetune_100/ckpt_epoch_0.pth saved !!!
[2022-10-02 15:58:36 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0157 (0.0157)	loss 0.0866 (0.0866)	grad_norm 0.0276 (0.0276)	mem 455MB
[2022-10-02 15:58:37 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:37 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0214 (0.0214)	loss 0.0624 (0.0624)	grad_norm 0.0284 (0.0284)	mem 458MB
[2022-10-02 15:58:37 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:37 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0205 (0.0205)	loss 0.0618 (0.0618)	grad_norm 0.0299 (0.0299)	mem 458MB
[2022-10-02 15:58:37 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:37 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0242 (0.0242)	loss 0.0762 (0.0762)	grad_norm 0.0290 (0.0290)	mem 458MB
[2022-10-02 15:58:37 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:37 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0177 (0.0177)	loss 0.0862 (0.0862)	grad_norm 0.0275 (0.0275)	mem 458MB
[2022-10-02 15:58:37 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:37 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0210 (0.0210)	loss 0.0789 (0.0789)	grad_norm 0.0252 (0.0252)	mem 458MB
[2022-10-02 15:58:37 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:38 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0239 (0.0239)	loss 0.0862 (0.0862)	grad_norm 0.0318 (0.0318)	mem 458MB
[2022-10-02 15:58:38 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:38 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0206 (0.0206)	loss 0.0678 (0.0678)	grad_norm 0.0265 (0.0265)	mem 458MB
[2022-10-02 15:58:38 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:38 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0210 (0.0210)	loss 0.0680 (0.0680)	grad_norm 0.0274 (0.0274)	mem 458MB
[2022-10-02 15:58:38 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:38 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0210 (0.0210)	loss 0.0859 (0.0859)	grad_norm 0.0302 (0.0302)	mem 458MB
[2022-10-02 15:58:38 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:38 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0208 (0.0208)	loss 0.0781 (0.0781)	grad_norm 0.0258 (0.0258)	mem 458MB
[2022-10-02 15:58:38 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:39 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0218 (0.0218)	loss 0.0954 (0.0954)	grad_norm 0.0296 (0.0296)	mem 458MB
[2022-10-02 15:58:39 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:39 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0208 (0.0208)	loss 0.0627 (0.0627)	grad_norm 0.0269 (0.0269)	mem 458MB
[2022-10-02 15:58:39 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:39 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0217 (0.0217)	loss 0.0768 (0.0768)	grad_norm 0.0262 (0.0262)	mem 458MB
[2022-10-02 15:58:39 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:39 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0126 (0.0126)	loss 0.0726 (0.0726)	grad_norm 0.0257 (0.0257)	mem 458MB
[2022-10-02 15:58:39 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:39 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0166 (0.0166)	loss 0.0860 (0.0860)	grad_norm 0.0254 (0.0254)	mem 458MB
[2022-10-02 15:58:39 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:39 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0189 (0.0189)	loss 0.0683 (0.0683)	grad_norm 0.0260 (0.0260)	mem 458MB
[2022-10-02 15:58:39 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:40 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0210 (0.0210)	loss 0.0704 (0.0704)	grad_norm 0.0270 (0.0270)	mem 458MB
[2022-10-02 15:58:40 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:40 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0219 (0.0219)	loss 0.0861 (0.0861)	grad_norm 0.0297 (0.0297)	mem 458MB
[2022-10-02 15:58:40 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:40 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0218 (0.0218)	loss 0.0703 (0.0703)	grad_norm 0.0242 (0.0242)	mem 458MB
[2022-10-02 15:58:40 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:40 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0226 (0.0226)	loss 0.0761 (0.0761)	grad_norm 0.0273 (0.0273)	mem 458MB
[2022-10-02 15:58:40 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:40 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0214 (0.0214)	loss 0.0659 (0.0659)	grad_norm 0.0310 (0.0310)	mem 458MB
[2022-10-02 15:58:40 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:41 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0194 (0.0194)	loss 0.0497 (0.0497)	grad_norm 0.0258 (0.0258)	mem 458MB
[2022-10-02 15:58:41 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:41 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0213 (0.0213)	loss 0.0796 (0.0796)	grad_norm 0.0295 (0.0295)	mem 458MB
[2022-10-02 15:58:41 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:41 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0206 (0.0206)	loss 0.0532 (0.0532)	grad_norm 0.0260 (0.0260)	mem 458MB
[2022-10-02 15:58:41 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:41 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0156 (0.0156)	loss 0.0760 (0.0760)	grad_norm 0.0352 (0.0352)	mem 458MB
[2022-10-02 15:58:41 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:41 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0196 (0.0196)	loss 0.0590 (0.0590)	grad_norm 0.0268 (0.0268)	mem 458MB
[2022-10-02 15:58:41 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:41 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0149 (0.0149)	loss 0.0707 (0.0707)	grad_norm 0.0347 (0.0347)	mem 458MB
[2022-10-02 15:58:41 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:42 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0217 (0.0217)	loss 0.0843 (0.0843)	grad_norm 0.0298 (0.0298)	mem 458MB
[2022-10-02 15:58:42 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:42 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0217 (0.0217)	loss 0.0710 (0.0710)	grad_norm 0.0278 (0.0278)	mem 458MB
[2022-10-02 15:58:42 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:42 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0212 (0.0212)	loss 0.0653 (0.0653)	grad_norm 0.0266 (0.0266)	mem 458MB
[2022-10-02 15:58:42 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:42 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0202 (0.0202)	loss 0.0598 (0.0598)	grad_norm 0.0286 (0.0286)	mem 458MB
[2022-10-02 15:58:42 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:42 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0196 (0.0196)	loss 0.0619 (0.0619)	grad_norm 0.0322 (0.0322)	mem 458MB
[2022-10-02 15:58:42 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:43 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0149 (0.0149)	loss 0.0518 (0.0518)	grad_norm 0.0258 (0.0258)	mem 458MB
[2022-10-02 15:58:43 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:43 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0202 (0.0202)	loss 0.0648 (0.0648)	grad_norm 0.0325 (0.0325)	mem 458MB
[2022-10-02 15:58:43 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:43 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0124 (0.0124)	loss 0.0653 (0.0653)	grad_norm 0.0291 (0.0291)	mem 458MB
[2022-10-02 15:58:43 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:43 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0191 (0.0191)	loss 0.0540 (0.0540)	grad_norm 0.0381 (0.0381)	mem 458MB
[2022-10-02 15:58:43 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:43 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0132 (0.0132)	loss 0.0585 (0.0585)	grad_norm 0.0334 (0.0334)	mem 458MB
[2022-10-02 15:58:43 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:43 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0194 (0.0194)	loss 0.0545 (0.0545)	grad_norm 0.0364 (0.0364)	mem 458MB
[2022-10-02 15:58:43 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 15:58:44 demo] (houston_program2.py 333): INFO Train Ep: 1 	Loss1: 0.585775	Loss2: 0.577120	 Dis: 13.916288 Entropy: 4.273290 
[2022-10-02 15:58:44 demo] (houston_program2.py 335): INFO time_1_epoch:7.9454262256622314
[2022-10-02 15:58:44 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0154 (0.0154)	loss 0.0543 (0.0543)	grad_norm 0.0351 (0.0351)	mem 458MB
[2022-10-02 15:58:44 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:45 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0215 (0.0215)	loss 0.0503 (0.0503)	grad_norm 0.0372 (0.0372)	mem 458MB
[2022-10-02 15:58:45 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:45 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0214 (0.0214)	loss 0.0528 (0.0528)	grad_norm 0.0319 (0.0319)	mem 458MB
[2022-10-02 15:58:45 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:45 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0194 (0.0194)	loss 0.0499 (0.0499)	grad_norm 0.0309 (0.0309)	mem 458MB
[2022-10-02 15:58:45 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:45 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0209 (0.0209)	loss 0.0643 (0.0643)	grad_norm 0.0313 (0.0313)	mem 458MB
[2022-10-02 15:58:45 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:45 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0188 (0.0188)	loss 0.0459 (0.0459)	grad_norm 0.0462 (0.0462)	mem 458MB
[2022-10-02 15:58:45 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:45 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0203 (0.0203)	loss 0.0608 (0.0608)	grad_norm 0.0387 (0.0387)	mem 458MB
[2022-10-02 15:58:45 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:46 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0201 (0.0201)	loss 0.0557 (0.0557)	grad_norm 0.0408 (0.0408)	mem 458MB
[2022-10-02 15:58:46 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:46 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0209 (0.0209)	loss 0.0425 (0.0425)	grad_norm 0.0345 (0.0345)	mem 458MB
[2022-10-02 15:58:46 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:46 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0174 (0.0174)	loss 0.0411 (0.0411)	grad_norm 0.0363 (0.0363)	mem 458MB
[2022-10-02 15:58:46 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:46 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0183 (0.0183)	loss 0.0521 (0.0521)	grad_norm 0.0384 (0.0384)	mem 458MB
[2022-10-02 15:58:46 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:46 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0184 (0.0184)	loss 0.0484 (0.0484)	grad_norm 0.0302 (0.0302)	mem 458MB
[2022-10-02 15:58:46 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:47 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0196 (0.0196)	loss 0.0632 (0.0632)	grad_norm 0.0352 (0.0352)	mem 458MB
[2022-10-02 15:58:47 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:47 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0157 (0.0157)	loss 0.0561 (0.0561)	grad_norm 0.0346 (0.0346)	mem 458MB
[2022-10-02 15:58:47 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:47 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0197 (0.0197)	loss 0.0402 (0.0402)	grad_norm 0.0333 (0.0333)	mem 458MB
[2022-10-02 15:58:47 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:47 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0124 (0.0124)	loss 0.0439 (0.0439)	grad_norm 0.0392 (0.0392)	mem 458MB
[2022-10-02 15:58:47 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:47 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0205 (0.0205)	loss 0.0629 (0.0629)	grad_norm 0.0364 (0.0364)	mem 458MB
[2022-10-02 15:58:47 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:47 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0150 (0.0150)	loss 0.0398 (0.0398)	grad_norm 0.0322 (0.0322)	mem 458MB
[2022-10-02 15:58:47 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:48 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0193 (0.0193)	loss 0.0368 (0.0368)	grad_norm 0.0355 (0.0355)	mem 458MB
[2022-10-02 15:58:48 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:48 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0292 (0.0292)	loss 0.0411 (0.0411)	grad_norm 0.0306 (0.0306)	mem 458MB
[2022-10-02 15:58:48 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:48 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0193 (0.0193)	loss 0.0391 (0.0391)	grad_norm 0.0324 (0.0324)	mem 458MB
[2022-10-02 15:58:48 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:48 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0205 (0.0205)	loss 0.0499 (0.0499)	grad_norm 0.0500 (0.0500)	mem 458MB
[2022-10-02 15:58:48 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:48 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0189 (0.0189)	loss 0.0478 (0.0478)	grad_norm 0.0460 (0.0460)	mem 458MB
[2022-10-02 15:58:48 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:48 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0210 (0.0210)	loss 0.0594 (0.0594)	grad_norm 0.0425 (0.0425)	mem 458MB
[2022-10-02 15:58:48 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:49 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0199 (0.0199)	loss 0.0469 (0.0469)	grad_norm 0.0442 (0.0442)	mem 458MB
[2022-10-02 15:58:49 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:49 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0215 (0.0215)	loss 0.0426 (0.0426)	grad_norm 0.0243 (0.0243)	mem 458MB
[2022-10-02 15:58:49 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:49 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0141 (0.0141)	loss 0.0453 (0.0453)	grad_norm 0.0275 (0.0275)	mem 458MB
[2022-10-02 15:58:49 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:49 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0199 (0.0199)	loss 0.0545 (0.0545)	grad_norm 0.0441 (0.0441)	mem 458MB
[2022-10-02 15:58:49 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:49 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0166 (0.0166)	loss 0.0430 (0.0430)	grad_norm 0.0368 (0.0368)	mem 458MB
[2022-10-02 15:58:49 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:50 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0195 (0.0195)	loss 0.0315 (0.0315)	grad_norm 0.0269 (0.0269)	mem 458MB
[2022-10-02 15:58:50 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:50 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0152 (0.0152)	loss 0.0409 (0.0409)	grad_norm 0.0317 (0.0317)	mem 458MB
[2022-10-02 15:58:50 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:50 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0082 (0.0082)	loss 0.0413 (0.0413)	grad_norm 0.0236 (0.0236)	mem 458MB
[2022-10-02 15:58:50 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:50 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0103 (0.0103)	loss 0.0550 (0.0550)	grad_norm 0.0279 (0.0279)	mem 458MB
[2022-10-02 15:58:50 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:50 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0082 (0.0082)	loss 0.0444 (0.0444)	grad_norm 0.0296 (0.0296)	mem 458MB
[2022-10-02 15:58:50 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:50 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0111 (0.0111)	loss 0.0380 (0.0380)	grad_norm 0.0244 (0.0244)	mem 458MB
[2022-10-02 15:58:50 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:50 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0170 (0.0170)	loss 0.0524 (0.0524)	grad_norm 0.0402 (0.0402)	mem 458MB
[2022-10-02 15:58:50 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:50 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0212 (0.0212)	loss 0.0367 (0.0367)	grad_norm 0.0271 (0.0271)	mem 458MB
[2022-10-02 15:58:50 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:51 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0189 (0.0189)	loss 0.0445 (0.0445)	grad_norm 0.0237 (0.0237)	mem 458MB
[2022-10-02 15:58:51 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:51 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0218 (0.0218)	loss 0.0444 (0.0444)	grad_norm 0.0430 (0.0430)	mem 458MB
[2022-10-02 15:58:51 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 15:58:51 demo] (houston_program2.py 333): INFO Train Ep: 2 	Loss1: 1.195568	Loss2: 1.199755	 Dis: 17.859039 Entropy: 4.186808 
[2022-10-02 15:58:51 demo] (houston_program2.py 335): INFO time_2_epoch:7.311514139175415
[2022-10-02 15:58:52 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0208 (0.0208)	loss 0.0359 (0.0359)	grad_norm 0.0468 (0.0468)	mem 458MB
[2022-10-02 15:58:52 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:52 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0210 (0.0210)	loss 0.0539 (0.0539)	grad_norm 0.0273 (0.0273)	mem 458MB
[2022-10-02 15:58:52 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:52 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0190 (0.0190)	loss 0.0388 (0.0388)	grad_norm 0.0372 (0.0372)	mem 458MB
[2022-10-02 15:58:52 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:52 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0153 (0.0153)	loss 0.0346 (0.0346)	grad_norm 0.0304 (0.0304)	mem 458MB
[2022-10-02 15:58:52 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:53 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0219 (0.0219)	loss 0.0366 (0.0366)	grad_norm 0.0351 (0.0351)	mem 458MB
[2022-10-02 15:58:53 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:53 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0192 (0.0192)	loss 0.0341 (0.0341)	grad_norm 0.0312 (0.0312)	mem 458MB
[2022-10-02 15:58:53 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:53 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0224 (0.0224)	loss 0.0382 (0.0382)	grad_norm 0.0221 (0.0221)	mem 458MB
[2022-10-02 15:58:53 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:53 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0161 (0.0161)	loss 0.0388 (0.0388)	grad_norm 0.0371 (0.0371)	mem 458MB
[2022-10-02 15:58:53 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:53 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0205 (0.0205)	loss 0.0402 (0.0402)	grad_norm 0.0371 (0.0371)	mem 458MB
[2022-10-02 15:58:53 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:53 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0240 (0.0240)	loss 0.0423 (0.0423)	grad_norm 0.0298 (0.0298)	mem 458MB
[2022-10-02 15:58:53 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:54 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0183 (0.0183)	loss 0.0395 (0.0395)	grad_norm 0.0331 (0.0331)	mem 458MB
[2022-10-02 15:58:54 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:54 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0223 (0.0223)	loss 0.0389 (0.0389)	grad_norm 0.0420 (0.0420)	mem 458MB
[2022-10-02 15:58:54 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:54 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0216 (0.0216)	loss 0.0454 (0.0454)	grad_norm 0.0325 (0.0325)	mem 458MB
[2022-10-02 15:58:54 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:54 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0203 (0.0203)	loss 0.0471 (0.0471)	grad_norm 0.0351 (0.0351)	mem 458MB
[2022-10-02 15:58:54 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:54 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0150 (0.0150)	loss 0.0402 (0.0402)	grad_norm 0.0290 (0.0290)	mem 458MB
[2022-10-02 15:58:54 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:54 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0217 (0.0217)	loss 0.0416 (0.0416)	grad_norm 0.0231 (0.0231)	mem 458MB
[2022-10-02 15:58:55 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:55 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0158 (0.0158)	loss 0.0322 (0.0322)	grad_norm 0.0390 (0.0390)	mem 458MB
[2022-10-02 15:58:55 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:55 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0200 (0.0200)	loss 0.0313 (0.0313)	grad_norm 0.0296 (0.0296)	mem 458MB
[2022-10-02 15:58:55 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:55 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0156 (0.0156)	loss 0.0414 (0.0414)	grad_norm 0.0345 (0.0345)	mem 458MB
[2022-10-02 15:58:55 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:55 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0214 (0.0214)	loss 0.0391 (0.0391)	grad_norm 0.0274 (0.0274)	mem 458MB
[2022-10-02 15:58:55 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:55 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0144 (0.0144)	loss 0.0409 (0.0409)	grad_norm 0.0250 (0.0250)	mem 458MB
[2022-10-02 15:58:55 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:55 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0087 (0.0087)	loss 0.0399 (0.0399)	grad_norm 0.0392 (0.0392)	mem 458MB
[2022-10-02 15:58:55 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:56 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0094 (0.0094)	loss 0.0333 (0.0333)	grad_norm 0.0280 (0.0280)	mem 458MB
[2022-10-02 15:58:56 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:56 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0091 (0.0091)	loss 0.0298 (0.0298)	grad_norm 0.0252 (0.0252)	mem 458MB
[2022-10-02 15:58:56 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:56 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0224 (0.0224)	loss 0.0426 (0.0426)	grad_norm 0.0403 (0.0403)	mem 458MB
[2022-10-02 15:58:56 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:56 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0220 (0.0220)	loss 0.0437 (0.0437)	grad_norm 0.0294 (0.0294)	mem 458MB
[2022-10-02 15:58:56 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:56 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0202 (0.0202)	loss 0.0286 (0.0286)	grad_norm 0.0312 (0.0312)	mem 458MB
[2022-10-02 15:58:56 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:56 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0196 (0.0196)	loss 0.0360 (0.0360)	grad_norm 0.0439 (0.0439)	mem 458MB
[2022-10-02 15:58:56 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:56 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0199 (0.0199)	loss 0.0414 (0.0414)	grad_norm 0.0237 (0.0237)	mem 458MB
[2022-10-02 15:58:56 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:57 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0207 (0.0207)	loss 0.0279 (0.0279)	grad_norm 0.0316 (0.0316)	mem 458MB
[2022-10-02 15:58:57 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:57 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0229 (0.0229)	loss 0.0330 (0.0330)	grad_norm 0.0304 (0.0304)	mem 458MB
[2022-10-02 15:58:57 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:57 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0199 (0.0199)	loss 0.0361 (0.0361)	grad_norm 0.0349 (0.0349)	mem 458MB
[2022-10-02 15:58:57 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:57 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0184 (0.0184)	loss 0.0367 (0.0367)	grad_norm 0.0526 (0.0526)	mem 458MB
[2022-10-02 15:58:57 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:57 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0143 (0.0143)	loss 0.0368 (0.0368)	grad_norm 0.0505 (0.0505)	mem 458MB
[2022-10-02 15:58:57 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:58 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0214 (0.0214)	loss 0.0343 (0.0343)	grad_norm 0.0235 (0.0235)	mem 458MB
[2022-10-02 15:58:58 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:58 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0182 (0.0182)	loss 0.0471 (0.0471)	grad_norm 0.0374 (0.0374)	mem 458MB
[2022-10-02 15:58:58 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:58 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0196 (0.0196)	loss 0.0410 (0.0410)	grad_norm 0.0570 (0.0570)	mem 458MB
[2022-10-02 15:58:58 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:58 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0219 (0.0219)	loss 0.0365 (0.0365)	grad_norm 0.0387 (0.0387)	mem 458MB
[2022-10-02 15:58:58 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:58 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0172 (0.0172)	loss 0.0331 (0.0331)	grad_norm 0.0267 (0.0267)	mem 458MB
[2022-10-02 15:58:58 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 15:58:59 demo] (houston_program2.py 333): INFO Train Ep: 3 	Loss1: 0.967287	Loss2: 0.999359	 Dis: 17.760349 Entropy: 4.466930 
[2022-10-02 15:58:59 demo] (houston_program2.py 335): INFO time_3_epoch:7.487156867980957
[2022-10-02 15:58:59 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0133 (0.0133)	loss 0.0356 (0.0356)	grad_norm 0.0377 (0.0377)	mem 458MB
[2022-10-02 15:58:59 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:58:59 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0210 (0.0210)	loss 0.0286 (0.0286)	grad_norm 0.0332 (0.0332)	mem 458MB
[2022-10-02 15:58:59 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:00 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0132 (0.0132)	loss 0.0445 (0.0445)	grad_norm 0.0335 (0.0335)	mem 458MB
[2022-10-02 15:59:00 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:00 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0186 (0.0186)	loss 0.0269 (0.0269)	grad_norm 0.0338 (0.0338)	mem 458MB
[2022-10-02 15:59:00 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:00 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0176 (0.0176)	loss 0.0235 (0.0235)	grad_norm 0.0361 (0.0361)	mem 458MB
[2022-10-02 15:59:00 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:00 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0221 (0.0221)	loss 0.0401 (0.0401)	grad_norm 0.0709 (0.0709)	mem 458MB
[2022-10-02 15:59:00 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:00 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0130 (0.0130)	loss 0.0314 (0.0314)	grad_norm 0.0247 (0.0247)	mem 458MB
[2022-10-02 15:59:00 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:00 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0202 (0.0202)	loss 0.0401 (0.0401)	grad_norm 0.0323 (0.0323)	mem 458MB
[2022-10-02 15:59:00 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:01 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0232 (0.0232)	loss 0.0423 (0.0423)	grad_norm 0.0273 (0.0273)	mem 458MB
[2022-10-02 15:59:01 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:01 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0209 (0.0209)	loss 0.0336 (0.0336)	grad_norm 0.0401 (0.0401)	mem 458MB
[2022-10-02 15:59:01 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:01 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0176 (0.0176)	loss 0.0287 (0.0287)	grad_norm 0.0349 (0.0349)	mem 458MB
[2022-10-02 15:59:01 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:01 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0113 (0.0113)	loss 0.0317 (0.0317)	grad_norm 0.0360 (0.0360)	mem 458MB
[2022-10-02 15:59:01 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:01 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0219 (0.0219)	loss 0.0414 (0.0414)	grad_norm 0.0633 (0.0633)	mem 458MB
[2022-10-02 15:59:01 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:01 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0220 (0.0220)	loss 0.0410 (0.0410)	grad_norm 0.0265 (0.0265)	mem 458MB
[2022-10-02 15:59:02 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:02 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0139 (0.0139)	loss 0.0369 (0.0369)	grad_norm 0.0414 (0.0414)	mem 458MB
[2022-10-02 15:59:02 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:02 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0200 (0.0200)	loss 0.0322 (0.0322)	grad_norm 0.0433 (0.0433)	mem 458MB
[2022-10-02 15:59:02 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:02 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0101 (0.0101)	loss 0.0429 (0.0429)	grad_norm 0.0315 (0.0315)	mem 458MB
[2022-10-02 15:59:02 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:02 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0210 (0.0210)	loss 0.0316 (0.0316)	grad_norm 0.0331 (0.0331)	mem 458MB
[2022-10-02 15:59:02 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:02 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0223 (0.0223)	loss 0.0345 (0.0345)	grad_norm 0.0305 (0.0305)	mem 458MB
[2022-10-02 15:59:02 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:03 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0207 (0.0207)	loss 0.0284 (0.0284)	grad_norm 0.0270 (0.0270)	mem 458MB
[2022-10-02 15:59:03 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:03 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0213 (0.0213)	loss 0.0280 (0.0280)	grad_norm 0.0403 (0.0403)	mem 458MB
[2022-10-02 15:59:03 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:03 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0126 (0.0126)	loss 0.0329 (0.0329)	grad_norm 0.0625 (0.0625)	mem 458MB
[2022-10-02 15:59:03 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:03 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0175 (0.0175)	loss 0.0426 (0.0426)	grad_norm 0.0427 (0.0427)	mem 458MB
[2022-10-02 15:59:03 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:03 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0138 (0.0138)	loss 0.0302 (0.0302)	grad_norm 0.0296 (0.0296)	mem 458MB
[2022-10-02 15:59:03 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:03 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0162 (0.0162)	loss 0.0239 (0.0239)	grad_norm 0.0345 (0.0345)	mem 458MB
[2022-10-02 15:59:03 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:04 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0205 (0.0205)	loss 0.0334 (0.0334)	grad_norm 0.0632 (0.0632)	mem 458MB
[2022-10-02 15:59:04 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:04 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0102 (0.0102)	loss 0.0337 (0.0337)	grad_norm 0.0274 (0.0274)	mem 458MB
[2022-10-02 15:59:04 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:04 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0176 (0.0176)	loss 0.0274 (0.0274)	grad_norm 0.0321 (0.0321)	mem 458MB
[2022-10-02 15:59:04 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:04 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0123 (0.0123)	loss 0.0361 (0.0361)	grad_norm 0.0405 (0.0405)	mem 458MB
[2022-10-02 15:59:04 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:04 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0141 (0.0141)	loss 0.0329 (0.0329)	grad_norm 0.0497 (0.0497)	mem 458MB
[2022-10-02 15:59:04 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:04 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0153 (0.0153)	loss 0.0335 (0.0335)	grad_norm 0.0404 (0.0404)	mem 458MB
[2022-10-02 15:59:04 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:04 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0162 (0.0162)	loss 0.0312 (0.0312)	grad_norm 0.0315 (0.0315)	mem 458MB
[2022-10-02 15:59:04 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:05 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0134 (0.0134)	loss 0.0336 (0.0336)	grad_norm 0.0296 (0.0296)	mem 458MB
[2022-10-02 15:59:05 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:05 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0143 (0.0143)	loss 0.0312 (0.0312)	grad_norm 0.0337 (0.0337)	mem 458MB
[2022-10-02 15:59:05 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:05 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0147 (0.0147)	loss 0.0411 (0.0411)	grad_norm 0.0484 (0.0484)	mem 458MB
[2022-10-02 15:59:05 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:05 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0173 (0.0173)	loss 0.0271 (0.0271)	grad_norm 0.0353 (0.0353)	mem 458MB
[2022-10-02 15:59:05 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:05 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0206 (0.0206)	loss 0.0371 (0.0371)	grad_norm 0.0511 (0.0511)	mem 458MB
[2022-10-02 15:59:05 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:05 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0087 (0.0087)	loss 0.0346 (0.0346)	grad_norm 0.0333 (0.0333)	mem 458MB
[2022-10-02 15:59:05 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:06 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0198 (0.0198)	loss 0.0286 (0.0286)	grad_norm 0.0278 (0.0278)	mem 458MB
[2022-10-02 15:59:06 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 15:59:06 demo] (houston_program2.py 333): INFO Train Ep: 4 	Loss1: 0.877312	Loss2: 0.832907	 Dis: 17.313248 Entropy: 4.185730 
[2022-10-02 15:59:06 demo] (houston_program2.py 335): INFO time_4_epoch:7.346017360687256
[2022-10-02 15:59:07 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0125 (0.0125)	loss 0.0277 (0.0277)	grad_norm 0.0471 (0.0471)	mem 458MB
[2022-10-02 15:59:07 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:07 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0210 (0.0210)	loss 0.0361 (0.0361)	grad_norm 0.0606 (0.0606)	mem 458MB
[2022-10-02 15:59:07 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:07 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0258 (0.0258)	loss 0.0268 (0.0268)	grad_norm 0.0324 (0.0324)	mem 458MB
[2022-10-02 15:59:07 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:07 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0144 (0.0144)	loss 0.0305 (0.0305)	grad_norm 0.0351 (0.0351)	mem 458MB
[2022-10-02 15:59:07 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:07 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0210 (0.0210)	loss 0.0295 (0.0295)	grad_norm 0.0611 (0.0611)	mem 458MB
[2022-10-02 15:59:07 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:07 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0164 (0.0164)	loss 0.0240 (0.0240)	grad_norm 0.0434 (0.0434)	mem 458MB
[2022-10-02 15:59:07 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:08 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0220 (0.0220)	loss 0.0325 (0.0325)	grad_norm 0.0338 (0.0338)	mem 458MB
[2022-10-02 15:59:08 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:08 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0100 (0.0100)	loss 0.0367 (0.0367)	grad_norm 0.0389 (0.0389)	mem 458MB
[2022-10-02 15:59:08 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:08 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0219 (0.0219)	loss 0.0273 (0.0273)	grad_norm 0.0617 (0.0617)	mem 458MB
[2022-10-02 15:59:08 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:08 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0157 (0.0157)	loss 0.0343 (0.0343)	grad_norm 0.0248 (0.0248)	mem 458MB
[2022-10-02 15:59:08 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:08 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0201 (0.0201)	loss 0.0310 (0.0310)	grad_norm 0.0565 (0.0565)	mem 458MB
[2022-10-02 15:59:08 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:08 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0218 (0.0218)	loss 0.0360 (0.0360)	grad_norm 0.0369 (0.0369)	mem 458MB
[2022-10-02 15:59:08 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:09 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0096 (0.0096)	loss 0.0287 (0.0287)	grad_norm 0.0403 (0.0403)	mem 458MB
[2022-10-02 15:59:09 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:09 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0205 (0.0205)	loss 0.0314 (0.0314)	grad_norm 0.0382 (0.0382)	mem 458MB
[2022-10-02 15:59:09 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:09 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0165 (0.0165)	loss 0.0308 (0.0308)	grad_norm 0.0415 (0.0415)	mem 458MB
[2022-10-02 15:59:09 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:09 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0205 (0.0205)	loss 0.0325 (0.0325)	grad_norm 0.0545 (0.0545)	mem 458MB
[2022-10-02 15:59:09 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:09 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0199 (0.0199)	loss 0.0308 (0.0308)	grad_norm 0.0475 (0.0475)	mem 458MB
[2022-10-02 15:59:09 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:09 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0187 (0.0187)	loss 0.0250 (0.0250)	grad_norm 0.0413 (0.0413)	mem 458MB
[2022-10-02 15:59:09 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:10 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0219 (0.0219)	loss 0.0255 (0.0255)	grad_norm 0.0641 (0.0641)	mem 458MB
[2022-10-02 15:59:10 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:10 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0127 (0.0127)	loss 0.0263 (0.0263)	grad_norm 0.0551 (0.0551)	mem 458MB
[2022-10-02 15:59:10 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:10 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0207 (0.0207)	loss 0.0223 (0.0223)	grad_norm 0.0378 (0.0378)	mem 458MB
[2022-10-02 15:59:10 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:10 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0221 (0.0221)	loss 0.0345 (0.0345)	grad_norm 0.0360 (0.0360)	mem 458MB
[2022-10-02 15:59:10 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:10 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0209 (0.0209)	loss 0.0250 (0.0250)	grad_norm 0.0699 (0.0699)	mem 458MB
[2022-10-02 15:59:10 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:11 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0208 (0.0208)	loss 0.0403 (0.0403)	grad_norm 0.0465 (0.0465)	mem 458MB
[2022-10-02 15:59:11 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:11 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0120 (0.0120)	loss 0.0280 (0.0280)	grad_norm 0.0285 (0.0285)	mem 458MB
[2022-10-02 15:59:11 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:11 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0216 (0.0216)	loss 0.0304 (0.0304)	grad_norm 0.0533 (0.0533)	mem 458MB
[2022-10-02 15:59:11 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:11 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0113 (0.0113)	loss 0.0333 (0.0333)	grad_norm 0.0358 (0.0358)	mem 458MB
[2022-10-02 15:59:11 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:11 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0213 (0.0213)	loss 0.0348 (0.0348)	grad_norm 0.0342 (0.0342)	mem 458MB
[2022-10-02 15:59:11 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:11 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0187 (0.0187)	loss 0.0313 (0.0313)	grad_norm 0.0339 (0.0339)	mem 458MB
[2022-10-02 15:59:11 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:12 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0182 (0.0182)	loss 0.0231 (0.0231)	grad_norm 0.0559 (0.0559)	mem 458MB
[2022-10-02 15:59:12 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:12 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0216 (0.0216)	loss 0.0301 (0.0301)	grad_norm 0.0565 (0.0565)	mem 458MB
[2022-10-02 15:59:12 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:12 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0142 (0.0142)	loss 0.0248 (0.0248)	grad_norm 0.0481 (0.0481)	mem 458MB
[2022-10-02 15:59:12 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:12 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0203 (0.0203)	loss 0.0314 (0.0314)	grad_norm 0.0433 (0.0433)	mem 458MB
[2022-10-02 15:59:12 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:12 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0141 (0.0141)	loss 0.0295 (0.0295)	grad_norm 0.0326 (0.0326)	mem 458MB
[2022-10-02 15:59:12 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:12 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0213 (0.0213)	loss 0.0315 (0.0315)	grad_norm 0.0327 (0.0327)	mem 458MB
[2022-10-02 15:59:12 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:13 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0221 (0.0221)	loss 0.0303 (0.0303)	grad_norm 0.0415 (0.0415)	mem 458MB
[2022-10-02 15:59:13 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:13 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0199 (0.0199)	loss 0.0279 (0.0279)	grad_norm 0.0482 (0.0482)	mem 458MB
[2022-10-02 15:59:13 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:13 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0166 (0.0166)	loss 0.0302 (0.0302)	grad_norm 0.0337 (0.0337)	mem 458MB
[2022-10-02 15:59:13 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:13 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0144 (0.0144)	loss 0.0333 (0.0333)	grad_norm 0.0307 (0.0307)	mem 458MB
[2022-10-02 15:59:13 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 15:59:13 demo] (houston_program2.py 333): INFO Train Ep: 5 	Loss1: 0.590153	Loss2: 0.594324	 Dis: 13.962709 Entropy: 4.357708 
[2022-10-02 15:59:13 demo] (houston_program2.py 335): INFO time_5_epoch:7.503745079040527
[2022-10-02 15:59:13 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 15:59:13 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 15:59:13 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 15:59:13 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 15:59:13 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 15:59:13 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 15:59:13 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 15:59:13 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 15:59:13 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 15:59:13 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 15:59:20 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.465600	Loss2: 0.412327	 Dis: 11.843290 Entropy: 4.783965 
[2022-10-02 15:59:20 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:59:25 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.435745	Loss2: 0.471503	 Dis: 10.976456 Entropy: 5.050088 
[2022-10-02 15:59:25 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:59:32 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.134789	Loss2: 0.169100	 Dis: 8.100454 Entropy: 5.577726 
[2022-10-02 15:59:32 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:59:38 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.261943	Loss2: 0.266088	 Dis: 7.488325 Entropy: 4.724213 
[2022-10-02 15:59:38 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:59:44 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.313753	Loss2: 0.320907	 Dis: 8.491449 Entropy: 5.554777 
[2022-10-02 15:59:44 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:59:51 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.125055	Loss2: 0.134198	 Dis: 10.087936 Entropy: 4.940911 
[2022-10-02 15:59:51 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 15:59:57 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.214447	Loss2: 0.175792	 Dis: 9.392092 Entropy: 4.693109 
[2022-10-02 15:59:57 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:00:04 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.118132	Loss2: 0.108391	 Dis: 6.779591 Entropy: 5.565352 
[2022-10-02 16:00:04 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:00:11 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.208044	Loss2: 0.232300	 Dis: 6.492441 Entropy: 4.780952 
[2022-10-02 16:00:11 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:00:17 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.112718	Loss2: 0.095962	 Dis: 5.398720 Entropy: 4.618234 
[2022-10-02 16:00:17 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:00:23 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.076272	Loss2: 0.070812	 Dis: 6.413229 Entropy: 4.669837 
[2022-10-02 16:00:23 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:00:29 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.062063	Loss2: 0.038337	 Dis: 6.104595 Entropy: 4.488734 
[2022-10-02 16:00:29 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:00:36 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.110838	Loss2: 0.111422	 Dis: 7.015501 Entropy: 4.918636 
[2022-10-02 16:00:36 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:00:42 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.100101	Loss2: 0.113776	 Dis: 5.844299 Entropy: 4.780686 
[2022-10-02 16:00:42 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:00:48 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.031249	Loss2: 0.024871	 Dis: 5.051159 Entropy: 4.831741 
[2022-10-02 16:00:48 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:00:54 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.134439	Loss2: 0.153891	 Dis: 3.141041 Entropy: 5.241595 
[2022-10-02 16:00:54 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:01:01 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.217948	Loss2: 0.225077	 Dis: 7.233755 Entropy: 4.375644 
[2022-10-02 16:01:01 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:01:07 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.048692	Loss2: 0.051110	 Dis: 6.451000 Entropy: 5.003984 
[2022-10-02 16:01:07 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:01:14 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.071991	Loss2: 0.072262	 Dis: 3.967581 Entropy: 5.480093 
[2022-10-02 16:01:14 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:01:21 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.097551	Loss2: 0.077250	 Dis: 3.843212 Entropy: 6.235911 
[2022-10-02 16:01:21 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:01:27 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.019080	Loss2: 0.021331	 Dis: 5.100266 Entropy: 4.464605 
[2022-10-02 16:01:27 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:01:34 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.174477	Loss2: 0.150645	 Dis: 3.347549 Entropy: 4.595998 
[2022-10-02 16:01:34 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:01:40 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.239158	Loss2: 0.247782	 Dis: 4.982861 Entropy: 4.265482 
[2022-10-02 16:01:40 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:01:47 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.295404	Loss2: 0.258946	 Dis: 4.117596 Entropy: 5.151814 
[2022-10-02 16:01:47 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:01:53 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.064157	Loss2: 0.066040	 Dis: 3.986944 Entropy: 5.561964 
[2022-10-02 16:01:53 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:02:00 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.110576	Loss2: 0.129403	 Dis: 4.053116 Entropy: 4.979476 
[2022-10-02 16:02:00 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:02:06 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.064028	Loss2: 0.050425	 Dis: 4.811321 Entropy: 4.414222 
[2022-10-02 16:02:06 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:02:13 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.040358	Loss2: 0.040199	 Dis: 5.249489 Entropy: 4.309847 
[2022-10-02 16:02:13 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:02:19 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.053696	Loss2: 0.058551	 Dis: 4.085232 Entropy: 4.434196 
[2022-10-02 16:02:19 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:02:26 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.101107	Loss2: 0.086910	 Dis: 5.773842 Entropy: 5.061832 
[2022-10-02 16:02:26 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:02:33 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.029505	Loss2: 0.031703	 Dis: 6.522942 Entropy: 5.466875 
[2022-10-02 16:02:33 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:02:40 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.125837	Loss2: 0.190351	 Dis: 3.514582 Entropy: 6.354932 
[2022-10-02 16:02:40 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:02:46 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.065979	Loss2: 0.068806	 Dis: 4.123762 Entropy: 5.526983 
[2022-10-02 16:02:46 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:02:53 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.070073	Loss2: 0.073494	 Dis: 3.120232 Entropy: 4.472326 
[2022-10-02 16:02:53 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:03:00 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.032179	Loss2: 0.035890	 Dis: 7.050232 Entropy: 4.509296 
[2022-10-02 16:03:00 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:03:06 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.048276	Loss2: 0.049334	 Dis: 2.494732 Entropy: 5.253533 
[2022-10-02 16:03:06 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:03:13 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.026104	Loss2: 0.025669	 Dis: 2.547853 Entropy: 5.475443 
[2022-10-02 16:03:13 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:03:20 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.114127	Loss2: 0.115427	 Dis: 2.590195 Entropy: 5.948874 
[2022-10-02 16:03:20 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:03:26 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 1.001671	Loss2: 1.088901	 Dis: 17.072224 Entropy: 4.237973 
[2022-10-02 16:03:26 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:03:33 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.483972	Loss2: 0.482094	 Dis: 10.765223 Entropy: 4.597611 
[2022-10-02 16:03:33 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:03:40 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.314289	Loss2: 0.305359	 Dis: 8.518520 Entropy: 4.730520 
[2022-10-02 16:03:40 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:03:46 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.563998	Loss2: 0.548305	 Dis: 7.756870 Entropy: 4.514501 
[2022-10-02 16:03:46 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:03:53 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.474270	Loss2: 0.444480	 Dis: 7.020500 Entropy: 4.882360 
[2022-10-02 16:03:53 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:03:59 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.239005	Loss2: 0.248526	 Dis: 7.937529 Entropy: 4.493382 
[2022-10-02 16:03:59 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:04:06 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.262772	Loss2: 0.254290	 Dis: 8.346413 Entropy: 4.654996 
[2022-10-02 16:04:06 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:04:13 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.200733	Loss2: 0.267444	 Dis: 5.596401 Entropy: 4.467577 
[2022-10-02 16:04:13 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:04:19 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.086030	Loss2: 0.094205	 Dis: 6.666599 Entropy: 4.503334 
[2022-10-02 16:04:19 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:04:26 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.104331	Loss2: 0.093847	 Dis: 4.078043 Entropy: 6.234006 
[2022-10-02 16:04:26 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:04:32 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.147598	Loss2: 0.161853	 Dis: 6.461708 Entropy: 4.492270 
[2022-10-02 16:04:32 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:04:38 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.143803	Loss2: 0.124650	 Dis: 6.379488 Entropy: 4.873503 
[2022-10-02 16:04:38 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:04:44 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.163377	Loss2: 0.209827	 Dis: 4.921907 Entropy: 4.663344 
[2022-10-02 16:04:44 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:04:50 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.139203	Loss2: 0.162387	 Dis: 3.853390 Entropy: 5.352814 
[2022-10-02 16:04:50 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:04:56 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.125688	Loss2: 0.128062	 Dis: 5.802425 Entropy: 4.918438 
[2022-10-02 16:04:56 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:05:02 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.198437	Loss2: 0.249038	 Dis: 6.144457 Entropy: 4.839396 
[2022-10-02 16:05:02 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:05:09 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.410885	Loss2: 0.367872	 Dis: 8.489563 Entropy: 4.727345 
[2022-10-02 16:05:09 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:05:15 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.097861	Loss2: 0.089159	 Dis: 4.046087 Entropy: 5.235682 
[2022-10-02 16:05:15 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:05:22 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.065088	Loss2: 0.061429	 Dis: 4.613256 Entropy: 5.236093 
[2022-10-02 16:05:22 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:05:29 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.249496	Loss2: 0.239054	 Dis: 6.619921 Entropy: 5.580223 
[2022-10-02 16:05:29 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:05:35 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.227094	Loss2: 0.230157	 Dis: 3.172359 Entropy: 4.802403 
[2022-10-02 16:05:35 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:05:41 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.152392	Loss2: 0.138093	 Dis: 5.991400 Entropy: 4.765449 
[2022-10-02 16:05:41 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:05:47 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.075779	Loss2: 0.081116	 Dis: 5.154564 Entropy: 5.062723 
[2022-10-02 16:05:47 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:05:53 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.321632	Loss2: 0.275293	 Dis: 4.262947 Entropy: 5.595936 
[2022-10-02 16:05:53 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:06:00 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.045689	Loss2: 0.044476	 Dis: 4.075848 Entropy: 4.310186 
[2022-10-02 16:06:00 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:06:06 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.301340	Loss2: 0.313700	 Dis: 10.365910 Entropy: 4.751435 
[2022-10-02 16:06:06 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:06:12 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.136400	Loss2: 0.134031	 Dis: 4.051121 Entropy: 5.436302 
[2022-10-02 16:06:12 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:06:19 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.065309	Loss2: 0.061864	 Dis: 5.856628 Entropy: 4.524297 
[2022-10-02 16:06:19 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:06:26 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.163251	Loss2: 0.177723	 Dis: 4.409933 Entropy: 4.304793 
[2022-10-02 16:06:26 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:06:32 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.102423	Loss2: 0.098144	 Dis: 5.810661 Entropy: 4.795173 
[2022-10-02 16:06:32 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:06:38 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.255580	Loss2: 0.232690	 Dis: 5.326607 Entropy: 4.721012 
[2022-10-02 16:06:38 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:06:44 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.097730	Loss2: 0.087776	 Dis: 4.871159 Entropy: 4.550321 
[2022-10-02 16:06:44 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:06:50 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.150942	Loss2: 0.175270	 Dis: 4.291914 Entropy: 4.856332 
[2022-10-02 16:06:50 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:06:56 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.069593	Loss2: 0.077532	 Dis: 4.370243 Entropy: 5.277103 
[2022-10-02 16:06:56 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:07:03 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.056036	Loss2: 0.055608	 Dis: 3.088074 Entropy: 4.450639 
[2022-10-02 16:07:03 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:07:09 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.405382	Loss2: 0.451910	 Dis: 3.384785 Entropy: 4.610198 
[2022-10-02 16:07:09 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:07:16 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.240275	Loss2: 0.186530	 Dis: 5.251068 Entropy: 4.354414 
[2022-10-02 16:07:16 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:07:22 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.172755	Loss2: 0.161000	 Dis: 5.634644 Entropy: 4.308463 
[2022-10-02 16:07:22 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:07:29 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.111263	Loss2: 0.098285	 Dis: 4.093575 Entropy: 4.861614 
[2022-10-02 16:07:29 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:07:35 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.171063	Loss2: 0.168527	 Dis: 3.348616 Entropy: 5.311442 
[2022-10-02 16:07:35 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:07:41 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.081078	Loss2: 0.070488	 Dis: 3.770811 Entropy: 4.644967 
[2022-10-02 16:07:41 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:07:47 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.131259	Loss2: 0.133824	 Dis: 4.837641 Entropy: 4.219227 
[2022-10-02 16:07:47 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:07:54 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.027902	Loss2: 0.021431	 Dis: 4.031439 Entropy: 6.460969 
[2022-10-02 16:07:54 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:08:00 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.121462	Loss2: 0.120734	 Dis: 2.605171 Entropy: 4.615738 
[2022-10-02 16:08:00 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:08:07 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.022135	Loss2: 0.028124	 Dis: 5.760822 Entropy: 4.552016 
[2022-10-02 16:08:07 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:08:13 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.111233	Loss2: 0.087578	 Dis: 2.245325 Entropy: 5.525527 
[2022-10-02 16:08:13 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:08:19 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.083842	Loss2: 0.103610	 Dis: 3.851250 Entropy: 5.010649 
[2022-10-02 16:08:19 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:08:26 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.143096	Loss2: 0.177235	 Dis: 4.632114 Entropy: 5.189800 
[2022-10-02 16:08:26 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:08:32 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.046992	Loss2: 0.037242	 Dis: 3.144939 Entropy: 4.381351 
[2022-10-02 16:08:32 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:08:39 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.139626	Loss2: 0.121530	 Dis: 2.239765 Entropy: 6.021857 
[2022-10-02 16:08:39 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:08:46 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.044321	Loss2: 0.057824	 Dis: 4.787815 Entropy: 4.537214 
[2022-10-02 16:08:46 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:08:52 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.121936	Loss2: 0.141259	 Dis: 3.798986 Entropy: 4.527020 
[2022-10-02 16:08:52 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:08:58 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.080223	Loss2: 0.082292	 Dis: 5.136543 Entropy: 4.952452 
[2022-10-02 16:08:58 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:09:04 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.040147	Loss2: 0.037220	 Dis: 4.303810 Entropy: 4.436934 
[2022-10-02 16:09:04 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:09:11 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.057185	Loss2: 0.048780	 Dis: 3.159313 Entropy: 4.735847 
[2022-10-02 16:09:11 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:09:16 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.145083	Loss2: 0.129850	 Dis: 4.991549 Entropy: 4.803048 
[2022-10-02 16:09:16 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:09:23 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.039568	Loss2: 0.049781	 Dis: 4.866825 Entropy: 5.098907 
[2022-10-02 16:09:23 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:09:29 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.144682	Loss2: 0.142749	 Dis: 6.672071 Entropy: 5.534811 
[2022-10-02 16:09:29 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:09:35 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.205054	Loss2: 0.207698	 Dis: 5.734140 Entropy: 4.708283 
[2022-10-02 16:09:35 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:09:41 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.067310	Loss2: 0.073070	 Dis: 4.881460 Entropy: 5.166819 
[2022-10-02 16:09:41 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:09:48 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.377741	Loss2: 0.357460	 Dis: 3.569532 Entropy: 5.885703 
[2022-10-02 16:09:48 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:09:54 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.117987	Loss2: 0.123039	 Dis: 4.482178 Entropy: 5.077945 
[2022-10-02 16:09:54 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:09:54 demo] (houston_program2.py 509): INFO time_5_epoch:640.7723722457886
[2022-10-02 16:10:02 demo] (houston_program2.py 667): INFO 	val_Accuracy: 27750/53200 (52.16%)	
[2022-10-02 16:10:02 demo] (utils.py 47): INFO outputs/demo/finetune_100/ckpt_epoch_5.pth saving......
[2022-10-02 16:10:03 demo] (utils.py 49): INFO outputs/demo/finetune_100/ckpt_epoch_5.pth saved !!!
[2022-10-02 16:10:03 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0145 (0.0145)	loss 0.0368 (0.0368)	grad_norm 0.0283 (0.0283)	mem 458MB
[2022-10-02 16:10:03 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:03 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0088 (0.0088)	loss 0.0343 (0.0343)	grad_norm 0.0303 (0.0303)	mem 459MB
[2022-10-02 16:10:03 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:04 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0095 (0.0095)	loss 0.0313 (0.0313)	grad_norm 0.0517 (0.0517)	mem 459MB
[2022-10-02 16:10:04 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:04 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0174 (0.0174)	loss 0.0209 (0.0209)	grad_norm 0.0321 (0.0321)	mem 459MB
[2022-10-02 16:10:04 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:04 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0199 (0.0199)	loss 0.0232 (0.0232)	grad_norm 0.0365 (0.0365)	mem 459MB
[2022-10-02 16:10:04 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:04 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0151 (0.0151)	loss 0.0351 (0.0351)	grad_norm 0.0484 (0.0484)	mem 459MB
[2022-10-02 16:10:04 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:04 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0217 (0.0217)	loss 0.0299 (0.0299)	grad_norm 0.0384 (0.0384)	mem 459MB
[2022-10-02 16:10:04 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:04 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0224 (0.0224)	loss 0.0235 (0.0235)	grad_norm 0.0341 (0.0341)	mem 459MB
[2022-10-02 16:10:04 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:05 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0203 (0.0203)	loss 0.0288 (0.0288)	grad_norm 0.0597 (0.0597)	mem 459MB
[2022-10-02 16:10:05 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:05 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0212 (0.0212)	loss 0.0314 (0.0314)	grad_norm 0.0398 (0.0398)	mem 459MB
[2022-10-02 16:10:05 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:05 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0201 (0.0201)	loss 0.0281 (0.0281)	grad_norm 0.0326 (0.0326)	mem 459MB
[2022-10-02 16:10:05 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:05 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0227 (0.0227)	loss 0.0345 (0.0345)	grad_norm 0.0456 (0.0456)	mem 459MB
[2022-10-02 16:10:05 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:05 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0191 (0.0191)	loss 0.0221 (0.0221)	grad_norm 0.0410 (0.0410)	mem 459MB
[2022-10-02 16:10:05 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:06 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0222 (0.0222)	loss 0.0301 (0.0301)	grad_norm 0.0272 (0.0272)	mem 459MB
[2022-10-02 16:10:06 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:06 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0203 (0.0203)	loss 0.0308 (0.0308)	grad_norm 0.0506 (0.0506)	mem 459MB
[2022-10-02 16:10:06 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:06 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0209 (0.0209)	loss 0.0286 (0.0286)	grad_norm 0.0343 (0.0343)	mem 459MB
[2022-10-02 16:10:06 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:06 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0202 (0.0202)	loss 0.0242 (0.0242)	grad_norm 0.0381 (0.0381)	mem 459MB
[2022-10-02 16:10:06 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:06 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0228 (0.0228)	loss 0.0322 (0.0322)	grad_norm 0.0289 (0.0289)	mem 459MB
[2022-10-02 16:10:06 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:06 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0219 (0.0219)	loss 0.0239 (0.0239)	grad_norm 0.0425 (0.0425)	mem 459MB
[2022-10-02 16:10:06 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:07 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0226 (0.0226)	loss 0.0224 (0.0224)	grad_norm 0.0373 (0.0373)	mem 459MB
[2022-10-02 16:10:07 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:07 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0225 (0.0225)	loss 0.0244 (0.0244)	grad_norm 0.0314 (0.0314)	mem 459MB
[2022-10-02 16:10:07 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:07 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0245 (0.0245)	loss 0.0250 (0.0250)	grad_norm 0.0456 (0.0456)	mem 459MB
[2022-10-02 16:10:07 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:07 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0205 (0.0205)	loss 0.0271 (0.0271)	grad_norm 0.0403 (0.0403)	mem 459MB
[2022-10-02 16:10:07 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:07 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0244 (0.0244)	loss 0.0305 (0.0305)	grad_norm 0.0486 (0.0486)	mem 459MB
[2022-10-02 16:10:07 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:07 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0200 (0.0200)	loss 0.0259 (0.0259)	grad_norm 0.0346 (0.0346)	mem 459MB
[2022-10-02 16:10:07 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:08 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0206 (0.0206)	loss 0.0330 (0.0330)	grad_norm 0.0501 (0.0501)	mem 459MB
[2022-10-02 16:10:08 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:08 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0193 (0.0193)	loss 0.0225 (0.0225)	grad_norm 0.0534 (0.0534)	mem 459MB
[2022-10-02 16:10:08 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:08 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0199 (0.0199)	loss 0.0281 (0.0281)	grad_norm 0.0347 (0.0347)	mem 459MB
[2022-10-02 16:10:08 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:08 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0172 (0.0172)	loss 0.0223 (0.0223)	grad_norm 0.0616 (0.0616)	mem 459MB
[2022-10-02 16:10:08 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:08 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0213 (0.0213)	loss 0.0298 (0.0298)	grad_norm 0.0498 (0.0498)	mem 459MB
[2022-10-02 16:10:08 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:09 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0209 (0.0209)	loss 0.0263 (0.0263)	grad_norm 0.0434 (0.0434)	mem 459MB
[2022-10-02 16:10:09 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:09 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0209 (0.0209)	loss 0.0230 (0.0230)	grad_norm 0.0607 (0.0607)	mem 459MB
[2022-10-02 16:10:09 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:09 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0209 (0.0209)	loss 0.0228 (0.0228)	grad_norm 0.0708 (0.0708)	mem 459MB
[2022-10-02 16:10:09 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:09 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0208 (0.0208)	loss 0.0294 (0.0294)	grad_norm 0.0513 (0.0513)	mem 459MB
[2022-10-02 16:10:09 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:09 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0194 (0.0194)	loss 0.0198 (0.0198)	grad_norm 0.0488 (0.0488)	mem 459MB
[2022-10-02 16:10:09 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:09 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0218 (0.0218)	loss 0.0235 (0.0235)	grad_norm 0.0415 (0.0415)	mem 459MB
[2022-10-02 16:10:09 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:10 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0155 (0.0155)	loss 0.0237 (0.0237)	grad_norm 0.0640 (0.0640)	mem 459MB
[2022-10-02 16:10:10 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:10 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0217 (0.0217)	loss 0.0228 (0.0228)	grad_norm 0.0516 (0.0516)	mem 459MB
[2022-10-02 16:10:10 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:10 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0114 (0.0114)	loss 0.0212 (0.0212)	grad_norm 0.0553 (0.0553)	mem 459MB
[2022-10-02 16:10:10 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 16:10:10 demo] (houston_program2.py 333): INFO Train Ep: 6 	Loss1: 0.410634	Loss2: 0.383499	 Dis: 11.279245 Entropy: 4.424335 
[2022-10-02 16:10:10 demo] (houston_program2.py 335): INFO time_6_epoch:7.522140741348267
[2022-10-02 16:10:11 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0131 (0.0131)	loss 0.0229 (0.0229)	grad_norm 0.0594 (0.0594)	mem 459MB
[2022-10-02 16:10:11 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:11 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0210 (0.0210)	loss 0.0251 (0.0251)	grad_norm 0.0645 (0.0645)	mem 459MB
[2022-10-02 16:10:11 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:11 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0210 (0.0210)	loss 0.0232 (0.0232)	grad_norm 0.0583 (0.0583)	mem 459MB
[2022-10-02 16:10:11 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:12 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0193 (0.0193)	loss 0.0203 (0.0203)	grad_norm 0.0508 (0.0508)	mem 459MB
[2022-10-02 16:10:12 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:12 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0164 (0.0164)	loss 0.0292 (0.0292)	grad_norm 0.0551 (0.0551)	mem 459MB
[2022-10-02 16:10:12 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:12 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0220 (0.0220)	loss 0.0208 (0.0208)	grad_norm 0.0456 (0.0456)	mem 459MB
[2022-10-02 16:10:12 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:12 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0169 (0.0169)	loss 0.0340 (0.0340)	grad_norm 0.0338 (0.0338)	mem 459MB
[2022-10-02 16:10:12 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:12 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0198 (0.0198)	loss 0.0250 (0.0250)	grad_norm 0.0532 (0.0532)	mem 459MB
[2022-10-02 16:10:12 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:12 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0212 (0.0212)	loss 0.0245 (0.0245)	grad_norm 0.0503 (0.0503)	mem 459MB
[2022-10-02 16:10:12 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:13 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0212 (0.0212)	loss 0.0186 (0.0186)	grad_norm 0.0401 (0.0401)	mem 459MB
[2022-10-02 16:10:13 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:13 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0199 (0.0199)	loss 0.0239 (0.0239)	grad_norm 0.0605 (0.0605)	mem 459MB
[2022-10-02 16:10:13 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:13 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0213 (0.0213)	loss 0.0207 (0.0207)	grad_norm 0.0530 (0.0530)	mem 459MB
[2022-10-02 16:10:13 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:13 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0120 (0.0120)	loss 0.0329 (0.0329)	grad_norm 0.0520 (0.0520)	mem 459MB
[2022-10-02 16:10:13 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:13 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0172 (0.0172)	loss 0.0254 (0.0254)	grad_norm 0.0664 (0.0664)	mem 459MB
[2022-10-02 16:10:13 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:13 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0172 (0.0172)	loss 0.0269 (0.0269)	grad_norm 0.0514 (0.0514)	mem 459MB
[2022-10-02 16:10:13 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:14 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0202 (0.0202)	loss 0.0258 (0.0258)	grad_norm 0.0838 (0.0838)	mem 459MB
[2022-10-02 16:10:14 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:14 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0150 (0.0150)	loss 0.0204 (0.0204)	grad_norm 0.0575 (0.0575)	mem 459MB
[2022-10-02 16:10:14 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:14 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0203 (0.0203)	loss 0.0306 (0.0306)	grad_norm 0.0490 (0.0490)	mem 459MB
[2022-10-02 16:10:14 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:14 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0168 (0.0168)	loss 0.0292 (0.0292)	grad_norm 0.0562 (0.0562)	mem 459MB
[2022-10-02 16:10:14 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:14 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0220 (0.0220)	loss 0.0283 (0.0283)	grad_norm 0.0386 (0.0386)	mem 459MB
[2022-10-02 16:10:14 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:14 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0144 (0.0144)	loss 0.0272 (0.0272)	grad_norm 0.0676 (0.0676)	mem 459MB
[2022-10-02 16:10:14 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:15 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0218 (0.0218)	loss 0.0225 (0.0225)	grad_norm 0.0770 (0.0770)	mem 459MB
[2022-10-02 16:10:15 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:15 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0142 (0.0142)	loss 0.0226 (0.0226)	grad_norm 0.0536 (0.0536)	mem 459MB
[2022-10-02 16:10:15 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:15 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0220 (0.0220)	loss 0.0281 (0.0281)	grad_norm 0.0416 (0.0416)	mem 459MB
[2022-10-02 16:10:15 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:15 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0119 (0.0119)	loss 0.0240 (0.0240)	grad_norm 0.0602 (0.0602)	mem 459MB
[2022-10-02 16:10:15 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:15 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0205 (0.0205)	loss 0.0220 (0.0220)	grad_norm 0.0468 (0.0468)	mem 459MB
[2022-10-02 16:10:15 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:16 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0191 (0.0191)	loss 0.0288 (0.0288)	grad_norm 0.0554 (0.0554)	mem 459MB
[2022-10-02 16:10:16 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:16 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0218 (0.0218)	loss 0.0192 (0.0192)	grad_norm 0.0826 (0.0826)	mem 459MB
[2022-10-02 16:10:16 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:16 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0099 (0.0099)	loss 0.0248 (0.0248)	grad_norm 0.0379 (0.0379)	mem 459MB
[2022-10-02 16:10:16 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:16 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0204 (0.0204)	loss 0.0211 (0.0211)	grad_norm 0.0528 (0.0528)	mem 459MB
[2022-10-02 16:10:16 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:16 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0123 (0.0123)	loss 0.0219 (0.0219)	grad_norm 0.0750 (0.0750)	mem 459MB
[2022-10-02 16:10:16 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:16 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0194 (0.0194)	loss 0.0188 (0.0188)	grad_norm 0.0528 (0.0528)	mem 459MB
[2022-10-02 16:10:16 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:16 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0141 (0.0141)	loss 0.0231 (0.0231)	grad_norm 0.0461 (0.0461)	mem 459MB
[2022-10-02 16:10:17 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:17 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0120 (0.0120)	loss 0.0245 (0.0245)	grad_norm 0.0739 (0.0739)	mem 459MB
[2022-10-02 16:10:17 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:17 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0205 (0.0205)	loss 0.0258 (0.0258)	grad_norm 0.0556 (0.0556)	mem 459MB
[2022-10-02 16:10:17 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:17 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0158 (0.0158)	loss 0.0240 (0.0240)	grad_norm 0.0523 (0.0523)	mem 459MB
[2022-10-02 16:10:17 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:17 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0203 (0.0203)	loss 0.0287 (0.0287)	grad_norm 0.0693 (0.0693)	mem 459MB
[2022-10-02 16:10:17 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:17 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0168 (0.0168)	loss 0.0203 (0.0203)	grad_norm 0.0589 (0.0589)	mem 459MB
[2022-10-02 16:10:17 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:18 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0219 (0.0219)	loss 0.0249 (0.0249)	grad_norm 0.0542 (0.0542)	mem 459MB
[2022-10-02 16:10:18 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 16:10:18 demo] (houston_program2.py 333): INFO Train Ep: 7 	Loss1: 0.357324	Loss2: 0.380413	 Dis: 11.031301 Entropy: 4.352975 
[2022-10-02 16:10:18 demo] (houston_program2.py 335): INFO time_7_epoch:7.634087800979614
[2022-10-02 16:10:19 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0135 (0.0135)	loss 0.0228 (0.0228)	grad_norm 0.0732 (0.0732)	mem 459MB
[2022-10-02 16:10:19 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:19 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0130 (0.0130)	loss 0.0215 (0.0215)	grad_norm 0.0435 (0.0435)	mem 459MB
[2022-10-02 16:10:19 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:19 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0206 (0.0206)	loss 0.0216 (0.0216)	grad_norm 0.0560 (0.0560)	mem 459MB
[2022-10-02 16:10:19 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:19 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0159 (0.0159)	loss 0.0272 (0.0272)	grad_norm 0.0376 (0.0376)	mem 459MB
[2022-10-02 16:10:19 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:19 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0213 (0.0213)	loss 0.0202 (0.0202)	grad_norm 0.0399 (0.0399)	mem 459MB
[2022-10-02 16:10:19 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:19 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0170 (0.0170)	loss 0.0262 (0.0262)	grad_norm 0.0548 (0.0548)	mem 459MB
[2022-10-02 16:10:19 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:20 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0223 (0.0223)	loss 0.0272 (0.0272)	grad_norm 0.0438 (0.0438)	mem 459MB
[2022-10-02 16:10:20 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:20 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0180 (0.0180)	loss 0.0267 (0.0267)	grad_norm 0.0322 (0.0322)	mem 459MB
[2022-10-02 16:10:20 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:20 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0216 (0.0216)	loss 0.0216 (0.0216)	grad_norm 0.0338 (0.0338)	mem 459MB
[2022-10-02 16:10:20 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:20 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0201 (0.0201)	loss 0.0210 (0.0210)	grad_norm 0.0385 (0.0385)	mem 459MB
[2022-10-02 16:10:20 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:20 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0213 (0.0213)	loss 0.0250 (0.0250)	grad_norm 0.0657 (0.0657)	mem 459MB
[2022-10-02 16:10:20 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:21 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0218 (0.0218)	loss 0.0245 (0.0245)	grad_norm 0.0389 (0.0389)	mem 459MB
[2022-10-02 16:10:21 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:21 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0204 (0.0204)	loss 0.0186 (0.0186)	grad_norm 0.0511 (0.0511)	mem 459MB
[2022-10-02 16:10:21 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:21 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0205 (0.0205)	loss 0.0188 (0.0188)	grad_norm 0.0469 (0.0469)	mem 459MB
[2022-10-02 16:10:21 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:21 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0213 (0.0213)	loss 0.0216 (0.0216)	grad_norm 0.0347 (0.0347)	mem 459MB
[2022-10-02 16:10:21 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:21 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0214 (0.0214)	loss 0.0264 (0.0264)	grad_norm 0.0482 (0.0482)	mem 459MB
[2022-10-02 16:10:21 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:21 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0223 (0.0223)	loss 0.0212 (0.0212)	grad_norm 0.0503 (0.0503)	mem 459MB
[2022-10-02 16:10:21 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:22 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0155 (0.0155)	loss 0.0200 (0.0200)	grad_norm 0.0345 (0.0345)	mem 459MB
[2022-10-02 16:10:22 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:22 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0197 (0.0197)	loss 0.0220 (0.0220)	grad_norm 0.0400 (0.0400)	mem 459MB
[2022-10-02 16:10:22 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:22 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0179 (0.0179)	loss 0.0235 (0.0235)	grad_norm 0.0455 (0.0455)	mem 459MB
[2022-10-02 16:10:22 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:22 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0215 (0.0215)	loss 0.0257 (0.0257)	grad_norm 0.0523 (0.0523)	mem 459MB
[2022-10-02 16:10:22 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:22 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0185 (0.0185)	loss 0.0225 (0.0225)	grad_norm 0.0362 (0.0362)	mem 459MB
[2022-10-02 16:10:22 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:23 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0204 (0.0204)	loss 0.0248 (0.0248)	grad_norm 0.0455 (0.0455)	mem 459MB
[2022-10-02 16:10:23 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:23 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0172 (0.0172)	loss 0.0207 (0.0207)	grad_norm 0.0512 (0.0512)	mem 459MB
[2022-10-02 16:10:23 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:23 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0210 (0.0210)	loss 0.0168 (0.0168)	grad_norm 0.0500 (0.0500)	mem 459MB
[2022-10-02 16:10:23 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:23 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0132 (0.0132)	loss 0.0269 (0.0269)	grad_norm 0.0626 (0.0626)	mem 459MB
[2022-10-02 16:10:23 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:23 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0215 (0.0215)	loss 0.0237 (0.0237)	grad_norm 0.0655 (0.0655)	mem 459MB
[2022-10-02 16:10:23 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:23 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0192 (0.0192)	loss 0.0236 (0.0236)	grad_norm 0.0595 (0.0595)	mem 459MB
[2022-10-02 16:10:23 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:24 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0205 (0.0205)	loss 0.0187 (0.0187)	grad_norm 0.0775 (0.0775)	mem 459MB
[2022-10-02 16:10:24 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:24 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0207 (0.0207)	loss 0.0191 (0.0191)	grad_norm 0.0379 (0.0379)	mem 459MB
[2022-10-02 16:10:24 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:24 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0192 (0.0192)	loss 0.0194 (0.0194)	grad_norm 0.0794 (0.0794)	mem 459MB
[2022-10-02 16:10:24 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:24 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0207 (0.0207)	loss 0.0234 (0.0234)	grad_norm 0.0486 (0.0486)	mem 459MB
[2022-10-02 16:10:24 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:24 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0292 (0.0292)	loss 0.0239 (0.0239)	grad_norm 0.0683 (0.0683)	mem 459MB
[2022-10-02 16:10:24 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:25 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0208 (0.0208)	loss 0.0257 (0.0257)	grad_norm 0.0694 (0.0694)	mem 459MB
[2022-10-02 16:10:25 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:25 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0211 (0.0211)	loss 0.0229 (0.0229)	grad_norm 0.0723 (0.0723)	mem 459MB
[2022-10-02 16:10:25 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:25 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0215 (0.0215)	loss 0.0173 (0.0173)	grad_norm 0.0528 (0.0528)	mem 459MB
[2022-10-02 16:10:25 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:25 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0193 (0.0193)	loss 0.0358 (0.0358)	grad_norm 0.0673 (0.0673)	mem 459MB
[2022-10-02 16:10:25 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:25 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0209 (0.0209)	loss 0.0252 (0.0252)	grad_norm 0.0671 (0.0671)	mem 459MB
[2022-10-02 16:10:25 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:25 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0193 (0.0193)	loss 0.0205 (0.0205)	grad_norm 0.0454 (0.0454)	mem 459MB
[2022-10-02 16:10:25 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 16:10:26 demo] (houston_program2.py 333): INFO Train Ep: 8 	Loss1: 1.206203	Loss2: 1.143407	 Dis: 19.620659 Entropy: 4.166624 
[2022-10-02 16:10:26 demo] (houston_program2.py 335): INFO time_8_epoch:7.915705680847168
[2022-10-02 16:10:27 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0156 (0.0156)	loss 0.0253 (0.0253)	grad_norm 0.0521 (0.0521)	mem 459MB
[2022-10-02 16:10:27 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:27 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0118 (0.0118)	loss 0.0204 (0.0204)	grad_norm 0.0398 (0.0398)	mem 459MB
[2022-10-02 16:10:27 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:27 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0201 (0.0201)	loss 0.0189 (0.0189)	grad_norm 0.0780 (0.0780)	mem 459MB
[2022-10-02 16:10:27 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:27 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0104 (0.0104)	loss 0.0232 (0.0232)	grad_norm 0.0594 (0.0594)	mem 459MB
[2022-10-02 16:10:27 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:27 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0215 (0.0215)	loss 0.0232 (0.0232)	grad_norm 0.0442 (0.0442)	mem 459MB
[2022-10-02 16:10:27 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:27 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0223 (0.0223)	loss 0.0270 (0.0270)	grad_norm 0.0799 (0.0799)	mem 459MB
[2022-10-02 16:10:27 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:28 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0213 (0.0213)	loss 0.0235 (0.0235)	grad_norm 0.0435 (0.0435)	mem 459MB
[2022-10-02 16:10:28 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:28 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0246 (0.0246)	loss 0.0233 (0.0233)	grad_norm 0.0445 (0.0445)	mem 459MB
[2022-10-02 16:10:28 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:28 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0276 (0.0276)	loss 0.0302 (0.0302)	grad_norm 0.0637 (0.0637)	mem 459MB
[2022-10-02 16:10:28 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:28 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0205 (0.0205)	loss 0.0182 (0.0182)	grad_norm 0.0590 (0.0590)	mem 459MB
[2022-10-02 16:10:28 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:28 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0224 (0.0224)	loss 0.0193 (0.0193)	grad_norm 0.0475 (0.0475)	mem 459MB
[2022-10-02 16:10:28 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:29 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0137 (0.0137)	loss 0.0224 (0.0224)	grad_norm 0.0660 (0.0660)	mem 459MB
[2022-10-02 16:10:29 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:29 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0204 (0.0204)	loss 0.0196 (0.0196)	grad_norm 0.0533 (0.0533)	mem 459MB
[2022-10-02 16:10:29 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:29 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0168 (0.0168)	loss 0.0187 (0.0187)	grad_norm 0.0507 (0.0507)	mem 459MB
[2022-10-02 16:10:29 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:29 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0205 (0.0205)	loss 0.0223 (0.0223)	grad_norm 0.0585 (0.0585)	mem 459MB
[2022-10-02 16:10:29 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:29 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0140 (0.0140)	loss 0.0220 (0.0220)	grad_norm 0.0727 (0.0727)	mem 459MB
[2022-10-02 16:10:29 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:29 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0203 (0.0203)	loss 0.0254 (0.0254)	grad_norm 0.0490 (0.0490)	mem 459MB
[2022-10-02 16:10:29 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:30 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0198 (0.0198)	loss 0.0222 (0.0222)	grad_norm 0.0653 (0.0653)	mem 459MB
[2022-10-02 16:10:30 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:30 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0196 (0.0196)	loss 0.0282 (0.0282)	grad_norm 0.0517 (0.0517)	mem 459MB
[2022-10-02 16:10:30 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:30 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0138 (0.0138)	loss 0.0228 (0.0228)	grad_norm 0.0661 (0.0661)	mem 459MB
[2022-10-02 16:10:30 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:30 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0203 (0.0203)	loss 0.0259 (0.0259)	grad_norm 0.0577 (0.0577)	mem 459MB
[2022-10-02 16:10:30 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:30 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0140 (0.0140)	loss 0.0192 (0.0192)	grad_norm 0.0547 (0.0547)	mem 459MB
[2022-10-02 16:10:30 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:30 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0198 (0.0198)	loss 0.0275 (0.0275)	grad_norm 0.0466 (0.0466)	mem 459MB
[2022-10-02 16:10:30 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:31 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0148 (0.0148)	loss 0.0191 (0.0191)	grad_norm 0.0510 (0.0510)	mem 459MB
[2022-10-02 16:10:31 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:31 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0204 (0.0204)	loss 0.0172 (0.0172)	grad_norm 0.0569 (0.0569)	mem 459MB
[2022-10-02 16:10:31 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:31 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0156 (0.0156)	loss 0.0159 (0.0159)	grad_norm 0.0470 (0.0470)	mem 459MB
[2022-10-02 16:10:31 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:31 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0208 (0.0208)	loss 0.0292 (0.0292)	grad_norm 0.0433 (0.0433)	mem 459MB
[2022-10-02 16:10:31 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:31 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0205 (0.0205)	loss 0.0221 (0.0221)	grad_norm 0.0431 (0.0431)	mem 459MB
[2022-10-02 16:10:31 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:32 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0206 (0.0206)	loss 0.0158 (0.0158)	grad_norm 0.0559 (0.0559)	mem 459MB
[2022-10-02 16:10:32 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:32 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0142 (0.0142)	loss 0.0180 (0.0180)	grad_norm 0.0394 (0.0394)	mem 459MB
[2022-10-02 16:10:32 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:32 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0202 (0.0202)	loss 0.0146 (0.0146)	grad_norm 0.0749 (0.0749)	mem 459MB
[2022-10-02 16:10:32 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:32 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0169 (0.0169)	loss 0.0191 (0.0191)	grad_norm 0.0462 (0.0462)	mem 459MB
[2022-10-02 16:10:32 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:32 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0214 (0.0214)	loss 0.0231 (0.0231)	grad_norm 0.0634 (0.0634)	mem 459MB
[2022-10-02 16:10:32 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:32 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0207 (0.0207)	loss 0.0200 (0.0200)	grad_norm 0.0713 (0.0713)	mem 459MB
[2022-10-02 16:10:32 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:33 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0202 (0.0202)	loss 0.0171 (0.0171)	grad_norm 0.0413 (0.0413)	mem 459MB
[2022-10-02 16:10:33 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:33 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0155 (0.0155)	loss 0.0202 (0.0202)	grad_norm 0.0942 (0.0942)	mem 459MB
[2022-10-02 16:10:33 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:33 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000050	time 0.0210 (0.0210)	loss 0.0199 (0.0199)	grad_norm 0.0755 (0.0755)	mem 459MB
[2022-10-02 16:10:33 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:33 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000050	time 0.0168 (0.0168)	loss 0.0194 (0.0194)	grad_norm 0.0537 (0.0537)	mem 459MB
[2022-10-02 16:10:33 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:33 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000050	time 0.0204 (0.0204)	loss 0.0223 (0.0223)	grad_norm 0.0776 (0.0776)	mem 459MB
[2022-10-02 16:10:33 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 16:10:34 demo] (houston_program2.py 333): INFO Train Ep: 9 	Loss1: 0.375222	Loss2: 0.396588	 Dis: 14.501604 Entropy: 4.213511 
[2022-10-02 16:10:34 demo] (houston_program2.py 335): INFO time_9_epoch:7.933380126953125
[2022-10-02 16:10:34 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000050	time 0.0178 (0.0178)	loss 0.0232 (0.0232)	grad_norm 0.0460 (0.0460)	mem 459MB
[2022-10-02 16:10:34 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:35 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0151 (0.0151)	loss 0.0190 (0.0190)	grad_norm 0.0461 (0.0461)	mem 459MB
[2022-10-02 16:10:35 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:35 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0221 (0.0221)	loss 0.0198 (0.0198)	grad_norm 0.0488 (0.0488)	mem 459MB
[2022-10-02 16:10:35 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:35 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0161 (0.0161)	loss 0.0241 (0.0241)	grad_norm 0.0654 (0.0654)	mem 459MB
[2022-10-02 16:10:35 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:35 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0208 (0.0208)	loss 0.0177 (0.0177)	grad_norm 0.0573 (0.0573)	mem 459MB
[2022-10-02 16:10:35 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:35 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0132 (0.0132)	loss 0.0256 (0.0256)	grad_norm 0.0631 (0.0631)	mem 459MB
[2022-10-02 16:10:35 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:35 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0214 (0.0214)	loss 0.0183 (0.0183)	grad_norm 0.0583 (0.0583)	mem 459MB
[2022-10-02 16:10:35 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:36 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0110 (0.0110)	loss 0.0168 (0.0168)	grad_norm 0.0702 (0.0702)	mem 459MB
[2022-10-02 16:10:36 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:36 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0217 (0.0217)	loss 0.0250 (0.0250)	grad_norm 0.0742 (0.0742)	mem 459MB
[2022-10-02 16:10:36 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:36 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0109 (0.0109)	loss 0.0216 (0.0216)	grad_norm 0.0507 (0.0507)	mem 459MB
[2022-10-02 16:10:36 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:36 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0202 (0.0202)	loss 0.0221 (0.0221)	grad_norm 0.0723 (0.0723)	mem 459MB
[2022-10-02 16:10:36 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:36 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0169 (0.0169)	loss 0.0262 (0.0262)	grad_norm 0.0662 (0.0662)	mem 459MB
[2022-10-02 16:10:36 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:36 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0217 (0.0217)	loss 0.0210 (0.0210)	grad_norm 0.0425 (0.0425)	mem 459MB
[2022-10-02 16:10:36 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:37 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0167 (0.0167)	loss 0.0236 (0.0236)	grad_norm 0.0964 (0.0964)	mem 459MB
[2022-10-02 16:10:37 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:37 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0190 (0.0190)	loss 0.0258 (0.0258)	grad_norm 0.0838 (0.0838)	mem 459MB
[2022-10-02 16:10:37 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:37 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0228 (0.0228)	loss 0.0222 (0.0222)	grad_norm 0.0559 (0.0559)	mem 459MB
[2022-10-02 16:10:37 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:37 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0199 (0.0199)	loss 0.0265 (0.0265)	grad_norm 0.0701 (0.0701)	mem 459MB
[2022-10-02 16:10:37 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:37 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0209 (0.0209)	loss 0.0213 (0.0213)	grad_norm 0.0571 (0.0571)	mem 459MB
[2022-10-02 16:10:37 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:38 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0218 (0.0218)	loss 0.0264 (0.0264)	grad_norm 0.0801 (0.0801)	mem 459MB
[2022-10-02 16:10:38 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:38 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0125 (0.0125)	loss 0.0191 (0.0191)	grad_norm 0.0380 (0.0380)	mem 459MB
[2022-10-02 16:10:38 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:38 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0201 (0.0201)	loss 0.0219 (0.0219)	grad_norm 0.0691 (0.0691)	mem 459MB
[2022-10-02 16:10:38 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:38 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0121 (0.0121)	loss 0.0214 (0.0214)	grad_norm 0.0564 (0.0564)	mem 459MB
[2022-10-02 16:10:38 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:38 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0211 (0.0211)	loss 0.0158 (0.0158)	grad_norm 0.0458 (0.0458)	mem 459MB
[2022-10-02 16:10:38 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:38 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0116 (0.0116)	loss 0.0190 (0.0190)	grad_norm 0.0581 (0.0581)	mem 459MB
[2022-10-02 16:10:38 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:39 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0196 (0.0196)	loss 0.0175 (0.0175)	grad_norm 0.0773 (0.0773)	mem 459MB
[2022-10-02 16:10:39 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:39 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0174 (0.0174)	loss 0.0209 (0.0209)	grad_norm 0.0487 (0.0487)	mem 459MB
[2022-10-02 16:10:39 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:39 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0200 (0.0200)	loss 0.0211 (0.0211)	grad_norm 0.0705 (0.0705)	mem 459MB
[2022-10-02 16:10:39 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:39 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0094 (0.0094)	loss 0.0251 (0.0251)	grad_norm 0.0643 (0.0643)	mem 459MB
[2022-10-02 16:10:39 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:39 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0089 (0.0089)	loss 0.0169 (0.0169)	grad_norm 0.0470 (0.0470)	mem 459MB
[2022-10-02 16:10:39 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:39 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0104 (0.0104)	loss 0.0226 (0.0226)	grad_norm 0.0720 (0.0720)	mem 459MB
[2022-10-02 16:10:39 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:39 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0094 (0.0094)	loss 0.0221 (0.0221)	grad_norm 0.0383 (0.0383)	mem 459MB
[2022-10-02 16:10:39 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:39 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0118 (0.0118)	loss 0.0162 (0.0162)	grad_norm 0.0652 (0.0652)	mem 459MB
[2022-10-02 16:10:39 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:40 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0129 (0.0129)	loss 0.0154 (0.0154)	grad_norm 0.0718 (0.0718)	mem 459MB
[2022-10-02 16:10:40 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:40 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0148 (0.0148)	loss 0.0186 (0.0186)	grad_norm 0.0416 (0.0416)	mem 459MB
[2022-10-02 16:10:40 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:40 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0106 (0.0106)	loss 0.0253 (0.0253)	grad_norm 0.0578 (0.0578)	mem 459MB
[2022-10-02 16:10:40 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:40 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0102 (0.0102)	loss 0.0194 (0.0194)	grad_norm 0.0535 (0.0535)	mem 459MB
[2022-10-02 16:10:40 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:40 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0103 (0.0103)	loss 0.0314 (0.0314)	grad_norm 0.0568 (0.0568)	mem 459MB
[2022-10-02 16:10:40 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:40 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0120 (0.0120)	loss 0.0219 (0.0219)	grad_norm 0.0674 (0.0674)	mem 459MB
[2022-10-02 16:10:40 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:40 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0103 (0.0103)	loss 0.0197 (0.0197)	grad_norm 0.0740 (0.0740)	mem 459MB
[2022-10-02 16:10:40 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 16:10:41 demo] (houston_program2.py 333): INFO Train Ep: 10 	Loss1: 0.540664	Loss2: 0.496954	 Dis: 10.907745 Entropy: 4.263019 
[2022-10-02 16:10:41 demo] (houston_program2.py 335): INFO time_10_epoch:6.849940538406372
[2022-10-02 16:10:41 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 16:10:41 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 16:10:41 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 16:10:41 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 16:10:41 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 16:10:41 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 16:10:41 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 16:10:41 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 16:10:41 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 16:10:41 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 16:10:47 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.284368	Loss2: 0.254691	 Dis: 12.631535 Entropy: 4.383392 
[2022-10-02 16:10:47 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:10:53 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.153476	Loss2: 0.148951	 Dis: 8.580790 Entropy: 4.828622 
[2022-10-02 16:10:53 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:11:00 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.402156	Loss2: 0.405287	 Dis: 6.176300 Entropy: 5.225630 
[2022-10-02 16:11:00 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:11:06 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.200070	Loss2: 0.211463	 Dis: 9.656168 Entropy: 4.564232 
[2022-10-02 16:11:06 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:11:13 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.200584	Loss2: 0.206198	 Dis: 8.910213 Entropy: 5.167757 
[2022-10-02 16:11:13 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:11:19 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.273252	Loss2: 0.283404	 Dis: 7.719290 Entropy: 5.026318 
[2022-10-02 16:11:19 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:11:26 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.230453	Loss2: 0.229560	 Dis: 7.322121 Entropy: 4.746242 
[2022-10-02 16:11:26 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:11:33 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.180092	Loss2: 0.221323	 Dis: 6.430239 Entropy: 4.578628 
[2022-10-02 16:11:33 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:11:39 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.156078	Loss2: 0.120968	 Dis: 5.994774 Entropy: 5.003366 
[2022-10-02 16:11:39 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:11:45 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.412594	Loss2: 0.373870	 Dis: 5.052767 Entropy: 5.706413 
[2022-10-02 16:11:45 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:11:52 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.101897	Loss2: 0.145190	 Dis: 5.514126 Entropy: 4.274659 
[2022-10-02 16:11:52 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:11:58 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.130503	Loss2: 0.128505	 Dis: 6.553484 Entropy: 5.054695 
[2022-10-02 16:11:58 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:12:04 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.171316	Loss2: 0.204765	 Dis: 7.871731 Entropy: 5.430421 
[2022-10-02 16:12:04 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:12:11 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.248937	Loss2: 0.286003	 Dis: 6.453617 Entropy: 4.523680 
[2022-10-02 16:12:11 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:12:17 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.080295	Loss2: 0.097341	 Dis: 3.934120 Entropy: 5.242605 
[2022-10-02 16:12:17 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:12:23 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.102679	Loss2: 0.101849	 Dis: 4.646652 Entropy: 5.407182 
[2022-10-02 16:12:23 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:12:30 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.125672	Loss2: 0.142175	 Dis: 3.911522 Entropy: 5.252308 
[2022-10-02 16:12:30 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:12:36 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.514034	Loss2: 0.527933	 Dis: 5.292765 Entropy: 4.818725 
[2022-10-02 16:12:36 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:12:42 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.173264	Loss2: 0.187082	 Dis: 4.653873 Entropy: 4.985982 
[2022-10-02 16:12:42 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:12:48 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.376825	Loss2: 0.355304	 Dis: 7.414677 Entropy: 4.731384 
[2022-10-02 16:12:48 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:12:55 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.162831	Loss2: 0.163179	 Dis: 7.286545 Entropy: 6.073218 
[2022-10-02 16:12:55 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:13:01 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.034162	Loss2: 0.033642	 Dis: 6.264278 Entropy: 4.689228 
[2022-10-02 16:13:01 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:13:08 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.136746	Loss2: 0.117199	 Dis: 4.951544 Entropy: 5.068315 
[2022-10-02 16:13:08 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:13:14 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.126001	Loss2: 0.102650	 Dis: 7.058817 Entropy: 4.898930 
[2022-10-02 16:13:14 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:13:20 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.057242	Loss2: 0.046139	 Dis: 3.874088 Entropy: 5.164533 
[2022-10-02 16:13:20 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:13:27 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.357179	Loss2: 0.400065	 Dis: 12.715141 Entropy: 4.382905 
[2022-10-02 16:13:27 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:13:33 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.420298	Loss2: 0.465185	 Dis: 9.195723 Entropy: 5.484561 
[2022-10-02 16:13:33 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:13:40 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.177927	Loss2: 0.141140	 Dis: 9.917467 Entropy: 5.328533 
[2022-10-02 16:13:40 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:13:46 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.277784	Loss2: 0.266846	 Dis: 7.154524 Entropy: 4.894393 
[2022-10-02 16:13:46 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:13:52 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.202571	Loss2: 0.210168	 Dis: 4.449064 Entropy: 4.642699 
[2022-10-02 16:13:52 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:13:58 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.312082	Loss2: 0.375678	 Dis: 8.131607 Entropy: 4.575400 
[2022-10-02 16:13:58 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:14:05 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.228212	Loss2: 0.219137	 Dis: 6.865620 Entropy: 4.682869 
[2022-10-02 16:14:05 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:14:11 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.219200	Loss2: 0.260238	 Dis: 9.513107 Entropy: 4.702456 
[2022-10-02 16:14:11 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:14:17 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.094873	Loss2: 0.131631	 Dis: 5.023079 Entropy: 6.035435 
[2022-10-02 16:14:17 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:14:24 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.248367	Loss2: 0.218342	 Dis: 4.492350 Entropy: 4.863235 
[2022-10-02 16:14:24 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:14:30 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.158613	Loss2: 0.164208	 Dis: 5.102772 Entropy: 4.749657 
[2022-10-02 16:14:30 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:14:37 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.151254	Loss2: 0.159706	 Dis: 7.598509 Entropy: 5.004640 
[2022-10-02 16:14:37 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:14:43 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.124895	Loss2: 0.102645	 Dis: 5.111164 Entropy: 4.525684 
[2022-10-02 16:14:43 demo] (houston_program2.py 507): INFO E_lr: 0.005000	 C_lr:0.000050
[2022-10-02 16:39:53 demo] (houston_program2.py 727): INFO Full config saved to outputs/demo/finetune_100/config.json
[2022-10-02 16:39:53 demo] (houston_program2.py 730): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 32
  CHANNEL_DIM: 48
  CLASS_NUM: 7
  DATASET: huston13-18
  DATA_PATH: ''
  DATA_SOURCE_PATH: dataset/houston13-18/Houston13.mat
  DATA_TARGET_PATH: dataset/houston13-18/Houston18.mat
  DIM: 48
  EPOCHS: 80
  GAMMA: 0.9
  HALFWIDTH: 2
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  LABEL_SOURCE_PATH: dataset/houston13-18/Houston13_7gt.mat
  LABEL_TARGET_PATH: dataset/houston13-18/Houston18_7gt.mat
  LEARNING_RATE: 2.5e-05
  MASK_PATCH_SIZE: 4
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  N_CLASS: 32
  PATCH_DIM: 512
  PIN_MEMORY: true
  SAMPLE_NUM: 180
  SEED: 0
EVAL_MODE: false
IS_DIST: false
IS_HSI: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  Dtransformer:
    APE: false
    DEPTH: 2
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 6
  LABEL_SMOOTHING: 0.0
  LEARNING_RATE: 2.5e-05
  NAME: demo
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: Dtransformer
  VIT:
    DEPTH: 12
    EMBED_DIM: 768
    INIT_VALUES: 0.1
    IN_CHANS: 3
    MLP_RATIO: 4
    NUM_HEADS: 12
    PATCH_SIZE: 16
    QKV_BIAS: true
    USE_APE: false
    USE_MEAN_POOLING: false
    USE_RPB: false
    USE_SHARED_RPB: true
OUTPUT: outputs/demo/finetune_100
PRETRAINED: ''
PRINT_FREQ: 100
SAVE_FREQ: 5
SEED: 0
TAG: finetune_100
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_LR: 5.0e-05
  CLIP_GRAD: 5.0
  EPOCHS: 100
  FINETUNE:
    BASE_LR: 0.005
    MIN_LR: 5.0e-07
    WARMUP_EPOCHS: 10
    WARMUP_LR: 5.0e-06
    WEIGHT_DECAY: 0.05
  LAYER_DECAY: 0.7
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
  MIN_LR: 1.5625e-08
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2022-10-02 16:40:05 demo] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 16:40:05 demo] (optimizer.py 27): INFO No weight decay: {'encoder.absolute_pos_embed'}
[2022-10-02 16:40:05 demo] (optimizer.py 30): INFO No weight decay keywords: {'encoder.relative_position_bias_table'}
[2022-10-02 16:40:05 demo] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed.proj.bias', 'encoder.patch_to_embedding.bias', 'encoder.attn_layers.layers.0.0.weight', 'encoder.attn_layers.layers.0.0.bias', 'encoder.attn_layers.layers.0.1.to_out.bias', 'encoder.attn_layers.layers.1.0.weight', 'encoder.attn_layers.layers.1.0.bias', 'encoder.attn_layers.layers.1.1.net.0.0.bias', 'encoder.attn_layers.layers.1.1.net.2.bias', 'encoder.attn_layers.layers.2.0.weight', 'encoder.attn_layers.layers.2.0.bias', 'encoder.attn_layers.layers.2.1.to_out.bias', 'encoder.attn_layers.layers.3.0.weight', 'encoder.attn_layers.layers.3.0.bias', 'encoder.attn_layers.layers.3.1.net.0.0.bias', 'encoder.attn_layers.layers.3.1.net.2.bias', 'encoder.norm.weight', 'encoder.norm.bias', 'decoder.0.bias']
[2022-10-02 16:40:05 demo] (optimizer.py 64): INFO Has decay params: ['encoder.pos_embedding', 'encoder.cls_token', 'encoder.mask_token', 'encoder.patch_embed.proj.weight', 'encoder.patch_to_embedding.weight', 'encoder.attn_layers.layers.0.1.to_q.weight', 'encoder.attn_layers.layers.0.1.to_k.weight', 'encoder.attn_layers.layers.0.1.to_v.weight', 'encoder.attn_layers.layers.0.1.to_out.weight', 'encoder.attn_layers.layers.1.1.net.0.0.weight', 'encoder.attn_layers.layers.1.1.net.2.weight', 'encoder.attn_layers.layers.2.1.to_q.weight', 'encoder.attn_layers.layers.2.1.to_k.weight', 'encoder.attn_layers.layers.2.1.to_v.weight', 'encoder.attn_layers.layers.2.1.to_out.weight', 'encoder.attn_layers.layers.3.1.net.0.0.weight', 'encoder.attn_layers.layers.3.1.net.2.weight', 'decoder.0.weight']
[2022-10-02 16:40:05 demo] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 16:40:05 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 16:40:05 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 16:40:05 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 16:40:05 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 16:40:05 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 16:40:05 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 16:40:05 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 16:40:05 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 16:40:05 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 16:40:05 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 16:40:05 demo] (houston_program2.py 109): INFO Start training
[2022-10-02 16:40:05 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 16:40:05 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 16:40:05 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 16:40:05 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 16:40:05 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 16:40:05 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 16:40:05 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 16:40:05 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 16:40:05 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 16:40:05 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 16:41:36 demo] (houston_program2.py 727): INFO Full config saved to outputs/demo/finetune_100/config.json
[2022-10-02 16:41:36 demo] (houston_program2.py 730): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 32
  CHANNEL_DIM: 48
  CLASS_NUM: 7
  DATASET: huston13-18
  DATA_PATH: ''
  DATA_SOURCE_PATH: dataset/houston13-18/Houston13.mat
  DATA_TARGET_PATH: dataset/houston13-18/Houston18.mat
  DIM: 48
  EPOCHS: 80
  GAMMA: 0.9
  HALFWIDTH: 2
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  LABEL_SOURCE_PATH: dataset/houston13-18/Houston13_7gt.mat
  LABEL_TARGET_PATH: dataset/houston13-18/Houston18_7gt.mat
  LEARNING_RATE: 2.5e-05
  MASK_PATCH_SIZE: 4
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  N_CLASS: 32
  PATCH_DIM: 512
  PIN_MEMORY: true
  SAMPLE_NUM: 180
  SEED: 0
EVAL_MODE: false
IS_DIST: false
IS_HSI: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  Dtransformer:
    APE: false
    DEPTH: 2
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 6
  LABEL_SMOOTHING: 0.0
  LEARNING_RATE: 2.5e-05
  NAME: demo
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: Dtransformer
  VIT:
    DEPTH: 12
    EMBED_DIM: 768
    INIT_VALUES: 0.1
    IN_CHANS: 3
    MLP_RATIO: 4
    NUM_HEADS: 12
    PATCH_SIZE: 16
    QKV_BIAS: true
    USE_APE: false
    USE_MEAN_POOLING: false
    USE_RPB: false
    USE_SHARED_RPB: true
OUTPUT: outputs/demo/finetune_100
PRETRAINED: ''
PRINT_FREQ: 100
SAVE_FREQ: 5
SEED: 0
TAG: finetune_100
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_LR: 5.0e-05
  CLIP_GRAD: 5.0
  EPOCHS: 100
  FINETUNE:
    BASE_LR: 0.005
    MIN_LR: 5.0e-07
    WARMUP_EPOCHS: 10
    WARMUP_LR: 5.0e-06
    WEIGHT_DECAY: 0.05
  LAYER_DECAY: 0.7
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
  MIN_LR: 1.5625e-08
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2022-10-02 16:41:45 demo] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 16:41:45 demo] (optimizer.py 27): INFO No weight decay: {'encoder.absolute_pos_embed'}
[2022-10-02 16:41:45 demo] (optimizer.py 30): INFO No weight decay keywords: {'encoder.relative_position_bias_table'}
[2022-10-02 16:41:45 demo] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed.proj.bias', 'encoder.patch_to_embedding.bias', 'encoder.attn_layers.layers.0.0.weight', 'encoder.attn_layers.layers.0.0.bias', 'encoder.attn_layers.layers.0.1.to_out.bias', 'encoder.attn_layers.layers.1.0.weight', 'encoder.attn_layers.layers.1.0.bias', 'encoder.attn_layers.layers.1.1.net.0.0.bias', 'encoder.attn_layers.layers.1.1.net.2.bias', 'encoder.attn_layers.layers.2.0.weight', 'encoder.attn_layers.layers.2.0.bias', 'encoder.attn_layers.layers.2.1.to_out.bias', 'encoder.attn_layers.layers.3.0.weight', 'encoder.attn_layers.layers.3.0.bias', 'encoder.attn_layers.layers.3.1.net.0.0.bias', 'encoder.attn_layers.layers.3.1.net.2.bias', 'encoder.norm.weight', 'encoder.norm.bias', 'decoder.0.bias']
[2022-10-02 16:41:45 demo] (optimizer.py 64): INFO Has decay params: ['encoder.pos_embedding', 'encoder.cls_token', 'encoder.mask_token', 'encoder.patch_embed.proj.weight', 'encoder.patch_to_embedding.weight', 'encoder.attn_layers.layers.0.1.to_q.weight', 'encoder.attn_layers.layers.0.1.to_k.weight', 'encoder.attn_layers.layers.0.1.to_v.weight', 'encoder.attn_layers.layers.0.1.to_out.weight', 'encoder.attn_layers.layers.1.1.net.0.0.weight', 'encoder.attn_layers.layers.1.1.net.2.weight', 'encoder.attn_layers.layers.2.1.to_q.weight', 'encoder.attn_layers.layers.2.1.to_k.weight', 'encoder.attn_layers.layers.2.1.to_v.weight', 'encoder.attn_layers.layers.2.1.to_out.weight', 'encoder.attn_layers.layers.3.1.net.0.0.weight', 'encoder.attn_layers.layers.3.1.net.2.weight', 'decoder.0.weight']
[2022-10-02 16:41:45 demo] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 16:41:45 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 16:41:45 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 16:41:45 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 16:41:45 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 16:41:45 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 16:41:45 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 16:41:45 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 16:41:45 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 16:41:45 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 16:41:45 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 16:41:45 demo] (houston_program2.py 109): INFO Start training
[2022-10-02 16:41:45 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 16:41:45 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 16:41:45 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 16:41:45 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 16:41:45 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 16:41:45 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 16:41:45 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 16:41:45 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 16:41:45 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 16:41:45 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 16:43:50 demo] (houston_program2.py 727): INFO Full config saved to outputs/demo/finetune_100/config.json
[2022-10-02 16:43:50 demo] (houston_program2.py 730): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 32
  CHANNEL_DIM: 48
  CLASS_NUM: 7
  DATASET: huston13-18
  DATA_PATH: ''
  DATA_SOURCE_PATH: dataset/houston13-18/Houston13.mat
  DATA_TARGET_PATH: dataset/houston13-18/Houston18.mat
  DIM: 48
  EPOCHS: 80
  GAMMA: 0.9
  HALFWIDTH: 2
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  LABEL_SOURCE_PATH: dataset/houston13-18/Houston13_7gt.mat
  LABEL_TARGET_PATH: dataset/houston13-18/Houston18_7gt.mat
  LEARNING_RATE: 2.5e-05
  MASK_PATCH_SIZE: 4
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  N_CLASS: 32
  PATCH_DIM: 512
  PIN_MEMORY: true
  SAMPLE_NUM: 180
  SEED: 0
EVAL_MODE: false
IS_DIST: false
IS_HSI: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  Dtransformer:
    APE: false
    DEPTH: 2
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 6
  LABEL_SMOOTHING: 0.0
  LEARNING_RATE: 2.5e-05
  NAME: demo
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: Dtransformer
  VIT:
    DEPTH: 12
    EMBED_DIM: 768
    INIT_VALUES: 0.1
    IN_CHANS: 3
    MLP_RATIO: 4
    NUM_HEADS: 12
    PATCH_SIZE: 16
    QKV_BIAS: true
    USE_APE: false
    USE_MEAN_POOLING: false
    USE_RPB: false
    USE_SHARED_RPB: true
OUTPUT: outputs/demo/finetune_100
PRETRAINED: ''
PRINT_FREQ: 100
SAVE_FREQ: 5
SEED: 0
TAG: finetune_100
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: false
  BASE_LR: 5.0e-05
  CLIP_GRAD: 5.0
  EPOCHS: 100
  FINETUNE:
    BASE_LR: 0.005
    MIN_LR: 5.0e-07
    WARMUP_EPOCHS: 10
    WARMUP_LR: 5.0e-06
    WEIGHT_DECAY: 0.05
  LAYER_DECAY: 0.7
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
  MIN_LR: 1.5625e-08
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.5625e-08
  WEIGHT_DECAY: 0.05

[2022-10-02 16:43:59 demo] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 16:43:59 demo] (optimizer.py 27): INFO No weight decay: {'encoder.absolute_pos_embed'}
[2022-10-02 16:43:59 demo] (optimizer.py 30): INFO No weight decay keywords: {'encoder.relative_position_bias_table'}
[2022-10-02 16:43:59 demo] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed.proj.bias', 'encoder.patch_to_embedding.bias', 'encoder.attn_layers.layers.0.0.weight', 'encoder.attn_layers.layers.0.0.bias', 'encoder.attn_layers.layers.0.1.to_out.bias', 'encoder.attn_layers.layers.1.0.weight', 'encoder.attn_layers.layers.1.0.bias', 'encoder.attn_layers.layers.1.1.net.0.0.bias', 'encoder.attn_layers.layers.1.1.net.2.bias', 'encoder.attn_layers.layers.2.0.weight', 'encoder.attn_layers.layers.2.0.bias', 'encoder.attn_layers.layers.2.1.to_out.bias', 'encoder.attn_layers.layers.3.0.weight', 'encoder.attn_layers.layers.3.0.bias', 'encoder.attn_layers.layers.3.1.net.0.0.bias', 'encoder.attn_layers.layers.3.1.net.2.bias', 'encoder.norm.weight', 'encoder.norm.bias', 'decoder.0.bias']
[2022-10-02 16:43:59 demo] (optimizer.py 64): INFO Has decay params: ['encoder.pos_embedding', 'encoder.cls_token', 'encoder.mask_token', 'encoder.patch_embed.proj.weight', 'encoder.patch_to_embedding.weight', 'encoder.attn_layers.layers.0.1.to_q.weight', 'encoder.attn_layers.layers.0.1.to_k.weight', 'encoder.attn_layers.layers.0.1.to_v.weight', 'encoder.attn_layers.layers.0.1.to_out.weight', 'encoder.attn_layers.layers.1.1.net.0.0.weight', 'encoder.attn_layers.layers.1.1.net.2.weight', 'encoder.attn_layers.layers.2.1.to_q.weight', 'encoder.attn_layers.layers.2.1.to_k.weight', 'encoder.attn_layers.layers.2.1.to_v.weight', 'encoder.attn_layers.layers.2.1.to_out.weight', 'encoder.attn_layers.layers.3.1.net.0.0.weight', 'encoder.attn_layers.layers.3.1.net.2.weight', 'decoder.0.weight']
[2022-10-02 16:43:59 demo] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 16:43:59 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 16:43:59 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 16:43:59 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 16:43:59 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 16:43:59 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 16:43:59 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 16:43:59 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 16:43:59 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 16:43:59 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 16:43:59 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 16:43:59 demo] (houston_program2.py 109): INFO Start training
[2022-10-02 16:43:59 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0370 (0.0370)	loss 0.0751 (0.0751)	grad_norm 0.0324 (0.0324)	mem 177MB
[2022-10-02 16:43:59 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:00 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0219 (0.0219)	loss 0.0774 (0.0774)	grad_norm 0.0330 (0.0330)	mem 371MB
[2022-10-02 16:44:00 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:00 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0193 (0.0193)	loss 0.0832 (0.0832)	grad_norm 0.0323 (0.0323)	mem 371MB
[2022-10-02 16:44:00 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:00 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0213 (0.0213)	loss 0.0893 (0.0893)	grad_norm 0.0302 (0.0302)	mem 371MB
[2022-10-02 16:44:00 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:00 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0213 (0.0213)	loss 0.0840 (0.0840)	grad_norm 0.0298 (0.0298)	mem 371MB
[2022-10-02 16:44:00 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:00 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0237 (0.0237)	loss 0.0791 (0.0791)	grad_norm 0.0332 (0.0332)	mem 371MB
[2022-10-02 16:44:00 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:01 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0198 (0.0198)	loss 0.0983 (0.0983)	grad_norm 0.0326 (0.0326)	mem 371MB
[2022-10-02 16:44:01 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:01 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0189 (0.0189)	loss 0.0788 (0.0788)	grad_norm 0.0371 (0.0371)	mem 371MB
[2022-10-02 16:44:01 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:01 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0218 (0.0218)	loss 0.0711 (0.0711)	grad_norm 0.0361 (0.0361)	mem 371MB
[2022-10-02 16:44:01 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:01 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0114 (0.0114)	loss 0.0798 (0.0798)	grad_norm 0.0376 (0.0376)	mem 371MB
[2022-10-02 16:44:01 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:01 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0206 (0.0206)	loss 0.0842 (0.0842)	grad_norm 0.0334 (0.0334)	mem 371MB
[2022-10-02 16:44:01 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:01 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0126 (0.0126)	loss 0.0711 (0.0711)	grad_norm 0.0382 (0.0382)	mem 371MB
[2022-10-02 16:44:01 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:02 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0211 (0.0211)	loss 0.0799 (0.0799)	grad_norm 0.0308 (0.0308)	mem 371MB
[2022-10-02 16:44:02 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:02 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0207 (0.0207)	loss 0.0882 (0.0882)	grad_norm 0.0399 (0.0399)	mem 371MB
[2022-10-02 16:44:02 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:02 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0221 (0.0221)	loss 0.0744 (0.0744)	grad_norm 0.0359 (0.0359)	mem 371MB
[2022-10-02 16:44:02 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:02 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0171 (0.0171)	loss 0.0875 (0.0875)	grad_norm 0.0339 (0.0339)	mem 371MB
[2022-10-02 16:44:02 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:02 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0203 (0.0203)	loss 0.0839 (0.0839)	grad_norm 0.0310 (0.0310)	mem 371MB
[2022-10-02 16:44:02 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:03 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0206 (0.0206)	loss 0.0752 (0.0752)	grad_norm 0.0322 (0.0322)	mem 371MB
[2022-10-02 16:44:03 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:03 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0221 (0.0221)	loss 0.0700 (0.0700)	grad_norm 0.0362 (0.0362)	mem 371MB
[2022-10-02 16:44:03 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:03 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0204 (0.0204)	loss 0.0942 (0.0942)	grad_norm 0.0292 (0.0292)	mem 371MB
[2022-10-02 16:44:03 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:03 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0203 (0.0203)	loss 0.0753 (0.0753)	grad_norm 0.0347 (0.0347)	mem 371MB
[2022-10-02 16:44:03 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:03 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0085 (0.0085)	loss 0.0947 (0.0947)	grad_norm 0.0293 (0.0293)	mem 371MB
[2022-10-02 16:44:03 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:03 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0082 (0.0082)	loss 0.0623 (0.0623)	grad_norm 0.0355 (0.0355)	mem 371MB
[2022-10-02 16:44:03 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:03 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0083 (0.0083)	loss 0.0855 (0.0855)	grad_norm 0.0320 (0.0320)	mem 371MB
[2022-10-02 16:44:03 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:03 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0127 (0.0127)	loss 0.0792 (0.0792)	grad_norm 0.0301 (0.0301)	mem 371MB
[2022-10-02 16:44:03 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:04 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0214 (0.0214)	loss 0.0879 (0.0879)	grad_norm 0.0278 (0.0278)	mem 371MB
[2022-10-02 16:44:04 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:04 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0207 (0.0207)	loss 0.0724 (0.0724)	grad_norm 0.0332 (0.0332)	mem 371MB
[2022-10-02 16:44:04 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:04 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0202 (0.0202)	loss 0.0783 (0.0783)	grad_norm 0.0304 (0.0304)	mem 371MB
[2022-10-02 16:44:04 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:04 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0103 (0.0103)	loss 0.0786 (0.0786)	grad_norm 0.0297 (0.0297)	mem 371MB
[2022-10-02 16:44:04 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:04 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0215 (0.0215)	loss 0.0711 (0.0711)	grad_norm 0.0296 (0.0296)	mem 371MB
[2022-10-02 16:44:04 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:04 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0121 (0.0121)	loss 0.0621 (0.0621)	grad_norm 0.0333 (0.0333)	mem 371MB
[2022-10-02 16:44:05 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:05 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0247 (0.0247)	loss 0.0977 (0.0977)	grad_norm 0.0289 (0.0289)	mem 371MB
[2022-10-02 16:44:05 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:05 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0181 (0.0181)	loss 0.0834 (0.0834)	grad_norm 0.0309 (0.0309)	mem 371MB
[2022-10-02 16:44:05 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:05 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0275 (0.0275)	loss 0.0666 (0.0666)	grad_norm 0.0290 (0.0290)	mem 371MB
[2022-10-02 16:44:05 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:05 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0211 (0.0211)	loss 0.0758 (0.0758)	grad_norm 0.0280 (0.0280)	mem 371MB
[2022-10-02 16:44:05 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:05 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0216 (0.0216)	loss 0.0715 (0.0715)	grad_norm 0.0297 (0.0297)	mem 371MB
[2022-10-02 16:44:05 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:06 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0184 (0.0184)	loss 0.0986 (0.0986)	grad_norm 0.0273 (0.0273)	mem 371MB
[2022-10-02 16:44:06 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:06 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0212 (0.0212)	loss 0.0788 (0.0788)	grad_norm 0.0369 (0.0369)	mem 371MB
[2022-10-02 16:44:06 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:06 demo] (houston_program2.py 243): INFO Train: [0/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0220 (0.0220)	loss 0.0779 (0.0779)	grad_norm 0.0350 (0.0350)	mem 371MB
[2022-10-02 16:44:06 demo] (houston_program2.py 252): INFO EPOCH 0 training takes 0:00:00
[2022-10-02 16:44:06 demo] (houston_program2.py 333): INFO Train Ep: 0 	Loss1: 0.617298	Loss2: 0.647448	 Dis: 13.595806 Entropy: 4.372677 
[2022-10-02 16:44:06 demo] (houston_program2.py 335): INFO time_0_epoch:7.10519552230835
[2022-10-02 16:44:06 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 16:44:06 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 16:44:06 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 16:44:06 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 16:44:06 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 16:44:06 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 16:44:06 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 16:44:06 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 16:44:06 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 16:44:06 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 16:44:12 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.298733	Loss2: 0.328192	 Dis: 11.356176 Entropy: 4.798171 
[2022-10-02 16:44:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.000487	 E_lr_2:0.000487
[2022-10-02 16:44:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 16:44:18 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.395702	Loss2: 0.387910	 Dis: 12.767796 Entropy: 4.777389 
[2022-10-02 16:44:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.000987	 E_lr_2:0.000987
[2022-10-02 16:44:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 16:44:24 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.990997	Loss2: 1.063936	 Dis: 17.080671 Entropy: 4.147566 
[2022-10-02 16:44:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.001487	 E_lr_2:0.001487
[2022-10-02 16:44:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 16:44:30 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.471071	Loss2: 0.487867	 Dis: 12.814507 Entropy: 4.432094 
[2022-10-02 16:44:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.001987	 E_lr_2:0.001987
[2022-10-02 16:44:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 16:44:36 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.742546	Loss2: 0.809286	 Dis: 16.439919 Entropy: 4.455802 
[2022-10-02 16:44:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.002487	 E_lr_2:0.002487
[2022-10-02 16:44:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 16:44:42 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.677001	Loss2: 0.643491	 Dis: 12.349957 Entropy: 4.553415 
[2022-10-02 16:44:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.002987	 E_lr_2:0.002987
[2022-10-02 16:44:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 16:44:48 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.445738	Loss2: 0.484603	 Dis: 11.865767 Entropy: 4.727935 
[2022-10-02 16:44:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.003487	 E_lr_2:0.003487
[2022-10-02 16:44:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 16:44:54 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.507357	Loss2: 0.521237	 Dis: 11.442032 Entropy: 4.661946 
[2022-10-02 16:44:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.003987	 E_lr_2:0.003987
[2022-10-02 16:44:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 16:45:00 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.656724	Loss2: 0.615915	 Dis: 16.302435 Entropy: 5.038336 
[2022-10-02 16:45:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.004487	 E_lr_2:0.004487
[2022-10-02 16:45:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 16:45:06 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.200928	Loss2: 0.173385	 Dis: 8.841782 Entropy: 5.394973 
[2022-10-02 16:45:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.004987	 E_lr_2:0.004987
[2022-10-02 16:45:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000050	 C_lr_2:0.000050	 C_lr_3:0.000050	 C_lr_4:0.000050
[2022-10-02 16:45:12 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.205814	Loss2: 0.186674	 Dis: 8.415739 Entropy: 5.092452 
[2022-10-02 16:45:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.004853	 E_lr_2:0.004853
[2022-10-02 16:45:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000049	 C_lr_2:0.000049	 C_lr_3:0.000049	 C_lr_4:0.000049
[2022-10-02 16:45:18 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.360799	Loss2: 0.391489	 Dis: 7.851171 Entropy: 4.870742 
[2022-10-02 16:45:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.004825	 E_lr_2:0.004825
[2022-10-02 16:45:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 16:45:24 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.130010	Loss2: 0.137771	 Dis: 10.590078 Entropy: 4.678907 
[2022-10-02 16:45:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.004795	 E_lr_2:0.004795
[2022-10-02 16:45:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 16:45:30 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.198438	Loss2: 0.193206	 Dis: 6.977999 Entropy: 5.256645 
[2022-10-02 16:45:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.004763	 E_lr_2:0.004763
[2022-10-02 16:45:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 16:45:36 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.307933	Loss2: 0.293958	 Dis: 8.111151 Entropy: 4.829533 
[2022-10-02 16:45:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.004728	 E_lr_2:0.004728
[2022-10-02 16:45:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 16:45:42 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.209763	Loss2: 0.223438	 Dis: 6.877569 Entropy: 5.591595 
[2022-10-02 16:45:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.004692	 E_lr_2:0.004692
[2022-10-02 16:45:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 16:45:48 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.585635	Loss2: 0.510964	 Dis: 7.998398 Entropy: 4.867489 
[2022-10-02 16:45:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.004653	 E_lr_2:0.004653
[2022-10-02 16:45:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 16:45:54 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.180206	Loss2: 0.187364	 Dis: 7.698307 Entropy: 5.389904 
[2022-10-02 16:45:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.004612	 E_lr_2:0.004612
[2022-10-02 16:45:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 16:46:00 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.207372	Loss2: 0.169398	 Dis: 11.437784 Entropy: 4.382146 
[2022-10-02 16:46:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.004569	 E_lr_2:0.004569
[2022-10-02 16:46:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 16:46:06 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.239339	Loss2: 0.248735	 Dis: 8.409872 Entropy: 4.672912 
[2022-10-02 16:46:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.004524	 E_lr_2:0.004524
[2022-10-02 16:46:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 16:46:12 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.226929	Loss2: 0.201910	 Dis: 8.819935 Entropy: 4.457459 
[2022-10-02 16:46:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.004477	 E_lr_2:0.004477
[2022-10-02 16:46:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 16:46:18 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.292877	Loss2: 0.277856	 Dis: 6.159500 Entropy: 5.135612 
[2022-10-02 16:46:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.004428	 E_lr_2:0.004428
[2022-10-02 16:46:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 16:46:24 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.225949	Loss2: 0.214823	 Dis: 8.552813 Entropy: 4.862506 
[2022-10-02 16:46:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.004377	 E_lr_2:0.004377
[2022-10-02 16:46:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 16:46:30 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.299356	Loss2: 0.280977	 Dis: 8.911644 Entropy: 5.328878 
[2022-10-02 16:46:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.004324	 E_lr_2:0.004324
[2022-10-02 16:46:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 16:46:36 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.133427	Loss2: 0.150004	 Dis: 6.394976 Entropy: 5.186522 
[2022-10-02 16:46:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.004269	 E_lr_2:0.004269
[2022-10-02 16:46:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 16:46:42 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.125502	Loss2: 0.129597	 Dis: 7.864241 Entropy: 4.948614 
[2022-10-02 16:46:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.004213	 E_lr_2:0.004213
[2022-10-02 16:46:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 16:46:48 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.302767	Loss2: 0.314016	 Dis: 6.731386 Entropy: 4.798609 
[2022-10-02 16:46:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.004155	 E_lr_2:0.004155
[2022-10-02 16:46:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 16:46:54 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.164292	Loss2: 0.183435	 Dis: 4.670769 Entropy: 4.520426 
[2022-10-02 16:46:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.004095	 E_lr_2:0.004095
[2022-10-02 16:46:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000041	 C_lr_2:0.000041	 C_lr_3:0.000041	 C_lr_4:0.000041
[2022-10-02 16:47:00 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.200759	Loss2: 0.185649	 Dis: 5.459581 Entropy: 5.765976 
[2022-10-02 16:47:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.004034	 E_lr_2:0.004034
[2022-10-02 16:47:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 16:47:06 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.208726	Loss2: 0.182116	 Dis: 6.841391 Entropy: 4.744438 
[2022-10-02 16:47:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.003971	 E_lr_2:0.003971
[2022-10-02 16:47:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 16:47:12 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.147370	Loss2: 0.179550	 Dis: 6.844177 Entropy: 4.761980 
[2022-10-02 16:47:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.003907	 E_lr_2:0.003907
[2022-10-02 16:47:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000039	 C_lr_2:0.000039	 C_lr_3:0.000039	 C_lr_4:0.000039
[2022-10-02 16:47:17 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.274793	Loss2: 0.262123	 Dis: 5.672077 Entropy: 4.733922 
[2022-10-02 16:47:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.003841	 E_lr_2:0.003841
[2022-10-02 16:47:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 16:47:23 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.366006	Loss2: 0.324235	 Dis: 5.890518 Entropy: 4.380235 
[2022-10-02 16:47:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.003774	 E_lr_2:0.003774
[2022-10-02 16:47:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 16:47:30 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.276771	Loss2: 0.282246	 Dis: 7.233631 Entropy: 4.448495 
[2022-10-02 16:47:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.003706	 E_lr_2:0.003706
[2022-10-02 16:47:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000037	 C_lr_2:0.000037	 C_lr_3:0.000037	 C_lr_4:0.000037
[2022-10-02 16:47:36 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.118536	Loss2: 0.145094	 Dis: 7.068361 Entropy: 4.917162 
[2022-10-02 16:47:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.003637	 E_lr_2:0.003637
[2022-10-02 16:47:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 16:47:42 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.099566	Loss2: 0.109150	 Dis: 7.092297 Entropy: 4.530343 
[2022-10-02 16:47:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.003566	 E_lr_2:0.003566
[2022-10-02 16:47:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 16:47:48 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.228488	Loss2: 0.256461	 Dis: 6.540634 Entropy: 5.247565 
[2022-10-02 16:47:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.003495	 E_lr_2:0.003495
[2022-10-02 16:47:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 16:47:54 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.240185	Loss2: 0.273164	 Dis: 7.292902 Entropy: 5.362731 
[2022-10-02 16:47:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.003422	 E_lr_2:0.003422
[2022-10-02 16:47:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000034	 C_lr_2:0.000034	 C_lr_3:0.000034	 C_lr_4:0.000034
[2022-10-02 16:48:00 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.088434	Loss2: 0.107302	 Dis: 4.732218 Entropy: 4.540390 
[2022-10-02 16:48:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.003349	 E_lr_2:0.003349
[2022-10-02 16:48:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 16:48:06 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.081136	Loss2: 0.094419	 Dis: 5.156837 Entropy: 5.261042 
[2022-10-02 16:48:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.003274	 E_lr_2:0.003274
[2022-10-02 16:48:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 16:48:13 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.042310	Loss2: 0.049350	 Dis: 4.119335 Entropy: 5.198203 
[2022-10-02 16:48:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.003199	 E_lr_2:0.003199
[2022-10-02 16:48:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000032	 C_lr_2:0.000032	 C_lr_3:0.000032	 C_lr_4:0.000032
[2022-10-02 16:48:19 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.194810	Loss2: 0.154145	 Dis: 7.323429 Entropy: 5.212058 
[2022-10-02 16:48:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.003124	 E_lr_2:0.003124
[2022-10-02 16:48:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000031	 C_lr_2:0.000031	 C_lr_3:0.000031	 C_lr_4:0.000031
[2022-10-02 16:48:25 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.250904	Loss2: 0.296152	 Dis: 7.492477 Entropy: 5.020620 
[2022-10-02 16:48:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.003047	 E_lr_2:0.003047
[2022-10-02 16:48:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 16:48:31 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.073764	Loss2: 0.067046	 Dis: 4.086174 Entropy: 4.239877 
[2022-10-02 16:48:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.002970	 E_lr_2:0.002970
[2022-10-02 16:48:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 16:48:37 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.093649	Loss2: 0.095005	 Dis: 4.968521 Entropy: 5.151938 
[2022-10-02 16:48:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.002893	 E_lr_2:0.002893
[2022-10-02 16:48:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000029	 C_lr_2:0.000029	 C_lr_3:0.000029	 C_lr_4:0.000029
[2022-10-02 16:48:43 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.101025	Loss2: 0.072021	 Dis: 5.686157 Entropy: 5.700159 
[2022-10-02 16:48:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.002815	 E_lr_2:0.002815
[2022-10-02 16:48:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000028	 C_lr_2:0.000028	 C_lr_3:0.000028	 C_lr_4:0.000028
[2022-10-02 16:48:49 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.108491	Loss2: 0.118852	 Dis: 8.243366 Entropy: 4.704504 
[2022-10-02 16:48:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.002737	 E_lr_2:0.002737
[2022-10-02 16:48:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 16:48:55 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.104121	Loss2: 0.113806	 Dis: 5.757278 Entropy: 5.625669 
[2022-10-02 16:48:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.002659	 E_lr_2:0.002659
[2022-10-02 16:48:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 16:49:01 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.161146	Loss2: 0.151197	 Dis: 5.379707 Entropy: 4.927970 
[2022-10-02 16:49:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.002581	 E_lr_2:0.002581
[2022-10-02 16:49:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000026	 C_lr_2:0.000026	 C_lr_3:0.000026	 C_lr_4:0.000026
[2022-10-02 16:49:07 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.061333	Loss2: 0.055052	 Dis: 3.581156 Entropy: 5.790491 
[2022-10-02 16:49:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.002502	 E_lr_2:0.002502
[2022-10-02 16:49:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 16:49:13 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.169268	Loss2: 0.168203	 Dis: 4.766401 Entropy: 4.308476 
[2022-10-02 16:49:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.002423	 E_lr_2:0.002423
[2022-10-02 16:49:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000024	 C_lr_2:0.000024	 C_lr_3:0.000024	 C_lr_4:0.000024
[2022-10-02 16:49:19 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.107932	Loss2: 0.112565	 Dis: 3.964546 Entropy: 5.315622 
[2022-10-02 16:49:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.002345	 E_lr_2:0.002345
[2022-10-02 16:49:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 16:49:25 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.143709	Loss2: 0.127608	 Dis: 4.133949 Entropy: 5.466321 
[2022-10-02 16:49:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.002267	 E_lr_2:0.002267
[2022-10-02 16:49:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 16:49:32 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.068720	Loss2: 0.055858	 Dis: 4.663029 Entropy: 5.113404 
[2022-10-02 16:49:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.002189	 E_lr_2:0.002189
[2022-10-02 16:49:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000022	 C_lr_2:0.000022	 C_lr_3:0.000022	 C_lr_4:0.000022
[2022-10-02 16:49:38 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.032097	Loss2: 0.031680	 Dis: 5.245380 Entropy: 5.641013 
[2022-10-02 16:49:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.002111	 E_lr_2:0.002111
[2022-10-02 16:49:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000021	 C_lr_2:0.000021	 C_lr_3:0.000021	 C_lr_4:0.000021
[2022-10-02 16:49:44 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.068075	Loss2: 0.074119	 Dis: 4.195023 Entropy: 5.340954 
[2022-10-02 16:49:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.002034	 E_lr_2:0.002034
[2022-10-02 16:49:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 16:49:49 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.046697	Loss2: 0.038520	 Dis: 5.015263 Entropy: 5.793250 
[2022-10-02 16:49:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.001957	 E_lr_2:0.001957
[2022-10-02 16:49:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 16:49:56 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.041876	Loss2: 0.034795	 Dis: 5.759748 Entropy: 4.912935 
[2022-10-02 16:49:56 demo] (houston_program2.py 512): INFO E_lr_1: 0.001880	 E_lr_2:0.001880
[2022-10-02 16:49:56 demo] (houston_program2.py 513): INFO C_lr_1: 0.000019	 C_lr_2:0.000019	 C_lr_3:0.000019	 C_lr_4:0.000019
[2022-10-02 16:50:02 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.028804	Loss2: 0.023494	 Dis: 4.187725 Entropy: 5.809209 
[2022-10-02 16:50:02 demo] (houston_program2.py 512): INFO E_lr_1: 0.001804	 E_lr_2:0.001804
[2022-10-02 16:50:02 demo] (houston_program2.py 513): INFO C_lr_1: 0.000018	 C_lr_2:0.000018	 C_lr_3:0.000018	 C_lr_4:0.000018
[2022-10-02 16:50:07 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.051194	Loss2: 0.061036	 Dis: 4.281624 Entropy: 4.891362 
[2022-10-02 16:50:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.001729	 E_lr_2:0.001729
[2022-10-02 16:50:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 16:50:14 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.103922	Loss2: 0.078625	 Dis: 4.207970 Entropy: 5.099312 
[2022-10-02 16:50:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.001655	 E_lr_2:0.001655
[2022-10-02 16:50:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 16:50:20 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.076856	Loss2: 0.058891	 Dis: 3.377476 Entropy: 4.571676 
[2022-10-02 16:50:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.001582	 E_lr_2:0.001582
[2022-10-02 16:50:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000016	 C_lr_2:0.000016	 C_lr_3:0.000016	 C_lr_4:0.000016
[2022-10-02 16:50:26 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.209545	Loss2: 0.238552	 Dis: 3.347980 Entropy: 5.268349 
[2022-10-02 16:50:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.001509	 E_lr_2:0.001509
[2022-10-02 16:50:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 16:50:33 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.100471	Loss2: 0.100643	 Dis: 4.826328 Entropy: 5.184543 
[2022-10-02 16:50:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.001437	 E_lr_2:0.001437
[2022-10-02 16:50:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 16:50:39 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.076240	Loss2: 0.063539	 Dis: 4.597244 Entropy: 4.920358 
[2022-10-02 16:50:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.001367	 E_lr_2:0.001367
[2022-10-02 16:50:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 16:50:45 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.044118	Loss2: 0.044013	 Dis: 3.565220 Entropy: 4.714086 
[2022-10-02 16:50:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.001297	 E_lr_2:0.001297
[2022-10-02 16:50:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000013	 C_lr_2:0.000013	 C_lr_3:0.000013	 C_lr_4:0.000013
[2022-10-02 16:50:51 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.056916	Loss2: 0.058275	 Dis: 5.113541 Entropy: 5.061732 
[2022-10-02 16:50:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.001229	 E_lr_2:0.001229
[2022-10-02 16:50:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 16:50:57 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.059871	Loss2: 0.066981	 Dis: 3.535400 Entropy: 5.176672 
[2022-10-02 16:50:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.001162	 E_lr_2:0.001162
[2022-10-02 16:50:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 16:51:02 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.057722	Loss2: 0.039132	 Dis: 5.777754 Entropy: 4.984611 
[2022-10-02 16:51:02 demo] (houston_program2.py 512): INFO E_lr_1: 0.001096	 E_lr_2:0.001096
[2022-10-02 16:51:02 demo] (houston_program2.py 513): INFO C_lr_1: 0.000011	 C_lr_2:0.000011	 C_lr_3:0.000011	 C_lr_4:0.000011
[2022-10-02 16:51:08 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.243261	Loss2: 0.215426	 Dis: 3.980942 Entropy: 5.087965 
[2022-10-02 16:51:08 demo] (houston_program2.py 512): INFO E_lr_1: 0.001032	 E_lr_2:0.001032
[2022-10-02 16:51:08 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 16:51:15 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.050569	Loss2: 0.046168	 Dis: 4.757244 Entropy: 5.105997 
[2022-10-02 16:51:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.000969	 E_lr_2:0.000969
[2022-10-02 16:51:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 16:51:21 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.040505	Loss2: 0.036818	 Dis: 1.807156 Entropy: 5.406727 
[2022-10-02 16:51:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.000908	 E_lr_2:0.000908
[2022-10-02 16:51:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000009	 C_lr_2:0.000009	 C_lr_3:0.000009	 C_lr_4:0.000009
[2022-10-02 16:51:27 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.023994	Loss2: 0.030913	 Dis: 4.386034 Entropy: 4.854004 
[2022-10-02 16:51:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.000848	 E_lr_2:0.000848
[2022-10-02 16:51:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 16:51:32 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.096703	Loss2: 0.080655	 Dis: 4.527416 Entropy: 4.842984 
[2022-10-02 16:51:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.000790	 E_lr_2:0.000790
[2022-10-02 16:51:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 16:51:38 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.065495	Loss2: 0.075089	 Dis: 2.632763 Entropy: 4.763265 
[2022-10-02 16:51:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.000734	 E_lr_2:0.000734
[2022-10-02 16:51:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 16:51:44 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.014949	Loss2: 0.014970	 Dis: 3.994535 Entropy: 5.560406 
[2022-10-02 16:51:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.000679	 E_lr_2:0.000679
[2022-10-02 16:51:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 16:51:51 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.051909	Loss2: 0.048276	 Dis: 3.652943 Entropy: 5.788159 
[2022-10-02 16:51:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.000626	 E_lr_2:0.000626
[2022-10-02 16:51:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 16:51:56 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.094547	Loss2: 0.074102	 Dis: 4.151285 Entropy: 4.934187 
[2022-10-02 16:51:56 demo] (houston_program2.py 512): INFO E_lr_1: 0.000575	 E_lr_2:0.000575
[2022-10-02 16:51:56 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 16:52:03 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.086990	Loss2: 0.105885	 Dis: 3.476311 Entropy: 5.267807 
[2022-10-02 16:52:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.000526	 E_lr_2:0.000526
[2022-10-02 16:52:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 16:52:09 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.014615	Loss2: 0.022660	 Dis: 4.797440 Entropy: 5.504991 
[2022-10-02 16:52:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.000479	 E_lr_2:0.000479
[2022-10-02 16:52:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 16:52:15 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.018611	Loss2: 0.023428	 Dis: 2.955454 Entropy: 5.115098 
[2022-10-02 16:52:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.000433	 E_lr_2:0.000433
[2022-10-02 16:52:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 16:52:21 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.047257	Loss2: 0.043773	 Dis: 4.191559 Entropy: 5.194406 
[2022-10-02 16:52:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.000390	 E_lr_2:0.000390
[2022-10-02 16:52:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 16:52:27 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.051311	Loss2: 0.037228	 Dis: 3.779976 Entropy: 4.764204 
[2022-10-02 16:52:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.000349	 E_lr_2:0.000349
[2022-10-02 16:52:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 16:52:34 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.056143	Loss2: 0.052246	 Dis: 5.608685 Entropy: 5.620908 
[2022-10-02 16:52:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.000310	 E_lr_2:0.000310
[2022-10-02 16:52:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 16:52:40 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.054536	Loss2: 0.057895	 Dis: 3.950895 Entropy: 5.899298 
[2022-10-02 16:52:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.000273	 E_lr_2:0.000273
[2022-10-02 16:52:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 16:52:46 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.015007	Loss2: 0.014156	 Dis: 3.733135 Entropy: 4.835826 
[2022-10-02 16:52:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.000239	 E_lr_2:0.000239
[2022-10-02 16:52:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 16:52:52 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.027834	Loss2: 0.027630	 Dis: 3.739796 Entropy: 6.085199 
[2022-10-02 16:52:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.000206	 E_lr_2:0.000206
[2022-10-02 16:52:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 16:52:58 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.033484	Loss2: 0.022249	 Dis: 2.522179 Entropy: 4.688257 
[2022-10-02 16:52:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.000176	 E_lr_2:0.000176
[2022-10-02 16:52:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 16:53:04 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.006038	Loss2: 0.010172	 Dis: 2.167643 Entropy: 5.197197 
[2022-10-02 16:53:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.000148	 E_lr_2:0.000148
[2022-10-02 16:53:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 16:53:10 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.041753	Loss2: 0.030154	 Dis: 3.720121 Entropy: 4.470081 
[2022-10-02 16:53:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.000123	 E_lr_2:0.000123
[2022-10-02 16:53:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 16:53:16 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.007479	Loss2: 0.005768	 Dis: 2.171131 Entropy: 4.832469 
[2022-10-02 16:53:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.000100	 E_lr_2:0.000100
[2022-10-02 16:53:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 16:53:22 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.011028	Loss2: 0.011985	 Dis: 3.479626 Entropy: 5.122791 
[2022-10-02 16:53:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.000079	 E_lr_2:0.000079
[2022-10-02 16:53:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 16:53:28 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.108679	Loss2: 0.107387	 Dis: 1.829197 Entropy: 5.372071 
[2022-10-02 16:53:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.000061	 E_lr_2:0.000061
[2022-10-02 16:53:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 16:53:33 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.020313	Loss2: 0.021775	 Dis: 3.261993 Entropy: 4.722107 
[2022-10-02 16:53:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.000045	 E_lr_2:0.000045
[2022-10-02 16:53:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 16:53:40 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.014712	Loss2: 0.013442	 Dis: 2.201126 Entropy: 4.787007 
[2022-10-02 16:53:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.000031	 E_lr_2:0.000031
[2022-10-02 16:53:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 16:53:45 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.014004	Loss2: 0.014242	 Dis: 5.328568 Entropy: 4.713880 
[2022-10-02 16:53:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.000020	 E_lr_2:0.000020
[2022-10-02 16:53:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 16:53:51 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.020521	Loss2: 0.025322	 Dis: 2.152796 Entropy: 5.554556 
[2022-10-02 16:53:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.000011	 E_lr_2:0.000011
[2022-10-02 16:53:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 16:53:57 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.008859	Loss2: 0.011729	 Dis: 5.145872 Entropy: 4.828970 
[2022-10-02 16:53:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.000005	 E_lr_2:0.000005
[2022-10-02 16:53:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 16:54:03 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.066779	Loss2: 0.069776	 Dis: 2.805107 Entropy: 4.918765 
[2022-10-02 16:54:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.000001	 E_lr_2:0.000001
[2022-10-02 16:54:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 16:54:09 demo] (houston_program2.py 504): INFO Train Ep: 0 	Loss1: 0.010649	Loss2: 0.011288	 Dis: 2.436815 Entropy: 5.548099 
[2022-10-02 16:54:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.000000	 E_lr_2:0.000000
[2022-10-02 16:54:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 16:54:09 demo] (houston_program2.py 515): INFO time_0_epoch:602.6627533435822
[2022-10-02 16:54:17 demo] (houston_program2.py 673): INFO 	val_Accuracy: 32470/53200 (61.03%)	
[2022-10-02 16:54:17 demo] (utils.py 47): INFO outputs/demo/finetune_100/ckpt_epoch_0.pth saving......
[2022-10-02 16:54:17 demo] (utils.py 49): INFO outputs/demo/finetune_100/ckpt_epoch_0.pth saved !!!
[2022-10-02 16:54:17 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0126 (0.0126)	loss 0.0866 (0.0866)	grad_norm 0.0276 (0.0276)	mem 455MB
[2022-10-02 16:54:17 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:18 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0210 (0.0210)	loss 0.0624 (0.0624)	grad_norm 0.0284 (0.0284)	mem 458MB
[2022-10-02 16:54:18 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:18 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0178 (0.0178)	loss 0.0618 (0.0618)	grad_norm 0.0299 (0.0299)	mem 458MB
[2022-10-02 16:54:18 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:18 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0215 (0.0215)	loss 0.0762 (0.0762)	grad_norm 0.0290 (0.0290)	mem 458MB
[2022-10-02 16:54:18 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:18 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0143 (0.0143)	loss 0.0862 (0.0862)	grad_norm 0.0275 (0.0275)	mem 458MB
[2022-10-02 16:54:18 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:18 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0224 (0.0224)	loss 0.0789 (0.0789)	grad_norm 0.0252 (0.0252)	mem 458MB
[2022-10-02 16:54:18 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:19 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0216 (0.0216)	loss 0.0862 (0.0862)	grad_norm 0.0318 (0.0318)	mem 458MB
[2022-10-02 16:54:19 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:19 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0204 (0.0204)	loss 0.0678 (0.0678)	grad_norm 0.0265 (0.0265)	mem 458MB
[2022-10-02 16:54:19 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:19 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0218 (0.0218)	loss 0.0680 (0.0680)	grad_norm 0.0274 (0.0274)	mem 458MB
[2022-10-02 16:54:19 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:19 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0189 (0.0189)	loss 0.0859 (0.0859)	grad_norm 0.0302 (0.0302)	mem 458MB
[2022-10-02 16:54:19 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:19 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0213 (0.0213)	loss 0.0781 (0.0781)	grad_norm 0.0258 (0.0258)	mem 458MB
[2022-10-02 16:54:19 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:19 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0218 (0.0218)	loss 0.0954 (0.0954)	grad_norm 0.0296 (0.0296)	mem 458MB
[2022-10-02 16:54:19 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:20 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0208 (0.0208)	loss 0.0627 (0.0627)	grad_norm 0.0269 (0.0269)	mem 458MB
[2022-10-02 16:54:20 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:20 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0216 (0.0216)	loss 0.0768 (0.0768)	grad_norm 0.0262 (0.0262)	mem 458MB
[2022-10-02 16:54:20 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:20 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0224 (0.0224)	loss 0.0726 (0.0726)	grad_norm 0.0257 (0.0257)	mem 458MB
[2022-10-02 16:54:20 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:20 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0105 (0.0105)	loss 0.0860 (0.0860)	grad_norm 0.0254 (0.0254)	mem 458MB
[2022-10-02 16:54:20 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:20 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0221 (0.0221)	loss 0.0683 (0.0683)	grad_norm 0.0260 (0.0260)	mem 458MB
[2022-10-02 16:54:20 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:20 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0110 (0.0110)	loss 0.0704 (0.0704)	grad_norm 0.0270 (0.0270)	mem 458MB
[2022-10-02 16:54:20 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:21 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0193 (0.0193)	loss 0.0861 (0.0861)	grad_norm 0.0297 (0.0297)	mem 458MB
[2022-10-02 16:54:21 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:21 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0127 (0.0127)	loss 0.0703 (0.0703)	grad_norm 0.0242 (0.0242)	mem 458MB
[2022-10-02 16:54:21 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:21 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0091 (0.0091)	loss 0.0761 (0.0761)	grad_norm 0.0273 (0.0273)	mem 458MB
[2022-10-02 16:54:21 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:21 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0094 (0.0094)	loss 0.0659 (0.0659)	grad_norm 0.0310 (0.0310)	mem 458MB
[2022-10-02 16:54:21 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:21 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0084 (0.0084)	loss 0.0497 (0.0497)	grad_norm 0.0258 (0.0258)	mem 458MB
[2022-10-02 16:54:21 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:21 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0095 (0.0095)	loss 0.0796 (0.0796)	grad_norm 0.0295 (0.0295)	mem 458MB
[2022-10-02 16:54:21 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:21 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0089 (0.0089)	loss 0.0532 (0.0532)	grad_norm 0.0260 (0.0260)	mem 458MB
[2022-10-02 16:54:21 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:21 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0095 (0.0095)	loss 0.0760 (0.0760)	grad_norm 0.0352 (0.0352)	mem 458MB
[2022-10-02 16:54:21 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:21 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0125 (0.0125)	loss 0.0590 (0.0590)	grad_norm 0.0268 (0.0268)	mem 458MB
[2022-10-02 16:54:22 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:22 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0222 (0.0222)	loss 0.0707 (0.0707)	grad_norm 0.0347 (0.0347)	mem 458MB
[2022-10-02 16:54:22 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:22 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0148 (0.0148)	loss 0.0843 (0.0843)	grad_norm 0.0298 (0.0298)	mem 458MB
[2022-10-02 16:54:22 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:22 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0243 (0.0243)	loss 0.0710 (0.0710)	grad_norm 0.0278 (0.0278)	mem 458MB
[2022-10-02 16:54:22 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:22 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0159 (0.0159)	loss 0.0653 (0.0653)	grad_norm 0.0266 (0.0266)	mem 458MB
[2022-10-02 16:54:22 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:22 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0223 (0.0223)	loss 0.0598 (0.0598)	grad_norm 0.0286 (0.0286)	mem 458MB
[2022-10-02 16:54:22 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:23 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0218 (0.0218)	loss 0.0619 (0.0619)	grad_norm 0.0322 (0.0322)	mem 458MB
[2022-10-02 16:54:23 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:23 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0218 (0.0218)	loss 0.0518 (0.0518)	grad_norm 0.0258 (0.0258)	mem 458MB
[2022-10-02 16:54:23 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:23 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0216 (0.0216)	loss 0.0648 (0.0648)	grad_norm 0.0325 (0.0325)	mem 458MB
[2022-10-02 16:54:23 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:23 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0221 (0.0221)	loss 0.0653 (0.0653)	grad_norm 0.0291 (0.0291)	mem 458MB
[2022-10-02 16:54:23 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:23 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0206 (0.0206)	loss 0.0540 (0.0540)	grad_norm 0.0381 (0.0381)	mem 458MB
[2022-10-02 16:54:23 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:24 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0150 (0.0150)	loss 0.0585 (0.0585)	grad_norm 0.0334 (0.0334)	mem 458MB
[2022-10-02 16:54:24 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:24 demo] (houston_program2.py 243): INFO Train: [1/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0212 (0.0212)	loss 0.0545 (0.0545)	grad_norm 0.0364 (0.0364)	mem 458MB
[2022-10-02 16:54:24 demo] (houston_program2.py 252): INFO EPOCH 1 training takes 0:00:00
[2022-10-02 16:54:24 demo] (houston_program2.py 333): INFO Train Ep: 1 	Loss1: 0.670667	Loss2: 0.688396	 Dis: 14.970793 Entropy: 4.321917 
[2022-10-02 16:54:24 demo] (houston_program2.py 335): INFO time_1_epoch:6.83452582359314
[2022-10-02 16:54:24 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0111 (0.0111)	loss 0.0543 (0.0543)	grad_norm 0.0351 (0.0351)	mem 458MB
[2022-10-02 16:54:24 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:24 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0211 (0.0211)	loss 0.0503 (0.0503)	grad_norm 0.0372 (0.0372)	mem 458MB
[2022-10-02 16:54:24 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:25 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0207 (0.0207)	loss 0.0528 (0.0528)	grad_norm 0.0319 (0.0319)	mem 458MB
[2022-10-02 16:54:25 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:25 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0189 (0.0189)	loss 0.0499 (0.0499)	grad_norm 0.0309 (0.0309)	mem 458MB
[2022-10-02 16:54:25 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:25 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0223 (0.0223)	loss 0.0643 (0.0643)	grad_norm 0.0313 (0.0313)	mem 458MB
[2022-10-02 16:54:25 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:25 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0180 (0.0180)	loss 0.0459 (0.0459)	grad_norm 0.0462 (0.0462)	mem 458MB
[2022-10-02 16:54:25 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:25 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0226 (0.0226)	loss 0.0608 (0.0608)	grad_norm 0.0387 (0.0387)	mem 458MB
[2022-10-02 16:54:25 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:26 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0187 (0.0187)	loss 0.0557 (0.0557)	grad_norm 0.0408 (0.0408)	mem 458MB
[2022-10-02 16:54:26 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:26 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0219 (0.0219)	loss 0.0425 (0.0425)	grad_norm 0.0345 (0.0345)	mem 458MB
[2022-10-02 16:54:26 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:26 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0220 (0.0220)	loss 0.0411 (0.0411)	grad_norm 0.0363 (0.0363)	mem 458MB
[2022-10-02 16:54:26 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:26 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0195 (0.0195)	loss 0.0521 (0.0521)	grad_norm 0.0384 (0.0384)	mem 458MB
[2022-10-02 16:54:26 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:26 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0179 (0.0179)	loss 0.0484 (0.0484)	grad_norm 0.0302 (0.0302)	mem 458MB
[2022-10-02 16:54:26 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:26 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0218 (0.0218)	loss 0.0632 (0.0632)	grad_norm 0.0352 (0.0352)	mem 458MB
[2022-10-02 16:54:27 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:27 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0215 (0.0215)	loss 0.0561 (0.0561)	grad_norm 0.0346 (0.0346)	mem 458MB
[2022-10-02 16:54:27 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:27 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0207 (0.0207)	loss 0.0402 (0.0402)	grad_norm 0.0333 (0.0333)	mem 458MB
[2022-10-02 16:54:27 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:27 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0216 (0.0216)	loss 0.0439 (0.0439)	grad_norm 0.0392 (0.0392)	mem 458MB
[2022-10-02 16:54:27 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:27 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0106 (0.0106)	loss 0.0629 (0.0629)	grad_norm 0.0364 (0.0364)	mem 458MB
[2022-10-02 16:54:27 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:27 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0209 (0.0209)	loss 0.0398 (0.0398)	grad_norm 0.0322 (0.0322)	mem 458MB
[2022-10-02 16:54:27 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:28 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0108 (0.0108)	loss 0.0368 (0.0368)	grad_norm 0.0355 (0.0355)	mem 458MB
[2022-10-02 16:54:28 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:28 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0210 (0.0210)	loss 0.0411 (0.0411)	grad_norm 0.0306 (0.0306)	mem 458MB
[2022-10-02 16:54:28 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:28 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0109 (0.0109)	loss 0.0391 (0.0391)	grad_norm 0.0324 (0.0324)	mem 458MB
[2022-10-02 16:54:28 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:28 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0215 (0.0215)	loss 0.0499 (0.0499)	grad_norm 0.0500 (0.0500)	mem 458MB
[2022-10-02 16:54:28 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:28 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0118 (0.0118)	loss 0.0478 (0.0478)	grad_norm 0.0460 (0.0460)	mem 458MB
[2022-10-02 16:54:28 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:28 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0222 (0.0222)	loss 0.0594 (0.0594)	grad_norm 0.0425 (0.0425)	mem 458MB
[2022-10-02 16:54:28 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:29 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0176 (0.0176)	loss 0.0469 (0.0469)	grad_norm 0.0442 (0.0442)	mem 458MB
[2022-10-02 16:54:29 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:29 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0213 (0.0213)	loss 0.0426 (0.0426)	grad_norm 0.0243 (0.0243)	mem 458MB
[2022-10-02 16:54:29 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:29 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0184 (0.0184)	loss 0.0453 (0.0453)	grad_norm 0.0275 (0.0275)	mem 458MB
[2022-10-02 16:54:29 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:29 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0207 (0.0207)	loss 0.0545 (0.0545)	grad_norm 0.0441 (0.0441)	mem 458MB
[2022-10-02 16:54:29 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:29 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0220 (0.0220)	loss 0.0430 (0.0430)	grad_norm 0.0368 (0.0368)	mem 458MB
[2022-10-02 16:54:29 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:30 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0207 (0.0207)	loss 0.0315 (0.0315)	grad_norm 0.0269 (0.0269)	mem 458MB
[2022-10-02 16:54:30 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:30 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0205 (0.0205)	loss 0.0409 (0.0409)	grad_norm 0.0317 (0.0317)	mem 458MB
[2022-10-02 16:54:30 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:30 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0268 (0.0268)	loss 0.0413 (0.0413)	grad_norm 0.0236 (0.0236)	mem 458MB
[2022-10-02 16:54:30 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:30 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0223 (0.0223)	loss 0.0550 (0.0550)	grad_norm 0.0279 (0.0279)	mem 458MB
[2022-10-02 16:54:30 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:30 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0093 (0.0093)	loss 0.0444 (0.0444)	grad_norm 0.0296 (0.0296)	mem 458MB
[2022-10-02 16:54:30 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:30 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0212 (0.0212)	loss 0.0380 (0.0380)	grad_norm 0.0244 (0.0244)	mem 458MB
[2022-10-02 16:54:30 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:31 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0142 (0.0142)	loss 0.0524 (0.0524)	grad_norm 0.0402 (0.0402)	mem 458MB
[2022-10-02 16:54:31 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:31 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0231 (0.0231)	loss 0.0367 (0.0367)	grad_norm 0.0271 (0.0271)	mem 458MB
[2022-10-02 16:54:31 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:31 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0120 (0.0120)	loss 0.0445 (0.0445)	grad_norm 0.0237 (0.0237)	mem 458MB
[2022-10-02 16:54:31 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:31 demo] (houston_program2.py 243): INFO Train: [2/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0214 (0.0214)	loss 0.0444 (0.0444)	grad_norm 0.0430 (0.0430)	mem 458MB
[2022-10-02 16:54:31 demo] (houston_program2.py 252): INFO EPOCH 2 training takes 0:00:00
[2022-10-02 16:54:31 demo] (houston_program2.py 333): INFO Train Ep: 2 	Loss1: 1.126414	Loss2: 1.135576	 Dis: 13.985729 Entropy: 4.311239 
[2022-10-02 16:54:31 demo] (houston_program2.py 335): INFO time_2_epoch:7.449463129043579
[2022-10-02 16:54:32 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0115 (0.0115)	loss 0.0359 (0.0359)	grad_norm 0.0468 (0.0468)	mem 458MB
[2022-10-02 16:54:32 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:32 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0228 (0.0228)	loss 0.0539 (0.0539)	grad_norm 0.0273 (0.0273)	mem 458MB
[2022-10-02 16:54:32 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:32 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0206 (0.0206)	loss 0.0388 (0.0388)	grad_norm 0.0372 (0.0372)	mem 458MB
[2022-10-02 16:54:32 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:32 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0168 (0.0168)	loss 0.0346 (0.0346)	grad_norm 0.0304 (0.0304)	mem 458MB
[2022-10-02 16:54:32 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:32 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0213 (0.0213)	loss 0.0366 (0.0366)	grad_norm 0.0351 (0.0351)	mem 458MB
[2022-10-02 16:54:33 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:33 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0188 (0.0188)	loss 0.0341 (0.0341)	grad_norm 0.0312 (0.0312)	mem 458MB
[2022-10-02 16:54:33 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:33 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0218 (0.0218)	loss 0.0382 (0.0382)	grad_norm 0.0221 (0.0221)	mem 458MB
[2022-10-02 16:54:33 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:33 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0173 (0.0173)	loss 0.0388 (0.0388)	grad_norm 0.0371 (0.0371)	mem 458MB
[2022-10-02 16:54:33 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:33 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0215 (0.0215)	loss 0.0402 (0.0402)	grad_norm 0.0371 (0.0371)	mem 458MB
[2022-10-02 16:54:33 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:33 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0210 (0.0210)	loss 0.0423 (0.0423)	grad_norm 0.0298 (0.0298)	mem 458MB
[2022-10-02 16:54:33 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:34 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0184 (0.0184)	loss 0.0395 (0.0395)	grad_norm 0.0331 (0.0331)	mem 458MB
[2022-10-02 16:54:34 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:34 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0207 (0.0207)	loss 0.0389 (0.0389)	grad_norm 0.0420 (0.0420)	mem 458MB
[2022-10-02 16:54:34 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:34 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0202 (0.0202)	loss 0.0454 (0.0454)	grad_norm 0.0325 (0.0325)	mem 458MB
[2022-10-02 16:54:34 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:34 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0222 (0.0222)	loss 0.0471 (0.0471)	grad_norm 0.0351 (0.0351)	mem 458MB
[2022-10-02 16:54:34 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:34 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0130 (0.0130)	loss 0.0402 (0.0402)	grad_norm 0.0290 (0.0290)	mem 458MB
[2022-10-02 16:54:34 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:34 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0205 (0.0205)	loss 0.0416 (0.0416)	grad_norm 0.0231 (0.0231)	mem 458MB
[2022-10-02 16:54:34 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:35 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0130 (0.0130)	loss 0.0322 (0.0322)	grad_norm 0.0390 (0.0390)	mem 458MB
[2022-10-02 16:54:35 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:35 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0215 (0.0215)	loss 0.0313 (0.0313)	grad_norm 0.0296 (0.0296)	mem 458MB
[2022-10-02 16:54:35 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:35 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0138 (0.0138)	loss 0.0414 (0.0414)	grad_norm 0.0345 (0.0345)	mem 458MB
[2022-10-02 16:54:35 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:35 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0221 (0.0221)	loss 0.0391 (0.0391)	grad_norm 0.0274 (0.0274)	mem 458MB
[2022-10-02 16:54:35 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:35 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0173 (0.0173)	loss 0.0409 (0.0409)	grad_norm 0.0250 (0.0250)	mem 458MB
[2022-10-02 16:54:35 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:36 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0220 (0.0220)	loss 0.0399 (0.0399)	grad_norm 0.0392 (0.0392)	mem 458MB
[2022-10-02 16:54:36 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:36 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0222 (0.0222)	loss 0.0333 (0.0333)	grad_norm 0.0280 (0.0280)	mem 458MB
[2022-10-02 16:54:36 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:36 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0200 (0.0200)	loss 0.0298 (0.0298)	grad_norm 0.0252 (0.0252)	mem 458MB
[2022-10-02 16:54:36 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:36 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0212 (0.0212)	loss 0.0426 (0.0426)	grad_norm 0.0403 (0.0403)	mem 458MB
[2022-10-02 16:54:36 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:36 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0222 (0.0222)	loss 0.0437 (0.0437)	grad_norm 0.0294 (0.0294)	mem 458MB
[2022-10-02 16:54:36 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:36 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0193 (0.0193)	loss 0.0286 (0.0286)	grad_norm 0.0312 (0.0312)	mem 458MB
[2022-10-02 16:54:36 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:37 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0257 (0.0257)	loss 0.0360 (0.0360)	grad_norm 0.0439 (0.0439)	mem 458MB
[2022-10-02 16:54:37 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:37 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0253 (0.0253)	loss 0.0414 (0.0414)	grad_norm 0.0237 (0.0237)	mem 458MB
[2022-10-02 16:54:37 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:37 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0102 (0.0102)	loss 0.0279 (0.0279)	grad_norm 0.0316 (0.0316)	mem 458MB
[2022-10-02 16:54:37 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:37 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0204 (0.0204)	loss 0.0330 (0.0330)	grad_norm 0.0304 (0.0304)	mem 458MB
[2022-10-02 16:54:37 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:37 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0104 (0.0104)	loss 0.0361 (0.0361)	grad_norm 0.0349 (0.0349)	mem 458MB
[2022-10-02 16:54:37 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:38 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0202 (0.0202)	loss 0.0367 (0.0367)	grad_norm 0.0526 (0.0526)	mem 458MB
[2022-10-02 16:54:38 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:38 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0165 (0.0165)	loss 0.0368 (0.0368)	grad_norm 0.0505 (0.0505)	mem 458MB
[2022-10-02 16:54:38 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:38 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0261 (0.0261)	loss 0.0343 (0.0343)	grad_norm 0.0235 (0.0235)	mem 458MB
[2022-10-02 16:54:38 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:38 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0214 (0.0214)	loss 0.0471 (0.0471)	grad_norm 0.0374 (0.0374)	mem 458MB
[2022-10-02 16:54:38 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:38 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0260 (0.0260)	loss 0.0410 (0.0410)	grad_norm 0.0570 (0.0570)	mem 458MB
[2022-10-02 16:54:38 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:39 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0226 (0.0226)	loss 0.0365 (0.0365)	grad_norm 0.0387 (0.0387)	mem 458MB
[2022-10-02 16:54:39 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:39 demo] (houston_program2.py 243): INFO Train: [3/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0276 (0.0276)	loss 0.0331 (0.0331)	grad_norm 0.0267 (0.0267)	mem 458MB
[2022-10-02 16:54:39 demo] (houston_program2.py 252): INFO EPOCH 3 training takes 0:00:00
[2022-10-02 16:54:39 demo] (houston_program2.py 333): INFO Train Ep: 3 	Loss1: 0.627693	Loss2: 0.603687	 Dis: 12.250589 Entropy: 4.603708 
[2022-10-02 16:54:39 demo] (houston_program2.py 335): INFO time_3_epoch:7.563570499420166
[2022-10-02 16:54:39 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0119 (0.0119)	loss 0.0356 (0.0356)	grad_norm 0.0377 (0.0377)	mem 458MB
[2022-10-02 16:54:39 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:39 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0212 (0.0212)	loss 0.0286 (0.0286)	grad_norm 0.0332 (0.0332)	mem 458MB
[2022-10-02 16:54:39 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:40 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0205 (0.0205)	loss 0.0445 (0.0445)	grad_norm 0.0335 (0.0335)	mem 458MB
[2022-10-02 16:54:40 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:40 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0180 (0.0180)	loss 0.0269 (0.0269)	grad_norm 0.0338 (0.0338)	mem 458MB
[2022-10-02 16:54:40 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:40 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0212 (0.0212)	loss 0.0235 (0.0235)	grad_norm 0.0361 (0.0361)	mem 458MB
[2022-10-02 16:54:40 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:40 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0217 (0.0217)	loss 0.0401 (0.0401)	grad_norm 0.0709 (0.0709)	mem 458MB
[2022-10-02 16:54:40 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:40 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0215 (0.0215)	loss 0.0314 (0.0314)	grad_norm 0.0247 (0.0247)	mem 458MB
[2022-10-02 16:54:40 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:41 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0213 (0.0213)	loss 0.0401 (0.0401)	grad_norm 0.0323 (0.0323)	mem 458MB
[2022-10-02 16:54:41 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:41 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0217 (0.0217)	loss 0.0423 (0.0423)	grad_norm 0.0273 (0.0273)	mem 458MB
[2022-10-02 16:54:41 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:41 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0226 (0.0226)	loss 0.0336 (0.0336)	grad_norm 0.0401 (0.0401)	mem 458MB
[2022-10-02 16:54:41 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:41 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0273 (0.0273)	loss 0.0287 (0.0287)	grad_norm 0.0349 (0.0349)	mem 458MB
[2022-10-02 16:54:41 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:41 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0206 (0.0206)	loss 0.0317 (0.0317)	grad_norm 0.0360 (0.0360)	mem 458MB
[2022-10-02 16:54:41 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:41 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0167 (0.0167)	loss 0.0414 (0.0414)	grad_norm 0.0633 (0.0633)	mem 458MB
[2022-10-02 16:54:41 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:42 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0209 (0.0209)	loss 0.0410 (0.0410)	grad_norm 0.0265 (0.0265)	mem 458MB
[2022-10-02 16:54:42 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:42 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0100 (0.0100)	loss 0.0369 (0.0369)	grad_norm 0.0414 (0.0414)	mem 458MB
[2022-10-02 16:54:42 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:42 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0214 (0.0214)	loss 0.0322 (0.0322)	grad_norm 0.0433 (0.0433)	mem 458MB
[2022-10-02 16:54:42 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:42 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0118 (0.0118)	loss 0.0429 (0.0429)	grad_norm 0.0315 (0.0315)	mem 458MB
[2022-10-02 16:54:42 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:42 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0226 (0.0226)	loss 0.0316 (0.0316)	grad_norm 0.0331 (0.0331)	mem 458MB
[2022-10-02 16:54:42 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:42 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0196 (0.0196)	loss 0.0345 (0.0345)	grad_norm 0.0305 (0.0305)	mem 458MB
[2022-10-02 16:54:43 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:43 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0193 (0.0193)	loss 0.0284 (0.0284)	grad_norm 0.0270 (0.0270)	mem 458MB
[2022-10-02 16:54:43 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:43 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0148 (0.0148)	loss 0.0280 (0.0280)	grad_norm 0.0403 (0.0403)	mem 458MB
[2022-10-02 16:54:43 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:43 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0224 (0.0224)	loss 0.0329 (0.0329)	grad_norm 0.0625 (0.0625)	mem 458MB
[2022-10-02 16:54:43 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:43 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0213 (0.0213)	loss 0.0426 (0.0426)	grad_norm 0.0427 (0.0427)	mem 458MB
[2022-10-02 16:54:43 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:43 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0207 (0.0207)	loss 0.0302 (0.0302)	grad_norm 0.0296 (0.0296)	mem 458MB
[2022-10-02 16:54:43 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:44 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0226 (0.0226)	loss 0.0239 (0.0239)	grad_norm 0.0345 (0.0345)	mem 458MB
[2022-10-02 16:54:44 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:44 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0216 (0.0216)	loss 0.0334 (0.0334)	grad_norm 0.0632 (0.0632)	mem 458MB
[2022-10-02 16:54:44 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:44 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0223 (0.0223)	loss 0.0337 (0.0337)	grad_norm 0.0274 (0.0274)	mem 458MB
[2022-10-02 16:54:44 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:44 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0158 (0.0158)	loss 0.0274 (0.0274)	grad_norm 0.0321 (0.0321)	mem 458MB
[2022-10-02 16:54:44 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:44 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0234 (0.0234)	loss 0.0361 (0.0361)	grad_norm 0.0405 (0.0405)	mem 458MB
[2022-10-02 16:54:44 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:45 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0190 (0.0190)	loss 0.0329 (0.0329)	grad_norm 0.0497 (0.0497)	mem 458MB
[2022-10-02 16:54:45 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:45 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0278 (0.0278)	loss 0.0335 (0.0335)	grad_norm 0.0404 (0.0404)	mem 458MB
[2022-10-02 16:54:45 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:45 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0184 (0.0184)	loss 0.0312 (0.0312)	grad_norm 0.0315 (0.0315)	mem 458MB
[2022-10-02 16:54:45 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:45 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0102 (0.0102)	loss 0.0336 (0.0336)	grad_norm 0.0296 (0.0296)	mem 458MB
[2022-10-02 16:54:45 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:45 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0099 (0.0099)	loss 0.0312 (0.0312)	grad_norm 0.0337 (0.0337)	mem 458MB
[2022-10-02 16:54:45 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:45 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0214 (0.0214)	loss 0.0411 (0.0411)	grad_norm 0.0484 (0.0484)	mem 458MB
[2022-10-02 16:54:45 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:45 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0215 (0.0215)	loss 0.0271 (0.0271)	grad_norm 0.0353 (0.0353)	mem 458MB
[2022-10-02 16:54:45 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:46 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0217 (0.0217)	loss 0.0371 (0.0371)	grad_norm 0.0511 (0.0511)	mem 458MB
[2022-10-02 16:54:46 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:46 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0188 (0.0188)	loss 0.0346 (0.0346)	grad_norm 0.0333 (0.0333)	mem 458MB
[2022-10-02 16:54:46 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:46 demo] (houston_program2.py 243): INFO Train: [4/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0201 (0.0201)	loss 0.0286 (0.0286)	grad_norm 0.0278 (0.0278)	mem 458MB
[2022-10-02 16:54:46 demo] (houston_program2.py 252): INFO EPOCH 4 training takes 0:00:00
[2022-10-02 16:54:46 demo] (houston_program2.py 333): INFO Train Ep: 4 	Loss1: 0.490407	Loss2: 0.455644	 Dis: 12.679358 Entropy: 4.332611 
[2022-10-02 16:54:46 demo] (houston_program2.py 335): INFO time_4_epoch:7.260124683380127
[2022-10-02 16:54:47 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0116 (0.0116)	loss 0.0277 (0.0277)	grad_norm 0.0471 (0.0471)	mem 458MB
[2022-10-02 16:54:47 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:47 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0218 (0.0218)	loss 0.0361 (0.0361)	grad_norm 0.0606 (0.0606)	mem 458MB
[2022-10-02 16:54:47 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:47 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0200 (0.0200)	loss 0.0268 (0.0268)	grad_norm 0.0324 (0.0324)	mem 458MB
[2022-10-02 16:54:47 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:47 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0119 (0.0119)	loss 0.0305 (0.0305)	grad_norm 0.0351 (0.0351)	mem 458MB
[2022-10-02 16:54:47 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:47 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0209 (0.0209)	loss 0.0295 (0.0295)	grad_norm 0.0611 (0.0611)	mem 458MB
[2022-10-02 16:54:47 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:47 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0122 (0.0122)	loss 0.0240 (0.0240)	grad_norm 0.0434 (0.0434)	mem 458MB
[2022-10-02 16:54:47 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:48 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0208 (0.0208)	loss 0.0325 (0.0325)	grad_norm 0.0338 (0.0338)	mem 458MB
[2022-10-02 16:54:48 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:48 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0201 (0.0201)	loss 0.0367 (0.0367)	grad_norm 0.0389 (0.0389)	mem 458MB
[2022-10-02 16:54:48 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:48 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0222 (0.0222)	loss 0.0273 (0.0273)	grad_norm 0.0617 (0.0617)	mem 458MB
[2022-10-02 16:54:48 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:48 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0203 (0.0203)	loss 0.0343 (0.0343)	grad_norm 0.0248 (0.0248)	mem 458MB
[2022-10-02 16:54:48 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:48 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0189 (0.0189)	loss 0.0310 (0.0310)	grad_norm 0.0565 (0.0565)	mem 458MB
[2022-10-02 16:54:48 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:49 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0225 (0.0225)	loss 0.0360 (0.0360)	grad_norm 0.0369 (0.0369)	mem 458MB
[2022-10-02 16:54:49 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:49 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0222 (0.0222)	loss 0.0287 (0.0287)	grad_norm 0.0403 (0.0403)	mem 458MB
[2022-10-02 16:54:49 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:49 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0215 (0.0215)	loss 0.0314 (0.0314)	grad_norm 0.0382 (0.0382)	mem 458MB
[2022-10-02 16:54:49 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:49 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0265 (0.0265)	loss 0.0308 (0.0308)	grad_norm 0.0415 (0.0415)	mem 458MB
[2022-10-02 16:54:49 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:49 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0204 (0.0204)	loss 0.0325 (0.0325)	grad_norm 0.0545 (0.0545)	mem 458MB
[2022-10-02 16:54:49 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:49 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0099 (0.0099)	loss 0.0308 (0.0308)	grad_norm 0.0475 (0.0475)	mem 458MB
[2022-10-02 16:54:49 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:50 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0209 (0.0209)	loss 0.0250 (0.0250)	grad_norm 0.0413 (0.0413)	mem 458MB
[2022-10-02 16:54:50 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:50 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0115 (0.0115)	loss 0.0255 (0.0255)	grad_norm 0.0641 (0.0641)	mem 458MB
[2022-10-02 16:54:50 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:50 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0210 (0.0210)	loss 0.0263 (0.0263)	grad_norm 0.0551 (0.0551)	mem 458MB
[2022-10-02 16:54:50 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:50 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0132 (0.0132)	loss 0.0223 (0.0223)	grad_norm 0.0378 (0.0378)	mem 458MB
[2022-10-02 16:54:50 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:50 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0237 (0.0237)	loss 0.0345 (0.0345)	grad_norm 0.0360 (0.0360)	mem 458MB
[2022-10-02 16:54:50 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:51 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0222 (0.0222)	loss 0.0250 (0.0250)	grad_norm 0.0699 (0.0699)	mem 458MB
[2022-10-02 16:54:51 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:51 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0259 (0.0259)	loss 0.0403 (0.0403)	grad_norm 0.0465 (0.0465)	mem 458MB
[2022-10-02 16:54:51 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:51 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0206 (0.0206)	loss 0.0280 (0.0280)	grad_norm 0.0285 (0.0285)	mem 458MB
[2022-10-02 16:54:51 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:51 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0179 (0.0179)	loss 0.0304 (0.0304)	grad_norm 0.0533 (0.0533)	mem 458MB
[2022-10-02 16:54:51 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:51 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0222 (0.0222)	loss 0.0333 (0.0333)	grad_norm 0.0358 (0.0358)	mem 458MB
[2022-10-02 16:54:51 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:52 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0102 (0.0102)	loss 0.0348 (0.0348)	grad_norm 0.0342 (0.0342)	mem 458MB
[2022-10-02 16:54:52 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:52 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0204 (0.0204)	loss 0.0313 (0.0313)	grad_norm 0.0339 (0.0339)	mem 458MB
[2022-10-02 16:54:52 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:52 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0224 (0.0224)	loss 0.0231 (0.0231)	grad_norm 0.0559 (0.0559)	mem 458MB
[2022-10-02 16:54:52 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:52 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0200 (0.0200)	loss 0.0301 (0.0301)	grad_norm 0.0565 (0.0565)	mem 458MB
[2022-10-02 16:54:52 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:52 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0160 (0.0160)	loss 0.0248 (0.0248)	grad_norm 0.0481 (0.0481)	mem 458MB
[2022-10-02 16:54:52 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:52 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0225 (0.0225)	loss 0.0314 (0.0314)	grad_norm 0.0433 (0.0433)	mem 458MB
[2022-10-02 16:54:52 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:53 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0163 (0.0163)	loss 0.0295 (0.0295)	grad_norm 0.0326 (0.0326)	mem 458MB
[2022-10-02 16:54:53 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:53 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0219 (0.0219)	loss 0.0315 (0.0315)	grad_norm 0.0327 (0.0327)	mem 458MB
[2022-10-02 16:54:53 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:53 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0178 (0.0178)	loss 0.0303 (0.0303)	grad_norm 0.0415 (0.0415)	mem 458MB
[2022-10-02 16:54:53 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:53 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0223 (0.0223)	loss 0.0279 (0.0279)	grad_norm 0.0482 (0.0482)	mem 458MB
[2022-10-02 16:54:53 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:53 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0207 (0.0207)	loss 0.0302 (0.0302)	grad_norm 0.0337 (0.0337)	mem 458MB
[2022-10-02 16:54:53 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:54 demo] (houston_program2.py 243): INFO Train: [5/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0248 (0.0248)	loss 0.0333 (0.0333)	grad_norm 0.0307 (0.0307)	mem 458MB
[2022-10-02 16:54:54 demo] (houston_program2.py 252): INFO EPOCH 5 training takes 0:00:00
[2022-10-02 16:54:54 demo] (houston_program2.py 333): INFO Train Ep: 5 	Loss1: 0.458071	Loss2: 0.490601	 Dis: 12.559879 Entropy: 4.488073 
[2022-10-02 16:54:54 demo] (houston_program2.py 335): INFO time_5_epoch:7.531949520111084
[2022-10-02 16:54:54 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 16:54:54 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 16:54:54 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 16:54:54 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 16:54:54 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 16:54:54 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 16:54:54 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 16:54:54 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 16:54:54 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 16:54:54 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 16:55:00 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.358153	Loss2: 0.358496	 Dis: 9.022421 Entropy: 4.702317 
[2022-10-02 16:55:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.000487	 E_lr_2:0.000487
[2022-10-02 16:55:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 16:55:06 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.343266	Loss2: 0.353579	 Dis: 11.527296 Entropy: 4.823680 
[2022-10-02 16:55:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.000987	 E_lr_2:0.000987
[2022-10-02 16:55:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 16:55:11 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.191544	Loss2: 0.231728	 Dis: 7.742472 Entropy: 4.643992 
[2022-10-02 16:55:11 demo] (houston_program2.py 512): INFO E_lr_1: 0.001487	 E_lr_2:0.001487
[2022-10-02 16:55:11 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 16:55:17 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.205842	Loss2: 0.214094	 Dis: 7.210138 Entropy: 4.973228 
[2022-10-02 16:55:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.001987	 E_lr_2:0.001987
[2022-10-02 16:55:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 16:55:23 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.217174	Loss2: 0.240884	 Dis: 8.179283 Entropy: 4.710050 
[2022-10-02 16:55:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.002487	 E_lr_2:0.002487
[2022-10-02 16:55:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 16:55:29 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.123650	Loss2: 0.104696	 Dis: 9.921995 Entropy: 4.751387 
[2022-10-02 16:55:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.002987	 E_lr_2:0.002987
[2022-10-02 16:55:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 16:55:35 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.230777	Loss2: 0.239794	 Dis: 4.857695 Entropy: 4.613107 
[2022-10-02 16:55:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.003487	 E_lr_2:0.003487
[2022-10-02 16:55:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 16:55:41 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.666474	Loss2: 0.676096	 Dis: 11.703655 Entropy: 4.302458 
[2022-10-02 16:55:41 demo] (houston_program2.py 512): INFO E_lr_1: 0.003987	 E_lr_2:0.003987
[2022-10-02 16:55:41 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 16:55:47 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.443540	Loss2: 0.454545	 Dis: 10.158556 Entropy: 4.569768 
[2022-10-02 16:55:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.004487	 E_lr_2:0.004487
[2022-10-02 16:55:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 16:55:53 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.314243	Loss2: 0.294609	 Dis: 7.261398 Entropy: 4.630310 
[2022-10-02 16:55:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.004987	 E_lr_2:0.004987
[2022-10-02 16:55:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000050	 C_lr_2:0.000050	 C_lr_3:0.000050	 C_lr_4:0.000050
[2022-10-02 16:55:59 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.161448	Loss2: 0.176922	 Dis: 6.500633 Entropy: 5.414892 
[2022-10-02 16:55:59 demo] (houston_program2.py 512): INFO E_lr_1: 0.004853	 E_lr_2:0.004853
[2022-10-02 16:55:59 demo] (houston_program2.py 513): INFO C_lr_1: 0.000049	 C_lr_2:0.000049	 C_lr_3:0.000049	 C_lr_4:0.000049
[2022-10-02 16:56:05 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.174986	Loss2: 0.188007	 Dis: 7.448038 Entropy: 4.510747 
[2022-10-02 16:56:05 demo] (houston_program2.py 512): INFO E_lr_1: 0.004825	 E_lr_2:0.004825
[2022-10-02 16:56:05 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 16:56:11 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.119951	Loss2: 0.106531	 Dis: 9.574917 Entropy: 5.322534 
[2022-10-02 16:56:11 demo] (houston_program2.py 512): INFO E_lr_1: 0.004795	 E_lr_2:0.004795
[2022-10-02 16:56:11 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 16:56:17 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.131192	Loss2: 0.139924	 Dis: 8.014402 Entropy: 4.920006 
[2022-10-02 16:56:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.004763	 E_lr_2:0.004763
[2022-10-02 16:56:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 16:56:22 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.304379	Loss2: 0.309313	 Dis: 6.248892 Entropy: 4.722927 
[2022-10-02 16:56:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.004728	 E_lr_2:0.004728
[2022-10-02 16:56:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 16:56:28 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.329274	Loss2: 0.327533	 Dis: 5.597277 Entropy: 5.099328 
[2022-10-02 16:56:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.004692	 E_lr_2:0.004692
[2022-10-02 16:56:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 16:56:34 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.496291	Loss2: 0.509276	 Dis: 8.581348 Entropy: 4.199288 
[2022-10-02 16:56:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.004653	 E_lr_2:0.004653
[2022-10-02 16:56:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 16:56:40 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.102321	Loss2: 0.087938	 Dis: 6.348480 Entropy: 5.167076 
[2022-10-02 16:56:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.004612	 E_lr_2:0.004612
[2022-10-02 16:56:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 16:56:46 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.302603	Loss2: 0.216414	 Dis: 7.586966 Entropy: 5.409294 
[2022-10-02 16:56:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.004569	 E_lr_2:0.004569
[2022-10-02 16:56:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 16:56:52 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.206474	Loss2: 0.200829	 Dis: 4.994453 Entropy: 6.426479 
[2022-10-02 16:56:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.004524	 E_lr_2:0.004524
[2022-10-02 16:56:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 16:56:58 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.081030	Loss2: 0.080461	 Dis: 5.895491 Entropy: 4.438028 
[2022-10-02 16:56:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.004477	 E_lr_2:0.004477
[2022-10-02 16:56:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 16:57:04 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.715745	Loss2: 0.679650	 Dis: 8.567698 Entropy: 4.592790 
[2022-10-02 16:57:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.004428	 E_lr_2:0.004428
[2022-10-02 16:57:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 16:57:10 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.216475	Loss2: 0.203476	 Dis: 9.103277 Entropy: 4.638561 
[2022-10-02 16:57:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.004377	 E_lr_2:0.004377
[2022-10-02 16:57:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 16:57:16 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.370887	Loss2: 0.383639	 Dis: 8.284601 Entropy: 5.178390 
[2022-10-02 16:57:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.004324	 E_lr_2:0.004324
[2022-10-02 16:57:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 16:57:22 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.133364	Loss2: 0.138334	 Dis: 7.953739 Entropy: 4.348571 
[2022-10-02 16:57:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.004269	 E_lr_2:0.004269
[2022-10-02 16:57:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 16:57:29 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.250084	Loss2: 0.275919	 Dis: 8.756405 Entropy: 4.495098 
[2022-10-02 16:57:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.004213	 E_lr_2:0.004213
[2022-10-02 16:57:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 16:57:34 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.156643	Loss2: 0.146714	 Dis: 6.098663 Entropy: 4.160561 
[2022-10-02 16:57:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.004155	 E_lr_2:0.004155
[2022-10-02 16:57:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 16:57:40 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.152777	Loss2: 0.151005	 Dis: 4.425892 Entropy: 5.596433 
[2022-10-02 16:57:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.004095	 E_lr_2:0.004095
[2022-10-02 16:57:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000041	 C_lr_2:0.000041	 C_lr_3:0.000041	 C_lr_4:0.000041
[2022-10-02 16:57:46 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.072353	Loss2: 0.072697	 Dis: 4.321125 Entropy: 4.997449 
[2022-10-02 16:57:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.004034	 E_lr_2:0.004034
[2022-10-02 16:57:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 16:57:52 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.055579	Loss2: 0.059386	 Dis: 5.474300 Entropy: 5.234533 
[2022-10-02 16:57:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.003971	 E_lr_2:0.003971
[2022-10-02 16:57:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 16:57:58 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.028851	Loss2: 0.026272	 Dis: 6.159616 Entropy: 5.054325 
[2022-10-02 16:57:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.003907	 E_lr_2:0.003907
[2022-10-02 16:57:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000039	 C_lr_2:0.000039	 C_lr_3:0.000039	 C_lr_4:0.000039
[2022-10-02 16:58:04 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.160495	Loss2: 0.148248	 Dis: 4.092476 Entropy: 5.506250 
[2022-10-02 16:58:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.003841	 E_lr_2:0.003841
[2022-10-02 16:58:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 16:58:10 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.290735	Loss2: 0.335518	 Dis: 4.480476 Entropy: 5.818361 
[2022-10-02 16:58:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.003774	 E_lr_2:0.003774
[2022-10-02 16:58:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 16:58:17 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.141184	Loss2: 0.134737	 Dis: 4.176781 Entropy: 4.494098 
[2022-10-02 16:58:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.003706	 E_lr_2:0.003706
[2022-10-02 16:58:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000037	 C_lr_2:0.000037	 C_lr_3:0.000037	 C_lr_4:0.000037
[2022-10-02 16:58:23 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.051916	Loss2: 0.050559	 Dis: 5.777699 Entropy: 4.451502 
[2022-10-02 16:58:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.003637	 E_lr_2:0.003637
[2022-10-02 16:58:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 16:58:28 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.044623	Loss2: 0.059733	 Dis: 4.245342 Entropy: 5.671548 
[2022-10-02 16:58:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.003566	 E_lr_2:0.003566
[2022-10-02 16:58:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 16:58:35 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.052769	Loss2: 0.056825	 Dis: 4.111712 Entropy: 5.735900 
[2022-10-02 16:58:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.003495	 E_lr_2:0.003495
[2022-10-02 16:58:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 16:58:41 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.142941	Loss2: 0.153225	 Dis: 6.005960 Entropy: 5.544834 
[2022-10-02 16:58:41 demo] (houston_program2.py 512): INFO E_lr_1: 0.003422	 E_lr_2:0.003422
[2022-10-02 16:58:41 demo] (houston_program2.py 513): INFO C_lr_1: 0.000034	 C_lr_2:0.000034	 C_lr_3:0.000034	 C_lr_4:0.000034
[2022-10-02 16:58:47 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.055409	Loss2: 0.068840	 Dis: 4.744034 Entropy: 4.970837 
[2022-10-02 16:58:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.003349	 E_lr_2:0.003349
[2022-10-02 16:58:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 16:58:53 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.091450	Loss2: 0.088707	 Dis: 4.496122 Entropy: 5.348881 
[2022-10-02 16:58:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.003274	 E_lr_2:0.003274
[2022-10-02 16:58:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 16:59:00 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.105063	Loss2: 0.109519	 Dis: 2.966576 Entropy: 4.804012 
[2022-10-02 16:59:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.003199	 E_lr_2:0.003199
[2022-10-02 16:59:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000032	 C_lr_2:0.000032	 C_lr_3:0.000032	 C_lr_4:0.000032
[2022-10-02 16:59:06 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.119131	Loss2: 0.121597	 Dis: 4.210123 Entropy: 4.706017 
[2022-10-02 16:59:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.003124	 E_lr_2:0.003124
[2022-10-02 16:59:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000031	 C_lr_2:0.000031	 C_lr_3:0.000031	 C_lr_4:0.000031
[2022-10-02 16:59:11 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.268442	Loss2: 0.230418	 Dis: 3.733128 Entropy: 5.398460 
[2022-10-02 16:59:11 demo] (houston_program2.py 512): INFO E_lr_1: 0.003047	 E_lr_2:0.003047
[2022-10-02 16:59:11 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 16:59:17 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.133489	Loss2: 0.146477	 Dis: 4.520607 Entropy: 4.426046 
[2022-10-02 16:59:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.002970	 E_lr_2:0.002970
[2022-10-02 16:59:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 16:59:23 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.045176	Loss2: 0.042870	 Dis: 4.262651 Entropy: 4.395730 
[2022-10-02 16:59:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.002893	 E_lr_2:0.002893
[2022-10-02 16:59:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000029	 C_lr_2:0.000029	 C_lr_3:0.000029	 C_lr_4:0.000029
[2022-10-02 16:59:29 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.235060	Loss2: 0.198513	 Dis: 3.658848 Entropy: 4.847339 
[2022-10-02 16:59:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.002815	 E_lr_2:0.002815
[2022-10-02 16:59:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000028	 C_lr_2:0.000028	 C_lr_3:0.000028	 C_lr_4:0.000028
[2022-10-02 16:59:35 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.030102	Loss2: 0.038728	 Dis: 4.663813 Entropy: 4.206600 
[2022-10-02 16:59:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.002737	 E_lr_2:0.002737
[2022-10-02 16:59:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 16:59:41 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.080560	Loss2: 0.080425	 Dis: 4.750271 Entropy: 5.459211 
[2022-10-02 16:59:41 demo] (houston_program2.py 512): INFO E_lr_1: 0.002659	 E_lr_2:0.002659
[2022-10-02 16:59:41 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 16:59:46 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.127216	Loss2: 0.131557	 Dis: 2.275795 Entropy: 5.584085 
[2022-10-02 16:59:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.002581	 E_lr_2:0.002581
[2022-10-02 16:59:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000026	 C_lr_2:0.000026	 C_lr_3:0.000026	 C_lr_4:0.000026
[2022-10-02 16:59:52 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.018939	Loss2: 0.020364	 Dis: 3.197311 Entropy: 5.520436 
[2022-10-02 16:59:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.002502	 E_lr_2:0.002502
[2022-10-02 16:59:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 16:59:58 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.209191	Loss2: 0.197925	 Dis: 3.969553 Entropy: 4.323302 
[2022-10-02 16:59:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.002423	 E_lr_2:0.002423
[2022-10-02 16:59:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000024	 C_lr_2:0.000024	 C_lr_3:0.000024	 C_lr_4:0.000024
[2022-10-02 17:00:04 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.161692	Loss2: 0.151491	 Dis: 4.678240 Entropy: 4.459069 
[2022-10-02 17:00:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.002345	 E_lr_2:0.002345
[2022-10-02 17:00:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 17:00:10 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.006159	Loss2: 0.005228	 Dis: 2.989162 Entropy: 5.455710 
[2022-10-02 17:00:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.002267	 E_lr_2:0.002267
[2022-10-02 17:00:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 17:00:16 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.008757	Loss2: 0.010381	 Dis: 4.513203 Entropy: 4.320937 
[2022-10-02 17:00:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.002189	 E_lr_2:0.002189
[2022-10-02 17:00:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000022	 C_lr_2:0.000022	 C_lr_3:0.000022	 C_lr_4:0.000022
[2022-10-02 17:00:22 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.010346	Loss2: 0.013102	 Dis: 3.927343 Entropy: 5.035275 
[2022-10-02 17:00:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.002111	 E_lr_2:0.002111
[2022-10-02 17:00:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000021	 C_lr_2:0.000021	 C_lr_3:0.000021	 C_lr_4:0.000021
[2022-10-02 17:00:27 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.016786	Loss2: 0.014163	 Dis: 3.636101 Entropy: 5.365135 
[2022-10-02 17:00:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.002034	 E_lr_2:0.002034
[2022-10-02 17:00:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 17:00:34 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.030883	Loss2: 0.027525	 Dis: 3.631811 Entropy: 4.601537 
[2022-10-02 17:00:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.001957	 E_lr_2:0.001957
[2022-10-02 17:00:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 17:00:39 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.061592	Loss2: 0.064398	 Dis: 2.548538 Entropy: 5.366900 
[2022-10-02 17:00:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.001880	 E_lr_2:0.001880
[2022-10-02 17:00:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000019	 C_lr_2:0.000019	 C_lr_3:0.000019	 C_lr_4:0.000019
[2022-10-02 17:00:45 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.151177	Loss2: 0.129712	 Dis: 3.055592 Entropy: 4.954726 
[2022-10-02 17:00:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.001804	 E_lr_2:0.001804
[2022-10-02 17:00:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000018	 C_lr_2:0.000018	 C_lr_3:0.000018	 C_lr_4:0.000018
[2022-10-02 17:00:51 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.051445	Loss2: 0.050281	 Dis: 3.443794 Entropy: 4.958750 
[2022-10-02 17:00:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.001729	 E_lr_2:0.001729
[2022-10-02 17:00:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 17:00:57 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.014918	Loss2: 0.021387	 Dis: 2.321482 Entropy: 4.238311 
[2022-10-02 17:00:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.001655	 E_lr_2:0.001655
[2022-10-02 17:00:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 17:01:03 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.104943	Loss2: 0.098306	 Dis: 2.316839 Entropy: 4.853915 
[2022-10-02 17:01:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.001582	 E_lr_2:0.001582
[2022-10-02 17:01:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000016	 C_lr_2:0.000016	 C_lr_3:0.000016	 C_lr_4:0.000016
[2022-10-02 17:01:09 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.005032	Loss2: 0.005171	 Dis: 4.778219 Entropy: 4.225533 
[2022-10-02 17:01:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.001509	 E_lr_2:0.001509
[2022-10-02 17:01:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 17:01:16 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.256575	Loss2: 0.234893	 Dis: 2.994061 Entropy: 4.848941 
[2022-10-02 17:01:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.001437	 E_lr_2:0.001437
[2022-10-02 17:01:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 17:01:22 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.008272	Loss2: 0.009150	 Dis: 2.359005 Entropy: 5.515901 
[2022-10-02 17:01:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.001367	 E_lr_2:0.001367
[2022-10-02 17:01:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 17:01:28 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.013738	Loss2: 0.012108	 Dis: 3.739613 Entropy: 5.038821 
[2022-10-02 17:01:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.001297	 E_lr_2:0.001297
[2022-10-02 17:01:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000013	 C_lr_2:0.000013	 C_lr_3:0.000013	 C_lr_4:0.000013
[2022-10-02 17:01:34 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.016171	Loss2: 0.015907	 Dis: 2.228390 Entropy: 4.487579 
[2022-10-02 17:01:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.001229	 E_lr_2:0.001229
[2022-10-02 17:01:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 17:01:40 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.003539	Loss2: 0.004242	 Dis: 1.989780 Entropy: 4.544927 
[2022-10-02 17:01:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.001162	 E_lr_2:0.001162
[2022-10-02 17:01:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 17:01:46 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.153791	Loss2: 0.179041	 Dis: 3.604748 Entropy: 4.683970 
[2022-10-02 17:01:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.001096	 E_lr_2:0.001096
[2022-10-02 17:01:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000011	 C_lr_2:0.000011	 C_lr_3:0.000011	 C_lr_4:0.000011
[2022-10-02 17:01:52 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.008993	Loss2: 0.008731	 Dis: 3.024216 Entropy: 4.706179 
[2022-10-02 17:01:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.001032	 E_lr_2:0.001032
[2022-10-02 17:01:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 17:01:58 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.169998	Loss2: 0.186921	 Dis: 2.390013 Entropy: 4.773970 
[2022-10-02 17:01:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.000969	 E_lr_2:0.000969
[2022-10-02 17:01:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 17:02:04 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.018059	Loss2: 0.022515	 Dis: 2.064955 Entropy: 5.487055 
[2022-10-02 17:02:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.000908	 E_lr_2:0.000908
[2022-10-02 17:02:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000009	 C_lr_2:0.000009	 C_lr_3:0.000009	 C_lr_4:0.000009
[2022-10-02 17:02:10 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.011844	Loss2: 0.015544	 Dis: 1.146366 Entropy: 4.566013 
[2022-10-02 17:02:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.000848	 E_lr_2:0.000848
[2022-10-02 17:02:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 17:02:16 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.009490	Loss2: 0.014158	 Dis: 2.474146 Entropy: 5.189519 
[2022-10-02 17:02:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.000790	 E_lr_2:0.000790
[2022-10-02 17:02:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 17:02:22 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.009935	Loss2: 0.008569	 Dis: 1.831295 Entropy: 4.946218 
[2022-10-02 17:02:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.000734	 E_lr_2:0.000734
[2022-10-02 17:02:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 17:02:28 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.002153	Loss2: 0.002162	 Dis: 4.695974 Entropy: 4.231993 
[2022-10-02 17:02:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.000679	 E_lr_2:0.000679
[2022-10-02 17:02:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 17:02:34 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.010495	Loss2: 0.008898	 Dis: 2.982021 Entropy: 5.069954 
[2022-10-02 17:02:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.000626	 E_lr_2:0.000626
[2022-10-02 17:02:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 17:02:40 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.019829	Loss2: 0.028783	 Dis: 1.706360 Entropy: 4.854089 
[2022-10-02 17:02:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.000575	 E_lr_2:0.000575
[2022-10-02 17:02:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 17:02:46 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.015892	Loss2: 0.016459	 Dis: 2.705986 Entropy: 4.477590 
[2022-10-02 17:02:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.000526	 E_lr_2:0.000526
[2022-10-02 17:02:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 17:02:52 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.014562	Loss2: 0.013898	 Dis: 2.089428 Entropy: 4.200371 
[2022-10-02 17:02:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.000479	 E_lr_2:0.000479
[2022-10-02 17:02:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 17:02:58 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.003946	Loss2: 0.002413	 Dis: 1.972946 Entropy: 6.109235 
[2022-10-02 17:02:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.000433	 E_lr_2:0.000433
[2022-10-02 17:02:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 17:03:04 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.026208	Loss2: 0.029868	 Dis: 4.230164 Entropy: 4.819595 
[2022-10-02 17:03:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.000390	 E_lr_2:0.000390
[2022-10-02 17:03:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 17:03:10 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.047660	Loss2: 0.041297	 Dis: 2.806322 Entropy: 5.638407 
[2022-10-02 17:03:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.000349	 E_lr_2:0.000349
[2022-10-02 17:03:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 17:03:15 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.003384	Loss2: 0.003061	 Dis: 0.572866 Entropy: 4.322258 
[2022-10-02 17:03:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.000310	 E_lr_2:0.000310
[2022-10-02 17:03:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 17:03:22 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.009031	Loss2: 0.005493	 Dis: 1.990170 Entropy: 4.428704 
[2022-10-02 17:03:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.000273	 E_lr_2:0.000273
[2022-10-02 17:03:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 17:03:27 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.003356	Loss2: 0.002419	 Dis: 0.890438 Entropy: 5.540633 
[2022-10-02 17:03:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.000239	 E_lr_2:0.000239
[2022-10-02 17:03:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 17:03:32 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.004231	Loss2: 0.004379	 Dis: 1.155376 Entropy: 4.582788 
[2022-10-02 17:03:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.000206	 E_lr_2:0.000206
[2022-10-02 17:03:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 17:03:38 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.016319	Loss2: 0.026974	 Dis: 2.900108 Entropy: 4.779704 
[2022-10-02 17:03:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.000176	 E_lr_2:0.000176
[2022-10-02 17:03:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 17:03:44 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.003270	Loss2: 0.003572	 Dis: 2.171698 Entropy: 4.936685 
[2022-10-02 17:03:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.000148	 E_lr_2:0.000148
[2022-10-02 17:03:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:03:50 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.002801	Loss2: 0.002635	 Dis: 1.721569 Entropy: 4.492880 
[2022-10-02 17:03:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.000123	 E_lr_2:0.000123
[2022-10-02 17:03:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:03:57 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.001677	Loss2: 0.001515	 Dis: 3.464764 Entropy: 5.071775 
[2022-10-02 17:03:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.000100	 E_lr_2:0.000100
[2022-10-02 17:03:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:04:03 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.003796	Loss2: 0.007462	 Dis: 3.110123 Entropy: 4.200731 
[2022-10-02 17:04:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.000079	 E_lr_2:0.000079
[2022-10-02 17:04:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:04:09 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.013962	Loss2: 0.014343	 Dis: 2.836332 Entropy: 4.495869 
[2022-10-02 17:04:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.000061	 E_lr_2:0.000061
[2022-10-02 17:04:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:04:14 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.011460	Loss2: 0.010971	 Dis: 2.617668 Entropy: 4.578643 
[2022-10-02 17:04:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.000045	 E_lr_2:0.000045
[2022-10-02 17:04:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:04:20 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.002684	Loss2: 0.002363	 Dis: 2.124041 Entropy: 4.286585 
[2022-10-02 17:04:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.000031	 E_lr_2:0.000031
[2022-10-02 17:04:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:04:26 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.003883	Loss2: 0.002485	 Dis: 2.356441 Entropy: 4.728758 
[2022-10-02 17:04:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.000020	 E_lr_2:0.000020
[2022-10-02 17:04:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:04:32 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.015742	Loss2: 0.024517	 Dis: 2.560204 Entropy: 5.036194 
[2022-10-02 17:04:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.000011	 E_lr_2:0.000011
[2022-10-02 17:04:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:04:38 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.006624	Loss2: 0.005588	 Dis: 2.429237 Entropy: 4.212242 
[2022-10-02 17:04:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.000005	 E_lr_2:0.000005
[2022-10-02 17:04:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:04:43 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.003474	Loss2: 0.004128	 Dis: 3.719009 Entropy: 4.272919 
[2022-10-02 17:04:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.000001	 E_lr_2:0.000001
[2022-10-02 17:04:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:04:49 demo] (houston_program2.py 504): INFO Train Ep: 5 	Loss1: 0.006824	Loss2: 0.016462	 Dis: 2.096634 Entropy: 4.501331 
[2022-10-02 17:04:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.000000	 E_lr_2:0.000000
[2022-10-02 17:04:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:04:49 demo] (houston_program2.py 515): INFO time_5_epoch:594.9593682289124
[2022-10-02 17:04:57 demo] (houston_program2.py 673): INFO 	val_Accuracy: 30523/53200 (57.37%)	
[2022-10-02 17:04:57 demo] (utils.py 47): INFO outputs/demo/finetune_100/ckpt_epoch_5.pth saving......
[2022-10-02 17:04:57 demo] (utils.py 49): INFO outputs/demo/finetune_100/ckpt_epoch_5.pth saved !!!
[2022-10-02 17:04:57 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0114 (0.0114)	loss 0.0368 (0.0368)	grad_norm 0.0283 (0.0283)	mem 458MB
[2022-10-02 17:04:57 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:04:57 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0222 (0.0222)	loss 0.0343 (0.0343)	grad_norm 0.0303 (0.0303)	mem 459MB
[2022-10-02 17:04:57 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:04:58 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0192 (0.0192)	loss 0.0313 (0.0313)	grad_norm 0.0517 (0.0517)	mem 459MB
[2022-10-02 17:04:58 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:04:58 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0194 (0.0194)	loss 0.0209 (0.0209)	grad_norm 0.0321 (0.0321)	mem 459MB
[2022-10-02 17:04:58 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:04:58 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0204 (0.0204)	loss 0.0232 (0.0232)	grad_norm 0.0365 (0.0365)	mem 459MB
[2022-10-02 17:04:58 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:04:58 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0125 (0.0125)	loss 0.0351 (0.0351)	grad_norm 0.0484 (0.0484)	mem 459MB
[2022-10-02 17:04:58 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:04:58 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0220 (0.0220)	loss 0.0299 (0.0299)	grad_norm 0.0384 (0.0384)	mem 459MB
[2022-10-02 17:04:58 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:04:59 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0123 (0.0123)	loss 0.0235 (0.0235)	grad_norm 0.0341 (0.0341)	mem 459MB
[2022-10-02 17:04:59 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:04:59 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0219 (0.0219)	loss 0.0288 (0.0288)	grad_norm 0.0597 (0.0597)	mem 459MB
[2022-10-02 17:04:59 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:04:59 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0119 (0.0119)	loss 0.0314 (0.0314)	grad_norm 0.0398 (0.0398)	mem 459MB
[2022-10-02 17:04:59 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:04:59 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0212 (0.0212)	loss 0.0281 (0.0281)	grad_norm 0.0326 (0.0326)	mem 459MB
[2022-10-02 17:04:59 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:04:59 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0183 (0.0183)	loss 0.0345 (0.0345)	grad_norm 0.0456 (0.0456)	mem 459MB
[2022-10-02 17:04:59 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:04:59 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0217 (0.0217)	loss 0.0221 (0.0221)	grad_norm 0.0410 (0.0410)	mem 459MB
[2022-10-02 17:04:59 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:00 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0229 (0.0229)	loss 0.0301 (0.0301)	grad_norm 0.0272 (0.0272)	mem 459MB
[2022-10-02 17:05:00 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:00 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0206 (0.0206)	loss 0.0308 (0.0308)	grad_norm 0.0506 (0.0506)	mem 459MB
[2022-10-02 17:05:00 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:00 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0209 (0.0209)	loss 0.0286 (0.0286)	grad_norm 0.0343 (0.0343)	mem 459MB
[2022-10-02 17:05:00 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:00 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0234 (0.0234)	loss 0.0242 (0.0242)	grad_norm 0.0381 (0.0381)	mem 459MB
[2022-10-02 17:05:00 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:00 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0259 (0.0259)	loss 0.0322 (0.0322)	grad_norm 0.0289 (0.0289)	mem 459MB
[2022-10-02 17:05:00 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:01 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0156 (0.0156)	loss 0.0239 (0.0239)	grad_norm 0.0425 (0.0425)	mem 459MB
[2022-10-02 17:05:01 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:01 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0218 (0.0218)	loss 0.0224 (0.0224)	grad_norm 0.0373 (0.0373)	mem 459MB
[2022-10-02 17:05:01 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:01 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0131 (0.0131)	loss 0.0244 (0.0244)	grad_norm 0.0314 (0.0314)	mem 459MB
[2022-10-02 17:05:01 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:01 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0225 (0.0225)	loss 0.0250 (0.0250)	grad_norm 0.0456 (0.0456)	mem 459MB
[2022-10-02 17:05:01 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:01 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0119 (0.0119)	loss 0.0271 (0.0271)	grad_norm 0.0403 (0.0403)	mem 459MB
[2022-10-02 17:05:01 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:01 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0217 (0.0217)	loss 0.0305 (0.0305)	grad_norm 0.0486 (0.0486)	mem 459MB
[2022-10-02 17:05:01 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:02 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0168 (0.0168)	loss 0.0259 (0.0259)	grad_norm 0.0346 (0.0346)	mem 459MB
[2022-10-02 17:05:02 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:02 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0271 (0.0271)	loss 0.0330 (0.0330)	grad_norm 0.0501 (0.0501)	mem 459MB
[2022-10-02 17:05:02 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:02 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0192 (0.0192)	loss 0.0225 (0.0225)	grad_norm 0.0534 (0.0534)	mem 459MB
[2022-10-02 17:05:02 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:02 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0267 (0.0267)	loss 0.0281 (0.0281)	grad_norm 0.0347 (0.0347)	mem 459MB
[2022-10-02 17:05:02 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:02 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0110 (0.0110)	loss 0.0223 (0.0223)	grad_norm 0.0616 (0.0616)	mem 459MB
[2022-10-02 17:05:02 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:02 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0116 (0.0116)	loss 0.0298 (0.0298)	grad_norm 0.0498 (0.0498)	mem 459MB
[2022-10-02 17:05:02 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:03 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0126 (0.0126)	loss 0.0263 (0.0263)	grad_norm 0.0434 (0.0434)	mem 459MB
[2022-10-02 17:05:03 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:03 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0127 (0.0127)	loss 0.0230 (0.0230)	grad_norm 0.0607 (0.0607)	mem 459MB
[2022-10-02 17:05:03 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:03 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0115 (0.0115)	loss 0.0228 (0.0228)	grad_norm 0.0708 (0.0708)	mem 459MB
[2022-10-02 17:05:03 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:03 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0113 (0.0113)	loss 0.0294 (0.0294)	grad_norm 0.0513 (0.0513)	mem 459MB
[2022-10-02 17:05:03 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:03 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0122 (0.0122)	loss 0.0198 (0.0198)	grad_norm 0.0488 (0.0488)	mem 459MB
[2022-10-02 17:05:03 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:03 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0113 (0.0113)	loss 0.0235 (0.0235)	grad_norm 0.0415 (0.0415)	mem 459MB
[2022-10-02 17:05:03 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:03 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0131 (0.0131)	loss 0.0237 (0.0237)	grad_norm 0.0640 (0.0640)	mem 459MB
[2022-10-02 17:05:03 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:03 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0256 (0.0256)	loss 0.0228 (0.0228)	grad_norm 0.0516 (0.0516)	mem 459MB
[2022-10-02 17:05:03 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:04 demo] (houston_program2.py 243): INFO Train: [6/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0221 (0.0221)	loss 0.0212 (0.0212)	grad_norm 0.0553 (0.0553)	mem 459MB
[2022-10-02 17:05:04 demo] (houston_program2.py 252): INFO EPOCH 6 training takes 0:00:00
[2022-10-02 17:05:04 demo] (houston_program2.py 333): INFO Train Ep: 6 	Loss1: 0.458879	Loss2: 0.362637	 Dis: 13.313437 Entropy: 4.320370 
[2022-10-02 17:05:04 demo] (houston_program2.py 335): INFO time_6_epoch:6.799268484115601
[2022-10-02 17:05:04 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0132 (0.0132)	loss 0.0229 (0.0229)	grad_norm 0.0594 (0.0594)	mem 459MB
[2022-10-02 17:05:04 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:04 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0174 (0.0174)	loss 0.0251 (0.0251)	grad_norm 0.0645 (0.0645)	mem 459MB
[2022-10-02 17:05:04 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:04 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0199 (0.0199)	loss 0.0232 (0.0232)	grad_norm 0.0583 (0.0583)	mem 459MB
[2022-10-02 17:05:05 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:05 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0188 (0.0188)	loss 0.0203 (0.0203)	grad_norm 0.0508 (0.0508)	mem 459MB
[2022-10-02 17:05:05 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:05 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0215 (0.0215)	loss 0.0292 (0.0292)	grad_norm 0.0551 (0.0551)	mem 459MB
[2022-10-02 17:05:05 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:05 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0216 (0.0216)	loss 0.0208 (0.0208)	grad_norm 0.0456 (0.0456)	mem 459MB
[2022-10-02 17:05:05 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:05 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0218 (0.0218)	loss 0.0340 (0.0340)	grad_norm 0.0338 (0.0338)	mem 459MB
[2022-10-02 17:05:05 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:05 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0213 (0.0213)	loss 0.0250 (0.0250)	grad_norm 0.0532 (0.0532)	mem 459MB
[2022-10-02 17:05:05 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:06 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0137 (0.0137)	loss 0.0245 (0.0245)	grad_norm 0.0503 (0.0503)	mem 459MB
[2022-10-02 17:05:06 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:06 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0222 (0.0222)	loss 0.0186 (0.0186)	grad_norm 0.0401 (0.0401)	mem 459MB
[2022-10-02 17:05:06 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:06 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0126 (0.0126)	loss 0.0239 (0.0239)	grad_norm 0.0605 (0.0605)	mem 459MB
[2022-10-02 17:05:06 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:06 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0202 (0.0202)	loss 0.0207 (0.0207)	grad_norm 0.0530 (0.0530)	mem 459MB
[2022-10-02 17:05:06 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:06 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0145 (0.0145)	loss 0.0329 (0.0329)	grad_norm 0.0520 (0.0520)	mem 459MB
[2022-10-02 17:05:06 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:06 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0205 (0.0205)	loss 0.0254 (0.0254)	grad_norm 0.0664 (0.0664)	mem 459MB
[2022-10-02 17:05:06 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:07 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0214 (0.0214)	loss 0.0269 (0.0269)	grad_norm 0.0514 (0.0514)	mem 459MB
[2022-10-02 17:05:07 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:07 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0240 (0.0240)	loss 0.0258 (0.0258)	grad_norm 0.0838 (0.0838)	mem 459MB
[2022-10-02 17:05:07 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:07 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0210 (0.0210)	loss 0.0204 (0.0204)	grad_norm 0.0575 (0.0575)	mem 459MB
[2022-10-02 17:05:07 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:07 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0177 (0.0177)	loss 0.0306 (0.0306)	grad_norm 0.0490 (0.0490)	mem 459MB
[2022-10-02 17:05:07 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:07 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0217 (0.0217)	loss 0.0292 (0.0292)	grad_norm 0.0562 (0.0562)	mem 459MB
[2022-10-02 17:05:07 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:08 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0111 (0.0111)	loss 0.0283 (0.0283)	grad_norm 0.0386 (0.0386)	mem 459MB
[2022-10-02 17:05:08 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:08 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0265 (0.0265)	loss 0.0272 (0.0272)	grad_norm 0.0676 (0.0676)	mem 459MB
[2022-10-02 17:05:08 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:08 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0136 (0.0136)	loss 0.0225 (0.0225)	grad_norm 0.0770 (0.0770)	mem 459MB
[2022-10-02 17:05:08 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:08 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0210 (0.0210)	loss 0.0226 (0.0226)	grad_norm 0.0536 (0.0536)	mem 459MB
[2022-10-02 17:05:08 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:08 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0202 (0.0202)	loss 0.0281 (0.0281)	grad_norm 0.0416 (0.0416)	mem 459MB
[2022-10-02 17:05:08 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:09 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0206 (0.0206)	loss 0.0240 (0.0240)	grad_norm 0.0602 (0.0602)	mem 459MB
[2022-10-02 17:05:09 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:09 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0210 (0.0210)	loss 0.0220 (0.0220)	grad_norm 0.0468 (0.0468)	mem 459MB
[2022-10-02 17:05:09 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:09 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0201 (0.0201)	loss 0.0288 (0.0288)	grad_norm 0.0554 (0.0554)	mem 459MB
[2022-10-02 17:05:09 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:09 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0217 (0.0217)	loss 0.0192 (0.0192)	grad_norm 0.0826 (0.0826)	mem 459MB
[2022-10-02 17:05:09 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:09 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0224 (0.0224)	loss 0.0248 (0.0248)	grad_norm 0.0379 (0.0379)	mem 459MB
[2022-10-02 17:05:09 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:09 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0221 (0.0221)	loss 0.0211 (0.0211)	grad_norm 0.0528 (0.0528)	mem 459MB
[2022-10-02 17:05:09 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:10 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0237 (0.0237)	loss 0.0219 (0.0219)	grad_norm 0.0750 (0.0750)	mem 459MB
[2022-10-02 17:05:10 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:10 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0208 (0.0208)	loss 0.0188 (0.0188)	grad_norm 0.0528 (0.0528)	mem 459MB
[2022-10-02 17:05:10 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:10 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0096 (0.0096)	loss 0.0231 (0.0231)	grad_norm 0.0461 (0.0461)	mem 459MB
[2022-10-02 17:05:10 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:10 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0214 (0.0214)	loss 0.0245 (0.0245)	grad_norm 0.0739 (0.0739)	mem 459MB
[2022-10-02 17:05:10 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:10 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0128 (0.0128)	loss 0.0258 (0.0258)	grad_norm 0.0556 (0.0556)	mem 459MB
[2022-10-02 17:05:10 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:10 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0207 (0.0207)	loss 0.0240 (0.0240)	grad_norm 0.0523 (0.0523)	mem 459MB
[2022-10-02 17:05:11 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:11 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0203 (0.0203)	loss 0.0287 (0.0287)	grad_norm 0.0693 (0.0693)	mem 459MB
[2022-10-02 17:05:11 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:11 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0205 (0.0205)	loss 0.0203 (0.0203)	grad_norm 0.0589 (0.0589)	mem 459MB
[2022-10-02 17:05:11 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:11 demo] (houston_program2.py 243): INFO Train: [7/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0198 (0.0198)	loss 0.0249 (0.0249)	grad_norm 0.0542 (0.0542)	mem 459MB
[2022-10-02 17:05:11 demo] (houston_program2.py 252): INFO EPOCH 7 training takes 0:00:00
[2022-10-02 17:05:11 demo] (houston_program2.py 333): INFO Train Ep: 7 	Loss1: 0.327208	Loss2: 0.324864	 Dis: 9.581726 Entropy: 4.388420 
[2022-10-02 17:05:11 demo] (houston_program2.py 335): INFO time_7_epoch:7.548974990844727
[2022-10-02 17:05:12 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0125 (0.0125)	loss 0.0228 (0.0228)	grad_norm 0.0732 (0.0732)	mem 459MB
[2022-10-02 17:05:12 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:12 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0220 (0.0220)	loss 0.0215 (0.0215)	grad_norm 0.0435 (0.0435)	mem 459MB
[2022-10-02 17:05:12 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:12 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0185 (0.0185)	loss 0.0216 (0.0216)	grad_norm 0.0560 (0.0560)	mem 459MB
[2022-10-02 17:05:12 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:12 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0193 (0.0193)	loss 0.0272 (0.0272)	grad_norm 0.0376 (0.0376)	mem 459MB
[2022-10-02 17:05:12 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:12 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0222 (0.0222)	loss 0.0202 (0.0202)	grad_norm 0.0399 (0.0399)	mem 459MB
[2022-10-02 17:05:12 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:13 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0225 (0.0225)	loss 0.0262 (0.0262)	grad_norm 0.0548 (0.0548)	mem 459MB
[2022-10-02 17:05:13 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:13 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0211 (0.0211)	loss 0.0272 (0.0272)	grad_norm 0.0438 (0.0438)	mem 459MB
[2022-10-02 17:05:13 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:13 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0250 (0.0250)	loss 0.0267 (0.0267)	grad_norm 0.0322 (0.0322)	mem 459MB
[2022-10-02 17:05:13 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:13 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0218 (0.0218)	loss 0.0216 (0.0216)	grad_norm 0.0338 (0.0338)	mem 459MB
[2022-10-02 17:05:13 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:13 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0119 (0.0119)	loss 0.0210 (0.0210)	grad_norm 0.0385 (0.0385)	mem 459MB
[2022-10-02 17:05:13 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:14 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0224 (0.0224)	loss 0.0250 (0.0250)	grad_norm 0.0657 (0.0657)	mem 459MB
[2022-10-02 17:05:14 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:14 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0126 (0.0126)	loss 0.0245 (0.0245)	grad_norm 0.0389 (0.0389)	mem 459MB
[2022-10-02 17:05:14 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:14 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0224 (0.0224)	loss 0.0186 (0.0186)	grad_norm 0.0511 (0.0511)	mem 459MB
[2022-10-02 17:05:14 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:14 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0173 (0.0173)	loss 0.0188 (0.0188)	grad_norm 0.0469 (0.0469)	mem 459MB
[2022-10-02 17:05:14 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:14 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0222 (0.0222)	loss 0.0216 (0.0216)	grad_norm 0.0347 (0.0347)	mem 459MB
[2022-10-02 17:05:14 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:14 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0164 (0.0164)	loss 0.0264 (0.0264)	grad_norm 0.0482 (0.0482)	mem 459MB
[2022-10-02 17:05:14 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:15 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0220 (0.0220)	loss 0.0212 (0.0212)	grad_norm 0.0503 (0.0503)	mem 459MB
[2022-10-02 17:05:15 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:15 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0203 (0.0203)	loss 0.0200 (0.0200)	grad_norm 0.0345 (0.0345)	mem 459MB
[2022-10-02 17:05:15 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:15 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0221 (0.0221)	loss 0.0220 (0.0220)	grad_norm 0.0400 (0.0400)	mem 459MB
[2022-10-02 17:05:15 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:15 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0219 (0.0219)	loss 0.0235 (0.0235)	grad_norm 0.0455 (0.0455)	mem 459MB
[2022-10-02 17:05:15 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:15 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0223 (0.0223)	loss 0.0257 (0.0257)	grad_norm 0.0523 (0.0523)	mem 459MB
[2022-10-02 17:05:15 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:15 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0196 (0.0196)	loss 0.0225 (0.0225)	grad_norm 0.0362 (0.0362)	mem 459MB
[2022-10-02 17:05:16 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:16 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0141 (0.0141)	loss 0.0248 (0.0248)	grad_norm 0.0455 (0.0455)	mem 459MB
[2022-10-02 17:05:16 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:16 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0230 (0.0230)	loss 0.0207 (0.0207)	grad_norm 0.0512 (0.0512)	mem 459MB
[2022-10-02 17:05:16 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:16 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0118 (0.0118)	loss 0.0168 (0.0168)	grad_norm 0.0500 (0.0500)	mem 459MB
[2022-10-02 17:05:16 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:16 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0222 (0.0222)	loss 0.0269 (0.0269)	grad_norm 0.0626 (0.0626)	mem 459MB
[2022-10-02 17:05:16 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:16 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0160 (0.0160)	loss 0.0237 (0.0237)	grad_norm 0.0655 (0.0655)	mem 459MB
[2022-10-02 17:05:16 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:17 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0223 (0.0223)	loss 0.0236 (0.0236)	grad_norm 0.0595 (0.0595)	mem 459MB
[2022-10-02 17:05:17 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:17 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0212 (0.0212)	loss 0.0187 (0.0187)	grad_norm 0.0775 (0.0775)	mem 459MB
[2022-10-02 17:05:17 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:17 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0203 (0.0203)	loss 0.0191 (0.0191)	grad_norm 0.0379 (0.0379)	mem 459MB
[2022-10-02 17:05:17 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:17 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0206 (0.0206)	loss 0.0194 (0.0194)	grad_norm 0.0794 (0.0794)	mem 459MB
[2022-10-02 17:05:17 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:17 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0255 (0.0255)	loss 0.0234 (0.0234)	grad_norm 0.0486 (0.0486)	mem 459MB
[2022-10-02 17:05:17 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:18 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0217 (0.0217)	loss 0.0239 (0.0239)	grad_norm 0.0683 (0.0683)	mem 459MB
[2022-10-02 17:05:18 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:18 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0233 (0.0233)	loss 0.0257 (0.0257)	grad_norm 0.0694 (0.0694)	mem 459MB
[2022-10-02 17:05:18 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:18 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0246 (0.0246)	loss 0.0229 (0.0229)	grad_norm 0.0723 (0.0723)	mem 459MB
[2022-10-02 17:05:18 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:18 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0157 (0.0157)	loss 0.0173 (0.0173)	grad_norm 0.0528 (0.0528)	mem 459MB
[2022-10-02 17:05:18 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:18 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0265 (0.0265)	loss 0.0358 (0.0358)	grad_norm 0.0673 (0.0673)	mem 459MB
[2022-10-02 17:05:18 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:19 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0239 (0.0239)	loss 0.0252 (0.0252)	grad_norm 0.0671 (0.0671)	mem 459MB
[2022-10-02 17:05:19 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:19 demo] (houston_program2.py 243): INFO Train: [8/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0233 (0.0233)	loss 0.0205 (0.0205)	grad_norm 0.0454 (0.0454)	mem 459MB
[2022-10-02 17:05:19 demo] (houston_program2.py 252): INFO EPOCH 8 training takes 0:00:00
[2022-10-02 17:05:19 demo] (houston_program2.py 333): INFO Train Ep: 8 	Loss1: 0.466164	Loss2: 0.505132	 Dis: 14.148857 Entropy: 4.659954 
[2022-10-02 17:05:19 demo] (houston_program2.py 335): INFO time_8_epoch:7.751699447631836
[2022-10-02 17:05:19 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0125 (0.0125)	loss 0.0253 (0.0253)	grad_norm 0.0521 (0.0521)	mem 459MB
[2022-10-02 17:05:19 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:20 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0244 (0.0244)	loss 0.0204 (0.0204)	grad_norm 0.0398 (0.0398)	mem 459MB
[2022-10-02 17:05:20 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:20 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0218 (0.0218)	loss 0.0189 (0.0189)	grad_norm 0.0780 (0.0780)	mem 459MB
[2022-10-02 17:05:20 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:20 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0218 (0.0218)	loss 0.0232 (0.0232)	grad_norm 0.0594 (0.0594)	mem 459MB
[2022-10-02 17:05:20 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:20 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0167 (0.0167)	loss 0.0232 (0.0232)	grad_norm 0.0442 (0.0442)	mem 459MB
[2022-10-02 17:05:20 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:20 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0219 (0.0219)	loss 0.0270 (0.0270)	grad_norm 0.0799 (0.0799)	mem 459MB
[2022-10-02 17:05:20 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:21 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0219 (0.0219)	loss 0.0235 (0.0235)	grad_norm 0.0435 (0.0435)	mem 459MB
[2022-10-02 17:05:21 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:21 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0203 (0.0203)	loss 0.0233 (0.0233)	grad_norm 0.0445 (0.0445)	mem 459MB
[2022-10-02 17:05:21 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:21 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0223 (0.0223)	loss 0.0302 (0.0302)	grad_norm 0.0637 (0.0637)	mem 459MB
[2022-10-02 17:05:21 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:21 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0121 (0.0121)	loss 0.0182 (0.0182)	grad_norm 0.0590 (0.0590)	mem 459MB
[2022-10-02 17:05:21 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:21 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0239 (0.0239)	loss 0.0193 (0.0193)	grad_norm 0.0475 (0.0475)	mem 459MB
[2022-10-02 17:05:21 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:22 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0107 (0.0107)	loss 0.0224 (0.0224)	grad_norm 0.0660 (0.0660)	mem 459MB
[2022-10-02 17:05:22 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:22 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0213 (0.0213)	loss 0.0196 (0.0196)	grad_norm 0.0533 (0.0533)	mem 459MB
[2022-10-02 17:05:22 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:22 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0129 (0.0129)	loss 0.0187 (0.0187)	grad_norm 0.0507 (0.0507)	mem 459MB
[2022-10-02 17:05:22 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:22 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0220 (0.0220)	loss 0.0223 (0.0223)	grad_norm 0.0585 (0.0585)	mem 459MB
[2022-10-02 17:05:22 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:22 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0142 (0.0142)	loss 0.0220 (0.0220)	grad_norm 0.0727 (0.0727)	mem 459MB
[2022-10-02 17:05:22 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:22 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0215 (0.0215)	loss 0.0254 (0.0254)	grad_norm 0.0490 (0.0490)	mem 459MB
[2022-10-02 17:05:22 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:23 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0215 (0.0215)	loss 0.0222 (0.0222)	grad_norm 0.0653 (0.0653)	mem 459MB
[2022-10-02 17:05:23 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:23 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0202 (0.0202)	loss 0.0282 (0.0282)	grad_norm 0.0517 (0.0517)	mem 459MB
[2022-10-02 17:05:23 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:23 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0215 (0.0215)	loss 0.0228 (0.0228)	grad_norm 0.0661 (0.0661)	mem 459MB
[2022-10-02 17:05:23 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:23 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0212 (0.0212)	loss 0.0259 (0.0259)	grad_norm 0.0577 (0.0577)	mem 459MB
[2022-10-02 17:05:23 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:23 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0185 (0.0185)	loss 0.0192 (0.0192)	grad_norm 0.0547 (0.0547)	mem 459MB
[2022-10-02 17:05:23 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:24 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0271 (0.0271)	loss 0.0275 (0.0275)	grad_norm 0.0466 (0.0466)	mem 459MB
[2022-10-02 17:05:24 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:24 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0202 (0.0202)	loss 0.0191 (0.0191)	grad_norm 0.0510 (0.0510)	mem 459MB
[2022-10-02 17:05:24 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:24 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0116 (0.0116)	loss 0.0172 (0.0172)	grad_norm 0.0569 (0.0569)	mem 459MB
[2022-10-02 17:05:24 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:24 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0217 (0.0217)	loss 0.0159 (0.0159)	grad_norm 0.0470 (0.0470)	mem 459MB
[2022-10-02 17:05:24 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:24 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0098 (0.0098)	loss 0.0292 (0.0292)	grad_norm 0.0433 (0.0433)	mem 459MB
[2022-10-02 17:05:24 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:24 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0219 (0.0219)	loss 0.0221 (0.0221)	grad_norm 0.0431 (0.0431)	mem 459MB
[2022-10-02 17:05:24 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:25 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0212 (0.0212)	loss 0.0158 (0.0158)	grad_norm 0.0559 (0.0559)	mem 459MB
[2022-10-02 17:05:25 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:25 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0276 (0.0276)	loss 0.0180 (0.0180)	grad_norm 0.0394 (0.0394)	mem 459MB
[2022-10-02 17:05:25 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:25 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0230 (0.0230)	loss 0.0146 (0.0146)	grad_norm 0.0749 (0.0749)	mem 459MB
[2022-10-02 17:05:25 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:25 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0260 (0.0260)	loss 0.0191 (0.0191)	grad_norm 0.0462 (0.0462)	mem 459MB
[2022-10-02 17:05:25 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:25 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0216 (0.0216)	loss 0.0231 (0.0231)	grad_norm 0.0634 (0.0634)	mem 459MB
[2022-10-02 17:05:25 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:26 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0259 (0.0259)	loss 0.0200 (0.0200)	grad_norm 0.0713 (0.0713)	mem 459MB
[2022-10-02 17:05:26 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:26 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0212 (0.0212)	loss 0.0171 (0.0171)	grad_norm 0.0413 (0.0413)	mem 459MB
[2022-10-02 17:05:26 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:26 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0129 (0.0129)	loss 0.0202 (0.0202)	grad_norm 0.0942 (0.0942)	mem 459MB
[2022-10-02 17:05:26 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:26 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000050	time 0.0198 (0.0198)	loss 0.0199 (0.0199)	grad_norm 0.0755 (0.0755)	mem 459MB
[2022-10-02 17:05:26 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:26 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000050	time 0.0090 (0.0090)	loss 0.0194 (0.0194)	grad_norm 0.0537 (0.0537)	mem 459MB
[2022-10-02 17:05:26 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:26 demo] (houston_program2.py 243): INFO Train: [9/100][0/2]	eta 0:00:00 lr 0.000050	time 0.0143 (0.0143)	loss 0.0223 (0.0223)	grad_norm 0.0776 (0.0776)	mem 459MB
[2022-10-02 17:05:26 demo] (houston_program2.py 252): INFO EPOCH 9 training takes 0:00:00
[2022-10-02 17:05:27 demo] (houston_program2.py 333): INFO Train Ep: 9 	Loss1: 0.230648	Loss2: 0.273583	 Dis: 9.787790 Entropy: 5.072371 
[2022-10-02 17:05:27 demo] (houston_program2.py 335): INFO time_9_epoch:7.5360143184661865
[2022-10-02 17:05:27 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000050	time 0.0137 (0.0137)	loss 0.0232 (0.0232)	grad_norm 0.0460 (0.0460)	mem 459MB
[2022-10-02 17:05:27 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:27 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0252 (0.0252)	loss 0.0190 (0.0190)	grad_norm 0.0461 (0.0461)	mem 459MB
[2022-10-02 17:05:27 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:27 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0232 (0.0232)	loss 0.0198 (0.0198)	grad_norm 0.0488 (0.0488)	mem 459MB
[2022-10-02 17:05:27 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:28 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0216 (0.0216)	loss 0.0241 (0.0241)	grad_norm 0.0654 (0.0654)	mem 459MB
[2022-10-02 17:05:28 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:28 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0216 (0.0216)	loss 0.0177 (0.0177)	grad_norm 0.0573 (0.0573)	mem 459MB
[2022-10-02 17:05:28 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:28 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0163 (0.0163)	loss 0.0256 (0.0256)	grad_norm 0.0631 (0.0631)	mem 459MB
[2022-10-02 17:05:28 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:28 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0217 (0.0217)	loss 0.0183 (0.0183)	grad_norm 0.0583 (0.0583)	mem 459MB
[2022-10-02 17:05:28 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:28 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0113 (0.0113)	loss 0.0168 (0.0168)	grad_norm 0.0702 (0.0702)	mem 459MB
[2022-10-02 17:05:28 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:28 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0218 (0.0218)	loss 0.0250 (0.0250)	grad_norm 0.0742 (0.0742)	mem 459MB
[2022-10-02 17:05:28 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:29 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0118 (0.0118)	loss 0.0216 (0.0216)	grad_norm 0.0507 (0.0507)	mem 459MB
[2022-10-02 17:05:29 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:29 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0216 (0.0216)	loss 0.0221 (0.0221)	grad_norm 0.0723 (0.0723)	mem 459MB
[2022-10-02 17:05:29 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:29 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0185 (0.0185)	loss 0.0262 (0.0262)	grad_norm 0.0662 (0.0662)	mem 459MB
[2022-10-02 17:05:29 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:29 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0214 (0.0214)	loss 0.0210 (0.0210)	grad_norm 0.0425 (0.0425)	mem 459MB
[2022-10-02 17:05:29 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:29 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0202 (0.0202)	loss 0.0236 (0.0236)	grad_norm 0.0964 (0.0964)	mem 459MB
[2022-10-02 17:05:29 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:30 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0203 (0.0203)	loss 0.0258 (0.0258)	grad_norm 0.0838 (0.0838)	mem 459MB
[2022-10-02 17:05:30 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:30 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0217 (0.0217)	loss 0.0222 (0.0222)	grad_norm 0.0559 (0.0559)	mem 459MB
[2022-10-02 17:05:30 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:30 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0213 (0.0213)	loss 0.0265 (0.0265)	grad_norm 0.0701 (0.0701)	mem 459MB
[2022-10-02 17:05:30 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:30 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0203 (0.0203)	loss 0.0213 (0.0213)	grad_norm 0.0571 (0.0571)	mem 459MB
[2022-10-02 17:05:30 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:30 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0231 (0.0231)	loss 0.0264 (0.0264)	grad_norm 0.0801 (0.0801)	mem 459MB
[2022-10-02 17:05:30 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:30 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0223 (0.0223)	loss 0.0191 (0.0191)	grad_norm 0.0380 (0.0380)	mem 459MB
[2022-10-02 17:05:30 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:31 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0120 (0.0120)	loss 0.0219 (0.0219)	grad_norm 0.0691 (0.0691)	mem 459MB
[2022-10-02 17:05:31 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:31 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0229 (0.0229)	loss 0.0214 (0.0214)	grad_norm 0.0564 (0.0564)	mem 459MB
[2022-10-02 17:05:31 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:31 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0150 (0.0150)	loss 0.0158 (0.0158)	grad_norm 0.0458 (0.0458)	mem 459MB
[2022-10-02 17:05:31 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:31 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0222 (0.0222)	loss 0.0190 (0.0190)	grad_norm 0.0581 (0.0581)	mem 459MB
[2022-10-02 17:05:31 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:31 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0155 (0.0155)	loss 0.0175 (0.0175)	grad_norm 0.0773 (0.0773)	mem 459MB
[2022-10-02 17:05:31 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:32 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0223 (0.0223)	loss 0.0209 (0.0209)	grad_norm 0.0487 (0.0487)	mem 459MB
[2022-10-02 17:05:32 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:32 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0206 (0.0206)	loss 0.0211 (0.0211)	grad_norm 0.0705 (0.0705)	mem 459MB
[2022-10-02 17:05:32 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:32 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0203 (0.0203)	loss 0.0251 (0.0251)	grad_norm 0.0643 (0.0643)	mem 459MB
[2022-10-02 17:05:32 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:32 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0218 (0.0218)	loss 0.0169 (0.0169)	grad_norm 0.0470 (0.0470)	mem 459MB
[2022-10-02 17:05:32 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:32 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0221 (0.0221)	loss 0.0226 (0.0226)	grad_norm 0.0720 (0.0720)	mem 459MB
[2022-10-02 17:05:32 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:32 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0214 (0.0214)	loss 0.0221 (0.0221)	grad_norm 0.0383 (0.0383)	mem 459MB
[2022-10-02 17:05:32 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:33 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0269 (0.0269)	loss 0.0162 (0.0162)	grad_norm 0.0652 (0.0652)	mem 459MB
[2022-10-02 17:05:33 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:33 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0219 (0.0219)	loss 0.0154 (0.0154)	grad_norm 0.0718 (0.0718)	mem 459MB
[2022-10-02 17:05:33 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:33 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0122 (0.0122)	loss 0.0186 (0.0186)	grad_norm 0.0416 (0.0416)	mem 459MB
[2022-10-02 17:05:33 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:33 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0221 (0.0221)	loss 0.0253 (0.0253)	grad_norm 0.0578 (0.0578)	mem 459MB
[2022-10-02 17:05:33 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:33 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0149 (0.0149)	loss 0.0194 (0.0194)	grad_norm 0.0535 (0.0535)	mem 459MB
[2022-10-02 17:05:33 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:34 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0219 (0.0219)	loss 0.0314 (0.0314)	grad_norm 0.0568 (0.0568)	mem 459MB
[2022-10-02 17:05:34 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:34 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0142 (0.0142)	loss 0.0219 (0.0219)	grad_norm 0.0674 (0.0674)	mem 459MB
[2022-10-02 17:05:34 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:34 demo] (houston_program2.py 243): INFO Train: [10/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0200 (0.0200)	loss 0.0197 (0.0197)	grad_norm 0.0740 (0.0740)	mem 459MB
[2022-10-02 17:05:34 demo] (houston_program2.py 252): INFO EPOCH 10 training takes 0:00:00
[2022-10-02 17:05:34 demo] (houston_program2.py 333): INFO Train Ep: 10 	Loss1: 0.390997	Loss2: 0.377922	 Dis: 7.765450 Entropy: 4.875431 
[2022-10-02 17:05:34 demo] (houston_program2.py 335): INFO time_10_epoch:7.468754291534424
[2022-10-02 17:05:34 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 17:05:34 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 17:05:34 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 17:05:34 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 17:05:34 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 17:05:34 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 17:05:34 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 17:05:34 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 17:05:34 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 17:05:34 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 17:05:40 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.205309	Loss2: 0.210358	 Dis: 6.019939 Entropy: 5.075073 
[2022-10-02 17:05:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.000487	 E_lr_2:0.000487
[2022-10-02 17:05:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 17:05:46 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.097255	Loss2: 0.092010	 Dis: 5.921867 Entropy: 5.016379 
[2022-10-02 17:05:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.000987	 E_lr_2:0.000987
[2022-10-02 17:05:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 17:05:52 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.251872	Loss2: 0.251161	 Dis: 7.590458 Entropy: 5.302180 
[2022-10-02 17:05:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.001487	 E_lr_2:0.001487
[2022-10-02 17:05:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 17:05:58 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.337214	Loss2: 0.296762	 Dis: 7.263788 Entropy: 5.419054 
[2022-10-02 17:05:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.001987	 E_lr_2:0.001987
[2022-10-02 17:05:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 17:06:04 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.213268	Loss2: 0.191996	 Dis: 7.597504 Entropy: 4.802043 
[2022-10-02 17:06:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.002487	 E_lr_2:0.002487
[2022-10-02 17:06:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 17:06:10 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.120335	Loss2: 0.114832	 Dis: 6.830242 Entropy: 5.303563 
[2022-10-02 17:06:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.002987	 E_lr_2:0.002987
[2022-10-02 17:06:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 17:06:16 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.190229	Loss2: 0.212300	 Dis: 8.200750 Entropy: 5.071086 
[2022-10-02 17:06:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.003487	 E_lr_2:0.003487
[2022-10-02 17:06:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 17:06:22 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.130490	Loss2: 0.133531	 Dis: 6.278439 Entropy: 5.192681 
[2022-10-02 17:06:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.003987	 E_lr_2:0.003987
[2022-10-02 17:06:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 17:06:28 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.318857	Loss2: 0.350187	 Dis: 6.677191 Entropy: 4.900008 
[2022-10-02 17:06:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.004487	 E_lr_2:0.004487
[2022-10-02 17:06:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 17:06:34 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.207704	Loss2: 0.215440	 Dis: 8.025589 Entropy: 5.747784 
[2022-10-02 17:06:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.004987	 E_lr_2:0.004987
[2022-10-02 17:06:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000050	 C_lr_2:0.000050	 C_lr_3:0.000050	 C_lr_4:0.000050
[2022-10-02 17:06:40 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.084615	Loss2: 0.086116	 Dis: 7.456272 Entropy: 4.749098 
[2022-10-02 17:06:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.004853	 E_lr_2:0.004853
[2022-10-02 17:06:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000049	 C_lr_2:0.000049	 C_lr_3:0.000049	 C_lr_4:0.000049
[2022-10-02 17:06:46 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.168747	Loss2: 0.166426	 Dis: 7.029936 Entropy: 4.985846 
[2022-10-02 17:06:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.004825	 E_lr_2:0.004825
[2022-10-02 17:06:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 17:06:52 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.530065	Loss2: 0.468870	 Dis: 6.072571 Entropy: 5.728855 
[2022-10-02 17:06:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.004795	 E_lr_2:0.004795
[2022-10-02 17:06:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 17:06:58 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.271978	Loss2: 0.292160	 Dis: 7.370863 Entropy: 4.798224 
[2022-10-02 17:06:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.004763	 E_lr_2:0.004763
[2022-10-02 17:06:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 17:07:04 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.140963	Loss2: 0.147532	 Dis: 6.016722 Entropy: 4.547997 
[2022-10-02 17:07:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.004728	 E_lr_2:0.004728
[2022-10-02 17:07:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 17:07:10 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.144452	Loss2: 0.136137	 Dis: 6.143835 Entropy: 4.792052 
[2022-10-02 17:07:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.004692	 E_lr_2:0.004692
[2022-10-02 17:07:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 17:07:17 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.166155	Loss2: 0.175832	 Dis: 6.551453 Entropy: 4.247233 
[2022-10-02 17:07:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.004653	 E_lr_2:0.004653
[2022-10-02 17:07:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 17:07:22 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.112588	Loss2: 0.102700	 Dis: 5.670654 Entropy: 4.834134 
[2022-10-02 17:07:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.004612	 E_lr_2:0.004612
[2022-10-02 17:07:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 17:07:29 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.119321	Loss2: 0.125474	 Dis: 5.036491 Entropy: 4.639895 
[2022-10-02 17:07:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.004569	 E_lr_2:0.004569
[2022-10-02 17:07:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 17:07:36 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.212802	Loss2: 0.243737	 Dis: 6.558859 Entropy: 5.021698 
[2022-10-02 17:07:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.004524	 E_lr_2:0.004524
[2022-10-02 17:07:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 17:07:42 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.196919	Loss2: 0.252932	 Dis: 6.260540 Entropy: 5.550009 
[2022-10-02 17:07:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.004477	 E_lr_2:0.004477
[2022-10-02 17:07:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 17:07:48 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.048256	Loss2: 0.044625	 Dis: 6.052654 Entropy: 5.212142 
[2022-10-02 17:07:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.004428	 E_lr_2:0.004428
[2022-10-02 17:07:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 17:07:54 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.157293	Loss2: 0.186371	 Dis: 6.989998 Entropy: 4.997906 
[2022-10-02 17:07:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.004377	 E_lr_2:0.004377
[2022-10-02 17:07:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 17:08:01 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.177659	Loss2: 0.165644	 Dis: 6.080278 Entropy: 4.722589 
[2022-10-02 17:08:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.004324	 E_lr_2:0.004324
[2022-10-02 17:08:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 17:08:07 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.038907	Loss2: 0.048868	 Dis: 5.608416 Entropy: 4.523429 
[2022-10-02 17:08:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.004269	 E_lr_2:0.004269
[2022-10-02 17:08:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 17:08:12 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.033536	Loss2: 0.039858	 Dis: 3.950933 Entropy: 5.273197 
[2022-10-02 17:08:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.004213	 E_lr_2:0.004213
[2022-10-02 17:08:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 17:08:17 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.223600	Loss2: 0.252093	 Dis: 4.624165 Entropy: 5.726730 
[2022-10-02 17:08:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.004155	 E_lr_2:0.004155
[2022-10-02 17:08:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 17:08:23 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.131934	Loss2: 0.123251	 Dis: 7.847908 Entropy: 4.547972 
[2022-10-02 17:08:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.004095	 E_lr_2:0.004095
[2022-10-02 17:08:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000041	 C_lr_2:0.000041	 C_lr_3:0.000041	 C_lr_4:0.000041
[2022-10-02 17:08:29 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.033036	Loss2: 0.035488	 Dis: 4.414076 Entropy: 4.741317 
[2022-10-02 17:08:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.004034	 E_lr_2:0.004034
[2022-10-02 17:08:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 17:08:34 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.084577	Loss2: 0.091837	 Dis: 2.361504 Entropy: 4.551943 
[2022-10-02 17:08:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.003971	 E_lr_2:0.003971
[2022-10-02 17:08:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 17:08:40 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.157801	Loss2: 0.189170	 Dis: 5.560856 Entropy: 4.701344 
[2022-10-02 17:08:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.003907	 E_lr_2:0.003907
[2022-10-02 17:08:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000039	 C_lr_2:0.000039	 C_lr_3:0.000039	 C_lr_4:0.000039
[2022-10-02 17:08:46 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.095599	Loss2: 0.107692	 Dis: 4.241785 Entropy: 4.798047 
[2022-10-02 17:08:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.003841	 E_lr_2:0.003841
[2022-10-02 17:08:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 17:08:52 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.071051	Loss2: 0.094795	 Dis: 5.239697 Entropy: 5.482508 
[2022-10-02 17:08:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.003774	 E_lr_2:0.003774
[2022-10-02 17:08:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 17:08:58 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.052151	Loss2: 0.053373	 Dis: 5.394314 Entropy: 5.264863 
[2022-10-02 17:08:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.003706	 E_lr_2:0.003706
[2022-10-02 17:08:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000037	 C_lr_2:0.000037	 C_lr_3:0.000037	 C_lr_4:0.000037
[2022-10-02 17:09:04 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.427773	Loss2: 0.451534	 Dis: 3.940983 Entropy: 4.740095 
[2022-10-02 17:09:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.003637	 E_lr_2:0.003637
[2022-10-02 17:09:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 17:09:11 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.026278	Loss2: 0.026940	 Dis: 3.973326 Entropy: 4.921839 
[2022-10-02 17:09:11 demo] (houston_program2.py 512): INFO E_lr_1: 0.003566	 E_lr_2:0.003566
[2022-10-02 17:09:11 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 17:09:17 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.013300	Loss2: 0.017686	 Dis: 4.141972 Entropy: 5.216209 
[2022-10-02 17:09:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.003495	 E_lr_2:0.003495
[2022-10-02 17:09:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 17:09:23 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.122797	Loss2: 0.109231	 Dis: 3.522514 Entropy: 4.377715 
[2022-10-02 17:09:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.003422	 E_lr_2:0.003422
[2022-10-02 17:09:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000034	 C_lr_2:0.000034	 C_lr_3:0.000034	 C_lr_4:0.000034
[2022-10-02 17:09:29 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.098537	Loss2: 0.109029	 Dis: 3.376925 Entropy: 4.345933 
[2022-10-02 17:09:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.003349	 E_lr_2:0.003349
[2022-10-02 17:09:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 17:09:35 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.041017	Loss2: 0.042912	 Dis: 2.871634 Entropy: 4.507211 
[2022-10-02 17:09:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.003274	 E_lr_2:0.003274
[2022-10-02 17:09:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 17:09:41 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.036566	Loss2: 0.037367	 Dis: 3.218758 Entropy: 4.163536 
[2022-10-02 17:09:41 demo] (houston_program2.py 512): INFO E_lr_1: 0.003199	 E_lr_2:0.003199
[2022-10-02 17:09:41 demo] (houston_program2.py 513): INFO C_lr_1: 0.000032	 C_lr_2:0.000032	 C_lr_3:0.000032	 C_lr_4:0.000032
[2022-10-02 17:09:47 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.032457	Loss2: 0.024874	 Dis: 3.666058 Entropy: 4.846527 
[2022-10-02 17:09:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.003124	 E_lr_2:0.003124
[2022-10-02 17:09:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000031	 C_lr_2:0.000031	 C_lr_3:0.000031	 C_lr_4:0.000031
[2022-10-02 17:09:54 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.009792	Loss2: 0.008427	 Dis: 2.669949 Entropy: 5.925499 
[2022-10-02 17:09:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.003047	 E_lr_2:0.003047
[2022-10-02 17:09:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 17:10:00 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.029330	Loss2: 0.043612	 Dis: 5.252682 Entropy: 5.115684 
[2022-10-02 17:10:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.002970	 E_lr_2:0.002970
[2022-10-02 17:10:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 17:10:06 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.085656	Loss2: 0.095535	 Dis: 4.479715 Entropy: 4.994848 
[2022-10-02 17:10:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.002893	 E_lr_2:0.002893
[2022-10-02 17:10:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000029	 C_lr_2:0.000029	 C_lr_3:0.000029	 C_lr_4:0.000029
[2022-10-02 17:10:12 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.028767	Loss2: 0.032174	 Dis: 3.984287 Entropy: 5.151313 
[2022-10-02 17:10:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.002815	 E_lr_2:0.002815
[2022-10-02 17:10:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000028	 C_lr_2:0.000028	 C_lr_3:0.000028	 C_lr_4:0.000028
[2022-10-02 17:10:18 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.068638	Loss2: 0.046085	 Dis: 5.459032 Entropy: 5.210867 
[2022-10-02 17:10:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.002737	 E_lr_2:0.002737
[2022-10-02 17:10:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 17:10:24 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.150015	Loss2: 0.136831	 Dis: 3.056581 Entropy: 4.285157 
[2022-10-02 17:10:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.002659	 E_lr_2:0.002659
[2022-10-02 17:10:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 17:10:30 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.057329	Loss2: 0.053823	 Dis: 3.685865 Entropy: 5.480384 
[2022-10-02 17:10:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.002581	 E_lr_2:0.002581
[2022-10-02 17:10:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000026	 C_lr_2:0.000026	 C_lr_3:0.000026	 C_lr_4:0.000026
[2022-10-02 17:10:36 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.022650	Loss2: 0.017635	 Dis: 3.277788 Entropy: 4.870007 
[2022-10-02 17:10:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.002502	 E_lr_2:0.002502
[2022-10-02 17:10:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 17:10:42 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.105907	Loss2: 0.094633	 Dis: 3.580566 Entropy: 4.578599 
[2022-10-02 17:10:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.002423	 E_lr_2:0.002423
[2022-10-02 17:10:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000024	 C_lr_2:0.000024	 C_lr_3:0.000024	 C_lr_4:0.000024
[2022-10-02 17:10:48 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.019132	Loss2: 0.019822	 Dis: 3.145447 Entropy: 4.929960 
[2022-10-02 17:10:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.002345	 E_lr_2:0.002345
[2022-10-02 17:10:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 17:10:54 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.048621	Loss2: 0.051592	 Dis: 2.248838 Entropy: 5.913287 
[2022-10-02 17:10:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.002267	 E_lr_2:0.002267
[2022-10-02 17:10:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 17:11:01 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.009056	Loss2: 0.013416	 Dis: 2.264891 Entropy: 4.598591 
[2022-10-02 17:11:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.002189	 E_lr_2:0.002189
[2022-10-02 17:11:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000022	 C_lr_2:0.000022	 C_lr_3:0.000022	 C_lr_4:0.000022
[2022-10-02 17:11:07 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.018408	Loss2: 0.013872	 Dis: 1.763563 Entropy: 4.432678 
[2022-10-02 17:11:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.002111	 E_lr_2:0.002111
[2022-10-02 17:11:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000021	 C_lr_2:0.000021	 C_lr_3:0.000021	 C_lr_4:0.000021
[2022-10-02 17:11:13 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.040736	Loss2: 0.031870	 Dis: 3.636131 Entropy: 4.273827 
[2022-10-02 17:11:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.002034	 E_lr_2:0.002034
[2022-10-02 17:11:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 17:11:19 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.014891	Loss2: 0.029656	 Dis: 3.361654 Entropy: 5.325185 
[2022-10-02 17:11:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.001957	 E_lr_2:0.001957
[2022-10-02 17:11:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 17:11:25 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.014724	Loss2: 0.010518	 Dis: 2.911697 Entropy: 4.858359 
[2022-10-02 17:11:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.001880	 E_lr_2:0.001880
[2022-10-02 17:11:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000019	 C_lr_2:0.000019	 C_lr_3:0.000019	 C_lr_4:0.000019
[2022-10-02 17:11:31 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.024914	Loss2: 0.028198	 Dis: 3.014597 Entropy: 4.688586 
[2022-10-02 17:11:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.001804	 E_lr_2:0.001804
[2022-10-02 17:11:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000018	 C_lr_2:0.000018	 C_lr_3:0.000018	 C_lr_4:0.000018
[2022-10-02 17:11:38 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.012636	Loss2: 0.015748	 Dis: 2.963684 Entropy: 4.921332 
[2022-10-02 17:11:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.001729	 E_lr_2:0.001729
[2022-10-02 17:11:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 17:11:44 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.008606	Loss2: 0.008155	 Dis: 4.100760 Entropy: 4.809676 
[2022-10-02 17:11:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.001655	 E_lr_2:0.001655
[2022-10-02 17:11:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 17:11:51 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.007091	Loss2: 0.006740	 Dis: 2.425943 Entropy: 5.435558 
[2022-10-02 17:11:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.001582	 E_lr_2:0.001582
[2022-10-02 17:11:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000016	 C_lr_2:0.000016	 C_lr_3:0.000016	 C_lr_4:0.000016
[2022-10-02 17:11:57 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.009364	Loss2: 0.009033	 Dis: 2.508928 Entropy: 4.277288 
[2022-10-02 17:11:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.001509	 E_lr_2:0.001509
[2022-10-02 17:11:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 17:12:02 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.031347	Loss2: 0.037031	 Dis: 3.909565 Entropy: 4.818252 
[2022-10-02 17:12:02 demo] (houston_program2.py 512): INFO E_lr_1: 0.001437	 E_lr_2:0.001437
[2022-10-02 17:12:02 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 17:12:09 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.006044	Loss2: 0.005089	 Dis: 5.557005 Entropy: 5.077915 
[2022-10-02 17:12:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.001367	 E_lr_2:0.001367
[2022-10-02 17:12:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 17:12:14 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.009731	Loss2: 0.008810	 Dis: 2.331900 Entropy: 4.505540 
[2022-10-02 17:12:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.001297	 E_lr_2:0.001297
[2022-10-02 17:12:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000013	 C_lr_2:0.000013	 C_lr_3:0.000013	 C_lr_4:0.000013
[2022-10-02 17:12:20 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.188809	Loss2: 0.127944	 Dis: 6.402428 Entropy: 4.403646 
[2022-10-02 17:12:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.001229	 E_lr_2:0.001229
[2022-10-02 17:12:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 17:12:26 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.166612	Loss2: 0.130898	 Dis: 4.687960 Entropy: 4.445440 
[2022-10-02 17:12:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.001162	 E_lr_2:0.001162
[2022-10-02 17:12:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 17:12:32 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.043865	Loss2: 0.040514	 Dis: 3.842375 Entropy: 4.426636 
[2022-10-02 17:12:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.001096	 E_lr_2:0.001096
[2022-10-02 17:12:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000011	 C_lr_2:0.000011	 C_lr_3:0.000011	 C_lr_4:0.000011
[2022-10-02 17:12:38 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.053413	Loss2: 0.045465	 Dis: 3.187984 Entropy: 4.511428 
[2022-10-02 17:12:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.001032	 E_lr_2:0.001032
[2022-10-02 17:12:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 17:12:43 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.113952	Loss2: 0.118886	 Dis: 4.188301 Entropy: 5.408815 
[2022-10-02 17:12:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.000969	 E_lr_2:0.000969
[2022-10-02 17:12:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 17:12:49 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.025926	Loss2: 0.023214	 Dis: 3.452480 Entropy: 5.341028 
[2022-10-02 17:12:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.000908	 E_lr_2:0.000908
[2022-10-02 17:12:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000009	 C_lr_2:0.000009	 C_lr_3:0.000009	 C_lr_4:0.000009
[2022-10-02 17:12:55 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.035761	Loss2: 0.025788	 Dis: 3.298206 Entropy: 5.350106 
[2022-10-02 17:12:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.000848	 E_lr_2:0.000848
[2022-10-02 17:12:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 17:13:01 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.022860	Loss2: 0.020069	 Dis: 3.314310 Entropy: 5.363808 
[2022-10-02 17:13:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.000790	 E_lr_2:0.000790
[2022-10-02 17:13:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 17:13:07 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.041232	Loss2: 0.038788	 Dis: 3.796165 Entropy: 4.621659 
[2022-10-02 17:13:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.000734	 E_lr_2:0.000734
[2022-10-02 17:13:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 17:13:13 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.030014	Loss2: 0.026127	 Dis: 2.463287 Entropy: 4.836203 
[2022-10-02 17:13:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.000679	 E_lr_2:0.000679
[2022-10-02 17:13:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 17:13:20 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.006098	Loss2: 0.007674	 Dis: 2.284843 Entropy: 4.526053 
[2022-10-02 17:13:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.000626	 E_lr_2:0.000626
[2022-10-02 17:13:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 17:13:26 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.036193	Loss2: 0.032598	 Dis: 2.347839 Entropy: 5.369786 
[2022-10-02 17:13:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.000575	 E_lr_2:0.000575
[2022-10-02 17:13:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 17:13:32 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.086977	Loss2: 0.087537	 Dis: 3.883036 Entropy: 5.414867 
[2022-10-02 17:13:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.000526	 E_lr_2:0.000526
[2022-10-02 17:13:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 17:13:38 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.006114	Loss2: 0.005685	 Dis: 3.248026 Entropy: 4.437663 
[2022-10-02 17:13:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.000479	 E_lr_2:0.000479
[2022-10-02 17:13:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 17:13:44 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.007965	Loss2: 0.007415	 Dis: 3.485071 Entropy: 4.416094 
[2022-10-02 17:13:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.000433	 E_lr_2:0.000433
[2022-10-02 17:13:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 17:13:50 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.110371	Loss2: 0.078129	 Dis: 1.760563 Entropy: 5.464270 
[2022-10-02 17:13:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.000390	 E_lr_2:0.000390
[2022-10-02 17:13:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 17:13:56 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.007024	Loss2: 0.007372	 Dis: 1.812105 Entropy: 4.754221 
[2022-10-02 17:13:56 demo] (houston_program2.py 512): INFO E_lr_1: 0.000349	 E_lr_2:0.000349
[2022-10-02 17:13:56 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 17:14:02 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.076964	Loss2: 0.088988	 Dis: 1.943041 Entropy: 4.811686 
[2022-10-02 17:14:02 demo] (houston_program2.py 512): INFO E_lr_1: 0.000310	 E_lr_2:0.000310
[2022-10-02 17:14:02 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 17:14:08 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.051088	Loss2: 0.044129	 Dis: 1.884802 Entropy: 5.157386 
[2022-10-02 17:14:08 demo] (houston_program2.py 512): INFO E_lr_1: 0.000273	 E_lr_2:0.000273
[2022-10-02 17:14:08 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 17:14:14 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.023479	Loss2: 0.018378	 Dis: 2.843216 Entropy: 4.549211 
[2022-10-02 17:14:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.000239	 E_lr_2:0.000239
[2022-10-02 17:14:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 17:14:20 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.046342	Loss2: 0.056148	 Dis: 4.616261 Entropy: 5.097492 
[2022-10-02 17:14:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.000206	 E_lr_2:0.000206
[2022-10-02 17:14:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 17:14:26 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.006564	Loss2: 0.007238	 Dis: 3.316662 Entropy: 4.813369 
[2022-10-02 17:14:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.000176	 E_lr_2:0.000176
[2022-10-02 17:14:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 17:14:32 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.021979	Loss2: 0.022539	 Dis: 2.791487 Entropy: 4.522000 
[2022-10-02 17:14:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.000148	 E_lr_2:0.000148
[2022-10-02 17:14:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:14:38 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.021838	Loss2: 0.022124	 Dis: 3.577156 Entropy: 4.473743 
[2022-10-02 17:14:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.000123	 E_lr_2:0.000123
[2022-10-02 17:14:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:14:44 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.010362	Loss2: 0.010550	 Dis: 3.323807 Entropy: 4.828155 
[2022-10-02 17:14:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.000100	 E_lr_2:0.000100
[2022-10-02 17:14:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:14:50 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.010602	Loss2: 0.015110	 Dis: 2.178181 Entropy: 5.813179 
[2022-10-02 17:14:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.000079	 E_lr_2:0.000079
[2022-10-02 17:14:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:14:55 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.011638	Loss2: 0.011766	 Dis: 3.239460 Entropy: 4.687002 
[2022-10-02 17:14:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.000061	 E_lr_2:0.000061
[2022-10-02 17:14:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:15:02 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.005628	Loss2: 0.006829	 Dis: 2.007008 Entropy: 4.542879 
[2022-10-02 17:15:02 demo] (houston_program2.py 512): INFO E_lr_1: 0.000045	 E_lr_2:0.000045
[2022-10-02 17:15:02 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:15:07 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.015365	Loss2: 0.013410	 Dis: 3.136311 Entropy: 4.904155 
[2022-10-02 17:15:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.000031	 E_lr_2:0.000031
[2022-10-02 17:15:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:15:13 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.009311	Loss2: 0.013591	 Dis: 1.967102 Entropy: 4.350670 
[2022-10-02 17:15:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.000020	 E_lr_2:0.000020
[2022-10-02 17:15:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:15:19 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.007951	Loss2: 0.009472	 Dis: 3.761353 Entropy: 4.366053 
[2022-10-02 17:15:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.000011	 E_lr_2:0.000011
[2022-10-02 17:15:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:15:25 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.021591	Loss2: 0.018794	 Dis: 2.092144 Entropy: 5.579632 
[2022-10-02 17:15:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.000005	 E_lr_2:0.000005
[2022-10-02 17:15:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:15:31 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.005567	Loss2: 0.004549	 Dis: 1.822548 Entropy: 5.192316 
[2022-10-02 17:15:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.000001	 E_lr_2:0.000001
[2022-10-02 17:15:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:15:38 demo] (houston_program2.py 504): INFO Train Ep: 10 	Loss1: 0.021487	Loss2: 0.021101	 Dis: 2.621147 Entropy: 5.360632 
[2022-10-02 17:15:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.000000	 E_lr_2:0.000000
[2022-10-02 17:15:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:15:38 demo] (houston_program2.py 515): INFO time_10_epoch:603.4476363658905
[2022-10-02 17:15:46 demo] (houston_program2.py 673): INFO 	val_Accuracy: 31654/53200 (59.50%)	
[2022-10-02 17:15:46 demo] (utils.py 47): INFO outputs/demo/finetune_100/ckpt_epoch_10.pth saving......
[2022-10-02 17:15:46 demo] (utils.py 49): INFO outputs/demo/finetune_100/ckpt_epoch_10.pth saved !!!
[2022-10-02 17:15:46 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0116 (0.0116)	loss 0.0211 (0.0211)	grad_norm 0.0542 (0.0542)	mem 459MB
[2022-10-02 17:15:46 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:46 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0098 (0.0098)	loss 0.0275 (0.0275)	grad_norm 0.0725 (0.0725)	mem 459MB
[2022-10-02 17:15:46 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:46 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0091 (0.0091)	loss 0.0181 (0.0181)	grad_norm 0.0472 (0.0472)	mem 460MB
[2022-10-02 17:15:46 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:46 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000049	time 0.0175 (0.0175)	loss 0.0212 (0.0212)	grad_norm 0.0569 (0.0569)	mem 460MB
[2022-10-02 17:15:46 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:47 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0114 (0.0114)	loss 0.0167 (0.0167)	grad_norm 0.0626 (0.0626)	mem 460MB
[2022-10-02 17:15:47 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:47 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0217 (0.0217)	loss 0.0229 (0.0229)	grad_norm 0.0429 (0.0429)	mem 460MB
[2022-10-02 17:15:47 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:47 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0108 (0.0108)	loss 0.0185 (0.0185)	grad_norm 0.1048 (0.1048)	mem 460MB
[2022-10-02 17:15:47 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:47 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0203 (0.0203)	loss 0.0209 (0.0209)	grad_norm 0.0753 (0.0753)	mem 460MB
[2022-10-02 17:15:47 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:47 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0132 (0.0132)	loss 0.0161 (0.0161)	grad_norm 0.0393 (0.0393)	mem 460MB
[2022-10-02 17:15:47 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:47 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0196 (0.0196)	loss 0.0174 (0.0174)	grad_norm 0.0899 (0.0899)	mem 460MB
[2022-10-02 17:15:47 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:48 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0114 (0.0114)	loss 0.0240 (0.0240)	grad_norm 0.0598 (0.0598)	mem 460MB
[2022-10-02 17:15:48 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:48 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0214 (0.0214)	loss 0.0203 (0.0203)	grad_norm 0.0565 (0.0565)	mem 460MB
[2022-10-02 17:15:48 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:48 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0192 (0.0192)	loss 0.0196 (0.0196)	grad_norm 0.0692 (0.0692)	mem 460MB
[2022-10-02 17:15:48 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:48 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0268 (0.0268)	loss 0.0186 (0.0186)	grad_norm 0.0856 (0.0856)	mem 460MB
[2022-10-02 17:15:48 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:48 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0195 (0.0195)	loss 0.0198 (0.0198)	grad_norm 0.0580 (0.0580)	mem 460MB
[2022-10-02 17:15:48 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:49 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0269 (0.0269)	loss 0.0178 (0.0178)	grad_norm 0.0836 (0.0836)	mem 460MB
[2022-10-02 17:15:49 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:49 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0227 (0.0227)	loss 0.0210 (0.0210)	grad_norm 0.1026 (0.1026)	mem 460MB
[2022-10-02 17:15:49 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:49 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0245 (0.0245)	loss 0.0176 (0.0176)	grad_norm 0.0679 (0.0679)	mem 460MB
[2022-10-02 17:15:49 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:49 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0208 (0.0208)	loss 0.0170 (0.0170)	grad_norm 0.1028 (0.1028)	mem 460MB
[2022-10-02 17:15:49 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:49 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0120 (0.0120)	loss 0.0178 (0.0178)	grad_norm 0.0534 (0.0534)	mem 460MB
[2022-10-02 17:15:49 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:49 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0219 (0.0219)	loss 0.0286 (0.0286)	grad_norm 0.0504 (0.0504)	mem 460MB
[2022-10-02 17:15:49 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:50 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0120 (0.0120)	loss 0.0182 (0.0182)	grad_norm 0.0944 (0.0944)	mem 460MB
[2022-10-02 17:15:50 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:50 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0217 (0.0217)	loss 0.0162 (0.0162)	grad_norm 0.0615 (0.0615)	mem 460MB
[2022-10-02 17:15:50 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:50 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0111 (0.0111)	loss 0.0247 (0.0247)	grad_norm 0.0680 (0.0680)	mem 460MB
[2022-10-02 17:15:50 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:50 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0198 (0.0198)	loss 0.0250 (0.0250)	grad_norm 0.0681 (0.0681)	mem 460MB
[2022-10-02 17:15:50 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:50 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0123 (0.0123)	loss 0.0219 (0.0219)	grad_norm 0.0622 (0.0622)	mem 460MB
[2022-10-02 17:15:50 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:50 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0222 (0.0222)	loss 0.0217 (0.0217)	grad_norm 0.0404 (0.0404)	mem 460MB
[2022-10-02 17:15:51 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:51 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0184 (0.0184)	loss 0.0207 (0.0207)	grad_norm 0.0551 (0.0551)	mem 460MB
[2022-10-02 17:15:51 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:51 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0220 (0.0220)	loss 0.0236 (0.0236)	grad_norm 0.0791 (0.0791)	mem 460MB
[2022-10-02 17:15:51 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:51 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0179 (0.0179)	loss 0.0158 (0.0158)	grad_norm 0.0397 (0.0397)	mem 460MB
[2022-10-02 17:15:51 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:51 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0201 (0.0201)	loss 0.0172 (0.0172)	grad_norm 0.0712 (0.0712)	mem 460MB
[2022-10-02 17:15:51 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:51 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0210 (0.0210)	loss 0.0201 (0.0201)	grad_norm 0.0596 (0.0596)	mem 460MB
[2022-10-02 17:15:51 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:52 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0223 (0.0223)	loss 0.0224 (0.0224)	grad_norm 0.0416 (0.0416)	mem 460MB
[2022-10-02 17:15:52 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:52 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0270 (0.0270)	loss 0.0159 (0.0159)	grad_norm 0.0589 (0.0589)	mem 460MB
[2022-10-02 17:15:52 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:52 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0198 (0.0198)	loss 0.0159 (0.0159)	grad_norm 0.0457 (0.0457)	mem 460MB
[2022-10-02 17:15:52 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:52 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0089 (0.0089)	loss 0.0168 (0.0168)	grad_norm 0.0431 (0.0431)	mem 460MB
[2022-10-02 17:15:52 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:52 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0116 (0.0116)	loss 0.0185 (0.0185)	grad_norm 0.0544 (0.0544)	mem 460MB
[2022-10-02 17:15:52 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:52 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0104 (0.0104)	loss 0.0156 (0.0156)	grad_norm 0.0689 (0.0689)	mem 460MB
[2022-10-02 17:15:52 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:52 demo] (houston_program2.py 243): INFO Train: [11/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0129 (0.0129)	loss 0.0194 (0.0194)	grad_norm 0.0372 (0.0372)	mem 460MB
[2022-10-02 17:15:52 demo] (houston_program2.py 252): INFO EPOCH 11 training takes 0:00:00
[2022-10-02 17:15:53 demo] (houston_program2.py 333): INFO Train Ep: 11 	Loss1: 0.254018	Loss2: 0.254390	 Dis: 9.111088 Entropy: 5.104875 
[2022-10-02 17:15:53 demo] (houston_program2.py 335): INFO time_11_epoch:6.79026985168457
[2022-10-02 17:15:53 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0122 (0.0122)	loss 0.0195 (0.0195)	grad_norm 0.0547 (0.0547)	mem 460MB
[2022-10-02 17:15:53 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:53 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0223 (0.0223)	loss 0.0156 (0.0156)	grad_norm 0.0404 (0.0404)	mem 460MB
[2022-10-02 17:15:53 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:53 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0190 (0.0190)	loss 0.0152 (0.0152)	grad_norm 0.0552 (0.0552)	mem 460MB
[2022-10-02 17:15:53 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:54 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0229 (0.0229)	loss 0.0168 (0.0168)	grad_norm 0.0383 (0.0383)	mem 460MB
[2022-10-02 17:15:54 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:54 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0202 (0.0202)	loss 0.0200 (0.0200)	grad_norm 0.0423 (0.0423)	mem 460MB
[2022-10-02 17:15:54 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:54 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0212 (0.0212)	loss 0.0142 (0.0142)	grad_norm 0.0382 (0.0382)	mem 460MB
[2022-10-02 17:15:54 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:54 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0189 (0.0189)	loss 0.0194 (0.0194)	grad_norm 0.0335 (0.0335)	mem 460MB
[2022-10-02 17:15:54 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:54 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0205 (0.0205)	loss 0.0203 (0.0203)	grad_norm 0.0383 (0.0383)	mem 460MB
[2022-10-02 17:15:54 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:54 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0216 (0.0216)	loss 0.0176 (0.0176)	grad_norm 0.0317 (0.0317)	mem 460MB
[2022-10-02 17:15:54 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:55 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0225 (0.0225)	loss 0.0181 (0.0181)	grad_norm 0.0470 (0.0470)	mem 460MB
[2022-10-02 17:15:55 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:55 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0197 (0.0197)	loss 0.0193 (0.0193)	grad_norm 0.0462 (0.0462)	mem 460MB
[2022-10-02 17:15:55 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:55 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0222 (0.0222)	loss 0.0148 (0.0148)	grad_norm 0.0477 (0.0477)	mem 460MB
[2022-10-02 17:15:55 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:55 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0202 (0.0202)	loss 0.0137 (0.0137)	grad_norm 0.0460 (0.0460)	mem 460MB
[2022-10-02 17:15:55 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:55 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0122 (0.0122)	loss 0.0148 (0.0148)	grad_norm 0.0370 (0.0370)	mem 460MB
[2022-10-02 17:15:55 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:55 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0212 (0.0212)	loss 0.0152 (0.0152)	grad_norm 0.0331 (0.0331)	mem 460MB
[2022-10-02 17:15:56 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:56 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0121 (0.0121)	loss 0.0223 (0.0223)	grad_norm 0.0394 (0.0394)	mem 460MB
[2022-10-02 17:15:56 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:56 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0219 (0.0219)	loss 0.0202 (0.0202)	grad_norm 0.0473 (0.0473)	mem 460MB
[2022-10-02 17:15:56 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:56 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0176 (0.0176)	loss 0.0184 (0.0184)	grad_norm 0.0395 (0.0395)	mem 460MB
[2022-10-02 17:15:56 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:56 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0219 (0.0219)	loss 0.0150 (0.0150)	grad_norm 0.0319 (0.0319)	mem 460MB
[2022-10-02 17:15:56 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:56 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0165 (0.0165)	loss 0.0162 (0.0162)	grad_norm 0.0565 (0.0565)	mem 460MB
[2022-10-02 17:15:56 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:57 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0211 (0.0211)	loss 0.0191 (0.0191)	grad_norm 0.0448 (0.0448)	mem 460MB
[2022-10-02 17:15:57 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:57 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0137 (0.0137)	loss 0.0169 (0.0169)	grad_norm 0.0568 (0.0568)	mem 460MB
[2022-10-02 17:15:57 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:57 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0162 (0.0162)	loss 0.0178 (0.0178)	grad_norm 0.0454 (0.0454)	mem 460MB
[2022-10-02 17:15:57 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:57 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0206 (0.0206)	loss 0.0166 (0.0166)	grad_norm 0.0395 (0.0395)	mem 460MB
[2022-10-02 17:15:57 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:57 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0207 (0.0207)	loss 0.0175 (0.0175)	grad_norm 0.0558 (0.0558)	mem 460MB
[2022-10-02 17:15:57 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:57 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0205 (0.0205)	loss 0.0258 (0.0258)	grad_norm 0.0369 (0.0369)	mem 460MB
[2022-10-02 17:15:57 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:58 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0206 (0.0206)	loss 0.0145 (0.0145)	grad_norm 0.0662 (0.0662)	mem 460MB
[2022-10-02 17:15:58 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:58 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0142 (0.0142)	loss 0.0204 (0.0204)	grad_norm 0.0536 (0.0536)	mem 460MB
[2022-10-02 17:15:58 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:58 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0198 (0.0198)	loss 0.0183 (0.0183)	grad_norm 0.0568 (0.0568)	mem 460MB
[2022-10-02 17:15:58 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:58 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0135 (0.0135)	loss 0.0126 (0.0126)	grad_norm 0.0545 (0.0545)	mem 460MB
[2022-10-02 17:15:58 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:58 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0204 (0.0204)	loss 0.0195 (0.0195)	grad_norm 0.1037 (0.1037)	mem 460MB
[2022-10-02 17:15:58 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:59 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0163 (0.0163)	loss 0.0264 (0.0264)	grad_norm 0.0582 (0.0582)	mem 460MB
[2022-10-02 17:15:59 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:59 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0217 (0.0217)	loss 0.0144 (0.0144)	grad_norm 0.0813 (0.0813)	mem 460MB
[2022-10-02 17:15:59 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:59 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0211 (0.0211)	loss 0.0287 (0.0287)	grad_norm 0.1272 (0.1272)	mem 460MB
[2022-10-02 17:15:59 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:59 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0206 (0.0206)	loss 0.0220 (0.0220)	grad_norm 0.0508 (0.0508)	mem 460MB
[2022-10-02 17:15:59 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:59 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0203 (0.0203)	loss 0.0233 (0.0233)	grad_norm 0.0652 (0.0652)	mem 460MB
[2022-10-02 17:15:59 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:15:59 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0213 (0.0213)	loss 0.0158 (0.0158)	grad_norm 0.0474 (0.0474)	mem 460MB
[2022-10-02 17:15:59 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:16:00 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0217 (0.0217)	loss 0.0165 (0.0165)	grad_norm 0.0557 (0.0557)	mem 460MB
[2022-10-02 17:16:00 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:16:00 demo] (houston_program2.py 243): INFO Train: [12/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0214 (0.0214)	loss 0.0141 (0.0141)	grad_norm 0.0559 (0.0559)	mem 460MB
[2022-10-02 17:16:00 demo] (houston_program2.py 252): INFO EPOCH 12 training takes 0:00:00
[2022-10-02 17:16:00 demo] (houston_program2.py 333): INFO Train Ep: 12 	Loss1: 0.364398	Loss2: 0.344716	 Dis: 9.136084 Entropy: 5.057462 
[2022-10-02 17:16:00 demo] (houston_program2.py 335): INFO time_12_epoch:7.486386299133301
[2022-10-02 17:16:00 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0137 (0.0137)	loss 0.0205 (0.0205)	grad_norm 0.0740 (0.0740)	mem 460MB
[2022-10-02 17:16:00 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:01 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0226 (0.0226)	loss 0.0171 (0.0171)	grad_norm 0.0571 (0.0571)	mem 460MB
[2022-10-02 17:16:01 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:01 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0149 (0.0149)	loss 0.0174 (0.0174)	grad_norm 0.0479 (0.0479)	mem 460MB
[2022-10-02 17:16:01 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:01 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0246 (0.0246)	loss 0.0158 (0.0158)	grad_norm 0.0568 (0.0568)	mem 460MB
[2022-10-02 17:16:01 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:01 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0261 (0.0261)	loss 0.0167 (0.0167)	grad_norm 0.0570 (0.0570)	mem 460MB
[2022-10-02 17:16:01 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:01 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0280 (0.0280)	loss 0.0138 (0.0138)	grad_norm 0.0453 (0.0453)	mem 460MB
[2022-10-02 17:16:01 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:02 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0249 (0.0249)	loss 0.0183 (0.0183)	grad_norm 0.0635 (0.0635)	mem 460MB
[2022-10-02 17:16:02 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:02 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0100 (0.0100)	loss 0.0183 (0.0183)	grad_norm 0.0623 (0.0623)	mem 460MB
[2022-10-02 17:16:02 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:02 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0219 (0.0219)	loss 0.0162 (0.0162)	grad_norm 0.0672 (0.0672)	mem 460MB
[2022-10-02 17:16:02 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:02 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0109 (0.0109)	loss 0.0184 (0.0184)	grad_norm 0.0577 (0.0577)	mem 460MB
[2022-10-02 17:16:02 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:02 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0185 (0.0185)	loss 0.0169 (0.0169)	grad_norm 0.0759 (0.0759)	mem 460MB
[2022-10-02 17:16:02 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:02 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0118 (0.0118)	loss 0.0215 (0.0215)	grad_norm 0.0402 (0.0402)	mem 460MB
[2022-10-02 17:16:03 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:03 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0219 (0.0219)	loss 0.0173 (0.0173)	grad_norm 0.0521 (0.0521)	mem 460MB
[2022-10-02 17:16:03 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:03 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0154 (0.0154)	loss 0.0163 (0.0163)	grad_norm 0.0990 (0.0990)	mem 460MB
[2022-10-02 17:16:03 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:03 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0212 (0.0212)	loss 0.0162 (0.0162)	grad_norm 0.0519 (0.0519)	mem 460MB
[2022-10-02 17:16:03 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:03 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0154 (0.0154)	loss 0.0189 (0.0189)	grad_norm 0.0589 (0.0589)	mem 460MB
[2022-10-02 17:16:03 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:03 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0203 (0.0203)	loss 0.0192 (0.0192)	grad_norm 0.0890 (0.0890)	mem 460MB
[2022-10-02 17:16:03 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:04 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0234 (0.0234)	loss 0.0156 (0.0156)	grad_norm 0.0633 (0.0633)	mem 460MB
[2022-10-02 17:16:04 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:04 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0200 (0.0200)	loss 0.0184 (0.0184)	grad_norm 0.0848 (0.0848)	mem 460MB
[2022-10-02 17:16:04 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:04 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0218 (0.0218)	loss 0.0185 (0.0185)	grad_norm 0.0585 (0.0585)	mem 460MB
[2022-10-02 17:16:04 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:04 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0225 (0.0225)	loss 0.0185 (0.0185)	grad_norm 0.0887 (0.0887)	mem 460MB
[2022-10-02 17:16:04 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:04 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0217 (0.0217)	loss 0.0181 (0.0181)	grad_norm 0.0581 (0.0581)	mem 460MB
[2022-10-02 17:16:04 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:04 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0125 (0.0125)	loss 0.0181 (0.0181)	grad_norm 0.0505 (0.0505)	mem 460MB
[2022-10-02 17:16:05 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:05 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0171 (0.0171)	loss 0.0171 (0.0171)	grad_norm 0.0636 (0.0636)	mem 460MB
[2022-10-02 17:16:05 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:05 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0097 (0.0097)	loss 0.0187 (0.0187)	grad_norm 0.0469 (0.0469)	mem 460MB
[2022-10-02 17:16:05 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:05 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0216 (0.0216)	loss 0.0161 (0.0161)	grad_norm 0.0587 (0.0587)	mem 460MB
[2022-10-02 17:16:05 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:05 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0115 (0.0115)	loss 0.0190 (0.0190)	grad_norm 0.0553 (0.0553)	mem 460MB
[2022-10-02 17:16:05 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:05 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0202 (0.0202)	loss 0.0182 (0.0182)	grad_norm 0.0609 (0.0609)	mem 460MB
[2022-10-02 17:16:05 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:06 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0144 (0.0144)	loss 0.0136 (0.0136)	grad_norm 0.0470 (0.0470)	mem 460MB
[2022-10-02 17:16:06 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:06 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0186 (0.0186)	loss 0.0264 (0.0264)	grad_norm 0.0499 (0.0499)	mem 460MB
[2022-10-02 17:16:06 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:06 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0205 (0.0205)	loss 0.0179 (0.0179)	grad_norm 0.0543 (0.0543)	mem 460MB
[2022-10-02 17:16:06 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:06 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0221 (0.0221)	loss 0.0202 (0.0202)	grad_norm 0.0662 (0.0662)	mem 460MB
[2022-10-02 17:16:06 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:06 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0204 (0.0204)	loss 0.0202 (0.0202)	grad_norm 0.0463 (0.0463)	mem 460MB
[2022-10-02 17:16:06 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:06 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0218 (0.0218)	loss 0.0211 (0.0211)	grad_norm 0.0662 (0.0662)	mem 460MB
[2022-10-02 17:16:06 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:07 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0220 (0.0220)	loss 0.0170 (0.0170)	grad_norm 0.0588 (0.0588)	mem 460MB
[2022-10-02 17:16:07 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:07 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0219 (0.0219)	loss 0.0171 (0.0171)	grad_norm 0.0515 (0.0515)	mem 460MB
[2022-10-02 17:16:07 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:07 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0214 (0.0214)	loss 0.0190 (0.0190)	grad_norm 0.0502 (0.0502)	mem 460MB
[2022-10-02 17:16:07 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:07 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0156 (0.0156)	loss 0.0150 (0.0150)	grad_norm 0.0739 (0.0739)	mem 460MB
[2022-10-02 17:16:07 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:07 demo] (houston_program2.py 243): INFO Train: [13/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0244 (0.0244)	loss 0.0148 (0.0148)	grad_norm 0.0459 (0.0459)	mem 460MB
[2022-10-02 17:16:07 demo] (houston_program2.py 252): INFO EPOCH 13 training takes 0:00:00
[2022-10-02 17:16:08 demo] (houston_program2.py 333): INFO Train Ep: 13 	Loss1: 0.262086	Loss2: 0.272610	 Dis: 6.323071 Entropy: 4.984469 
[2022-10-02 17:16:08 demo] (houston_program2.py 335): INFO time_13_epoch:7.549855709075928
[2022-10-02 17:16:08 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0165 (0.0165)	loss 0.0162 (0.0162)	grad_norm 0.0437 (0.0437)	mem 460MB
[2022-10-02 17:16:08 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:08 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0120 (0.0120)	loss 0.0196 (0.0196)	grad_norm 0.0679 (0.0679)	mem 460MB
[2022-10-02 17:16:08 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:08 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0189 (0.0189)	loss 0.0162 (0.0162)	grad_norm 0.0437 (0.0437)	mem 460MB
[2022-10-02 17:16:08 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:09 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0139 (0.0139)	loss 0.0188 (0.0188)	grad_norm 0.0519 (0.0519)	mem 460MB
[2022-10-02 17:16:09 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:09 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0204 (0.0204)	loss 0.0179 (0.0179)	grad_norm 0.0444 (0.0444)	mem 460MB
[2022-10-02 17:16:09 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:09 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0136 (0.0136)	loss 0.0216 (0.0216)	grad_norm 0.0514 (0.0514)	mem 460MB
[2022-10-02 17:16:09 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:09 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0229 (0.0229)	loss 0.0175 (0.0175)	grad_norm 0.0489 (0.0489)	mem 460MB
[2022-10-02 17:16:09 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:09 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0178 (0.0178)	loss 0.0156 (0.0156)	grad_norm 0.0380 (0.0380)	mem 460MB
[2022-10-02 17:16:09 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:09 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0217 (0.0217)	loss 0.0146 (0.0146)	grad_norm 0.0450 (0.0450)	mem 460MB
[2022-10-02 17:16:09 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:10 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0207 (0.0207)	loss 0.0151 (0.0151)	grad_norm 0.0469 (0.0469)	mem 460MB
[2022-10-02 17:16:10 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:10 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0201 (0.0201)	loss 0.0204 (0.0204)	grad_norm 0.0559 (0.0559)	mem 460MB
[2022-10-02 17:16:10 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:10 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0231 (0.0231)	loss 0.0193 (0.0193)	grad_norm 0.0467 (0.0467)	mem 460MB
[2022-10-02 17:16:10 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:10 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0222 (0.0222)	loss 0.0157 (0.0157)	grad_norm 0.0458 (0.0458)	mem 460MB
[2022-10-02 17:16:10 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:10 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0213 (0.0213)	loss 0.0164 (0.0164)	grad_norm 0.0662 (0.0662)	mem 460MB
[2022-10-02 17:16:10 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:11 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000048	time 0.0287 (0.0287)	loss 0.0146 (0.0146)	grad_norm 0.0635 (0.0635)	mem 460MB
[2022-10-02 17:16:11 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:11 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0214 (0.0214)	loss 0.0144 (0.0144)	grad_norm 0.0582 (0.0582)	mem 460MB
[2022-10-02 17:16:11 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:11 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0129 (0.0129)	loss 0.0162 (0.0162)	grad_norm 0.0711 (0.0711)	mem 460MB
[2022-10-02 17:16:11 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:11 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0217 (0.0217)	loss 0.0206 (0.0206)	grad_norm 0.0585 (0.0585)	mem 460MB
[2022-10-02 17:16:11 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:11 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0151 (0.0151)	loss 0.0197 (0.0197)	grad_norm 0.0998 (0.0998)	mem 460MB
[2022-10-02 17:16:11 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:11 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0217 (0.0217)	loss 0.0175 (0.0175)	grad_norm 0.0896 (0.0896)	mem 460MB
[2022-10-02 17:16:11 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:12 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0202 (0.0202)	loss 0.0134 (0.0134)	grad_norm 0.0881 (0.0881)	mem 460MB
[2022-10-02 17:16:12 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:12 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0207 (0.0207)	loss 0.0179 (0.0179)	grad_norm 0.0513 (0.0513)	mem 460MB
[2022-10-02 17:16:12 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:12 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0138 (0.0138)	loss 0.0170 (0.0170)	grad_norm 0.0714 (0.0714)	mem 460MB
[2022-10-02 17:16:12 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:12 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0200 (0.0200)	loss 0.0145 (0.0145)	grad_norm 0.0590 (0.0590)	mem 460MB
[2022-10-02 17:16:12 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:12 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0170 (0.0170)	loss 0.0218 (0.0218)	grad_norm 0.0748 (0.0748)	mem 460MB
[2022-10-02 17:16:12 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:12 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0234 (0.0234)	loss 0.0194 (0.0194)	grad_norm 0.0478 (0.0478)	mem 460MB
[2022-10-02 17:16:13 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:13 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0185 (0.0185)	loss 0.0182 (0.0182)	grad_norm 0.0552 (0.0552)	mem 460MB
[2022-10-02 17:16:13 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:13 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0217 (0.0217)	loss 0.0164 (0.0164)	grad_norm 0.0647 (0.0647)	mem 460MB
[2022-10-02 17:16:13 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:13 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0207 (0.0207)	loss 0.0174 (0.0174)	grad_norm 0.0755 (0.0755)	mem 460MB
[2022-10-02 17:16:13 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:13 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0187 (0.0187)	loss 0.0202 (0.0202)	grad_norm 0.0505 (0.0505)	mem 460MB
[2022-10-02 17:16:13 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:13 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0215 (0.0215)	loss 0.0160 (0.0160)	grad_norm 0.0390 (0.0390)	mem 460MB
[2022-10-02 17:16:13 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:14 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0202 (0.0202)	loss 0.0158 (0.0158)	grad_norm 0.0887 (0.0887)	mem 460MB
[2022-10-02 17:16:14 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:14 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0201 (0.0201)	loss 0.0189 (0.0189)	grad_norm 0.0508 (0.0508)	mem 460MB
[2022-10-02 17:16:14 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:14 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0237 (0.0237)	loss 0.0190 (0.0190)	grad_norm 0.0554 (0.0554)	mem 460MB
[2022-10-02 17:16:14 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:14 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0232 (0.0232)	loss 0.0167 (0.0167)	grad_norm 0.0530 (0.0530)	mem 460MB
[2022-10-02 17:16:14 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:14 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0237 (0.0237)	loss 0.0161 (0.0161)	grad_norm 0.0526 (0.0526)	mem 460MB
[2022-10-02 17:16:14 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:14 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0203 (0.0203)	loss 0.0160 (0.0160)	grad_norm 0.0494 (0.0494)	mem 460MB
[2022-10-02 17:16:14 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:15 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0112 (0.0112)	loss 0.0160 (0.0160)	grad_norm 0.0493 (0.0493)	mem 460MB
[2022-10-02 17:16:15 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:15 demo] (houston_program2.py 243): INFO Train: [14/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0216 (0.0216)	loss 0.0124 (0.0124)	grad_norm 0.0433 (0.0433)	mem 460MB
[2022-10-02 17:16:15 demo] (houston_program2.py 252): INFO EPOCH 14 training takes 0:00:00
[2022-10-02 17:16:15 demo] (houston_program2.py 333): INFO Train Ep: 14 	Loss1: 0.090338	Loss2: 0.103747	 Dis: 8.588314 Entropy: 5.241397 
[2022-10-02 17:16:15 demo] (houston_program2.py 335): INFO time_14_epoch:7.389256954193115
[2022-10-02 17:16:15 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0208 (0.0208)	loss 0.0105 (0.0105)	grad_norm 0.0475 (0.0475)	mem 460MB
[2022-10-02 17:16:15 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:16 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0159 (0.0159)	loss 0.0180 (0.0180)	grad_norm 0.0390 (0.0390)	mem 460MB
[2022-10-02 17:16:16 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:16 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0216 (0.0216)	loss 0.0143 (0.0143)	grad_norm 0.0450 (0.0450)	mem 460MB
[2022-10-02 17:16:16 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:16 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0194 (0.0194)	loss 0.0172 (0.0172)	grad_norm 0.1356 (0.1356)	mem 460MB
[2022-10-02 17:16:16 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:16 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0262 (0.0262)	loss 0.0152 (0.0152)	grad_norm 0.0509 (0.0509)	mem 460MB
[2022-10-02 17:16:16 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:16 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0237 (0.0237)	loss 0.0169 (0.0169)	grad_norm 0.0466 (0.0466)	mem 460MB
[2022-10-02 17:16:16 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:17 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0228 (0.0228)	loss 0.0138 (0.0138)	grad_norm 0.0607 (0.0607)	mem 460MB
[2022-10-02 17:16:17 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:17 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0226 (0.0226)	loss 0.0170 (0.0170)	grad_norm 0.0628 (0.0628)	mem 460MB
[2022-10-02 17:16:17 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:17 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0227 (0.0227)	loss 0.0181 (0.0181)	grad_norm 0.0518 (0.0518)	mem 460MB
[2022-10-02 17:16:17 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:17 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0147 (0.0147)	loss 0.0173 (0.0173)	grad_norm 0.0847 (0.0847)	mem 460MB
[2022-10-02 17:16:17 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:17 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0218 (0.0218)	loss 0.0135 (0.0135)	grad_norm 0.0473 (0.0473)	mem 460MB
[2022-10-02 17:16:17 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:18 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0091 (0.0091)	loss 0.0212 (0.0212)	grad_norm 0.0462 (0.0462)	mem 460MB
[2022-10-02 17:16:18 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:18 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0206 (0.0206)	loss 0.0160 (0.0160)	grad_norm 0.0706 (0.0706)	mem 460MB
[2022-10-02 17:16:18 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:18 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0126 (0.0126)	loss 0.0162 (0.0162)	grad_norm 0.0630 (0.0630)	mem 460MB
[2022-10-02 17:16:18 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:18 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0205 (0.0205)	loss 0.0186 (0.0186)	grad_norm 0.0546 (0.0546)	mem 460MB
[2022-10-02 17:16:18 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:18 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0202 (0.0202)	loss 0.0164 (0.0164)	grad_norm 0.0616 (0.0616)	mem 460MB
[2022-10-02 17:16:18 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:19 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0213 (0.0213)	loss 0.0168 (0.0168)	grad_norm 0.0413 (0.0413)	mem 460MB
[2022-10-02 17:16:19 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:19 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0211 (0.0211)	loss 0.0128 (0.0128)	grad_norm 0.0413 (0.0413)	mem 460MB
[2022-10-02 17:16:19 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:19 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0208 (0.0208)	loss 0.0178 (0.0178)	grad_norm 0.0487 (0.0487)	mem 460MB
[2022-10-02 17:16:19 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:19 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0126 (0.0126)	loss 0.0246 (0.0246)	grad_norm 0.0503 (0.0503)	mem 460MB
[2022-10-02 17:16:19 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:19 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0204 (0.0204)	loss 0.0156 (0.0156)	grad_norm 0.0634 (0.0634)	mem 460MB
[2022-10-02 17:16:19 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:19 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0124 (0.0124)	loss 0.0196 (0.0196)	grad_norm 0.0673 (0.0673)	mem 460MB
[2022-10-02 17:16:19 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:20 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0167 (0.0167)	loss 0.0114 (0.0114)	grad_norm 0.0498 (0.0498)	mem 460MB
[2022-10-02 17:16:20 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:20 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0174 (0.0174)	loss 0.0152 (0.0152)	grad_norm 0.0468 (0.0468)	mem 460MB
[2022-10-02 17:16:20 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:20 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0224 (0.0224)	loss 0.0183 (0.0183)	grad_norm 0.0563 (0.0563)	mem 460MB
[2022-10-02 17:16:20 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:20 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0153 (0.0153)	loss 0.0215 (0.0215)	grad_norm 0.0414 (0.0414)	mem 460MB
[2022-10-02 17:16:20 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:20 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0200 (0.0200)	loss 0.0146 (0.0146)	grad_norm 0.0474 (0.0474)	mem 460MB
[2022-10-02 17:16:20 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:20 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0221 (0.0221)	loss 0.0168 (0.0168)	grad_norm 0.0401 (0.0401)	mem 460MB
[2022-10-02 17:16:20 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:21 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0202 (0.0202)	loss 0.0129 (0.0129)	grad_norm 0.0583 (0.0583)	mem 460MB
[2022-10-02 17:16:21 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:21 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0211 (0.0211)	loss 0.0200 (0.0200)	grad_norm 0.0585 (0.0585)	mem 460MB
[2022-10-02 17:16:21 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:21 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0161 (0.0161)	loss 0.0146 (0.0146)	grad_norm 0.0685 (0.0685)	mem 460MB
[2022-10-02 17:16:21 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:21 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0214 (0.0214)	loss 0.0135 (0.0135)	grad_norm 0.0559 (0.0559)	mem 460MB
[2022-10-02 17:16:21 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:21 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0118 (0.0118)	loss 0.0133 (0.0133)	grad_norm 0.0561 (0.0561)	mem 460MB
[2022-10-02 17:16:21 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:22 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0218 (0.0218)	loss 0.0179 (0.0179)	grad_norm 0.0668 (0.0668)	mem 460MB
[2022-10-02 17:16:22 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:22 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0200 (0.0200)	loss 0.0160 (0.0160)	grad_norm 0.0741 (0.0741)	mem 460MB
[2022-10-02 17:16:22 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:22 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0210 (0.0210)	loss 0.0180 (0.0180)	grad_norm 0.0563 (0.0563)	mem 460MB
[2022-10-02 17:16:22 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:22 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0183 (0.0183)	loss 0.0149 (0.0149)	grad_norm 0.0794 (0.0794)	mem 460MB
[2022-10-02 17:16:22 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:22 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0202 (0.0202)	loss 0.0166 (0.0166)	grad_norm 0.0562 (0.0562)	mem 460MB
[2022-10-02 17:16:22 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:22 demo] (houston_program2.py 243): INFO Train: [15/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0202 (0.0202)	loss 0.0155 (0.0155)	grad_norm 0.0547 (0.0547)	mem 460MB
[2022-10-02 17:16:23 demo] (houston_program2.py 252): INFO EPOCH 15 training takes 0:00:00
[2022-10-02 17:16:23 demo] (houston_program2.py 333): INFO Train Ep: 15 	Loss1: 0.309265	Loss2: 0.345028	 Dis: 8.484266 Entropy: 4.419992 
[2022-10-02 17:16:23 demo] (houston_program2.py 335): INFO time_15_epoch:7.744597911834717
[2022-10-02 17:16:23 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 17:16:23 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 17:16:23 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 17:16:23 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 17:16:23 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 17:16:23 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 17:16:23 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 17:16:23 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 17:16:23 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 17:16:23 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 17:16:29 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.145130	Loss2: 0.175571	 Dis: 5.123230 Entropy: 4.688721 
[2022-10-02 17:16:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.000487	 E_lr_2:0.000487
[2022-10-02 17:16:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 17:16:35 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.134589	Loss2: 0.139764	 Dis: 6.549156 Entropy: 4.433798 
[2022-10-02 17:16:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.000987	 E_lr_2:0.000987
[2022-10-02 17:16:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 17:16:41 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.179019	Loss2: 0.175737	 Dis: 6.118843 Entropy: 5.198531 
[2022-10-02 17:16:41 demo] (houston_program2.py 512): INFO E_lr_1: 0.001487	 E_lr_2:0.001487
[2022-10-02 17:16:41 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 17:16:47 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.074443	Loss2: 0.082944	 Dis: 5.468451 Entropy: 4.562568 
[2022-10-02 17:16:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.001987	 E_lr_2:0.001987
[2022-10-02 17:16:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 17:16:53 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.356696	Loss2: 0.323430	 Dis: 8.903564 Entropy: 5.612681 
[2022-10-02 17:16:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.002487	 E_lr_2:0.002487
[2022-10-02 17:16:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 17:17:00 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.175707	Loss2: 0.154575	 Dis: 7.646793 Entropy: 4.782591 
[2022-10-02 17:17:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.002987	 E_lr_2:0.002987
[2022-10-02 17:17:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 17:17:06 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.099119	Loss2: 0.110169	 Dis: 7.374348 Entropy: 5.295393 
[2022-10-02 17:17:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.003487	 E_lr_2:0.003487
[2022-10-02 17:17:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 17:17:12 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.160332	Loss2: 0.161317	 Dis: 6.820335 Entropy: 5.104350 
[2022-10-02 17:17:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.003987	 E_lr_2:0.003987
[2022-10-02 17:17:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 17:17:18 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.140579	Loss2: 0.128358	 Dis: 6.101788 Entropy: 5.453838 
[2022-10-02 17:17:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.004487	 E_lr_2:0.004487
[2022-10-02 17:17:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 17:17:24 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.115917	Loss2: 0.116757	 Dis: 9.351311 Entropy: 5.140482 
[2022-10-02 17:17:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.004987	 E_lr_2:0.004987
[2022-10-02 17:17:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000050	 C_lr_2:0.000050	 C_lr_3:0.000050	 C_lr_4:0.000050
[2022-10-02 17:17:30 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.136823	Loss2: 0.134801	 Dis: 5.975090 Entropy: 5.605364 
[2022-10-02 17:17:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.004853	 E_lr_2:0.004853
[2022-10-02 17:17:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000049	 C_lr_2:0.000049	 C_lr_3:0.000049	 C_lr_4:0.000049
[2022-10-02 17:17:36 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.039121	Loss2: 0.035987	 Dis: 5.095613 Entropy: 5.609384 
[2022-10-02 17:17:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.004825	 E_lr_2:0.004825
[2022-10-02 17:17:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 17:17:42 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.166670	Loss2: 0.158978	 Dis: 4.738840 Entropy: 5.217355 
[2022-10-02 17:17:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.004795	 E_lr_2:0.004795
[2022-10-02 17:17:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 17:17:48 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.037479	Loss2: 0.039877	 Dis: 6.574261 Entropy: 5.843723 
[2022-10-02 17:17:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.004763	 E_lr_2:0.004763
[2022-10-02 17:17:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 17:17:54 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.109141	Loss2: 0.083144	 Dis: 4.542913 Entropy: 5.806882 
[2022-10-02 17:17:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.004728	 E_lr_2:0.004728
[2022-10-02 17:17:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 17:18:00 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.183428	Loss2: 0.225250	 Dis: 4.620075 Entropy: 5.129289 
[2022-10-02 17:18:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.004692	 E_lr_2:0.004692
[2022-10-02 17:18:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 17:18:06 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.103842	Loss2: 0.085597	 Dis: 4.640995 Entropy: 5.654968 
[2022-10-02 17:18:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.004653	 E_lr_2:0.004653
[2022-10-02 17:18:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 17:18:13 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.096959	Loss2: 0.097310	 Dis: 7.888672 Entropy: 5.031428 
[2022-10-02 17:18:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.004612	 E_lr_2:0.004612
[2022-10-02 17:18:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 17:18:18 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.054540	Loss2: 0.042328	 Dis: 4.125511 Entropy: 6.255195 
[2022-10-02 17:18:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.004569	 E_lr_2:0.004569
[2022-10-02 17:18:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 17:18:24 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.035140	Loss2: 0.060267	 Dis: 5.498276 Entropy: 5.908604 
[2022-10-02 17:18:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.004524	 E_lr_2:0.004524
[2022-10-02 17:18:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 17:18:31 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.052500	Loss2: 0.051458	 Dis: 5.492596 Entropy: 4.939923 
[2022-10-02 17:18:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.004477	 E_lr_2:0.004477
[2022-10-02 17:18:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 17:18:36 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.042171	Loss2: 0.040154	 Dis: 4.697794 Entropy: 5.329483 
[2022-10-02 17:18:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.004428	 E_lr_2:0.004428
[2022-10-02 17:18:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 17:18:42 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.171970	Loss2: 0.131240	 Dis: 5.203636 Entropy: 5.652935 
[2022-10-02 17:18:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.004377	 E_lr_2:0.004377
[2022-10-02 17:18:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 17:18:48 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.106676	Loss2: 0.104457	 Dis: 5.097889 Entropy: 4.748335 
[2022-10-02 17:18:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.004324	 E_lr_2:0.004324
[2022-10-02 17:18:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 17:18:54 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.034799	Loss2: 0.031783	 Dis: 4.628786 Entropy: 5.276278 
[2022-10-02 17:18:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.004269	 E_lr_2:0.004269
[2022-10-02 17:18:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 17:19:01 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.038684	Loss2: 0.030724	 Dis: 4.181856 Entropy: 4.644680 
[2022-10-02 17:19:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.004213	 E_lr_2:0.004213
[2022-10-02 17:19:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 17:19:07 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.023870	Loss2: 0.021165	 Dis: 3.181896 Entropy: 5.373551 
[2022-10-02 17:19:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.004155	 E_lr_2:0.004155
[2022-10-02 17:19:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 17:19:13 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.134874	Loss2: 0.147414	 Dis: 6.142273 Entropy: 4.674224 
[2022-10-02 17:19:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.004095	 E_lr_2:0.004095
[2022-10-02 17:19:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000041	 C_lr_2:0.000041	 C_lr_3:0.000041	 C_lr_4:0.000041
[2022-10-02 17:19:19 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.142269	Loss2: 0.108084	 Dis: 7.003229 Entropy: 6.310175 
[2022-10-02 17:19:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.004034	 E_lr_2:0.004034
[2022-10-02 17:19:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 17:19:24 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.120907	Loss2: 0.148821	 Dis: 5.215631 Entropy: 4.332274 
[2022-10-02 17:19:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.003971	 E_lr_2:0.003971
[2022-10-02 17:19:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 17:19:30 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.072741	Loss2: 0.068691	 Dis: 4.936720 Entropy: 5.164012 
[2022-10-02 17:19:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.003907	 E_lr_2:0.003907
[2022-10-02 17:19:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000039	 C_lr_2:0.000039	 C_lr_3:0.000039	 C_lr_4:0.000039
[2022-10-02 17:19:37 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.062820	Loss2: 0.045575	 Dis: 4.009893 Entropy: 5.144511 
[2022-10-02 17:19:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.003841	 E_lr_2:0.003841
[2022-10-02 17:19:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 17:19:43 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.071854	Loss2: 0.053742	 Dis: 2.956419 Entropy: 5.163662 
[2022-10-02 17:19:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.003774	 E_lr_2:0.003774
[2022-10-02 17:19:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 17:19:49 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.047407	Loss2: 0.039914	 Dis: 4.148371 Entropy: 4.655703 
[2022-10-02 17:19:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.003706	 E_lr_2:0.003706
[2022-10-02 17:19:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000037	 C_lr_2:0.000037	 C_lr_3:0.000037	 C_lr_4:0.000037
[2022-10-02 17:19:55 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.065641	Loss2: 0.063799	 Dis: 4.466782 Entropy: 5.065951 
[2022-10-02 17:19:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.003637	 E_lr_2:0.003637
[2022-10-02 17:19:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 17:20:01 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.077487	Loss2: 0.083957	 Dis: 3.066818 Entropy: 5.592102 
[2022-10-02 17:20:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.003566	 E_lr_2:0.003566
[2022-10-02 17:20:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 17:20:08 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.308189	Loss2: 0.262180	 Dis: 4.240467 Entropy: 5.750890 
[2022-10-02 17:20:08 demo] (houston_program2.py 512): INFO E_lr_1: 0.003495	 E_lr_2:0.003495
[2022-10-02 17:20:08 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 17:20:14 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.052937	Loss2: 0.060458	 Dis: 4.187300 Entropy: 4.486451 
[2022-10-02 17:20:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.003422	 E_lr_2:0.003422
[2022-10-02 17:20:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000034	 C_lr_2:0.000034	 C_lr_3:0.000034	 C_lr_4:0.000034
[2022-10-02 17:20:20 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.130897	Loss2: 0.134825	 Dis: 2.182413 Entropy: 4.592936 
[2022-10-02 17:20:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.003349	 E_lr_2:0.003349
[2022-10-02 17:20:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 17:20:26 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.015996	Loss2: 0.013421	 Dis: 2.979153 Entropy: 5.181830 
[2022-10-02 17:20:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.003274	 E_lr_2:0.003274
[2022-10-02 17:20:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 17:20:32 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.021926	Loss2: 0.017020	 Dis: 2.206459 Entropy: 4.933394 
[2022-10-02 17:20:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.003199	 E_lr_2:0.003199
[2022-10-02 17:20:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000032	 C_lr_2:0.000032	 C_lr_3:0.000032	 C_lr_4:0.000032
[2022-10-02 17:20:38 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.003470	Loss2: 0.003879	 Dis: 2.514782 Entropy: 4.455386 
[2022-10-02 17:20:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.003124	 E_lr_2:0.003124
[2022-10-02 17:20:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000031	 C_lr_2:0.000031	 C_lr_3:0.000031	 C_lr_4:0.000031
[2022-10-02 17:20:44 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.038891	Loss2: 0.041936	 Dis: 2.026716 Entropy: 5.207852 
[2022-10-02 17:20:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.003047	 E_lr_2:0.003047
[2022-10-02 17:20:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 17:20:51 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.078165	Loss2: 0.070699	 Dis: 2.659502 Entropy: 5.882894 
[2022-10-02 17:20:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.002970	 E_lr_2:0.002970
[2022-10-02 17:20:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 17:20:57 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.047535	Loss2: 0.080088	 Dis: 5.054783 Entropy: 6.199203 
[2022-10-02 17:20:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.002893	 E_lr_2:0.002893
[2022-10-02 17:20:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000029	 C_lr_2:0.000029	 C_lr_3:0.000029	 C_lr_4:0.000029
[2022-10-02 17:21:03 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.203425	Loss2: 0.186375	 Dis: 3.304123 Entropy: 4.543506 
[2022-10-02 17:21:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.002815	 E_lr_2:0.002815
[2022-10-02 17:21:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000028	 C_lr_2:0.000028	 C_lr_3:0.000028	 C_lr_4:0.000028
[2022-10-02 17:21:09 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.017258	Loss2: 0.018575	 Dis: 2.720751 Entropy: 6.250100 
[2022-10-02 17:21:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.002737	 E_lr_2:0.002737
[2022-10-02 17:21:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 17:21:14 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.022895	Loss2: 0.021371	 Dis: 4.337500 Entropy: 5.489760 
[2022-10-02 17:21:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.002659	 E_lr_2:0.002659
[2022-10-02 17:21:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 17:21:20 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.027845	Loss2: 0.033211	 Dis: 2.832035 Entropy: 4.673029 
[2022-10-02 17:21:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.002581	 E_lr_2:0.002581
[2022-10-02 17:21:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000026	 C_lr_2:0.000026	 C_lr_3:0.000026	 C_lr_4:0.000026
[2022-10-02 17:21:26 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.113761	Loss2: 0.117859	 Dis: 1.309082 Entropy: 5.586695 
[2022-10-02 17:21:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.002502	 E_lr_2:0.002502
[2022-10-02 17:21:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 17:21:32 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.019010	Loss2: 0.013839	 Dis: 2.240963 Entropy: 5.981744 
[2022-10-02 17:21:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.002423	 E_lr_2:0.002423
[2022-10-02 17:21:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000024	 C_lr_2:0.000024	 C_lr_3:0.000024	 C_lr_4:0.000024
[2022-10-02 17:21:38 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.024336	Loss2: 0.027531	 Dis: 2.947382 Entropy: 5.235146 
[2022-10-02 17:21:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.002345	 E_lr_2:0.002345
[2022-10-02 17:21:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 17:21:44 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.009986	Loss2: 0.009319	 Dis: 3.175331 Entropy: 4.658366 
[2022-10-02 17:21:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.002267	 E_lr_2:0.002267
[2022-10-02 17:21:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 17:21:50 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.005778	Loss2: 0.009182	 Dis: 2.531462 Entropy: 5.836412 
[2022-10-02 17:21:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.002189	 E_lr_2:0.002189
[2022-10-02 17:21:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000022	 C_lr_2:0.000022	 C_lr_3:0.000022	 C_lr_4:0.000022
[2022-10-02 17:21:56 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.024194	Loss2: 0.024737	 Dis: 3.529161 Entropy: 4.581689 
[2022-10-02 17:21:56 demo] (houston_program2.py 512): INFO E_lr_1: 0.002111	 E_lr_2:0.002111
[2022-10-02 17:21:56 demo] (houston_program2.py 513): INFO C_lr_1: 0.000021	 C_lr_2:0.000021	 C_lr_3:0.000021	 C_lr_4:0.000021
[2022-10-02 17:22:02 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.011467	Loss2: 0.012579	 Dis: 1.651104 Entropy: 5.679435 
[2022-10-02 17:22:02 demo] (houston_program2.py 512): INFO E_lr_1: 0.002034	 E_lr_2:0.002034
[2022-10-02 17:22:02 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 17:22:08 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.013664	Loss2: 0.016221	 Dis: 3.059280 Entropy: 4.680632 
[2022-10-02 17:22:08 demo] (houston_program2.py 512): INFO E_lr_1: 0.001957	 E_lr_2:0.001957
[2022-10-02 17:22:08 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 17:22:14 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.011066	Loss2: 0.013782	 Dis: 4.531452 Entropy: 4.718410 
[2022-10-02 17:22:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.001880	 E_lr_2:0.001880
[2022-10-02 17:22:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000019	 C_lr_2:0.000019	 C_lr_3:0.000019	 C_lr_4:0.000019
[2022-10-02 17:22:19 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.006034	Loss2: 0.008701	 Dis: 2.767014 Entropy: 4.587687 
[2022-10-02 17:22:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.001804	 E_lr_2:0.001804
[2022-10-02 17:22:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000018	 C_lr_2:0.000018	 C_lr_3:0.000018	 C_lr_4:0.000018
[2022-10-02 17:22:25 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.063524	Loss2: 0.064283	 Dis: 2.763573 Entropy: 5.000968 
[2022-10-02 17:22:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.001729	 E_lr_2:0.001729
[2022-10-02 17:22:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 17:22:31 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.019497	Loss2: 0.017495	 Dis: 1.828810 Entropy: 6.250355 
[2022-10-02 17:22:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.001655	 E_lr_2:0.001655
[2022-10-02 17:22:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 17:22:37 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.025524	Loss2: 0.018663	 Dis: 2.730894 Entropy: 6.503716 
[2022-10-02 17:22:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.001582	 E_lr_2:0.001582
[2022-10-02 17:22:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000016	 C_lr_2:0.000016	 C_lr_3:0.000016	 C_lr_4:0.000016
[2022-10-02 17:22:43 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.014490	Loss2: 0.013694	 Dis: 1.834702 Entropy: 5.550614 
[2022-10-02 17:22:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.001509	 E_lr_2:0.001509
[2022-10-02 17:22:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 17:22:48 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.017737	Loss2: 0.019549	 Dis: 2.388952 Entropy: 5.873142 
[2022-10-02 17:22:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.001437	 E_lr_2:0.001437
[2022-10-02 17:22:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 17:22:55 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.011777	Loss2: 0.012195	 Dis: 1.967331 Entropy: 5.686124 
[2022-10-02 17:22:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.001367	 E_lr_2:0.001367
[2022-10-02 17:22:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 17:23:01 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.036078	Loss2: 0.026559	 Dis: 2.161289 Entropy: 4.653957 
[2022-10-02 17:23:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.001297	 E_lr_2:0.001297
[2022-10-02 17:23:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000013	 C_lr_2:0.000013	 C_lr_3:0.000013	 C_lr_4:0.000013
[2022-10-02 17:23:07 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.004522	Loss2: 0.004628	 Dis: 2.311794 Entropy: 5.473154 
[2022-10-02 17:23:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.001229	 E_lr_2:0.001229
[2022-10-02 17:23:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 17:23:13 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.037041	Loss2: 0.026249	 Dis: 2.096954 Entropy: 4.423578 
[2022-10-02 17:23:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.001162	 E_lr_2:0.001162
[2022-10-02 17:23:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 17:23:19 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.003969	Loss2: 0.003443	 Dis: 1.184494 Entropy: 5.238314 
[2022-10-02 17:23:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.001096	 E_lr_2:0.001096
[2022-10-02 17:23:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000011	 C_lr_2:0.000011	 C_lr_3:0.000011	 C_lr_4:0.000011
[2022-10-02 17:23:25 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.010729	Loss2: 0.007922	 Dis: 1.449333 Entropy: 4.743816 
[2022-10-02 17:23:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.001032	 E_lr_2:0.001032
[2022-10-02 17:23:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 17:23:31 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.020257	Loss2: 0.017884	 Dis: 1.009640 Entropy: 5.839334 
[2022-10-02 17:23:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.000969	 E_lr_2:0.000969
[2022-10-02 17:23:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 17:23:37 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.011427	Loss2: 0.006765	 Dis: 1.185642 Entropy: 5.608625 
[2022-10-02 17:23:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.000908	 E_lr_2:0.000908
[2022-10-02 17:23:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000009	 C_lr_2:0.000009	 C_lr_3:0.000009	 C_lr_4:0.000009
[2022-10-02 17:23:43 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.002578	Loss2: 0.001670	 Dis: 0.820749 Entropy: 5.450478 
[2022-10-02 17:23:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.000848	 E_lr_2:0.000848
[2022-10-02 17:23:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 17:23:48 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.027188	Loss2: 0.031439	 Dis: 1.909716 Entropy: 4.834771 
[2022-10-02 17:23:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.000790	 E_lr_2:0.000790
[2022-10-02 17:23:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 17:23:54 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.004872	Loss2: 0.005813	 Dis: 0.673485 Entropy: 4.672073 
[2022-10-02 17:23:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.000734	 E_lr_2:0.000734
[2022-10-02 17:23:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 17:24:00 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.002734	Loss2: 0.002540	 Dis: 2.627523 Entropy: 5.311854 
[2022-10-02 17:24:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.000679	 E_lr_2:0.000679
[2022-10-02 17:24:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 17:24:06 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.002856	Loss2: 0.002873	 Dis: 1.627819 Entropy: 5.616624 
[2022-10-02 17:24:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.000626	 E_lr_2:0.000626
[2022-10-02 17:24:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 17:24:12 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.005814	Loss2: 0.004606	 Dis: 2.306568 Entropy: 4.484171 
[2022-10-02 17:24:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.000575	 E_lr_2:0.000575
[2022-10-02 17:24:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 17:24:18 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.086972	Loss2: 0.091397	 Dis: 2.228451 Entropy: 5.229612 
[2022-10-02 17:24:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.000526	 E_lr_2:0.000526
[2022-10-02 17:24:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 17:24:23 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.004800	Loss2: 0.004352	 Dis: 2.900679 Entropy: 6.366484 
[2022-10-02 17:24:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.000479	 E_lr_2:0.000479
[2022-10-02 17:24:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 17:24:29 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.003444	Loss2: 0.003376	 Dis: 3.857531 Entropy: 4.157708 
[2022-10-02 17:24:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.000433	 E_lr_2:0.000433
[2022-10-02 17:24:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 17:24:35 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.154901	Loss2: 0.152505	 Dis: 2.753471 Entropy: 4.876222 
[2022-10-02 17:24:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.000390	 E_lr_2:0.000390
[2022-10-02 17:24:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 17:24:41 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.003503	Loss2: 0.003284	 Dis: 1.634481 Entropy: 5.307793 
[2022-10-02 17:24:41 demo] (houston_program2.py 512): INFO E_lr_1: 0.000349	 E_lr_2:0.000349
[2022-10-02 17:24:41 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 17:24:48 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.008460	Loss2: 0.006736	 Dis: 1.480598 Entropy: 5.588502 
[2022-10-02 17:24:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.000310	 E_lr_2:0.000310
[2022-10-02 17:24:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 17:24:54 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.002740	Loss2: 0.003198	 Dis: 2.640268 Entropy: 4.903635 
[2022-10-02 17:24:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.000273	 E_lr_2:0.000273
[2022-10-02 17:24:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 17:25:00 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.001762	Loss2: 0.001564	 Dis: 1.348560 Entropy: 5.444395 
[2022-10-02 17:25:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.000239	 E_lr_2:0.000239
[2022-10-02 17:25:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 17:25:06 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.004528	Loss2: 0.008280	 Dis: 2.198792 Entropy: 4.274841 
[2022-10-02 17:25:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.000206	 E_lr_2:0.000206
[2022-10-02 17:25:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 17:25:12 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.090450	Loss2: 0.028660	 Dis: 2.013943 Entropy: 4.280896 
[2022-10-02 17:25:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.000176	 E_lr_2:0.000176
[2022-10-02 17:25:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 17:25:17 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.004313	Loss2: 0.003698	 Dis: 0.381817 Entropy: 4.667247 
[2022-10-02 17:25:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.000148	 E_lr_2:0.000148
[2022-10-02 17:25:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:25:24 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.004418	Loss2: 0.002929	 Dis: 2.962822 Entropy: 4.293458 
[2022-10-02 17:25:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.000123	 E_lr_2:0.000123
[2022-10-02 17:25:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:25:30 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.002633	Loss2: 0.002794	 Dis: 1.490173 Entropy: 4.940536 
[2022-10-02 17:25:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.000100	 E_lr_2:0.000100
[2022-10-02 17:25:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:25:36 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.004115	Loss2: 0.003921	 Dis: 1.730228 Entropy: 4.343441 
[2022-10-02 17:25:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.000079	 E_lr_2:0.000079
[2022-10-02 17:25:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:25:42 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.002132	Loss2: 0.002164	 Dis: 2.433540 Entropy: 6.061501 
[2022-10-02 17:25:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.000061	 E_lr_2:0.000061
[2022-10-02 17:25:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:25:48 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.002456	Loss2: 0.002320	 Dis: 0.815653 Entropy: 4.438991 
[2022-10-02 17:25:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.000045	 E_lr_2:0.000045
[2022-10-02 17:25:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:25:54 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.001808	Loss2: 0.001916	 Dis: 2.735748 Entropy: 5.076406 
[2022-10-02 17:25:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.000031	 E_lr_2:0.000031
[2022-10-02 17:25:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:26:00 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.002251	Loss2: 0.002275	 Dis: 1.782030 Entropy: 4.724050 
[2022-10-02 17:26:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.000020	 E_lr_2:0.000020
[2022-10-02 17:26:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:26:06 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.008820	Loss2: 0.004377	 Dis: 0.865078 Entropy: 5.610384 
[2022-10-02 17:26:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.000011	 E_lr_2:0.000011
[2022-10-02 17:26:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:26:12 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.004054	Loss2: 0.003622	 Dis: 1.411522 Entropy: 5.241635 
[2022-10-02 17:26:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.000005	 E_lr_2:0.000005
[2022-10-02 17:26:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:26:18 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.002443	Loss2: 0.002532	 Dis: 1.687874 Entropy: 6.031449 
[2022-10-02 17:26:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.000001	 E_lr_2:0.000001
[2022-10-02 17:26:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:26:24 demo] (houston_program2.py 504): INFO Train Ep: 15 	Loss1: 0.002899	Loss2: 0.002716	 Dis: 1.792196 Entropy: 4.530048 
[2022-10-02 17:26:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.000000	 E_lr_2:0.000000
[2022-10-02 17:26:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:26:24 demo] (houston_program2.py 515): INFO time_15_epoch:601.2963392734528
[2022-10-02 17:26:32 demo] (houston_program2.py 673): INFO 	val_Accuracy: 34007/53200 (63.92%)	
[2022-10-02 17:26:32 demo] (utils.py 47): INFO outputs/demo/finetune_100/ckpt_epoch_15.pth saving......
[2022-10-02 17:26:32 demo] (utils.py 49): INFO outputs/demo/finetune_100/ckpt_epoch_15.pth saved !!!
[2022-10-02 17:26:33 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0126 (0.0126)	loss 0.0173 (0.0173)	grad_norm 0.0956 (0.0956)	mem 460MB
[2022-10-02 17:26:33 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:33 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0196 (0.0196)	loss 0.0149 (0.0149)	grad_norm 0.0398 (0.0398)	mem 460MB
[2022-10-02 17:26:33 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:33 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0264 (0.0264)	loss 0.0191 (0.0191)	grad_norm 0.0756 (0.0756)	mem 460MB
[2022-10-02 17:26:33 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:33 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0197 (0.0197)	loss 0.0144 (0.0144)	grad_norm 0.0760 (0.0760)	mem 460MB
[2022-10-02 17:26:33 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:33 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0090 (0.0090)	loss 0.0118 (0.0118)	grad_norm 0.0440 (0.0440)	mem 460MB
[2022-10-02 17:26:33 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:34 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0218 (0.0218)	loss 0.0155 (0.0155)	grad_norm 0.0773 (0.0773)	mem 460MB
[2022-10-02 17:26:34 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:34 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0113 (0.0113)	loss 0.0178 (0.0178)	grad_norm 0.0684 (0.0684)	mem 460MB
[2022-10-02 17:26:34 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:34 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0247 (0.0247)	loss 0.0154 (0.0154)	grad_norm 0.0502 (0.0502)	mem 460MB
[2022-10-02 17:26:34 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:34 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0195 (0.0195)	loss 0.0151 (0.0151)	grad_norm 0.0644 (0.0644)	mem 460MB
[2022-10-02 17:26:34 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:34 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0233 (0.0233)	loss 0.0164 (0.0164)	grad_norm 0.0578 (0.0578)	mem 460MB
[2022-10-02 17:26:34 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:34 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0164 (0.0164)	loss 0.0163 (0.0163)	grad_norm 0.0384 (0.0384)	mem 460MB
[2022-10-02 17:26:34 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:35 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0199 (0.0199)	loss 0.0162 (0.0162)	grad_norm 0.0641 (0.0641)	mem 460MB
[2022-10-02 17:26:35 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:35 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0242 (0.0242)	loss 0.0189 (0.0189)	grad_norm 0.0547 (0.0547)	mem 460MB
[2022-10-02 17:26:35 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:35 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0146 (0.0146)	loss 0.0170 (0.0170)	grad_norm 0.0679 (0.0679)	mem 460MB
[2022-10-02 17:26:35 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:35 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0227 (0.0227)	loss 0.0213 (0.0213)	grad_norm 0.0911 (0.0911)	mem 460MB
[2022-10-02 17:26:35 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:35 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0240 (0.0240)	loss 0.0125 (0.0125)	grad_norm 0.0468 (0.0468)	mem 460MB
[2022-10-02 17:26:35 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:36 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0265 (0.0265)	loss 0.0183 (0.0183)	grad_norm 0.0833 (0.0833)	mem 460MB
[2022-10-02 17:26:36 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:36 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0239 (0.0239)	loss 0.0164 (0.0164)	grad_norm 0.0642 (0.0642)	mem 460MB
[2022-10-02 17:26:36 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:36 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0134 (0.0134)	loss 0.0143 (0.0143)	grad_norm 0.0722 (0.0722)	mem 460MB
[2022-10-02 17:26:36 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:36 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0187 (0.0187)	loss 0.0151 (0.0151)	grad_norm 0.0814 (0.0814)	mem 460MB
[2022-10-02 17:26:36 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:36 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0179 (0.0179)	loss 0.0149 (0.0149)	grad_norm 0.0380 (0.0380)	mem 460MB
[2022-10-02 17:26:36 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:37 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0230 (0.0230)	loss 0.0164 (0.0164)	grad_norm 0.0804 (0.0804)	mem 460MB
[2022-10-02 17:26:37 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:37 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0149 (0.0149)	loss 0.0133 (0.0133)	grad_norm 0.0648 (0.0648)	mem 460MB
[2022-10-02 17:26:37 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:37 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0203 (0.0203)	loss 0.0134 (0.0134)	grad_norm 0.0615 (0.0615)	mem 460MB
[2022-10-02 17:26:37 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:37 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0203 (0.0203)	loss 0.0173 (0.0173)	grad_norm 0.0797 (0.0797)	mem 460MB
[2022-10-02 17:26:37 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:37 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0208 (0.0208)	loss 0.0155 (0.0155)	grad_norm 0.0573 (0.0573)	mem 460MB
[2022-10-02 17:26:37 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:37 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0209 (0.0209)	loss 0.0139 (0.0139)	grad_norm 0.0755 (0.0755)	mem 460MB
[2022-10-02 17:26:37 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:38 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0216 (0.0216)	loss 0.0176 (0.0176)	grad_norm 0.0820 (0.0820)	mem 460MB
[2022-10-02 17:26:38 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:38 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0215 (0.0215)	loss 0.0158 (0.0158)	grad_norm 0.0360 (0.0360)	mem 460MB
[2022-10-02 17:26:38 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:38 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0218 (0.0218)	loss 0.0139 (0.0139)	grad_norm 0.0933 (0.0933)	mem 460MB
[2022-10-02 17:26:38 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:38 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0209 (0.0209)	loss 0.0120 (0.0120)	grad_norm 0.0817 (0.0817)	mem 460MB
[2022-10-02 17:26:38 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:38 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0207 (0.0207)	loss 0.0167 (0.0167)	grad_norm 0.0458 (0.0458)	mem 460MB
[2022-10-02 17:26:38 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:39 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0257 (0.0257)	loss 0.0182 (0.0182)	grad_norm 0.0874 (0.0874)	mem 460MB
[2022-10-02 17:26:39 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:39 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0251 (0.0251)	loss 0.0185 (0.0185)	grad_norm 0.0448 (0.0448)	mem 460MB
[2022-10-02 17:26:39 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:39 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0210 (0.0210)	loss 0.0155 (0.0155)	grad_norm 0.0438 (0.0438)	mem 460MB
[2022-10-02 17:26:39 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:39 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0122 (0.0122)	loss 0.0128 (0.0128)	grad_norm 0.0625 (0.0625)	mem 460MB
[2022-10-02 17:26:39 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:39 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0214 (0.0214)	loss 0.0159 (0.0159)	grad_norm 0.0542 (0.0542)	mem 460MB
[2022-10-02 17:26:39 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:39 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0103 (0.0103)	loss 0.0152 (0.0152)	grad_norm 0.0544 (0.0544)	mem 460MB
[2022-10-02 17:26:39 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:40 demo] (houston_program2.py 243): INFO Train: [16/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0206 (0.0206)	loss 0.0155 (0.0155)	grad_norm 0.0641 (0.0641)	mem 460MB
[2022-10-02 17:26:40 demo] (houston_program2.py 252): INFO EPOCH 16 training takes 0:00:00
[2022-10-02 17:26:40 demo] (houston_program2.py 333): INFO Train Ep: 16 	Loss1: 0.127140	Loss2: 0.125801	 Dis: 7.210159 Entropy: 4.510955 
[2022-10-02 17:26:40 demo] (houston_program2.py 335): INFO time_16_epoch:7.555436134338379
[2022-10-02 17:26:40 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0115 (0.0115)	loss 0.0121 (0.0121)	grad_norm 0.0484 (0.0484)	mem 460MB
[2022-10-02 17:26:40 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:40 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0223 (0.0223)	loss 0.0175 (0.0175)	grad_norm 0.0498 (0.0498)	mem 460MB
[2022-10-02 17:26:40 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:41 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000047	time 0.0220 (0.0220)	loss 0.0145 (0.0145)	grad_norm 0.0590 (0.0590)	mem 460MB
[2022-10-02 17:26:41 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:41 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0172 (0.0172)	loss 0.0129 (0.0129)	grad_norm 0.0468 (0.0468)	mem 460MB
[2022-10-02 17:26:41 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:41 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0214 (0.0214)	loss 0.0132 (0.0132)	grad_norm 0.0565 (0.0565)	mem 460MB
[2022-10-02 17:26:41 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:41 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0227 (0.0227)	loss 0.0173 (0.0173)	grad_norm 0.0612 (0.0612)	mem 460MB
[2022-10-02 17:26:41 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:41 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0218 (0.0218)	loss 0.0115 (0.0115)	grad_norm 0.0364 (0.0364)	mem 460MB
[2022-10-02 17:26:41 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:42 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0214 (0.0214)	loss 0.0179 (0.0179)	grad_norm 0.0605 (0.0605)	mem 460MB
[2022-10-02 17:26:42 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:42 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0260 (0.0260)	loss 0.0120 (0.0120)	grad_norm 0.0476 (0.0476)	mem 460MB
[2022-10-02 17:26:42 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:42 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0233 (0.0233)	loss 0.0164 (0.0164)	grad_norm 0.0526 (0.0526)	mem 460MB
[2022-10-02 17:26:42 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:42 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0156 (0.0156)	loss 0.0144 (0.0144)	grad_norm 0.0684 (0.0684)	mem 460MB
[2022-10-02 17:26:42 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:42 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0217 (0.0217)	loss 0.0119 (0.0119)	grad_norm 0.0622 (0.0622)	mem 460MB
[2022-10-02 17:26:42 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:42 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0161 (0.0161)	loss 0.0146 (0.0146)	grad_norm 0.0455 (0.0455)	mem 460MB
[2022-10-02 17:26:42 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:43 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0217 (0.0217)	loss 0.0136 (0.0136)	grad_norm 0.0545 (0.0545)	mem 460MB
[2022-10-02 17:26:43 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:43 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0202 (0.0202)	loss 0.0129 (0.0129)	grad_norm 0.0458 (0.0458)	mem 460MB
[2022-10-02 17:26:43 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:43 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0205 (0.0205)	loss 0.0134 (0.0134)	grad_norm 0.0455 (0.0455)	mem 460MB
[2022-10-02 17:26:43 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:43 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0218 (0.0218)	loss 0.0122 (0.0122)	grad_norm 0.0567 (0.0567)	mem 460MB
[2022-10-02 17:26:43 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:43 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0201 (0.0201)	loss 0.0131 (0.0131)	grad_norm 0.0438 (0.0438)	mem 460MB
[2022-10-02 17:26:43 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:44 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0204 (0.0204)	loss 0.0141 (0.0141)	grad_norm 0.0336 (0.0336)	mem 460MB
[2022-10-02 17:26:44 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:44 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0234 (0.0234)	loss 0.0154 (0.0154)	grad_norm 0.0470 (0.0470)	mem 460MB
[2022-10-02 17:26:44 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:44 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0194 (0.0194)	loss 0.0176 (0.0176)	grad_norm 0.0430 (0.0430)	mem 460MB
[2022-10-02 17:26:44 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:44 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0107 (0.0107)	loss 0.0180 (0.0180)	grad_norm 0.0639 (0.0639)	mem 460MB
[2022-10-02 17:26:44 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:44 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0217 (0.0217)	loss 0.0112 (0.0112)	grad_norm 0.0409 (0.0409)	mem 460MB
[2022-10-02 17:26:44 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:44 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0114 (0.0114)	loss 0.0156 (0.0156)	grad_norm 0.0537 (0.0537)	mem 460MB
[2022-10-02 17:26:44 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:45 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0218 (0.0218)	loss 0.0154 (0.0154)	grad_norm 0.0627 (0.0627)	mem 460MB
[2022-10-02 17:26:45 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:45 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0202 (0.0202)	loss 0.0123 (0.0123)	grad_norm 0.0465 (0.0465)	mem 460MB
[2022-10-02 17:26:45 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:45 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0240 (0.0240)	loss 0.0151 (0.0151)	grad_norm 0.0392 (0.0392)	mem 460MB
[2022-10-02 17:26:45 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:45 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0263 (0.0263)	loss 0.0155 (0.0155)	grad_norm 0.0570 (0.0570)	mem 460MB
[2022-10-02 17:26:45 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:45 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0277 (0.0277)	loss 0.0122 (0.0122)	grad_norm 0.0600 (0.0600)	mem 460MB
[2022-10-02 17:26:45 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:46 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0283 (0.0283)	loss 0.0237 (0.0237)	grad_norm 0.0575 (0.0575)	mem 460MB
[2022-10-02 17:26:46 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:46 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0125 (0.0125)	loss 0.0146 (0.0146)	grad_norm 0.0654 (0.0654)	mem 460MB
[2022-10-02 17:26:46 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:46 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0220 (0.0220)	loss 0.0106 (0.0106)	grad_norm 0.0369 (0.0369)	mem 460MB
[2022-10-02 17:26:46 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:46 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0135 (0.0135)	loss 0.0137 (0.0137)	grad_norm 0.0378 (0.0378)	mem 460MB
[2022-10-02 17:26:46 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:46 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0242 (0.0242)	loss 0.0146 (0.0146)	grad_norm 0.0554 (0.0554)	mem 460MB
[2022-10-02 17:26:46 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:46 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0228 (0.0228)	loss 0.0150 (0.0150)	grad_norm 0.0413 (0.0413)	mem 460MB
[2022-10-02 17:26:47 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:47 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0285 (0.0285)	loss 0.0123 (0.0123)	grad_norm 0.0455 (0.0455)	mem 460MB
[2022-10-02 17:26:47 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:47 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0241 (0.0241)	loss 0.0157 (0.0157)	grad_norm 0.0685 (0.0685)	mem 460MB
[2022-10-02 17:26:47 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:47 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0168 (0.0168)	loss 0.0163 (0.0163)	grad_norm 0.0371 (0.0371)	mem 460MB
[2022-10-02 17:26:47 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:47 demo] (houston_program2.py 243): INFO Train: [17/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0239 (0.0239)	loss 0.0140 (0.0140)	grad_norm 0.0534 (0.0534)	mem 460MB
[2022-10-02 17:26:47 demo] (houston_program2.py 252): INFO EPOCH 17 training takes 0:00:00
[2022-10-02 17:26:48 demo] (houston_program2.py 333): INFO Train Ep: 17 	Loss1: 0.381477	Loss2: 0.339879	 Dis: 7.940081 Entropy: 5.120812 
[2022-10-02 17:26:48 demo] (houston_program2.py 335): INFO time_17_epoch:7.746125221252441
[2022-10-02 17:26:48 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0124 (0.0124)	loss 0.0147 (0.0147)	grad_norm 0.0604 (0.0604)	mem 460MB
[2022-10-02 17:26:48 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:48 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0248 (0.0248)	loss 0.0163 (0.0163)	grad_norm 0.0510 (0.0510)	mem 460MB
[2022-10-02 17:26:48 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:48 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0237 (0.0237)	loss 0.0137 (0.0137)	grad_norm 0.0644 (0.0644)	mem 460MB
[2022-10-02 17:26:48 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:49 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0195 (0.0195)	loss 0.0143 (0.0143)	grad_norm 0.0456 (0.0456)	mem 460MB
[2022-10-02 17:26:49 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:49 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0140 (0.0140)	loss 0.0154 (0.0154)	grad_norm 0.0464 (0.0464)	mem 460MB
[2022-10-02 17:26:49 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:49 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0214 (0.0214)	loss 0.0168 (0.0168)	grad_norm 0.0603 (0.0603)	mem 460MB
[2022-10-02 17:26:49 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:49 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0140 (0.0140)	loss 0.0124 (0.0124)	grad_norm 0.0547 (0.0547)	mem 460MB
[2022-10-02 17:26:49 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:49 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0217 (0.0217)	loss 0.0118 (0.0118)	grad_norm 0.0890 (0.0890)	mem 460MB
[2022-10-02 17:26:49 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:49 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0244 (0.0244)	loss 0.0160 (0.0160)	grad_norm 0.0723 (0.0723)	mem 460MB
[2022-10-02 17:26:49 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:50 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0246 (0.0246)	loss 0.0144 (0.0144)	grad_norm 0.0490 (0.0490)	mem 460MB
[2022-10-02 17:26:50 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:50 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0203 (0.0203)	loss 0.0177 (0.0177)	grad_norm 0.0889 (0.0889)	mem 460MB
[2022-10-02 17:26:50 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:50 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0233 (0.0233)	loss 0.0141 (0.0141)	grad_norm 0.0651 (0.0651)	mem 460MB
[2022-10-02 17:26:50 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:50 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0211 (0.0211)	loss 0.0140 (0.0140)	grad_norm 0.0794 (0.0794)	mem 460MB
[2022-10-02 17:26:50 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:50 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0111 (0.0111)	loss 0.0157 (0.0157)	grad_norm 0.0596 (0.0596)	mem 460MB
[2022-10-02 17:26:50 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:51 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0201 (0.0201)	loss 0.0141 (0.0141)	grad_norm 0.0439 (0.0439)	mem 460MB
[2022-10-02 17:26:51 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:51 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0206 (0.0206)	loss 0.0120 (0.0120)	grad_norm 0.0902 (0.0902)	mem 460MB
[2022-10-02 17:26:51 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:51 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0244 (0.0244)	loss 0.0161 (0.0161)	grad_norm 0.0563 (0.0563)	mem 460MB
[2022-10-02 17:26:51 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:51 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0247 (0.0247)	loss 0.0146 (0.0146)	grad_norm 0.0641 (0.0641)	mem 460MB
[2022-10-02 17:26:51 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:51 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0125 (0.0125)	loss 0.0129 (0.0129)	grad_norm 0.0720 (0.0720)	mem 460MB
[2022-10-02 17:26:51 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:52 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0244 (0.0244)	loss 0.0157 (0.0157)	grad_norm 0.0597 (0.0597)	mem 460MB
[2022-10-02 17:26:52 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:52 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0217 (0.0217)	loss 0.0113 (0.0113)	grad_norm 0.0393 (0.0393)	mem 460MB
[2022-10-02 17:26:52 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:52 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0246 (0.0246)	loss 0.0170 (0.0170)	grad_norm 0.0880 (0.0880)	mem 460MB
[2022-10-02 17:26:52 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:52 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0245 (0.0245)	loss 0.0109 (0.0109)	grad_norm 0.0567 (0.0567)	mem 460MB
[2022-10-02 17:26:52 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:52 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0143 (0.0143)	loss 0.0133 (0.0133)	grad_norm 0.0775 (0.0775)	mem 460MB
[2022-10-02 17:26:52 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:53 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0219 (0.0219)	loss 0.0187 (0.0187)	grad_norm 0.0858 (0.0858)	mem 460MB
[2022-10-02 17:26:53 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:53 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0141 (0.0141)	loss 0.0139 (0.0139)	grad_norm 0.0452 (0.0452)	mem 460MB
[2022-10-02 17:26:53 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:53 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0215 (0.0215)	loss 0.0184 (0.0184)	grad_norm 0.0609 (0.0609)	mem 460MB
[2022-10-02 17:26:53 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:53 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0209 (0.0209)	loss 0.0121 (0.0121)	grad_norm 0.0721 (0.0721)	mem 460MB
[2022-10-02 17:26:53 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:53 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0220 (0.0220)	loss 0.0153 (0.0153)	grad_norm 0.0612 (0.0612)	mem 460MB
[2022-10-02 17:26:53 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:53 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0208 (0.0208)	loss 0.0110 (0.0110)	grad_norm 0.0478 (0.0478)	mem 460MB
[2022-10-02 17:26:54 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:54 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0201 (0.0201)	loss 0.0128 (0.0128)	grad_norm 0.0559 (0.0559)	mem 460MB
[2022-10-02 17:26:54 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:54 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0199 (0.0199)	loss 0.0145 (0.0145)	grad_norm 0.0450 (0.0450)	mem 460MB
[2022-10-02 17:26:54 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:54 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0224 (0.0224)	loss 0.0140 (0.0140)	grad_norm 0.0489 (0.0489)	mem 460MB
[2022-10-02 17:26:54 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:54 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0219 (0.0219)	loss 0.0158 (0.0158)	grad_norm 0.0717 (0.0717)	mem 460MB
[2022-10-02 17:26:54 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:54 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0121 (0.0121)	loss 0.0126 (0.0126)	grad_norm 0.0479 (0.0479)	mem 460MB
[2022-10-02 17:26:54 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:55 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0223 (0.0223)	loss 0.0132 (0.0132)	grad_norm 0.0725 (0.0725)	mem 460MB
[2022-10-02 17:26:55 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:55 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0163 (0.0163)	loss 0.0144 (0.0144)	grad_norm 0.0566 (0.0566)	mem 460MB
[2022-10-02 17:26:55 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:55 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0209 (0.0209)	loss 0.0125 (0.0125)	grad_norm 0.0658 (0.0658)	mem 460MB
[2022-10-02 17:26:55 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:55 demo] (houston_program2.py 243): INFO Train: [18/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0211 (0.0211)	loss 0.0152 (0.0152)	grad_norm 0.0801 (0.0801)	mem 460MB
[2022-10-02 17:26:55 demo] (houston_program2.py 252): INFO EPOCH 18 training takes 0:00:00
[2022-10-02 17:26:55 demo] (houston_program2.py 333): INFO Train Ep: 18 	Loss1: 0.310817	Loss2: 0.327083	 Dis: 6.859322 Entropy: 4.683466 
[2022-10-02 17:26:55 demo] (houston_program2.py 335): INFO time_18_epoch:7.792501211166382
[2022-10-02 17:26:56 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0110 (0.0110)	loss 0.0111 (0.0111)	grad_norm 0.0544 (0.0544)	mem 460MB
[2022-10-02 17:26:56 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:26:56 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0081 (0.0081)	loss 0.0146 (0.0146)	grad_norm 0.0527 (0.0527)	mem 460MB
[2022-10-02 17:26:56 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:26:56 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0085 (0.0085)	loss 0.0113 (0.0113)	grad_norm 0.0510 (0.0510)	mem 460MB
[2022-10-02 17:26:56 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:26:56 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0101 (0.0101)	loss 0.0135 (0.0135)	grad_norm 0.0881 (0.0881)	mem 460MB
[2022-10-02 17:26:56 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:26:56 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0080 (0.0080)	loss 0.0135 (0.0135)	grad_norm 0.0513 (0.0513)	mem 460MB
[2022-10-02 17:26:56 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:26:56 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0080 (0.0080)	loss 0.0103 (0.0103)	grad_norm 0.0625 (0.0625)	mem 460MB
[2022-10-02 17:26:56 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:26:56 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0079 (0.0079)	loss 0.0124 (0.0124)	grad_norm 0.0644 (0.0644)	mem 460MB
[2022-10-02 17:26:56 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:26:56 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0085 (0.0085)	loss 0.0150 (0.0150)	grad_norm 0.0336 (0.0336)	mem 460MB
[2022-10-02 17:26:56 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:26:56 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0192 (0.0192)	loss 0.0143 (0.0143)	grad_norm 0.0448 (0.0448)	mem 460MB
[2022-10-02 17:26:56 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:26:57 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0204 (0.0204)	loss 0.0136 (0.0136)	grad_norm 0.0881 (0.0881)	mem 460MB
[2022-10-02 17:26:57 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:26:57 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0189 (0.0189)	loss 0.0119 (0.0119)	grad_norm 0.0810 (0.0810)	mem 460MB
[2022-10-02 17:26:57 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:26:57 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0206 (0.0206)	loss 0.0123 (0.0123)	grad_norm 0.0631 (0.0631)	mem 460MB
[2022-10-02 17:26:57 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:26:57 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0230 (0.0230)	loss 0.0179 (0.0179)	grad_norm 0.0470 (0.0470)	mem 460MB
[2022-10-02 17:26:57 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:26:57 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0111 (0.0111)	loss 0.0122 (0.0122)	grad_norm 0.0768 (0.0768)	mem 460MB
[2022-10-02 17:26:57 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:26:58 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0216 (0.0216)	loss 0.0147 (0.0147)	grad_norm 0.0546 (0.0546)	mem 460MB
[2022-10-02 17:26:58 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:26:58 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0176 (0.0176)	loss 0.0177 (0.0177)	grad_norm 0.0442 (0.0442)	mem 460MB
[2022-10-02 17:26:58 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:26:58 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000046	time 0.0222 (0.0222)	loss 0.0150 (0.0150)	grad_norm 0.0628 (0.0628)	mem 460MB
[2022-10-02 17:26:58 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:26:58 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0160 (0.0160)	loss 0.0125 (0.0125)	grad_norm 0.0628 (0.0628)	mem 460MB
[2022-10-02 17:26:58 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:26:58 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0204 (0.0204)	loss 0.0139 (0.0139)	grad_norm 0.0488 (0.0488)	mem 460MB
[2022-10-02 17:26:58 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:26:58 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0207 (0.0207)	loss 0.0139 (0.0139)	grad_norm 0.0620 (0.0620)	mem 460MB
[2022-10-02 17:26:58 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:26:59 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0196 (0.0196)	loss 0.0127 (0.0127)	grad_norm 0.0566 (0.0566)	mem 460MB
[2022-10-02 17:26:59 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:26:59 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0210 (0.0210)	loss 0.0149 (0.0149)	grad_norm 0.0627 (0.0627)	mem 460MB
[2022-10-02 17:26:59 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:26:59 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0205 (0.0205)	loss 0.0168 (0.0168)	grad_norm 0.0586 (0.0586)	mem 460MB
[2022-10-02 17:26:59 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:26:59 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0206 (0.0206)	loss 0.0165 (0.0165)	grad_norm 0.0798 (0.0798)	mem 460MB
[2022-10-02 17:26:59 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:26:59 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0202 (0.0202)	loss 0.0170 (0.0170)	grad_norm 0.0446 (0.0446)	mem 460MB
[2022-10-02 17:26:59 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:27:00 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0183 (0.0183)	loss 0.0123 (0.0123)	grad_norm 0.0549 (0.0549)	mem 460MB
[2022-10-02 17:27:00 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:27:00 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0210 (0.0210)	loss 0.0122 (0.0122)	grad_norm 0.0689 (0.0689)	mem 460MB
[2022-10-02 17:27:00 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:27:00 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0217 (0.0217)	loss 0.0165 (0.0165)	grad_norm 0.0418 (0.0418)	mem 460MB
[2022-10-02 17:27:00 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:27:00 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0197 (0.0197)	loss 0.0151 (0.0151)	grad_norm 0.0826 (0.0826)	mem 460MB
[2022-10-02 17:27:00 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:27:00 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0208 (0.0208)	loss 0.0159 (0.0159)	grad_norm 0.0713 (0.0713)	mem 460MB
[2022-10-02 17:27:00 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:27:00 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0229 (0.0229)	loss 0.0177 (0.0177)	grad_norm 0.0803 (0.0803)	mem 460MB
[2022-10-02 17:27:00 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:27:01 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0186 (0.0186)	loss 0.0125 (0.0125)	grad_norm 0.0790 (0.0790)	mem 460MB
[2022-10-02 17:27:01 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:27:01 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0229 (0.0229)	loss 0.0130 (0.0130)	grad_norm 0.0635 (0.0635)	mem 460MB
[2022-10-02 17:27:01 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:27:01 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0210 (0.0210)	loss 0.0146 (0.0146)	grad_norm 0.0756 (0.0756)	mem 460MB
[2022-10-02 17:27:01 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:27:01 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0114 (0.0114)	loss 0.0186 (0.0186)	grad_norm 0.1066 (0.1066)	mem 460MB
[2022-10-02 17:27:01 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:27:01 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0221 (0.0221)	loss 0.0115 (0.0115)	grad_norm 0.0734 (0.0734)	mem 460MB
[2022-10-02 17:27:01 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:27:02 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0115 (0.0115)	loss 0.0110 (0.0110)	grad_norm 0.0737 (0.0737)	mem 460MB
[2022-10-02 17:27:02 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:27:02 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0210 (0.0210)	loss 0.0178 (0.0178)	grad_norm 0.0794 (0.0794)	mem 460MB
[2022-10-02 17:27:02 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:27:02 demo] (houston_program2.py 243): INFO Train: [19/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0197 (0.0197)	loss 0.0121 (0.0121)	grad_norm 0.0696 (0.0696)	mem 460MB
[2022-10-02 17:27:02 demo] (houston_program2.py 252): INFO EPOCH 19 training takes 0:00:00
[2022-10-02 17:27:02 demo] (houston_program2.py 333): INFO Train Ep: 19 	Loss1: 0.161031	Loss2: 0.153874	 Dis: 7.191610 Entropy: 4.497320 
[2022-10-02 17:27:02 demo] (houston_program2.py 335): INFO time_19_epoch:6.72725772857666
[2022-10-02 17:27:02 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0110 (0.0110)	loss 0.0158 (0.0158)	grad_norm 0.0364 (0.0364)	mem 460MB
[2022-10-02 17:27:02 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:03 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0217 (0.0217)	loss 0.0108 (0.0108)	grad_norm 0.0789 (0.0789)	mem 460MB
[2022-10-02 17:27:03 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:03 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0101 (0.0101)	loss 0.0190 (0.0190)	grad_norm 0.0606 (0.0606)	mem 460MB
[2022-10-02 17:27:03 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:03 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0220 (0.0220)	loss 0.0152 (0.0152)	grad_norm 0.0633 (0.0633)	mem 460MB
[2022-10-02 17:27:03 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:03 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0116 (0.0116)	loss 0.0176 (0.0176)	grad_norm 0.0472 (0.0472)	mem 460MB
[2022-10-02 17:27:03 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:03 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0221 (0.0221)	loss 0.0142 (0.0142)	grad_norm 0.0802 (0.0802)	mem 460MB
[2022-10-02 17:27:03 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:04 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0125 (0.0125)	loss 0.0151 (0.0151)	grad_norm 0.0653 (0.0653)	mem 460MB
[2022-10-02 17:27:04 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:04 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0205 (0.0205)	loss 0.0117 (0.0117)	grad_norm 0.0564 (0.0564)	mem 460MB
[2022-10-02 17:27:04 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:04 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0172 (0.0172)	loss 0.0166 (0.0166)	grad_norm 0.0620 (0.0620)	mem 460MB
[2022-10-02 17:27:04 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:04 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0216 (0.0216)	loss 0.0110 (0.0110)	grad_norm 0.0501 (0.0501)	mem 460MB
[2022-10-02 17:27:04 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:04 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0173 (0.0173)	loss 0.0126 (0.0126)	grad_norm 0.0531 (0.0531)	mem 460MB
[2022-10-02 17:27:04 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:04 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0219 (0.0219)	loss 0.0145 (0.0145)	grad_norm 0.0490 (0.0490)	mem 460MB
[2022-10-02 17:27:04 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:05 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0214 (0.0214)	loss 0.0129 (0.0129)	grad_norm 0.0490 (0.0490)	mem 460MB
[2022-10-02 17:27:05 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:05 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0209 (0.0209)	loss 0.0110 (0.0110)	grad_norm 0.0676 (0.0676)	mem 460MB
[2022-10-02 17:27:05 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:05 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0217 (0.0217)	loss 0.0185 (0.0185)	grad_norm 0.0450 (0.0450)	mem 460MB
[2022-10-02 17:27:05 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:05 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0212 (0.0212)	loss 0.0175 (0.0175)	grad_norm 0.0624 (0.0624)	mem 460MB
[2022-10-02 17:27:05 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:05 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0203 (0.0203)	loss 0.0132 (0.0132)	grad_norm 0.0856 (0.0856)	mem 460MB
[2022-10-02 17:27:05 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:06 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0189 (0.0189)	loss 0.0152 (0.0152)	grad_norm 0.0583 (0.0583)	mem 460MB
[2022-10-02 17:27:06 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:06 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0212 (0.0212)	loss 0.0134 (0.0134)	grad_norm 0.0495 (0.0495)	mem 460MB
[2022-10-02 17:27:06 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:06 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0127 (0.0127)	loss 0.0165 (0.0165)	grad_norm 0.0659 (0.0659)	mem 460MB
[2022-10-02 17:27:06 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:06 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0264 (0.0264)	loss 0.0150 (0.0150)	grad_norm 0.0666 (0.0666)	mem 460MB
[2022-10-02 17:27:06 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:06 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0164 (0.0164)	loss 0.0140 (0.0140)	grad_norm 0.0438 (0.0438)	mem 460MB
[2022-10-02 17:27:06 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:06 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0247 (0.0247)	loss 0.0161 (0.0161)	grad_norm 0.0447 (0.0447)	mem 460MB
[2022-10-02 17:27:06 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:07 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0206 (0.0206)	loss 0.0147 (0.0147)	grad_norm 0.0506 (0.0506)	mem 460MB
[2022-10-02 17:27:07 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:07 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0207 (0.0207)	loss 0.0167 (0.0167)	grad_norm 0.0551 (0.0551)	mem 460MB
[2022-10-02 17:27:07 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:07 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0204 (0.0204)	loss 0.0150 (0.0150)	grad_norm 0.0481 (0.0481)	mem 460MB
[2022-10-02 17:27:07 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:07 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0226 (0.0226)	loss 0.0117 (0.0117)	grad_norm 0.0611 (0.0611)	mem 460MB
[2022-10-02 17:27:07 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:07 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0218 (0.0218)	loss 0.0122 (0.0122)	grad_norm 0.0504 (0.0504)	mem 460MB
[2022-10-02 17:27:07 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:08 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0115 (0.0115)	loss 0.0122 (0.0122)	grad_norm 0.0482 (0.0482)	mem 460MB
[2022-10-02 17:27:08 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:08 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0214 (0.0214)	loss 0.0161 (0.0161)	grad_norm 0.0516 (0.0516)	mem 460MB
[2022-10-02 17:27:08 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:08 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0153 (0.0153)	loss 0.0134 (0.0134)	grad_norm 0.0429 (0.0429)	mem 460MB
[2022-10-02 17:27:08 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:08 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0207 (0.0207)	loss 0.0141 (0.0141)	grad_norm 0.0364 (0.0364)	mem 460MB
[2022-10-02 17:27:08 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:08 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0178 (0.0178)	loss 0.0161 (0.0161)	grad_norm 0.0483 (0.0483)	mem 460MB
[2022-10-02 17:27:08 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:08 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0210 (0.0210)	loss 0.0138 (0.0138)	grad_norm 0.0455 (0.0455)	mem 460MB
[2022-10-02 17:27:08 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:09 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0214 (0.0214)	loss 0.0137 (0.0137)	grad_norm 0.0576 (0.0576)	mem 460MB
[2022-10-02 17:27:09 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:09 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0209 (0.0209)	loss 0.0118 (0.0118)	grad_norm 0.0438 (0.0438)	mem 460MB
[2022-10-02 17:27:09 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:09 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0207 (0.0207)	loss 0.0154 (0.0154)	grad_norm 0.0478 (0.0478)	mem 460MB
[2022-10-02 17:27:09 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:09 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0211 (0.0211)	loss 0.0118 (0.0118)	grad_norm 0.0511 (0.0511)	mem 460MB
[2022-10-02 17:27:09 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:09 demo] (houston_program2.py 243): INFO Train: [20/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0219 (0.0219)	loss 0.0116 (0.0116)	grad_norm 0.0510 (0.0510)	mem 460MB
[2022-10-02 17:27:09 demo] (houston_program2.py 252): INFO EPOCH 20 training takes 0:00:00
[2022-10-02 17:27:10 demo] (houston_program2.py 333): INFO Train Ep: 20 	Loss1: 0.120951	Loss2: 0.130883	 Dis: 8.528280 Entropy: 5.496123 
[2022-10-02 17:27:10 demo] (houston_program2.py 335): INFO time_20_epoch:7.4162890911102295
[2022-10-02 17:27:10 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 17:27:10 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 17:27:10 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 17:27:10 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 17:27:10 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 17:27:10 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 17:27:10 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 17:27:10 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 17:27:10 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 17:27:10 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 17:27:16 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.164443	Loss2: 0.198742	 Dis: 6.823996 Entropy: 4.797452 
[2022-10-02 17:27:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.000487	 E_lr_2:0.000487
[2022-10-02 17:27:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 17:27:22 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.101319	Loss2: 0.090406	 Dis: 4.533522 Entropy: 4.425712 
[2022-10-02 17:27:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.000987	 E_lr_2:0.000987
[2022-10-02 17:27:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 17:27:28 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.156406	Loss2: 0.175449	 Dis: 7.536909 Entropy: 4.436859 
[2022-10-02 17:27:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.001487	 E_lr_2:0.001487
[2022-10-02 17:27:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 17:27:34 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.084625	Loss2: 0.082865	 Dis: 5.841734 Entropy: 4.972169 
[2022-10-02 17:27:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.001987	 E_lr_2:0.001987
[2022-10-02 17:27:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 17:27:40 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.166125	Loss2: 0.186511	 Dis: 5.228556 Entropy: 4.407256 
[2022-10-02 17:27:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.002487	 E_lr_2:0.002487
[2022-10-02 17:27:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 17:27:45 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.059389	Loss2: 0.059391	 Dis: 6.168200 Entropy: 4.888215 
[2022-10-02 17:27:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.002987	 E_lr_2:0.002987
[2022-10-02 17:27:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 17:27:51 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.179572	Loss2: 0.183573	 Dis: 5.659809 Entropy: 5.054683 
[2022-10-02 17:27:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.003487	 E_lr_2:0.003487
[2022-10-02 17:27:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 17:27:54 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.249114	Loss2: 0.244483	 Dis: 9.274414 Entropy: 4.657578 
[2022-10-02 17:27:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.003987	 E_lr_2:0.003987
[2022-10-02 17:27:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 17:28:00 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.133901	Loss2: 0.186160	 Dis: 6.770573 Entropy: 4.969313 
[2022-10-02 17:28:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.004487	 E_lr_2:0.004487
[2022-10-02 17:28:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 17:28:07 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.260093	Loss2: 0.260155	 Dis: 5.205879 Entropy: 5.526358 
[2022-10-02 17:28:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.004987	 E_lr_2:0.004987
[2022-10-02 17:28:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000050	 C_lr_2:0.000050	 C_lr_3:0.000050	 C_lr_4:0.000050
[2022-10-02 17:28:13 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.270954	Loss2: 0.261882	 Dis: 7.408588 Entropy: 5.285434 
[2022-10-02 17:28:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.004853	 E_lr_2:0.004853
[2022-10-02 17:28:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000049	 C_lr_2:0.000049	 C_lr_3:0.000049	 C_lr_4:0.000049
[2022-10-02 17:28:19 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.256842	Loss2: 0.259157	 Dis: 5.683918 Entropy: 5.695607 
[2022-10-02 17:28:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.004825	 E_lr_2:0.004825
[2022-10-02 17:28:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 17:28:23 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.031554	Loss2: 0.031359	 Dis: 6.384516 Entropy: 4.407253 
[2022-10-02 17:28:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.004795	 E_lr_2:0.004795
[2022-10-02 17:28:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 17:28:29 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.141682	Loss2: 0.141593	 Dis: 5.453499 Entropy: 4.777884 
[2022-10-02 17:28:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.004763	 E_lr_2:0.004763
[2022-10-02 17:28:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 17:28:35 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.019971	Loss2: 0.022249	 Dis: 7.177864 Entropy: 5.284728 
[2022-10-02 17:28:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.004728	 E_lr_2:0.004728
[2022-10-02 17:28:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 17:28:41 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.108240	Loss2: 0.088587	 Dis: 5.516163 Entropy: 4.580154 
[2022-10-02 17:28:41 demo] (houston_program2.py 512): INFO E_lr_1: 0.004692	 E_lr_2:0.004692
[2022-10-02 17:28:41 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 17:28:47 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.033064	Loss2: 0.031977	 Dis: 3.045076 Entropy: 5.180652 
[2022-10-02 17:28:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.004653	 E_lr_2:0.004653
[2022-10-02 17:28:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 17:28:53 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.080740	Loss2: 0.085746	 Dis: 4.821697 Entropy: 5.202456 
[2022-10-02 17:28:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.004612	 E_lr_2:0.004612
[2022-10-02 17:28:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 17:28:58 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.160708	Loss2: 0.136770	 Dis: 4.454346 Entropy: 5.662986 
[2022-10-02 17:28:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.004569	 E_lr_2:0.004569
[2022-10-02 17:28:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 17:29:04 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.247250	Loss2: 0.218683	 Dis: 5.915167 Entropy: 5.303074 
[2022-10-02 17:29:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.004524	 E_lr_2:0.004524
[2022-10-02 17:29:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 17:29:10 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.084078	Loss2: 0.079944	 Dis: 4.722580 Entropy: 4.664988 
[2022-10-02 17:29:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.004477	 E_lr_2:0.004477
[2022-10-02 17:29:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 17:29:16 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.357502	Loss2: 0.306977	 Dis: 3.426432 Entropy: 4.359662 
[2022-10-02 17:29:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.004428	 E_lr_2:0.004428
[2022-10-02 17:29:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 17:29:22 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.048375	Loss2: 0.040523	 Dis: 7.366592 Entropy: 4.957120 
[2022-10-02 17:29:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.004377	 E_lr_2:0.004377
[2022-10-02 17:29:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 17:29:28 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.081477	Loss2: 0.055845	 Dis: 4.787910 Entropy: 5.522232 
[2022-10-02 17:29:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.004324	 E_lr_2:0.004324
[2022-10-02 17:29:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 17:29:34 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.061952	Loss2: 0.071672	 Dis: 5.307859 Entropy: 5.726694 
[2022-10-02 17:29:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.004269	 E_lr_2:0.004269
[2022-10-02 17:29:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 17:29:41 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.108080	Loss2: 0.116902	 Dis: 4.455523 Entropy: 4.743578 
[2022-10-02 17:29:41 demo] (houston_program2.py 512): INFO E_lr_1: 0.004213	 E_lr_2:0.004213
[2022-10-02 17:29:41 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 17:29:47 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.113553	Loss2: 0.129264	 Dis: 3.451090 Entropy: 5.049071 
[2022-10-02 17:29:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.004155	 E_lr_2:0.004155
[2022-10-02 17:29:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 17:29:53 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.038971	Loss2: 0.033206	 Dis: 5.707127 Entropy: 4.417695 
[2022-10-02 17:29:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.004095	 E_lr_2:0.004095
[2022-10-02 17:29:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000041	 C_lr_2:0.000041	 C_lr_3:0.000041	 C_lr_4:0.000041
[2022-10-02 17:29:59 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.051987	Loss2: 0.046388	 Dis: 3.909374 Entropy: 4.434971 
[2022-10-02 17:29:59 demo] (houston_program2.py 512): INFO E_lr_1: 0.004034	 E_lr_2:0.004034
[2022-10-02 17:29:59 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 17:30:05 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.054002	Loss2: 0.060512	 Dis: 3.300877 Entropy: 6.109458 
[2022-10-02 17:30:05 demo] (houston_program2.py 512): INFO E_lr_1: 0.003971	 E_lr_2:0.003971
[2022-10-02 17:30:05 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 17:30:08 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.030594	Loss2: 0.029822	 Dis: 3.469728 Entropy: 5.494522 
[2022-10-02 17:30:08 demo] (houston_program2.py 512): INFO E_lr_1: 0.003907	 E_lr_2:0.003907
[2022-10-02 17:30:08 demo] (houston_program2.py 513): INFO C_lr_1: 0.000039	 C_lr_2:0.000039	 C_lr_3:0.000039	 C_lr_4:0.000039
[2022-10-02 17:30:14 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.062665	Loss2: 0.077072	 Dis: 4.287048 Entropy: 4.462464 
[2022-10-02 17:30:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.003841	 E_lr_2:0.003841
[2022-10-02 17:30:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 17:30:21 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.116226	Loss2: 0.125870	 Dis: 4.134892 Entropy: 4.703478 
[2022-10-02 17:30:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.003774	 E_lr_2:0.003774
[2022-10-02 17:30:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 17:30:26 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.102146	Loss2: 0.087559	 Dis: 2.262167 Entropy: 5.728406 
[2022-10-02 17:30:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.003706	 E_lr_2:0.003706
[2022-10-02 17:30:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000037	 C_lr_2:0.000037	 C_lr_3:0.000037	 C_lr_4:0.000037
[2022-10-02 17:30:33 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.036227	Loss2: 0.036639	 Dis: 3.888824 Entropy: 5.078947 
[2022-10-02 17:30:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.003637	 E_lr_2:0.003637
[2022-10-02 17:30:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 17:30:39 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.131825	Loss2: 0.096365	 Dis: 1.197021 Entropy: 4.967645 
[2022-10-02 17:30:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.003566	 E_lr_2:0.003566
[2022-10-02 17:30:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 17:30:45 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.015650	Loss2: 0.009554	 Dis: 2.689594 Entropy: 5.169728 
[2022-10-02 17:30:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.003495	 E_lr_2:0.003495
[2022-10-02 17:30:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 17:30:51 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.364529	Loss2: 0.360690	 Dis: 2.742172 Entropy: 5.484648 
[2022-10-02 17:30:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.003422	 E_lr_2:0.003422
[2022-10-02 17:30:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000034	 C_lr_2:0.000034	 C_lr_3:0.000034	 C_lr_4:0.000034
[2022-10-02 17:30:57 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.090411	Loss2: 0.087524	 Dis: 3.548761 Entropy: 4.265540 
[2022-10-02 17:30:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.003349	 E_lr_2:0.003349
[2022-10-02 17:30:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 17:31:03 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.563465	Loss2: 0.515568	 Dis: 8.716871 Entropy: 4.677738 
[2022-10-02 17:31:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.003274	 E_lr_2:0.003274
[2022-10-02 17:31:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 17:31:09 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.251142	Loss2: 0.249499	 Dis: 5.837498 Entropy: 4.511252 
[2022-10-02 17:31:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.003199	 E_lr_2:0.003199
[2022-10-02 17:31:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000032	 C_lr_2:0.000032	 C_lr_3:0.000032	 C_lr_4:0.000032
[2022-10-02 17:31:15 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.066989	Loss2: 0.054713	 Dis: 5.439838 Entropy: 5.352594 
[2022-10-02 17:31:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.003124	 E_lr_2:0.003124
[2022-10-02 17:31:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000031	 C_lr_2:0.000031	 C_lr_3:0.000031	 C_lr_4:0.000031
[2022-10-02 17:31:21 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.081847	Loss2: 0.060760	 Dis: 6.503323 Entropy: 5.005546 
[2022-10-02 17:31:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.003047	 E_lr_2:0.003047
[2022-10-02 17:31:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 17:31:27 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.153966	Loss2: 0.139084	 Dis: 5.588694 Entropy: 4.983978 
[2022-10-02 17:31:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.002970	 E_lr_2:0.002970
[2022-10-02 17:31:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 17:31:33 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.016738	Loss2: 0.016516	 Dis: 6.487413 Entropy: 5.443017 
[2022-10-02 17:31:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.002893	 E_lr_2:0.002893
[2022-10-02 17:31:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000029	 C_lr_2:0.000029	 C_lr_3:0.000029	 C_lr_4:0.000029
[2022-10-02 17:31:38 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.034839	Loss2: 0.036170	 Dis: 3.802200 Entropy: 5.204619 
[2022-10-02 17:31:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.002815	 E_lr_2:0.002815
[2022-10-02 17:31:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000028	 C_lr_2:0.000028	 C_lr_3:0.000028	 C_lr_4:0.000028
[2022-10-02 17:31:44 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.049614	Loss2: 0.060396	 Dis: 4.204937 Entropy: 4.632008 
[2022-10-02 17:31:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.002737	 E_lr_2:0.002737
[2022-10-02 17:31:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 17:31:50 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.152210	Loss2: 0.132879	 Dis: 5.649071 Entropy: 5.064398 
[2022-10-02 17:31:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.002659	 E_lr_2:0.002659
[2022-10-02 17:31:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 17:31:55 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.109601	Loss2: 0.126450	 Dis: 5.448719 Entropy: 5.584856 
[2022-10-02 17:31:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.002581	 E_lr_2:0.002581
[2022-10-02 17:31:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000026	 C_lr_2:0.000026	 C_lr_3:0.000026	 C_lr_4:0.000026
[2022-10-02 17:32:01 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.056090	Loss2: 0.031529	 Dis: 4.323681 Entropy: 4.592799 
[2022-10-02 17:32:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.002502	 E_lr_2:0.002502
[2022-10-02 17:32:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 17:32:07 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.147083	Loss2: 0.142193	 Dis: 4.619467 Entropy: 5.519667 
[2022-10-02 17:32:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.002423	 E_lr_2:0.002423
[2022-10-02 17:32:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000024	 C_lr_2:0.000024	 C_lr_3:0.000024	 C_lr_4:0.000024
[2022-10-02 17:32:14 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.287635	Loss2: 0.239144	 Dis: 3.687679 Entropy: 4.725546 
[2022-10-02 17:32:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.002345	 E_lr_2:0.002345
[2022-10-02 17:32:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 17:32:19 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.034156	Loss2: 0.053714	 Dis: 2.375433 Entropy: 5.336578 
[2022-10-02 17:32:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.002267	 E_lr_2:0.002267
[2022-10-02 17:32:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 17:32:25 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.094500	Loss2: 0.102451	 Dis: 4.866516 Entropy: 4.836093 
[2022-10-02 17:32:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.002189	 E_lr_2:0.002189
[2022-10-02 17:32:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000022	 C_lr_2:0.000022	 C_lr_3:0.000022	 C_lr_4:0.000022
[2022-10-02 17:32:32 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.059090	Loss2: 0.052827	 Dis: 3.855188 Entropy: 5.539134 
[2022-10-02 17:32:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.002111	 E_lr_2:0.002111
[2022-10-02 17:32:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000021	 C_lr_2:0.000021	 C_lr_3:0.000021	 C_lr_4:0.000021
[2022-10-02 17:32:38 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.177220	Loss2: 0.171867	 Dis: 5.017359 Entropy: 5.103574 
[2022-10-02 17:32:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.002034	 E_lr_2:0.002034
[2022-10-02 17:32:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 17:32:44 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.041692	Loss2: 0.037593	 Dis: 3.364065 Entropy: 4.973885 
[2022-10-02 17:32:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.001957	 E_lr_2:0.001957
[2022-10-02 17:32:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 17:32:51 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.053767	Loss2: 0.047312	 Dis: 3.915697 Entropy: 5.629146 
[2022-10-02 17:32:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.001880	 E_lr_2:0.001880
[2022-10-02 17:32:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000019	 C_lr_2:0.000019	 C_lr_3:0.000019	 C_lr_4:0.000019
[2022-10-02 17:32:57 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.011815	Loss2: 0.016247	 Dis: 2.004618 Entropy: 5.355808 
[2022-10-02 17:32:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.001804	 E_lr_2:0.001804
[2022-10-02 17:32:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000018	 C_lr_2:0.000018	 C_lr_3:0.000018	 C_lr_4:0.000018
[2022-10-02 17:33:03 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.049010	Loss2: 0.056527	 Dis: 2.483070 Entropy: 5.479088 
[2022-10-02 17:33:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.001729	 E_lr_2:0.001729
[2022-10-02 17:33:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 17:33:09 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.108486	Loss2: 0.096599	 Dis: 3.561863 Entropy: 4.644156 
[2022-10-02 17:33:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.001655	 E_lr_2:0.001655
[2022-10-02 17:33:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 17:33:15 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.075518	Loss2: 0.070524	 Dis: 3.969112 Entropy: 4.630338 
[2022-10-02 17:33:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.001582	 E_lr_2:0.001582
[2022-10-02 17:33:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000016	 C_lr_2:0.000016	 C_lr_3:0.000016	 C_lr_4:0.000016
[2022-10-02 17:33:21 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.022298	Loss2: 0.025284	 Dis: 2.147661 Entropy: 4.548999 
[2022-10-02 17:33:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.001509	 E_lr_2:0.001509
[2022-10-02 17:33:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 17:33:27 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.040678	Loss2: 0.053607	 Dis: 4.519552 Entropy: 4.683650 
[2022-10-02 17:33:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.001437	 E_lr_2:0.001437
[2022-10-02 17:33:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 17:33:33 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.021333	Loss2: 0.026170	 Dis: 3.175592 Entropy: 4.829403 
[2022-10-02 17:33:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.001367	 E_lr_2:0.001367
[2022-10-02 17:33:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 17:33:38 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.042359	Loss2: 0.049933	 Dis: 2.960461 Entropy: 4.946292 
[2022-10-02 17:33:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.001297	 E_lr_2:0.001297
[2022-10-02 17:33:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000013	 C_lr_2:0.000013	 C_lr_3:0.000013	 C_lr_4:0.000013
[2022-10-02 17:33:44 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.119480	Loss2: 0.050188	 Dis: 3.875704 Entropy: 4.953182 
[2022-10-02 17:33:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.001229	 E_lr_2:0.001229
[2022-10-02 17:33:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 17:33:50 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.027201	Loss2: 0.017964	 Dis: 2.191790 Entropy: 5.644642 
[2022-10-02 17:33:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.001162	 E_lr_2:0.001162
[2022-10-02 17:33:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 17:33:56 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.008578	Loss2: 0.005691	 Dis: 3.841301 Entropy: 4.309456 
[2022-10-02 17:33:56 demo] (houston_program2.py 512): INFO E_lr_1: 0.001096	 E_lr_2:0.001096
[2022-10-02 17:33:56 demo] (houston_program2.py 513): INFO C_lr_1: 0.000011	 C_lr_2:0.000011	 C_lr_3:0.000011	 C_lr_4:0.000011
[2022-10-02 17:34:03 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.012449	Loss2: 0.011139	 Dis: 1.834351 Entropy: 4.496658 
[2022-10-02 17:34:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.001032	 E_lr_2:0.001032
[2022-10-02 17:34:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 17:34:09 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.033125	Loss2: 0.040576	 Dis: 3.088751 Entropy: 4.265936 
[2022-10-02 17:34:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.000969	 E_lr_2:0.000969
[2022-10-02 17:34:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 17:34:16 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.063921	Loss2: 0.060206	 Dis: 1.728254 Entropy: 4.292683 
[2022-10-02 17:34:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.000908	 E_lr_2:0.000908
[2022-10-02 17:34:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000009	 C_lr_2:0.000009	 C_lr_3:0.000009	 C_lr_4:0.000009
[2022-10-02 17:34:22 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.054682	Loss2: 0.038659	 Dis: 1.059795 Entropy: 5.647309 
[2022-10-02 17:34:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.000848	 E_lr_2:0.000848
[2022-10-02 17:34:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 17:34:28 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.012737	Loss2: 0.013024	 Dis: 2.077900 Entropy: 4.446714 
[2022-10-02 17:34:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.000790	 E_lr_2:0.000790
[2022-10-02 17:34:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 17:34:34 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.003036	Loss2: 0.002956	 Dis: 0.685781 Entropy: 5.167449 
[2022-10-02 17:34:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.000734	 E_lr_2:0.000734
[2022-10-02 17:34:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 17:34:41 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.063145	Loss2: 0.047920	 Dis: 1.974651 Entropy: 5.028161 
[2022-10-02 17:34:41 demo] (houston_program2.py 512): INFO E_lr_1: 0.000679	 E_lr_2:0.000679
[2022-10-02 17:34:41 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 17:34:46 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.018896	Loss2: 0.033438	 Dis: 2.395149 Entropy: 5.027771 
[2022-10-02 17:34:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.000626	 E_lr_2:0.000626
[2022-10-02 17:34:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 17:34:52 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.005767	Loss2: 0.006333	 Dis: 2.044407 Entropy: 4.593807 
[2022-10-02 17:34:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.000575	 E_lr_2:0.000575
[2022-10-02 17:34:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 17:34:58 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.014366	Loss2: 0.011048	 Dis: 2.451258 Entropy: 4.600135 
[2022-10-02 17:34:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.000526	 E_lr_2:0.000526
[2022-10-02 17:34:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 17:35:04 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.012403	Loss2: 0.012120	 Dis: 2.955957 Entropy: 4.664181 
[2022-10-02 17:35:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.000479	 E_lr_2:0.000479
[2022-10-02 17:35:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 17:35:10 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.013450	Loss2: 0.011740	 Dis: 3.161789 Entropy: 5.526784 
[2022-10-02 17:35:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.000433	 E_lr_2:0.000433
[2022-10-02 17:35:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 17:35:16 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.003130	Loss2: 0.002479	 Dis: 1.966564 Entropy: 5.582256 
[2022-10-02 17:35:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.000390	 E_lr_2:0.000390
[2022-10-02 17:35:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 17:35:22 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.008780	Loss2: 0.009377	 Dis: 0.579473 Entropy: 6.647799 
[2022-10-02 17:35:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.000349	 E_lr_2:0.000349
[2022-10-02 17:35:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 17:35:28 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.004855	Loss2: 0.005147	 Dis: 2.372248 Entropy: 4.409057 
[2022-10-02 17:35:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.000310	 E_lr_2:0.000310
[2022-10-02 17:35:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 17:35:35 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.006308	Loss2: 0.009276	 Dis: 3.292589 Entropy: 4.409767 
[2022-10-02 17:35:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.000273	 E_lr_2:0.000273
[2022-10-02 17:35:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 17:35:40 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.003867	Loss2: 0.003479	 Dis: 2.169193 Entropy: 4.223114 
[2022-10-02 17:35:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.000239	 E_lr_2:0.000239
[2022-10-02 17:35:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 17:35:46 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.004394	Loss2: 0.005979	 Dis: 3.338045 Entropy: 4.773486 
[2022-10-02 17:35:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.000206	 E_lr_2:0.000206
[2022-10-02 17:35:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 17:35:52 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.004092	Loss2: 0.003664	 Dis: 1.533949 Entropy: 5.156740 
[2022-10-02 17:35:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.000176	 E_lr_2:0.000176
[2022-10-02 17:35:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 17:35:59 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.019825	Loss2: 0.013557	 Dis: 1.330688 Entropy: 4.846974 
[2022-10-02 17:35:59 demo] (houston_program2.py 512): INFO E_lr_1: 0.000148	 E_lr_2:0.000148
[2022-10-02 17:35:59 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:36:05 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.003270	Loss2: 0.003443	 Dis: 2.173292 Entropy: 4.428435 
[2022-10-02 17:36:05 demo] (houston_program2.py 512): INFO E_lr_1: 0.000123	 E_lr_2:0.000123
[2022-10-02 17:36:05 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:36:11 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.003392	Loss2: 0.005790	 Dis: 2.231110 Entropy: 4.441095 
[2022-10-02 17:36:11 demo] (houston_program2.py 512): INFO E_lr_1: 0.000100	 E_lr_2:0.000100
[2022-10-02 17:36:11 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:36:17 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.004068	Loss2: 0.002938	 Dis: 2.452969 Entropy: 5.002471 
[2022-10-02 17:36:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.000079	 E_lr_2:0.000079
[2022-10-02 17:36:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:36:23 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.004398	Loss2: 0.003932	 Dis: 3.412952 Entropy: 5.122677 
[2022-10-02 17:36:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.000061	 E_lr_2:0.000061
[2022-10-02 17:36:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:36:29 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.006464	Loss2: 0.005521	 Dis: 0.607090 Entropy: 5.884009 
[2022-10-02 17:36:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.000045	 E_lr_2:0.000045
[2022-10-02 17:36:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:36:35 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.004035	Loss2: 0.003108	 Dis: 0.697172 Entropy: 4.513790 
[2022-10-02 17:36:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.000031	 E_lr_2:0.000031
[2022-10-02 17:36:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:36:42 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.014649	Loss2: 0.017085	 Dis: 2.732201 Entropy: 5.205665 
[2022-10-02 17:36:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.000020	 E_lr_2:0.000020
[2022-10-02 17:36:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:36:47 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.003582	Loss2: 0.004129	 Dis: 1.438858 Entropy: 4.852458 
[2022-10-02 17:36:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.000011	 E_lr_2:0.000011
[2022-10-02 17:36:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:36:53 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.003239	Loss2: 0.003415	 Dis: 0.959358 Entropy: 5.798280 
[2022-10-02 17:36:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.000005	 E_lr_2:0.000005
[2022-10-02 17:36:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:36:59 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.013451	Loss2: 0.012913	 Dis: 2.359066 Entropy: 4.731844 
[2022-10-02 17:36:59 demo] (houston_program2.py 512): INFO E_lr_1: 0.000001	 E_lr_2:0.000001
[2022-10-02 17:36:59 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:37:05 demo] (houston_program2.py 504): INFO Train Ep: 20 	Loss1: 0.003549	Loss2: 0.003806	 Dis: 2.852709 Entropy: 4.498089 
[2022-10-02 17:37:05 demo] (houston_program2.py 512): INFO E_lr_1: 0.000000	 E_lr_2:0.000000
[2022-10-02 17:37:05 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:37:05 demo] (houston_program2.py 515): INFO time_20_epoch:595.8052897453308
[2022-10-02 17:37:13 demo] (houston_program2.py 673): INFO 	val_Accuracy: 32935/53200 (61.91%)	
[2022-10-02 17:37:13 demo] (utils.py 47): INFO outputs/demo/finetune_100/ckpt_epoch_20.pth saving......
[2022-10-02 17:37:14 demo] (utils.py 49): INFO outputs/demo/finetune_100/ckpt_epoch_20.pth saved !!!
[2022-10-02 17:37:14 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0123 (0.0123)	loss 0.0119 (0.0119)	grad_norm 0.0565 (0.0565)	mem 460MB
[2022-10-02 17:37:14 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:14 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0188 (0.0188)	loss 0.0168 (0.0168)	grad_norm 0.0536 (0.0536)	mem 460MB
[2022-10-02 17:37:14 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:14 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0192 (0.0192)	loss 0.0096 (0.0096)	grad_norm 0.0414 (0.0414)	mem 460MB
[2022-10-02 17:37:14 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:14 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0217 (0.0217)	loss 0.0131 (0.0131)	grad_norm 0.0443 (0.0443)	mem 460MB
[2022-10-02 17:37:14 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:15 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0215 (0.0215)	loss 0.0122 (0.0122)	grad_norm 0.0588 (0.0588)	mem 460MB
[2022-10-02 17:37:15 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:15 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0226 (0.0226)	loss 0.0125 (0.0125)	grad_norm 0.0356 (0.0356)	mem 460MB
[2022-10-02 17:37:15 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:15 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0260 (0.0260)	loss 0.0148 (0.0148)	grad_norm 0.0430 (0.0430)	mem 460MB
[2022-10-02 17:37:15 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:15 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0208 (0.0208)	loss 0.0122 (0.0122)	grad_norm 0.0433 (0.0433)	mem 460MB
[2022-10-02 17:37:15 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:15 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0198 (0.0198)	loss 0.0113 (0.0113)	grad_norm 0.0444 (0.0444)	mem 460MB
[2022-10-02 17:37:15 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:16 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0187 (0.0187)	loss 0.0111 (0.0111)	grad_norm 0.0424 (0.0424)	mem 460MB
[2022-10-02 17:37:16 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:16 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0108 (0.0108)	loss 0.0127 (0.0127)	grad_norm 0.0575 (0.0575)	mem 460MB
[2022-10-02 17:37:16 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:16 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0220 (0.0220)	loss 0.0159 (0.0159)	grad_norm 0.0577 (0.0577)	mem 460MB
[2022-10-02 17:37:16 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:16 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0105 (0.0105)	loss 0.0140 (0.0140)	grad_norm 0.0562 (0.0562)	mem 460MB
[2022-10-02 17:37:16 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:16 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0086 (0.0086)	loss 0.0154 (0.0154)	grad_norm 0.0433 (0.0433)	mem 460MB
[2022-10-02 17:37:16 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:16 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0087 (0.0087)	loss 0.0122 (0.0122)	grad_norm 0.0458 (0.0458)	mem 460MB
[2022-10-02 17:37:16 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:16 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0086 (0.0086)	loss 0.0106 (0.0106)	grad_norm 0.0459 (0.0459)	mem 460MB
[2022-10-02 17:37:16 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:16 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0103 (0.0103)	loss 0.0156 (0.0156)	grad_norm 0.0546 (0.0546)	mem 460MB
[2022-10-02 17:37:16 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:16 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0093 (0.0093)	loss 0.0133 (0.0133)	grad_norm 0.0523 (0.0523)	mem 460MB
[2022-10-02 17:37:16 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:17 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0094 (0.0094)	loss 0.0166 (0.0166)	grad_norm 0.0725 (0.0725)	mem 460MB
[2022-10-02 17:37:17 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:17 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0085 (0.0085)	loss 0.0127 (0.0127)	grad_norm 0.0449 (0.0449)	mem 460MB
[2022-10-02 17:37:17 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:17 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000045	time 0.0118 (0.0118)	loss 0.0124 (0.0124)	grad_norm 0.0455 (0.0455)	mem 460MB
[2022-10-02 17:37:17 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:17 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0207 (0.0207)	loss 0.0146 (0.0146)	grad_norm 0.0563 (0.0563)	mem 460MB
[2022-10-02 17:37:17 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:17 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0154 (0.0154)	loss 0.0113 (0.0113)	grad_norm 0.0614 (0.0614)	mem 460MB
[2022-10-02 17:37:17 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:17 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0211 (0.0211)	loss 0.0117 (0.0117)	grad_norm 0.0590 (0.0590)	mem 460MB
[2022-10-02 17:37:17 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:17 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0145 (0.0145)	loss 0.0148 (0.0148)	grad_norm 0.0715 (0.0715)	mem 460MB
[2022-10-02 17:37:17 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:18 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0216 (0.0216)	loss 0.0125 (0.0125)	grad_norm 0.0489 (0.0489)	mem 460MB
[2022-10-02 17:37:18 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:18 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0151 (0.0151)	loss 0.0135 (0.0135)	grad_norm 0.0861 (0.0861)	mem 460MB
[2022-10-02 17:37:18 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:18 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0203 (0.0203)	loss 0.0115 (0.0115)	grad_norm 0.0564 (0.0564)	mem 460MB
[2022-10-02 17:37:18 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:18 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0191 (0.0191)	loss 0.0106 (0.0106)	grad_norm 0.0448 (0.0448)	mem 460MB
[2022-10-02 17:37:18 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:18 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0183 (0.0183)	loss 0.0129 (0.0129)	grad_norm 0.0556 (0.0556)	mem 460MB
[2022-10-02 17:37:18 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:18 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0189 (0.0189)	loss 0.0141 (0.0141)	grad_norm 0.0621 (0.0621)	mem 460MB
[2022-10-02 17:37:18 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:19 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0210 (0.0210)	loss 0.0120 (0.0120)	grad_norm 0.0629 (0.0629)	mem 460MB
[2022-10-02 17:37:19 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:19 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0204 (0.0204)	loss 0.0159 (0.0159)	grad_norm 0.0638 (0.0638)	mem 460MB
[2022-10-02 17:37:19 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:19 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0207 (0.0207)	loss 0.0110 (0.0110)	grad_norm 0.0386 (0.0386)	mem 460MB
[2022-10-02 17:37:19 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:19 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0200 (0.0200)	loss 0.0197 (0.0197)	grad_norm 0.0624 (0.0624)	mem 460MB
[2022-10-02 17:37:19 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:19 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0223 (0.0223)	loss 0.0106 (0.0106)	grad_norm 0.0633 (0.0633)	mem 460MB
[2022-10-02 17:37:19 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:20 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0159 (0.0159)	loss 0.0106 (0.0106)	grad_norm 0.0519 (0.0519)	mem 460MB
[2022-10-02 17:37:20 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:20 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0209 (0.0209)	loss 0.0134 (0.0134)	grad_norm 0.0786 (0.0786)	mem 460MB
[2022-10-02 17:37:20 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:20 demo] (houston_program2.py 243): INFO Train: [21/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0201 (0.0201)	loss 0.0112 (0.0112)	grad_norm 0.0644 (0.0644)	mem 460MB
[2022-10-02 17:37:20 demo] (houston_program2.py 252): INFO EPOCH 21 training takes 0:00:00
[2022-10-02 17:37:20 demo] (houston_program2.py 333): INFO Train Ep: 21 	Loss1: 0.257274	Loss2: 0.248188	 Dis: 8.332947 Entropy: 4.186926 
[2022-10-02 17:37:20 demo] (houston_program2.py 335): INFO time_21_epoch:6.482593774795532
[2022-10-02 17:37:20 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0134 (0.0134)	loss 0.0118 (0.0118)	grad_norm 0.0698 (0.0698)	mem 460MB
[2022-10-02 17:37:20 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:21 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0108 (0.0108)	loss 0.0132 (0.0132)	grad_norm 0.0673 (0.0673)	mem 460MB
[2022-10-02 17:37:21 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:21 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0108 (0.0108)	loss 0.0114 (0.0114)	grad_norm 0.0585 (0.0585)	mem 460MB
[2022-10-02 17:37:21 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:21 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0096 (0.0096)	loss 0.0181 (0.0181)	grad_norm 0.0399 (0.0399)	mem 460MB
[2022-10-02 17:37:21 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:21 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0089 (0.0089)	loss 0.0166 (0.0166)	grad_norm 0.0877 (0.0877)	mem 460MB
[2022-10-02 17:37:21 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:21 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0084 (0.0084)	loss 0.0143 (0.0143)	grad_norm 0.0602 (0.0602)	mem 460MB
[2022-10-02 17:37:21 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:21 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0082 (0.0082)	loss 0.0161 (0.0161)	grad_norm 0.0574 (0.0574)	mem 460MB
[2022-10-02 17:37:21 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:21 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0085 (0.0085)	loss 0.0161 (0.0161)	grad_norm 0.0831 (0.0831)	mem 460MB
[2022-10-02 17:37:21 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:21 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0170 (0.0170)	loss 0.0139 (0.0139)	grad_norm 0.0700 (0.0700)	mem 460MB
[2022-10-02 17:37:21 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:21 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0220 (0.0220)	loss 0.0104 (0.0104)	grad_norm 0.0539 (0.0539)	mem 460MB
[2022-10-02 17:37:21 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:22 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0190 (0.0190)	loss 0.0132 (0.0132)	grad_norm 0.0836 (0.0836)	mem 460MB
[2022-10-02 17:37:22 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:22 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0206 (0.0206)	loss 0.0133 (0.0133)	grad_norm 0.0513 (0.0513)	mem 460MB
[2022-10-02 17:37:22 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:22 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0220 (0.0220)	loss 0.0143 (0.0143)	grad_norm 0.0623 (0.0623)	mem 460MB
[2022-10-02 17:37:22 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:22 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0216 (0.0216)	loss 0.0177 (0.0177)	grad_norm 0.0657 (0.0657)	mem 460MB
[2022-10-02 17:37:22 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:22 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0220 (0.0220)	loss 0.0156 (0.0156)	grad_norm 0.0393 (0.0393)	mem 460MB
[2022-10-02 17:37:22 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:22 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0107 (0.0107)	loss 0.0104 (0.0104)	grad_norm 0.0471 (0.0471)	mem 460MB
[2022-10-02 17:37:22 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:23 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0210 (0.0210)	loss 0.0138 (0.0138)	grad_norm 0.0674 (0.0674)	mem 460MB
[2022-10-02 17:37:23 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:23 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0179 (0.0179)	loss 0.0132 (0.0132)	grad_norm 0.0634 (0.0634)	mem 460MB
[2022-10-02 17:37:23 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:23 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0222 (0.0222)	loss 0.0101 (0.0101)	grad_norm 0.0581 (0.0581)	mem 460MB
[2022-10-02 17:37:23 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:23 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0159 (0.0159)	loss 0.0166 (0.0166)	grad_norm 0.0668 (0.0668)	mem 460MB
[2022-10-02 17:37:23 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:23 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0193 (0.0193)	loss 0.0137 (0.0137)	grad_norm 0.0522 (0.0522)	mem 460MB
[2022-10-02 17:37:23 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:24 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0206 (0.0206)	loss 0.0141 (0.0141)	grad_norm 0.0442 (0.0442)	mem 460MB
[2022-10-02 17:37:24 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:24 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0200 (0.0200)	loss 0.0113 (0.0113)	grad_norm 0.0637 (0.0637)	mem 460MB
[2022-10-02 17:37:24 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:24 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0213 (0.0213)	loss 0.0124 (0.0124)	grad_norm 0.0501 (0.0501)	mem 460MB
[2022-10-02 17:37:24 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:24 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0275 (0.0275)	loss 0.0161 (0.0161)	grad_norm 0.0742 (0.0742)	mem 460MB
[2022-10-02 17:37:24 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:24 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0213 (0.0213)	loss 0.0105 (0.0105)	grad_norm 0.0582 (0.0582)	mem 460MB
[2022-10-02 17:37:24 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:24 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0177 (0.0177)	loss 0.0107 (0.0107)	grad_norm 0.0404 (0.0404)	mem 460MB
[2022-10-02 17:37:24 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:25 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0202 (0.0202)	loss 0.0111 (0.0111)	grad_norm 0.0635 (0.0635)	mem 460MB
[2022-10-02 17:37:25 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:25 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0102 (0.0102)	loss 0.0142 (0.0142)	grad_norm 0.0468 (0.0468)	mem 460MB
[2022-10-02 17:37:25 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:25 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0198 (0.0198)	loss 0.0095 (0.0095)	grad_norm 0.0393 (0.0393)	mem 460MB
[2022-10-02 17:37:25 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:25 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0119 (0.0119)	loss 0.0133 (0.0133)	grad_norm 0.0454 (0.0454)	mem 460MB
[2022-10-02 17:37:25 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:25 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0207 (0.0207)	loss 0.0123 (0.0123)	grad_norm 0.0563 (0.0563)	mem 460MB
[2022-10-02 17:37:25 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:25 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0134 (0.0134)	loss 0.0110 (0.0110)	grad_norm 0.0476 (0.0476)	mem 460MB
[2022-10-02 17:37:25 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:26 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0211 (0.0211)	loss 0.0133 (0.0133)	grad_norm 0.0543 (0.0543)	mem 460MB
[2022-10-02 17:37:26 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:26 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0196 (0.0196)	loss 0.0113 (0.0113)	grad_norm 0.0356 (0.0356)	mem 460MB
[2022-10-02 17:37:26 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:26 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0206 (0.0206)	loss 0.0153 (0.0153)	grad_norm 0.0471 (0.0471)	mem 460MB
[2022-10-02 17:37:26 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:26 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0216 (0.0216)	loss 0.0156 (0.0156)	grad_norm 0.0673 (0.0673)	mem 460MB
[2022-10-02 17:37:26 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:26 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0220 (0.0220)	loss 0.0102 (0.0102)	grad_norm 0.0421 (0.0421)	mem 460MB
[2022-10-02 17:37:26 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:27 demo] (houston_program2.py 243): INFO Train: [22/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0217 (0.0217)	loss 0.0118 (0.0118)	grad_norm 0.0517 (0.0517)	mem 460MB
[2022-10-02 17:37:27 demo] (houston_program2.py 252): INFO EPOCH 22 training takes 0:00:00
[2022-10-02 17:37:27 demo] (houston_program2.py 333): INFO Train Ep: 22 	Loss1: 0.293642	Loss2: 0.307467	 Dis: 9.210739 Entropy: 4.479009 
[2022-10-02 17:37:27 demo] (houston_program2.py 335): INFO time_22_epoch:6.729608774185181
[2022-10-02 17:37:27 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0142 (0.0142)	loss 0.0132 (0.0132)	grad_norm 0.0402 (0.0402)	mem 460MB
[2022-10-02 17:37:27 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:27 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0243 (0.0243)	loss 0.0118 (0.0118)	grad_norm 0.0396 (0.0396)	mem 460MB
[2022-10-02 17:37:27 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:28 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0221 (0.0221)	loss 0.0104 (0.0104)	grad_norm 0.0402 (0.0402)	mem 460MB
[2022-10-02 17:37:28 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:28 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0158 (0.0158)	loss 0.0122 (0.0122)	grad_norm 0.0406 (0.0406)	mem 460MB
[2022-10-02 17:37:28 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:28 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0223 (0.0223)	loss 0.0131 (0.0131)	grad_norm 0.0513 (0.0513)	mem 460MB
[2022-10-02 17:37:28 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:28 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0205 (0.0205)	loss 0.0118 (0.0118)	grad_norm 0.0656 (0.0656)	mem 460MB
[2022-10-02 17:37:28 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:28 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0221 (0.0221)	loss 0.0116 (0.0116)	grad_norm 0.0513 (0.0513)	mem 460MB
[2022-10-02 17:37:28 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:29 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0219 (0.0219)	loss 0.0110 (0.0110)	grad_norm 0.0636 (0.0636)	mem 460MB
[2022-10-02 17:37:29 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:29 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0213 (0.0213)	loss 0.0140 (0.0140)	grad_norm 0.0661 (0.0661)	mem 460MB
[2022-10-02 17:37:29 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:29 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0207 (0.0207)	loss 0.0124 (0.0124)	grad_norm 0.0366 (0.0366)	mem 460MB
[2022-10-02 17:37:29 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:29 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0198 (0.0198)	loss 0.0161 (0.0161)	grad_norm 0.0665 (0.0665)	mem 460MB
[2022-10-02 17:37:29 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:29 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0211 (0.0211)	loss 0.0144 (0.0144)	grad_norm 0.0698 (0.0698)	mem 460MB
[2022-10-02 17:37:29 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:29 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0180 (0.0180)	loss 0.0125 (0.0125)	grad_norm 0.0586 (0.0586)	mem 460MB
[2022-10-02 17:37:29 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:30 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0224 (0.0224)	loss 0.0134 (0.0134)	grad_norm 0.0956 (0.0956)	mem 460MB
[2022-10-02 17:37:30 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:30 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0136 (0.0136)	loss 0.0168 (0.0168)	grad_norm 0.0647 (0.0647)	mem 460MB
[2022-10-02 17:37:30 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:30 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0243 (0.0243)	loss 0.0124 (0.0124)	grad_norm 0.0684 (0.0684)	mem 460MB
[2022-10-02 17:37:30 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:30 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0244 (0.0244)	loss 0.0153 (0.0153)	grad_norm 0.0599 (0.0599)	mem 460MB
[2022-10-02 17:37:30 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:30 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0278 (0.0278)	loss 0.0116 (0.0116)	grad_norm 0.0513 (0.0513)	mem 460MB
[2022-10-02 17:37:30 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:31 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0231 (0.0231)	loss 0.0138 (0.0138)	grad_norm 0.0544 (0.0544)	mem 460MB
[2022-10-02 17:37:31 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:31 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000044	time 0.0122 (0.0122)	loss 0.0126 (0.0126)	grad_norm 0.0659 (0.0659)	mem 460MB
[2022-10-02 17:37:31 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:31 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0222 (0.0222)	loss 0.0110 (0.0110)	grad_norm 0.0468 (0.0468)	mem 460MB
[2022-10-02 17:37:31 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:31 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0220 (0.0220)	loss 0.0167 (0.0167)	grad_norm 0.0739 (0.0739)	mem 460MB
[2022-10-02 17:37:31 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:31 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0297 (0.0297)	loss 0.0093 (0.0093)	grad_norm 0.0792 (0.0792)	mem 460MB
[2022-10-02 17:37:31 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:32 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0218 (0.0218)	loss 0.0128 (0.0128)	grad_norm 0.0607 (0.0607)	mem 460MB
[2022-10-02 17:37:32 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:32 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0097 (0.0097)	loss 0.0129 (0.0129)	grad_norm 0.0748 (0.0748)	mem 460MB
[2022-10-02 17:37:32 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:32 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0212 (0.0212)	loss 0.0126 (0.0126)	grad_norm 0.0749 (0.0749)	mem 460MB
[2022-10-02 17:37:32 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:32 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0106 (0.0106)	loss 0.0114 (0.0114)	grad_norm 0.0476 (0.0476)	mem 460MB
[2022-10-02 17:37:32 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:32 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0214 (0.0214)	loss 0.0104 (0.0104)	grad_norm 0.0715 (0.0715)	mem 460MB
[2022-10-02 17:37:32 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:32 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0107 (0.0107)	loss 0.0141 (0.0141)	grad_norm 0.0679 (0.0679)	mem 460MB
[2022-10-02 17:37:32 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:33 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0187 (0.0187)	loss 0.0118 (0.0118)	grad_norm 0.0446 (0.0446)	mem 460MB
[2022-10-02 17:37:33 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:33 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0118 (0.0118)	loss 0.0111 (0.0111)	grad_norm 0.0645 (0.0645)	mem 460MB
[2022-10-02 17:37:33 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:33 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0218 (0.0218)	loss 0.0201 (0.0201)	grad_norm 0.0825 (0.0825)	mem 460MB
[2022-10-02 17:37:33 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:33 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0144 (0.0144)	loss 0.0133 (0.0133)	grad_norm 0.0475 (0.0475)	mem 460MB
[2022-10-02 17:37:33 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:33 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0219 (0.0219)	loss 0.0140 (0.0140)	grad_norm 0.0658 (0.0658)	mem 460MB
[2022-10-02 17:37:33 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:33 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0186 (0.0186)	loss 0.0157 (0.0157)	grad_norm 0.1043 (0.1043)	mem 460MB
[2022-10-02 17:37:34 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:34 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0226 (0.0226)	loss 0.0124 (0.0124)	grad_norm 0.0554 (0.0554)	mem 460MB
[2022-10-02 17:37:34 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:34 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0208 (0.0208)	loss 0.0148 (0.0148)	grad_norm 0.0683 (0.0683)	mem 460MB
[2022-10-02 17:37:34 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:34 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0319 (0.0319)	loss 0.0122 (0.0122)	grad_norm 0.0748 (0.0748)	mem 460MB
[2022-10-02 17:37:34 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:34 demo] (houston_program2.py 243): INFO Train: [23/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0263 (0.0263)	loss 0.0115 (0.0115)	grad_norm 0.0451 (0.0451)	mem 460MB
[2022-10-02 17:37:34 demo] (houston_program2.py 252): INFO EPOCH 23 training takes 0:00:00
[2022-10-02 17:37:35 demo] (houston_program2.py 333): INFO Train Ep: 23 	Loss1: 0.222367	Loss2: 0.199345	 Dis: 8.162025 Entropy: 4.538730 
[2022-10-02 17:37:35 demo] (houston_program2.py 335): INFO time_23_epoch:7.682517766952515
[2022-10-02 17:37:35 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0151 (0.0151)	loss 0.0143 (0.0143)	grad_norm 0.0646 (0.0646)	mem 460MB
[2022-10-02 17:37:35 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:35 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0235 (0.0235)	loss 0.0095 (0.0095)	grad_norm 0.0651 (0.0651)	mem 460MB
[2022-10-02 17:37:35 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:35 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0212 (0.0212)	loss 0.0153 (0.0153)	grad_norm 0.0647 (0.0647)	mem 460MB
[2022-10-02 17:37:35 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:35 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0164 (0.0164)	loss 0.0102 (0.0102)	grad_norm 0.0710 (0.0710)	mem 460MB
[2022-10-02 17:37:35 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:36 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0233 (0.0233)	loss 0.0112 (0.0112)	grad_norm 0.0492 (0.0492)	mem 460MB
[2022-10-02 17:37:36 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:36 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0249 (0.0249)	loss 0.0124 (0.0124)	grad_norm 0.0530 (0.0530)	mem 460MB
[2022-10-02 17:37:36 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:36 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0161 (0.0161)	loss 0.0146 (0.0146)	grad_norm 0.0724 (0.0724)	mem 460MB
[2022-10-02 17:37:36 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:36 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0243 (0.0243)	loss 0.0094 (0.0094)	grad_norm 0.0392 (0.0392)	mem 460MB
[2022-10-02 17:37:36 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:36 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0159 (0.0159)	loss 0.0101 (0.0101)	grad_norm 0.0430 (0.0430)	mem 460MB
[2022-10-02 17:37:37 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:37 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0245 (0.0245)	loss 0.0120 (0.0120)	grad_norm 0.0812 (0.0812)	mem 460MB
[2022-10-02 17:37:37 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:37 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0248 (0.0248)	loss 0.0096 (0.0096)	grad_norm 0.0446 (0.0446)	mem 460MB
[2022-10-02 17:37:37 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:37 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0114 (0.0114)	loss 0.0189 (0.0189)	grad_norm 0.1148 (0.1148)	mem 460MB
[2022-10-02 17:37:37 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:37 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0208 (0.0208)	loss 0.0139 (0.0139)	grad_norm 0.0736 (0.0736)	mem 460MB
[2022-10-02 17:37:37 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:37 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0113 (0.0113)	loss 0.0105 (0.0105)	grad_norm 0.0465 (0.0465)	mem 460MB
[2022-10-02 17:37:37 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:38 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0206 (0.0206)	loss 0.0152 (0.0152)	grad_norm 0.0636 (0.0636)	mem 460MB
[2022-10-02 17:37:38 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:38 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0167 (0.0167)	loss 0.0167 (0.0167)	grad_norm 0.0933 (0.0933)	mem 460MB
[2022-10-02 17:37:38 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:38 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0215 (0.0215)	loss 0.0092 (0.0092)	grad_norm 0.0417 (0.0417)	mem 460MB
[2022-10-02 17:37:38 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:38 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0193 (0.0193)	loss 0.0127 (0.0127)	grad_norm 0.1150 (0.1150)	mem 460MB
[2022-10-02 17:37:38 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:38 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0206 (0.0206)	loss 0.0128 (0.0128)	grad_norm 0.0695 (0.0695)	mem 460MB
[2022-10-02 17:37:38 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:39 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0215 (0.0215)	loss 0.0153 (0.0153)	grad_norm 0.0865 (0.0865)	mem 460MB
[2022-10-02 17:37:39 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:39 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0204 (0.0204)	loss 0.0122 (0.0122)	grad_norm 0.0757 (0.0757)	mem 460MB
[2022-10-02 17:37:39 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:39 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0210 (0.0210)	loss 0.0139 (0.0139)	grad_norm 0.0691 (0.0691)	mem 460MB
[2022-10-02 17:37:39 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:39 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0262 (0.0262)	loss 0.0145 (0.0145)	grad_norm 0.0774 (0.0774)	mem 460MB
[2022-10-02 17:37:39 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:39 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0197 (0.0197)	loss 0.0148 (0.0148)	grad_norm 0.0505 (0.0505)	mem 460MB
[2022-10-02 17:37:39 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:39 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0142 (0.0142)	loss 0.0148 (0.0148)	grad_norm 0.0636 (0.0636)	mem 460MB
[2022-10-02 17:37:39 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:40 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0206 (0.0206)	loss 0.0191 (0.0191)	grad_norm 0.0695 (0.0695)	mem 460MB
[2022-10-02 17:37:40 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:40 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0204 (0.0204)	loss 0.0157 (0.0157)	grad_norm 0.0741 (0.0741)	mem 460MB
[2022-10-02 17:37:40 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:40 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0211 (0.0211)	loss 0.0136 (0.0136)	grad_norm 0.0663 (0.0663)	mem 460MB
[2022-10-02 17:37:40 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:40 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0203 (0.0203)	loss 0.0164 (0.0164)	grad_norm 0.0582 (0.0582)	mem 460MB
[2022-10-02 17:37:40 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:40 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0188 (0.0188)	loss 0.0159 (0.0159)	grad_norm 0.0466 (0.0466)	mem 460MB
[2022-10-02 17:37:40 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:41 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0127 (0.0127)	loss 0.0112 (0.0112)	grad_norm 0.0442 (0.0442)	mem 460MB
[2022-10-02 17:37:41 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:41 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0202 (0.0202)	loss 0.0095 (0.0095)	grad_norm 0.0379 (0.0379)	mem 460MB
[2022-10-02 17:37:41 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:41 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0135 (0.0135)	loss 0.0165 (0.0165)	grad_norm 0.0505 (0.0505)	mem 460MB
[2022-10-02 17:37:41 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:41 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0207 (0.0207)	loss 0.0130 (0.0130)	grad_norm 0.0525 (0.0525)	mem 460MB
[2022-10-02 17:37:41 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:41 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0194 (0.0194)	loss 0.0126 (0.0126)	grad_norm 0.0496 (0.0496)	mem 460MB
[2022-10-02 17:37:41 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:41 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0209 (0.0209)	loss 0.0135 (0.0135)	grad_norm 0.0679 (0.0679)	mem 460MB
[2022-10-02 17:37:41 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:42 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0207 (0.0207)	loss 0.0110 (0.0110)	grad_norm 0.0445 (0.0445)	mem 460MB
[2022-10-02 17:37:42 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:42 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0220 (0.0220)	loss 0.0102 (0.0102)	grad_norm 0.0599 (0.0599)	mem 460MB
[2022-10-02 17:37:42 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:42 demo] (houston_program2.py 243): INFO Train: [24/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0219 (0.0219)	loss 0.0149 (0.0149)	grad_norm 0.0447 (0.0447)	mem 460MB
[2022-10-02 17:37:42 demo] (houston_program2.py 252): INFO EPOCH 24 training takes 0:00:00
[2022-10-02 17:37:42 demo] (houston_program2.py 333): INFO Train Ep: 24 	Loss1: 0.411016	Loss2: 0.438489	 Dis: 8.722740 Entropy: 4.562035 
[2022-10-02 17:37:42 demo] (houston_program2.py 335): INFO time_24_epoch:7.71457576751709
[2022-10-02 17:37:43 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0122 (0.0122)	loss 0.0149 (0.0149)	grad_norm 0.0443 (0.0443)	mem 460MB
[2022-10-02 17:37:43 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:43 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0223 (0.0223)	loss 0.0112 (0.0112)	grad_norm 0.0471 (0.0471)	mem 460MB
[2022-10-02 17:37:43 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:43 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0189 (0.0189)	loss 0.0094 (0.0094)	grad_norm 0.0562 (0.0562)	mem 460MB
[2022-10-02 17:37:43 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:43 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0262 (0.0262)	loss 0.0135 (0.0135)	grad_norm 0.0586 (0.0586)	mem 460MB
[2022-10-02 17:37:43 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:43 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0218 (0.0218)	loss 0.0134 (0.0134)	grad_norm 0.0466 (0.0466)	mem 460MB
[2022-10-02 17:37:43 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:44 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0168 (0.0168)	loss 0.0103 (0.0103)	grad_norm 0.0375 (0.0375)	mem 460MB
[2022-10-02 17:37:44 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:44 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0213 (0.0213)	loss 0.0150 (0.0150)	grad_norm 0.0483 (0.0483)	mem 460MB
[2022-10-02 17:37:44 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:44 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0126 (0.0126)	loss 0.0161 (0.0161)	grad_norm 0.0737 (0.0737)	mem 460MB
[2022-10-02 17:37:44 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:44 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0216 (0.0216)	loss 0.0136 (0.0136)	grad_norm 0.0473 (0.0473)	mem 460MB
[2022-10-02 17:37:44 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:44 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0162 (0.0162)	loss 0.0091 (0.0091)	grad_norm 0.0565 (0.0565)	mem 460MB
[2022-10-02 17:37:44 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:44 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0182 (0.0182)	loss 0.0114 (0.0114)	grad_norm 0.0564 (0.0564)	mem 460MB
[2022-10-02 17:37:44 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:45 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0161 (0.0161)	loss 0.0116 (0.0116)	grad_norm 0.0393 (0.0393)	mem 460MB
[2022-10-02 17:37:45 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:45 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0199 (0.0199)	loss 0.0116 (0.0116)	grad_norm 0.0630 (0.0630)	mem 460MB
[2022-10-02 17:37:45 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:45 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000043	time 0.0212 (0.0212)	loss 0.0096 (0.0096)	grad_norm 0.0565 (0.0565)	mem 460MB
[2022-10-02 17:37:45 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:45 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0197 (0.0197)	loss 0.0177 (0.0177)	grad_norm 0.0459 (0.0459)	mem 460MB
[2022-10-02 17:37:45 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:45 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0207 (0.0207)	loss 0.0120 (0.0120)	grad_norm 0.0647 (0.0647)	mem 460MB
[2022-10-02 17:37:45 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:45 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0216 (0.0216)	loss 0.0113 (0.0113)	grad_norm 0.0358 (0.0358)	mem 460MB
[2022-10-02 17:37:46 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:46 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0217 (0.0217)	loss 0.0148 (0.0148)	grad_norm 0.0485 (0.0485)	mem 460MB
[2022-10-02 17:37:46 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:46 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0148 (0.0148)	loss 0.0163 (0.0163)	grad_norm 0.0502 (0.0502)	mem 460MB
[2022-10-02 17:37:46 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:46 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0219 (0.0219)	loss 0.0119 (0.0119)	grad_norm 0.0556 (0.0556)	mem 460MB
[2022-10-02 17:37:46 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:46 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0091 (0.0091)	loss 0.0152 (0.0152)	grad_norm 0.0418 (0.0418)	mem 460MB
[2022-10-02 17:37:46 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:46 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0211 (0.0211)	loss 0.0147 (0.0147)	grad_norm 0.0821 (0.0821)	mem 460MB
[2022-10-02 17:37:46 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:47 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0134 (0.0134)	loss 0.0139 (0.0139)	grad_norm 0.0697 (0.0697)	mem 460MB
[2022-10-02 17:37:47 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:47 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0219 (0.0219)	loss 0.0086 (0.0086)	grad_norm 0.0326 (0.0326)	mem 460MB
[2022-10-02 17:37:47 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:47 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0149 (0.0149)	loss 0.0109 (0.0109)	grad_norm 0.0724 (0.0724)	mem 460MB
[2022-10-02 17:37:47 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:47 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0214 (0.0214)	loss 0.0169 (0.0169)	grad_norm 0.0670 (0.0670)	mem 460MB
[2022-10-02 17:37:47 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:47 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0200 (0.0200)	loss 0.0126 (0.0126)	grad_norm 0.0522 (0.0522)	mem 460MB
[2022-10-02 17:37:47 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:47 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0204 (0.0204)	loss 0.0108 (0.0108)	grad_norm 0.0619 (0.0619)	mem 460MB
[2022-10-02 17:37:47 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:48 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0210 (0.0210)	loss 0.0110 (0.0110)	grad_norm 0.0584 (0.0584)	mem 460MB
[2022-10-02 17:37:48 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:48 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0198 (0.0198)	loss 0.0111 (0.0111)	grad_norm 0.0371 (0.0371)	mem 460MB
[2022-10-02 17:37:48 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:48 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0200 (0.0200)	loss 0.0148 (0.0148)	grad_norm 0.0712 (0.0712)	mem 460MB
[2022-10-02 17:37:48 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:48 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0225 (0.0225)	loss 0.0123 (0.0123)	grad_norm 0.0354 (0.0354)	mem 460MB
[2022-10-02 17:37:48 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:48 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0207 (0.0207)	loss 0.0112 (0.0112)	grad_norm 0.0466 (0.0466)	mem 460MB
[2022-10-02 17:37:48 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:49 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0123 (0.0123)	loss 0.0163 (0.0163)	grad_norm 0.0584 (0.0584)	mem 460MB
[2022-10-02 17:37:49 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:49 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0204 (0.0204)	loss 0.0107 (0.0107)	grad_norm 0.0470 (0.0470)	mem 460MB
[2022-10-02 17:37:49 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:49 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0131 (0.0131)	loss 0.0117 (0.0117)	grad_norm 0.0522 (0.0522)	mem 460MB
[2022-10-02 17:37:49 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:49 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0200 (0.0200)	loss 0.0096 (0.0096)	grad_norm 0.0456 (0.0456)	mem 460MB
[2022-10-02 17:37:49 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:49 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0145 (0.0145)	loss 0.0164 (0.0164)	grad_norm 0.0435 (0.0435)	mem 460MB
[2022-10-02 17:37:49 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:49 demo] (houston_program2.py 243): INFO Train: [25/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0205 (0.0205)	loss 0.0152 (0.0152)	grad_norm 0.0515 (0.0515)	mem 460MB
[2022-10-02 17:37:49 demo] (houston_program2.py 252): INFO EPOCH 25 training takes 0:00:00
[2022-10-02 17:37:50 demo] (houston_program2.py 333): INFO Train Ep: 25 	Loss1: 0.104599	Loss2: 0.109850	 Dis: 6.909222 Entropy: 4.211939 
[2022-10-02 17:37:50 demo] (houston_program2.py 335): INFO time_25_epoch:7.40118932723999
[2022-10-02 17:37:50 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 17:37:50 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 17:37:50 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 17:37:50 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 17:37:50 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 17:37:50 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 17:37:50 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 17:37:50 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 17:37:50 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 17:37:50 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 17:37:56 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.084113	Loss2: 0.067790	 Dis: 6.846447 Entropy: 4.653894 
[2022-10-02 17:37:56 demo] (houston_program2.py 512): INFO E_lr_1: 0.000487	 E_lr_2:0.000487
[2022-10-02 17:37:56 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 17:38:01 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.219389	Loss2: 0.194343	 Dis: 8.786913 Entropy: 4.880463 
[2022-10-02 17:38:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.000987	 E_lr_2:0.000987
[2022-10-02 17:38:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 17:38:06 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.220056	Loss2: 0.239856	 Dis: 5.333054 Entropy: 5.241066 
[2022-10-02 17:38:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.001487	 E_lr_2:0.001487
[2022-10-02 17:38:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 17:38:12 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.299825	Loss2: 0.349187	 Dis: 6.359497 Entropy: 4.991296 
[2022-10-02 17:38:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.001987	 E_lr_2:0.001987
[2022-10-02 17:38:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 17:38:18 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.143894	Loss2: 0.154954	 Dis: 6.368656 Entropy: 4.742075 
[2022-10-02 17:38:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.002487	 E_lr_2:0.002487
[2022-10-02 17:38:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 17:38:23 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.118744	Loss2: 0.121241	 Dis: 5.175781 Entropy: 5.409170 
[2022-10-02 17:38:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.002987	 E_lr_2:0.002987
[2022-10-02 17:38:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 17:38:29 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.143564	Loss2: 0.163868	 Dis: 5.737877 Entropy: 6.221502 
[2022-10-02 17:38:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.003487	 E_lr_2:0.003487
[2022-10-02 17:38:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 17:38:35 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.309593	Loss2: 0.301403	 Dis: 8.502638 Entropy: 5.778772 
[2022-10-02 17:38:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.003987	 E_lr_2:0.003987
[2022-10-02 17:38:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 17:38:41 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.120928	Loss2: 0.139214	 Dis: 5.059284 Entropy: 5.462005 
[2022-10-02 17:38:41 demo] (houston_program2.py 512): INFO E_lr_1: 0.004487	 E_lr_2:0.004487
[2022-10-02 17:38:41 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 17:38:48 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.352913	Loss2: 0.380735	 Dis: 6.193577 Entropy: 5.361953 
[2022-10-02 17:38:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.004987	 E_lr_2:0.004987
[2022-10-02 17:38:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000050	 C_lr_2:0.000050	 C_lr_3:0.000050	 C_lr_4:0.000050
[2022-10-02 17:38:54 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.124644	Loss2: 0.145959	 Dis: 4.874868 Entropy: 5.360846 
[2022-10-02 17:38:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.004853	 E_lr_2:0.004853
[2022-10-02 17:38:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000049	 C_lr_2:0.000049	 C_lr_3:0.000049	 C_lr_4:0.000049
[2022-10-02 17:39:00 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.142545	Loss2: 0.161790	 Dis: 4.414732 Entropy: 5.156523 
[2022-10-02 17:39:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.004825	 E_lr_2:0.004825
[2022-10-02 17:39:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 17:39:06 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.118022	Loss2: 0.139907	 Dis: 2.504799 Entropy: 6.551929 
[2022-10-02 17:39:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.004795	 E_lr_2:0.004795
[2022-10-02 17:39:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 17:39:13 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.130087	Loss2: 0.155891	 Dis: 4.396729 Entropy: 5.158085 
[2022-10-02 17:39:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.004763	 E_lr_2:0.004763
[2022-10-02 17:39:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 17:39:19 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.334170	Loss2: 0.303229	 Dis: 10.349716 Entropy: 4.666216 
[2022-10-02 17:39:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.004728	 E_lr_2:0.004728
[2022-10-02 17:39:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 17:39:25 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.349894	Loss2: 0.345204	 Dis: 11.106361 Entropy: 4.253784 
[2022-10-02 17:39:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.004692	 E_lr_2:0.004692
[2022-10-02 17:39:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 17:39:31 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.271796	Loss2: 0.281488	 Dis: 9.196976 Entropy: 4.447352 
[2022-10-02 17:39:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.004653	 E_lr_2:0.004653
[2022-10-02 17:39:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 17:39:37 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.223230	Loss2: 0.240738	 Dis: 7.497408 Entropy: 4.576780 
[2022-10-02 17:39:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.004612	 E_lr_2:0.004612
[2022-10-02 17:39:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 17:39:42 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.086896	Loss2: 0.079746	 Dis: 6.583487 Entropy: 5.949198 
[2022-10-02 17:39:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.004569	 E_lr_2:0.004569
[2022-10-02 17:39:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 17:39:48 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.195393	Loss2: 0.193160	 Dis: 5.272776 Entropy: 4.855082 
[2022-10-02 17:39:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.004524	 E_lr_2:0.004524
[2022-10-02 17:39:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 17:39:54 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.108688	Loss2: 0.095942	 Dis: 5.395203 Entropy: 5.200340 
[2022-10-02 17:39:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.004477	 E_lr_2:0.004477
[2022-10-02 17:39:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 17:40:00 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.052687	Loss2: 0.057880	 Dis: 6.231564 Entropy: 5.033850 
[2022-10-02 17:40:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.004428	 E_lr_2:0.004428
[2022-10-02 17:40:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 17:40:06 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.128353	Loss2: 0.120773	 Dis: 3.443935 Entropy: 4.416035 
[2022-10-02 17:40:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.004377	 E_lr_2:0.004377
[2022-10-02 17:40:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 17:40:12 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.071089	Loss2: 0.094754	 Dis: 4.928478 Entropy: 5.982203 
[2022-10-02 17:40:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.004324	 E_lr_2:0.004324
[2022-10-02 17:40:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 17:40:18 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.099370	Loss2: 0.088208	 Dis: 4.410406 Entropy: 4.530274 
[2022-10-02 17:40:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.004269	 E_lr_2:0.004269
[2022-10-02 17:40:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 17:40:24 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.152215	Loss2: 0.122121	 Dis: 6.151608 Entropy: 4.447371 
[2022-10-02 17:40:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.004213	 E_lr_2:0.004213
[2022-10-02 17:40:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 17:40:30 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.118306	Loss2: 0.112141	 Dis: 5.611637 Entropy: 5.595575 
[2022-10-02 17:40:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.004155	 E_lr_2:0.004155
[2022-10-02 17:40:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 17:40:36 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.037705	Loss2: 0.045366	 Dis: 4.425049 Entropy: 4.360804 
[2022-10-02 17:40:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.004095	 E_lr_2:0.004095
[2022-10-02 17:40:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000041	 C_lr_2:0.000041	 C_lr_3:0.000041	 C_lr_4:0.000041
[2022-10-02 17:40:42 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.100136	Loss2: 0.091631	 Dis: 5.175367 Entropy: 5.614424 
[2022-10-02 17:40:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.004034	 E_lr_2:0.004034
[2022-10-02 17:40:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 17:40:47 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.094809	Loss2: 0.095478	 Dis: 4.566097 Entropy: 4.855987 
[2022-10-02 17:40:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.003971	 E_lr_2:0.003971
[2022-10-02 17:40:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 17:40:54 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.047628	Loss2: 0.054355	 Dis: 4.191832 Entropy: 5.470723 
[2022-10-02 17:40:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.003907	 E_lr_2:0.003907
[2022-10-02 17:40:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000039	 C_lr_2:0.000039	 C_lr_3:0.000039	 C_lr_4:0.000039
[2022-10-02 17:41:00 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.275782	Loss2: 0.317849	 Dis: 3.199087 Entropy: 4.700114 
[2022-10-02 17:41:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.003841	 E_lr_2:0.003841
[2022-10-02 17:41:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 17:41:05 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.089432	Loss2: 0.059318	 Dis: 4.844986 Entropy: 5.175336 
[2022-10-02 17:41:05 demo] (houston_program2.py 512): INFO E_lr_1: 0.003774	 E_lr_2:0.003774
[2022-10-02 17:41:05 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 17:41:12 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.032465	Loss2: 0.032813	 Dis: 4.132360 Entropy: 4.635867 
[2022-10-02 17:41:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.003706	 E_lr_2:0.003706
[2022-10-02 17:41:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000037	 C_lr_2:0.000037	 C_lr_3:0.000037	 C_lr_4:0.000037
[2022-10-02 17:41:18 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.099145	Loss2: 0.073069	 Dis: 3.355019 Entropy: 5.072523 
[2022-10-02 17:41:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.003637	 E_lr_2:0.003637
[2022-10-02 17:41:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 17:41:24 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.046331	Loss2: 0.072455	 Dis: 2.626507 Entropy: 4.483954 
[2022-10-02 17:41:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.003566	 E_lr_2:0.003566
[2022-10-02 17:41:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 17:41:29 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.111343	Loss2: 0.122343	 Dis: 5.659231 Entropy: 5.103940 
[2022-10-02 17:41:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.003495	 E_lr_2:0.003495
[2022-10-02 17:41:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 17:41:35 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.031125	Loss2: 0.038409	 Dis: 4.598280 Entropy: 4.855357 
[2022-10-02 17:41:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.003422	 E_lr_2:0.003422
[2022-10-02 17:41:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000034	 C_lr_2:0.000034	 C_lr_3:0.000034	 C_lr_4:0.000034
[2022-10-02 17:41:41 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.021909	Loss2: 0.013372	 Dis: 2.107565 Entropy: 4.387236 
[2022-10-02 17:41:41 demo] (houston_program2.py 512): INFO E_lr_1: 0.003349	 E_lr_2:0.003349
[2022-10-02 17:41:41 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 17:41:48 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.162153	Loss2: 0.145234	 Dis: 6.345291 Entropy: 4.588206 
[2022-10-02 17:41:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.003274	 E_lr_2:0.003274
[2022-10-02 17:41:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 17:41:54 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.177387	Loss2: 0.187569	 Dis: 3.749269 Entropy: 6.335988 
[2022-10-02 17:41:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.003199	 E_lr_2:0.003199
[2022-10-02 17:41:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000032	 C_lr_2:0.000032	 C_lr_3:0.000032	 C_lr_4:0.000032
[2022-10-02 17:42:00 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.095146	Loss2: 0.087640	 Dis: 4.496771 Entropy: 5.188639 
[2022-10-02 17:42:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.003124	 E_lr_2:0.003124
[2022-10-02 17:42:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000031	 C_lr_2:0.000031	 C_lr_3:0.000031	 C_lr_4:0.000031
[2022-10-02 17:42:05 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.141563	Loss2: 0.078409	 Dis: 4.544863 Entropy: 6.086257 
[2022-10-02 17:42:05 demo] (houston_program2.py 512): INFO E_lr_1: 0.003047	 E_lr_2:0.003047
[2022-10-02 17:42:05 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 17:42:11 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.107529	Loss2: 0.124170	 Dis: 3.074169 Entropy: 4.722432 
[2022-10-02 17:42:11 demo] (houston_program2.py 512): INFO E_lr_1: 0.002970	 E_lr_2:0.002970
[2022-10-02 17:42:11 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 17:42:17 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.091823	Loss2: 0.100272	 Dis: 3.704142 Entropy: 4.583495 
[2022-10-02 17:42:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.002893	 E_lr_2:0.002893
[2022-10-02 17:42:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000029	 C_lr_2:0.000029	 C_lr_3:0.000029	 C_lr_4:0.000029
[2022-10-02 17:42:23 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.035247	Loss2: 0.016927	 Dis: 4.420177 Entropy: 4.577462 
[2022-10-02 17:42:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.002815	 E_lr_2:0.002815
[2022-10-02 17:42:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000028	 C_lr_2:0.000028	 C_lr_3:0.000028	 C_lr_4:0.000028
[2022-10-02 17:42:29 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.048185	Loss2: 0.041204	 Dis: 2.688290 Entropy: 4.952405 
[2022-10-02 17:42:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.002737	 E_lr_2:0.002737
[2022-10-02 17:42:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 17:42:35 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.125207	Loss2: 0.173964	 Dis: 4.264212 Entropy: 4.985662 
[2022-10-02 17:42:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.002659	 E_lr_2:0.002659
[2022-10-02 17:42:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 17:42:41 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.074773	Loss2: 0.095448	 Dis: 4.833069 Entropy: 4.923423 
[2022-10-02 17:42:41 demo] (houston_program2.py 512): INFO E_lr_1: 0.002581	 E_lr_2:0.002581
[2022-10-02 17:42:41 demo] (houston_program2.py 513): INFO C_lr_1: 0.000026	 C_lr_2:0.000026	 C_lr_3:0.000026	 C_lr_4:0.000026
[2022-10-02 17:42:47 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.046841	Loss2: 0.031563	 Dis: 2.592039 Entropy: 5.031148 
[2022-10-02 17:42:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.002502	 E_lr_2:0.002502
[2022-10-02 17:42:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 17:42:53 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.019715	Loss2: 0.023543	 Dis: 3.253792 Entropy: 5.527552 
[2022-10-02 17:42:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.002423	 E_lr_2:0.002423
[2022-10-02 17:42:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000024	 C_lr_2:0.000024	 C_lr_3:0.000024	 C_lr_4:0.000024
[2022-10-02 17:43:00 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.045448	Loss2: 0.038726	 Dis: 2.642492 Entropy: 4.230173 
[2022-10-02 17:43:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.002345	 E_lr_2:0.002345
[2022-10-02 17:43:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 17:43:06 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.042107	Loss2: 0.035428	 Dis: 2.635469 Entropy: 4.353189 
[2022-10-02 17:43:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.002267	 E_lr_2:0.002267
[2022-10-02 17:43:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 17:43:12 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.181999	Loss2: 0.238719	 Dis: 2.418428 Entropy: 5.403848 
[2022-10-02 17:43:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.002189	 E_lr_2:0.002189
[2022-10-02 17:43:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000022	 C_lr_2:0.000022	 C_lr_3:0.000022	 C_lr_4:0.000022
[2022-10-02 17:43:18 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.021592	Loss2: 0.023053	 Dis: 2.749702 Entropy: 4.553327 
[2022-10-02 17:43:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.002111	 E_lr_2:0.002111
[2022-10-02 17:43:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000021	 C_lr_2:0.000021	 C_lr_3:0.000021	 C_lr_4:0.000021
[2022-10-02 17:43:21 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.057708	Loss2: 0.029704	 Dis: 3.449142 Entropy: 4.533855 
[2022-10-02 17:43:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.002034	 E_lr_2:0.002034
[2022-10-02 17:43:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 17:43:28 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.081988	Loss2: 0.062271	 Dis: 2.240911 Entropy: 4.389467 
[2022-10-02 17:43:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.001957	 E_lr_2:0.001957
[2022-10-02 17:43:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 17:43:34 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.019665	Loss2: 0.028003	 Dis: 2.349867 Entropy: 4.341695 
[2022-10-02 17:43:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.001880	 E_lr_2:0.001880
[2022-10-02 17:43:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000019	 C_lr_2:0.000019	 C_lr_3:0.000019	 C_lr_4:0.000019
[2022-10-02 17:43:40 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.022252	Loss2: 0.016924	 Dis: 2.971100 Entropy: 4.370440 
[2022-10-02 17:43:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.001804	 E_lr_2:0.001804
[2022-10-02 17:43:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000018	 C_lr_2:0.000018	 C_lr_3:0.000018	 C_lr_4:0.000018
[2022-10-02 17:43:46 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.038443	Loss2: 0.024871	 Dis: 2.303787 Entropy: 5.530051 
[2022-10-02 17:43:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.001729	 E_lr_2:0.001729
[2022-10-02 17:43:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 17:43:52 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.027087	Loss2: 0.017840	 Dis: 2.678877 Entropy: 4.679517 
[2022-10-02 17:43:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.001655	 E_lr_2:0.001655
[2022-10-02 17:43:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 17:43:58 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.043125	Loss2: 0.060028	 Dis: 2.144411 Entropy: 4.308377 
[2022-10-02 17:43:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.001582	 E_lr_2:0.001582
[2022-10-02 17:43:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000016	 C_lr_2:0.000016	 C_lr_3:0.000016	 C_lr_4:0.000016
[2022-10-02 17:44:04 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.022760	Loss2: 0.016558	 Dis: 2.712299 Entropy: 4.827152 
[2022-10-02 17:44:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.001509	 E_lr_2:0.001509
[2022-10-02 17:44:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 17:44:10 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.073360	Loss2: 0.050545	 Dis: 1.806442 Entropy: 6.153530 
[2022-10-02 17:44:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.001437	 E_lr_2:0.001437
[2022-10-02 17:44:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 17:44:16 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.005401	Loss2: 0.004232	 Dis: 3.461271 Entropy: 4.852356 
[2022-10-02 17:44:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.001367	 E_lr_2:0.001367
[2022-10-02 17:44:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 17:44:22 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.015828	Loss2: 0.015219	 Dis: 1.531263 Entropy: 5.495792 
[2022-10-02 17:44:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.001297	 E_lr_2:0.001297
[2022-10-02 17:44:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000013	 C_lr_2:0.000013	 C_lr_3:0.000013	 C_lr_4:0.000013
[2022-10-02 17:44:28 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.003873	Loss2: 0.004583	 Dis: 1.701588 Entropy: 4.556645 
[2022-10-02 17:44:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.001229	 E_lr_2:0.001229
[2022-10-02 17:44:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 17:44:34 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.052567	Loss2: 0.050572	 Dis: 3.364473 Entropy: 5.483002 
[2022-10-02 17:44:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.001162	 E_lr_2:0.001162
[2022-10-02 17:44:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 17:44:40 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.035544	Loss2: 0.016109	 Dis: 2.544451 Entropy: 4.992054 
[2022-10-02 17:44:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.001096	 E_lr_2:0.001096
[2022-10-02 17:44:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000011	 C_lr_2:0.000011	 C_lr_3:0.000011	 C_lr_4:0.000011
[2022-10-02 17:44:46 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.082009	Loss2: 0.081268	 Dis: 0.830482 Entropy: 4.659021 
[2022-10-02 17:44:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.001032	 E_lr_2:0.001032
[2022-10-02 17:44:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 17:44:52 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.002644	Loss2: 0.003258	 Dis: 2.650230 Entropy: 5.340952 
[2022-10-02 17:44:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.000969	 E_lr_2:0.000969
[2022-10-02 17:44:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 17:44:57 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.001955	Loss2: 0.001941	 Dis: 1.482643 Entropy: 4.775542 
[2022-10-02 17:44:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.000908	 E_lr_2:0.000908
[2022-10-02 17:44:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000009	 C_lr_2:0.000009	 C_lr_3:0.000009	 C_lr_4:0.000009
[2022-10-02 17:45:03 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.022957	Loss2: 0.021838	 Dis: 3.171902 Entropy: 5.124641 
[2022-10-02 17:45:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.000848	 E_lr_2:0.000848
[2022-10-02 17:45:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 17:45:09 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.128403	Loss2: 0.128969	 Dis: 1.862242 Entropy: 4.955531 
[2022-10-02 17:45:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.000790	 E_lr_2:0.000790
[2022-10-02 17:45:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 17:45:15 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.005932	Loss2: 0.005654	 Dis: 5.341625 Entropy: 5.026444 
[2022-10-02 17:45:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.000734	 E_lr_2:0.000734
[2022-10-02 17:45:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 17:45:22 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.024690	Loss2: 0.040427	 Dis: 1.918884 Entropy: 6.097384 
[2022-10-02 17:45:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.000679	 E_lr_2:0.000679
[2022-10-02 17:45:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 17:45:27 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.002993	Loss2: 0.002928	 Dis: 1.994762 Entropy: 5.203286 
[2022-10-02 17:45:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.000626	 E_lr_2:0.000626
[2022-10-02 17:45:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 17:45:33 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.008559	Loss2: 0.007049	 Dis: 2.362461 Entropy: 4.306911 
[2022-10-02 17:45:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.000575	 E_lr_2:0.000575
[2022-10-02 17:45:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 17:45:39 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.001434	Loss2: 0.001298	 Dis: 2.340221 Entropy: 5.135421 
[2022-10-02 17:45:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.000526	 E_lr_2:0.000526
[2022-10-02 17:45:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 17:45:45 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.010072	Loss2: 0.007919	 Dis: 1.282871 Entropy: 4.369511 
[2022-10-02 17:45:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.000479	 E_lr_2:0.000479
[2022-10-02 17:45:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 17:45:51 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.024586	Loss2: 0.050533	 Dis: 2.625523 Entropy: 4.578413 
[2022-10-02 17:45:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.000433	 E_lr_2:0.000433
[2022-10-02 17:45:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 17:45:57 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.003259	Loss2: 0.003990	 Dis: 2.564888 Entropy: 5.471503 
[2022-10-02 17:45:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.000390	 E_lr_2:0.000390
[2022-10-02 17:45:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 17:46:03 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.021648	Loss2: 0.026946	 Dis: 2.767866 Entropy: 4.765808 
[2022-10-02 17:46:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.000349	 E_lr_2:0.000349
[2022-10-02 17:46:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 17:46:10 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.002855	Loss2: 0.002430	 Dis: 1.805031 Entropy: 5.437297 
[2022-10-02 17:46:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.000310	 E_lr_2:0.000310
[2022-10-02 17:46:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 17:46:16 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.002385	Loss2: 0.002888	 Dis: 3.124754 Entropy: 6.026499 
[2022-10-02 17:46:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.000273	 E_lr_2:0.000273
[2022-10-02 17:46:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 17:46:21 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.006913	Loss2: 0.011348	 Dis: 1.199242 Entropy: 4.700430 
[2022-10-02 17:46:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.000239	 E_lr_2:0.000239
[2022-10-02 17:46:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 17:46:27 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.006119	Loss2: 0.006464	 Dis: 1.656876 Entropy: 4.405753 
[2022-10-02 17:46:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.000206	 E_lr_2:0.000206
[2022-10-02 17:46:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 17:46:33 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.005841	Loss2: 0.005153	 Dis: 1.923805 Entropy: 4.849611 
[2022-10-02 17:46:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.000176	 E_lr_2:0.000176
[2022-10-02 17:46:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 17:46:40 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.005093	Loss2: 0.013463	 Dis: 0.804535 Entropy: 4.472063 
[2022-10-02 17:46:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.000148	 E_lr_2:0.000148
[2022-10-02 17:46:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:46:46 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.005622	Loss2: 0.003588	 Dis: 2.070816 Entropy: 4.225131 
[2022-10-02 17:46:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.000123	 E_lr_2:0.000123
[2022-10-02 17:46:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:46:52 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.003151	Loss2: 0.002261	 Dis: 2.069103 Entropy: 4.313980 
[2022-10-02 17:46:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.000100	 E_lr_2:0.000100
[2022-10-02 17:46:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:46:58 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.002276	Loss2: 0.001742	 Dis: 1.124481 Entropy: 5.988468 
[2022-10-02 17:46:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.000079	 E_lr_2:0.000079
[2022-10-02 17:46:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:47:04 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.006456	Loss2: 0.005507	 Dis: 2.617241 Entropy: 4.519215 
[2022-10-02 17:47:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.000061	 E_lr_2:0.000061
[2022-10-02 17:47:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:47:11 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.007387	Loss2: 0.011437	 Dis: 1.704163 Entropy: 5.577645 
[2022-10-02 17:47:11 demo] (houston_program2.py 512): INFO E_lr_1: 0.000045	 E_lr_2:0.000045
[2022-10-02 17:47:11 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:47:17 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.001460	Loss2: 0.001570	 Dis: 3.154686 Entropy: 4.767526 
[2022-10-02 17:47:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.000031	 E_lr_2:0.000031
[2022-10-02 17:47:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:47:23 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.005668	Loss2: 0.005160	 Dis: 1.911232 Entropy: 4.669182 
[2022-10-02 17:47:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.000020	 E_lr_2:0.000020
[2022-10-02 17:47:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:47:29 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.005527	Loss2: 0.006481	 Dis: 2.154524 Entropy: 4.396787 
[2022-10-02 17:47:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.000011	 E_lr_2:0.000011
[2022-10-02 17:47:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:47:35 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.011651	Loss2: 0.012184	 Dis: 1.831261 Entropy: 5.237111 
[2022-10-02 17:47:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.000005	 E_lr_2:0.000005
[2022-10-02 17:47:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:47:42 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.004048	Loss2: 0.003522	 Dis: 1.953291 Entropy: 5.450668 
[2022-10-02 17:47:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.000001	 E_lr_2:0.000001
[2022-10-02 17:47:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:47:47 demo] (houston_program2.py 504): INFO Train Ep: 25 	Loss1: 0.004038	Loss2: 0.003415	 Dis: 2.257772 Entropy: 5.438015 
[2022-10-02 17:47:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.000000	 E_lr_2:0.000000
[2022-10-02 17:47:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:47:47 demo] (houston_program2.py 515): INFO time_25_epoch:597.7501177787781
[2022-10-02 17:47:55 demo] (houston_program2.py 673): INFO 	val_Accuracy: 33859/53200 (63.64%)	
[2022-10-02 17:47:55 demo] (utils.py 47): INFO outputs/demo/finetune_100/ckpt_epoch_25.pth saving......
[2022-10-02 17:47:55 demo] (utils.py 49): INFO outputs/demo/finetune_100/ckpt_epoch_25.pth saved !!!
[2022-10-02 17:47:56 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0116 (0.0116)	loss 0.0151 (0.0151)	grad_norm 0.0581 (0.0581)	mem 460MB
[2022-10-02 17:47:56 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:47:56 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0200 (0.0200)	loss 0.0110 (0.0110)	grad_norm 0.0508 (0.0508)	mem 460MB
[2022-10-02 17:47:56 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:47:56 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0224 (0.0224)	loss 0.0129 (0.0129)	grad_norm 0.0878 (0.0878)	mem 460MB
[2022-10-02 17:47:56 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:47:56 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0203 (0.0203)	loss 0.0149 (0.0149)	grad_norm 0.0573 (0.0573)	mem 460MB
[2022-10-02 17:47:56 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:47:56 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0225 (0.0225)	loss 0.0113 (0.0113)	grad_norm 0.0631 (0.0631)	mem 460MB
[2022-10-02 17:47:56 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:47:57 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0199 (0.0199)	loss 0.0135 (0.0135)	grad_norm 0.0679 (0.0679)	mem 460MB
[2022-10-02 17:47:57 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:47:57 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0235 (0.0235)	loss 0.0119 (0.0119)	grad_norm 0.0492 (0.0492)	mem 460MB
[2022-10-02 17:47:57 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:47:57 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0217 (0.0217)	loss 0.0127 (0.0127)	grad_norm 0.0493 (0.0493)	mem 460MB
[2022-10-02 17:47:57 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:47:57 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0256 (0.0256)	loss 0.0143 (0.0143)	grad_norm 0.0772 (0.0772)	mem 460MB
[2022-10-02 17:47:57 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:47:57 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0220 (0.0220)	loss 0.0118 (0.0118)	grad_norm 0.0431 (0.0431)	mem 460MB
[2022-10-02 17:47:57 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:47:58 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0123 (0.0123)	loss 0.0110 (0.0110)	grad_norm 0.0423 (0.0423)	mem 460MB
[2022-10-02 17:47:58 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:47:58 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0204 (0.0204)	loss 0.0098 (0.0098)	grad_norm 0.0564 (0.0564)	mem 460MB
[2022-10-02 17:47:58 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:47:58 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0180 (0.0180)	loss 0.0105 (0.0105)	grad_norm 0.0669 (0.0669)	mem 460MB
[2022-10-02 17:47:58 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:47:58 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0203 (0.0203)	loss 0.0137 (0.0137)	grad_norm 0.0502 (0.0502)	mem 460MB
[2022-10-02 17:47:58 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:47:58 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0240 (0.0240)	loss 0.0119 (0.0119)	grad_norm 0.0767 (0.0767)	mem 460MB
[2022-10-02 17:47:58 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:47:58 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0276 (0.0276)	loss 0.0137 (0.0137)	grad_norm 0.0548 (0.0548)	mem 460MB
[2022-10-02 17:47:59 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:47:59 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0223 (0.0223)	loss 0.0104 (0.0104)	grad_norm 0.0486 (0.0486)	mem 460MB
[2022-10-02 17:47:59 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:47:59 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0094 (0.0094)	loss 0.0136 (0.0136)	grad_norm 0.0521 (0.0521)	mem 460MB
[2022-10-02 17:47:59 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:47:59 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0207 (0.0207)	loss 0.0176 (0.0176)	grad_norm 0.0530 (0.0530)	mem 460MB
[2022-10-02 17:47:59 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:47:59 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0165 (0.0165)	loss 0.0115 (0.0115)	grad_norm 0.0812 (0.0812)	mem 460MB
[2022-10-02 17:47:59 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:47:59 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0202 (0.0202)	loss 0.0112 (0.0112)	grad_norm 0.0451 (0.0451)	mem 460MB
[2022-10-02 17:47:59 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:48:00 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0201 (0.0201)	loss 0.0157 (0.0157)	grad_norm 0.0543 (0.0543)	mem 460MB
[2022-10-02 17:48:00 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:48:00 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0210 (0.0210)	loss 0.0126 (0.0126)	grad_norm 0.0805 (0.0805)	mem 460MB
[2022-10-02 17:48:00 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:48:00 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0207 (0.0207)	loss 0.0122 (0.0122)	grad_norm 0.0376 (0.0376)	mem 460MB
[2022-10-02 17:48:00 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:48:00 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0198 (0.0198)	loss 0.0139 (0.0139)	grad_norm 0.0520 (0.0520)	mem 460MB
[2022-10-02 17:48:00 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:48:00 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0123 (0.0123)	loss 0.0108 (0.0108)	grad_norm 0.0682 (0.0682)	mem 460MB
[2022-10-02 17:48:00 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:48:00 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0204 (0.0204)	loss 0.0138 (0.0138)	grad_norm 0.0361 (0.0361)	mem 460MB
[2022-10-02 17:48:00 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:48:01 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0136 (0.0136)	loss 0.0140 (0.0140)	grad_norm 0.0583 (0.0583)	mem 460MB
[2022-10-02 17:48:01 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:48:01 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0206 (0.0206)	loss 0.0107 (0.0107)	grad_norm 0.0657 (0.0657)	mem 460MB
[2022-10-02 17:48:01 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:48:01 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0135 (0.0135)	loss 0.0137 (0.0137)	grad_norm 0.0578 (0.0578)	mem 460MB
[2022-10-02 17:48:01 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:48:01 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0226 (0.0226)	loss 0.0160 (0.0160)	grad_norm 0.0720 (0.0720)	mem 460MB
[2022-10-02 17:48:01 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:48:01 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0140 (0.0140)	loss 0.0116 (0.0116)	grad_norm 0.0412 (0.0412)	mem 460MB
[2022-10-02 17:48:01 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:48:02 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0219 (0.0219)	loss 0.0123 (0.0123)	grad_norm 0.1010 (0.1010)	mem 460MB
[2022-10-02 17:48:02 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:48:02 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0217 (0.0217)	loss 0.0091 (0.0091)	grad_norm 0.0653 (0.0653)	mem 460MB
[2022-10-02 17:48:02 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:48:02 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0218 (0.0218)	loss 0.0153 (0.0153)	grad_norm 0.0699 (0.0699)	mem 460MB
[2022-10-02 17:48:02 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:48:02 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0208 (0.0208)	loss 0.0135 (0.0135)	grad_norm 0.0973 (0.0973)	mem 460MB
[2022-10-02 17:48:02 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:48:02 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0225 (0.0225)	loss 0.0144 (0.0144)	grad_norm 0.0826 (0.0826)	mem 460MB
[2022-10-02 17:48:02 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:48:02 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0212 (0.0212)	loss 0.0133 (0.0133)	grad_norm 0.0538 (0.0538)	mem 460MB
[2022-10-02 17:48:02 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:48:03 demo] (houston_program2.py 243): INFO Train: [26/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0153 (0.0153)	loss 0.0134 (0.0134)	grad_norm 0.0790 (0.0790)	mem 460MB
[2022-10-02 17:48:03 demo] (houston_program2.py 252): INFO EPOCH 26 training takes 0:00:00
[2022-10-02 17:48:03 demo] (houston_program2.py 333): INFO Train Ep: 26 	Loss1: 0.217753	Loss2: 0.196335	 Dis: 7.841986 Entropy: 4.988883 
[2022-10-02 17:48:03 demo] (houston_program2.py 335): INFO time_26_epoch:7.491503000259399
[2022-10-02 17:48:03 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0122 (0.0122)	loss 0.0131 (0.0131)	grad_norm 0.0586 (0.0586)	mem 460MB
[2022-10-02 17:48:03 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:03 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0222 (0.0222)	loss 0.0121 (0.0121)	grad_norm 0.0399 (0.0399)	mem 460MB
[2022-10-02 17:48:03 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:04 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000042	time 0.0269 (0.0269)	loss 0.0127 (0.0127)	grad_norm 0.0646 (0.0646)	mem 460MB
[2022-10-02 17:48:04 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:04 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0214 (0.0214)	loss 0.0107 (0.0107)	grad_norm 0.0842 (0.0842)	mem 460MB
[2022-10-02 17:48:04 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:04 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0119 (0.0119)	loss 0.0098 (0.0098)	grad_norm 0.0420 (0.0420)	mem 460MB
[2022-10-02 17:48:04 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:04 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0217 (0.0217)	loss 0.0113 (0.0113)	grad_norm 0.0661 (0.0661)	mem 460MB
[2022-10-02 17:48:04 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:04 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0113 (0.0113)	loss 0.0098 (0.0098)	grad_norm 0.0638 (0.0638)	mem 460MB
[2022-10-02 17:48:04 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:04 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0201 (0.0201)	loss 0.0149 (0.0149)	grad_norm 0.0626 (0.0626)	mem 460MB
[2022-10-02 17:48:04 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:05 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0157 (0.0157)	loss 0.0147 (0.0147)	grad_norm 0.0569 (0.0569)	mem 460MB
[2022-10-02 17:48:05 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:05 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0237 (0.0237)	loss 0.0123 (0.0123)	grad_norm 0.0613 (0.0613)	mem 460MB
[2022-10-02 17:48:05 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:05 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0172 (0.0172)	loss 0.0123 (0.0123)	grad_norm 0.0471 (0.0471)	mem 460MB
[2022-10-02 17:48:05 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:05 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0222 (0.0222)	loss 0.0154 (0.0154)	grad_norm 0.0515 (0.0515)	mem 460MB
[2022-10-02 17:48:05 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:05 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0220 (0.0220)	loss 0.0128 (0.0128)	grad_norm 0.0660 (0.0660)	mem 460MB
[2022-10-02 17:48:05 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:06 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0199 (0.0199)	loss 0.0141 (0.0141)	grad_norm 0.0606 (0.0606)	mem 460MB
[2022-10-02 17:48:06 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:06 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0218 (0.0218)	loss 0.0182 (0.0182)	grad_norm 0.0667 (0.0667)	mem 460MB
[2022-10-02 17:48:06 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:06 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0248 (0.0248)	loss 0.0134 (0.0134)	grad_norm 0.0559 (0.0559)	mem 460MB
[2022-10-02 17:48:06 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:06 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0202 (0.0202)	loss 0.0138 (0.0138)	grad_norm 0.0541 (0.0541)	mem 460MB
[2022-10-02 17:48:06 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:06 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0135 (0.0135)	loss 0.0130 (0.0130)	grad_norm 0.0749 (0.0749)	mem 460MB
[2022-10-02 17:48:06 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:06 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0197 (0.0197)	loss 0.0183 (0.0183)	grad_norm 0.0857 (0.0857)	mem 460MB
[2022-10-02 17:48:06 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:07 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0088 (0.0088)	loss 0.0084 (0.0084)	grad_norm 0.0397 (0.0397)	mem 460MB
[2022-10-02 17:48:07 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:07 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0097 (0.0097)	loss 0.0167 (0.0167)	grad_norm 0.0704 (0.0704)	mem 460MB
[2022-10-02 17:48:07 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:07 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0085 (0.0085)	loss 0.0102 (0.0102)	grad_norm 0.0437 (0.0437)	mem 460MB
[2022-10-02 17:48:07 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:07 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0083 (0.0083)	loss 0.0129 (0.0129)	grad_norm 0.0490 (0.0490)	mem 460MB
[2022-10-02 17:48:07 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:07 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0096 (0.0096)	loss 0.0122 (0.0122)	grad_norm 0.0643 (0.0643)	mem 460MB
[2022-10-02 17:48:07 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:07 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0084 (0.0084)	loss 0.0123 (0.0123)	grad_norm 0.0668 (0.0668)	mem 460MB
[2022-10-02 17:48:07 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:07 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0083 (0.0083)	loss 0.0120 (0.0120)	grad_norm 0.0394 (0.0394)	mem 460MB
[2022-10-02 17:48:07 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:07 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0094 (0.0094)	loss 0.0099 (0.0099)	grad_norm 0.0660 (0.0660)	mem 460MB
[2022-10-02 17:48:07 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:07 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0084 (0.0084)	loss 0.0122 (0.0122)	grad_norm 0.0736 (0.0736)	mem 460MB
[2022-10-02 17:48:07 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:07 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0125 (0.0125)	loss 0.0117 (0.0117)	grad_norm 0.0542 (0.0542)	mem 460MB
[2022-10-02 17:48:07 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:07 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0184 (0.0184)	loss 0.0175 (0.0175)	grad_norm 0.0781 (0.0781)	mem 460MB
[2022-10-02 17:48:07 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:08 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0200 (0.0200)	loss 0.0128 (0.0128)	grad_norm 0.0643 (0.0643)	mem 460MB
[2022-10-02 17:48:08 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:08 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0257 (0.0257)	loss 0.0107 (0.0107)	grad_norm 0.0656 (0.0656)	mem 460MB
[2022-10-02 17:48:08 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:08 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0204 (0.0204)	loss 0.0120 (0.0120)	grad_norm 0.0623 (0.0623)	mem 460MB
[2022-10-02 17:48:08 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:08 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0211 (0.0211)	loss 0.0110 (0.0110)	grad_norm 0.0690 (0.0690)	mem 460MB
[2022-10-02 17:48:08 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:08 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0163 (0.0163)	loss 0.0152 (0.0152)	grad_norm 0.0702 (0.0702)	mem 460MB
[2022-10-02 17:48:08 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:09 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0214 (0.0214)	loss 0.0133 (0.0133)	grad_norm 0.1022 (0.1022)	mem 460MB
[2022-10-02 17:48:09 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:09 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0204 (0.0204)	loss 0.0102 (0.0102)	grad_norm 0.0349 (0.0349)	mem 460MB
[2022-10-02 17:48:09 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:09 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0228 (0.0228)	loss 0.0128 (0.0128)	grad_norm 0.0856 (0.0856)	mem 460MB
[2022-10-02 17:48:09 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:09 demo] (houston_program2.py 243): INFO Train: [27/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0207 (0.0207)	loss 0.0127 (0.0127)	grad_norm 0.0670 (0.0670)	mem 460MB
[2022-10-02 17:48:09 demo] (houston_program2.py 252): INFO EPOCH 27 training takes 0:00:00
[2022-10-02 17:48:09 demo] (houston_program2.py 333): INFO Train Ep: 27 	Loss1: 0.152670	Loss2: 0.149341	 Dis: 7.192446 Entropy: 4.954951 
[2022-10-02 17:48:09 demo] (houston_program2.py 335): INFO time_27_epoch:6.485425233840942
[2022-10-02 17:48:10 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0144 (0.0144)	loss 0.0103 (0.0103)	grad_norm 0.0382 (0.0382)	mem 460MB
[2022-10-02 17:48:10 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:10 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0216 (0.0216)	loss 0.0095 (0.0095)	grad_norm 0.0628 (0.0628)	mem 460MB
[2022-10-02 17:48:10 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:10 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0225 (0.0225)	loss 0.0160 (0.0160)	grad_norm 0.0526 (0.0526)	mem 460MB
[2022-10-02 17:48:10 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:10 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0199 (0.0199)	loss 0.0129 (0.0129)	grad_norm 0.0453 (0.0453)	mem 460MB
[2022-10-02 17:48:10 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:11 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0222 (0.0222)	loss 0.0137 (0.0137)	grad_norm 0.0566 (0.0566)	mem 460MB
[2022-10-02 17:48:11 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:11 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0224 (0.0224)	loss 0.0121 (0.0121)	grad_norm 0.0455 (0.0455)	mem 460MB
[2022-10-02 17:48:11 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:11 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0216 (0.0216)	loss 0.0101 (0.0101)	grad_norm 0.0606 (0.0606)	mem 460MB
[2022-10-02 17:48:11 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:11 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0207 (0.0207)	loss 0.0183 (0.0183)	grad_norm 0.1160 (0.1160)	mem 460MB
[2022-10-02 17:48:11 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:11 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0257 (0.0257)	loss 0.0092 (0.0092)	grad_norm 0.0492 (0.0492)	mem 460MB
[2022-10-02 17:48:11 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:11 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0205 (0.0205)	loss 0.0104 (0.0104)	grad_norm 0.0716 (0.0716)	mem 460MB
[2022-10-02 17:48:11 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:12 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0121 (0.0121)	loss 0.0112 (0.0112)	grad_norm 0.0724 (0.0724)	mem 460MB
[2022-10-02 17:48:12 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:12 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0217 (0.0217)	loss 0.0140 (0.0140)	grad_norm 0.0407 (0.0407)	mem 460MB
[2022-10-02 17:48:12 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:12 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0197 (0.0197)	loss 0.0132 (0.0132)	grad_norm 0.0476 (0.0476)	mem 460MB
[2022-10-02 17:48:12 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:12 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0209 (0.0209)	loss 0.0120 (0.0120)	grad_norm 0.0865 (0.0865)	mem 460MB
[2022-10-02 17:48:12 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:12 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0193 (0.0193)	loss 0.0118 (0.0118)	grad_norm 0.0459 (0.0459)	mem 460MB
[2022-10-02 17:48:12 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:12 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0218 (0.0218)	loss 0.0139 (0.0139)	grad_norm 0.0462 (0.0462)	mem 460MB
[2022-10-02 17:48:13 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:13 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0174 (0.0174)	loss 0.0100 (0.0100)	grad_norm 0.0526 (0.0526)	mem 460MB
[2022-10-02 17:48:13 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:13 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0217 (0.0217)	loss 0.0092 (0.0092)	grad_norm 0.0467 (0.0467)	mem 460MB
[2022-10-02 17:48:13 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:13 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0208 (0.0208)	loss 0.0152 (0.0152)	grad_norm 0.0512 (0.0512)	mem 460MB
[2022-10-02 17:48:13 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:13 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0217 (0.0217)	loss 0.0115 (0.0115)	grad_norm 0.0422 (0.0422)	mem 460MB
[2022-10-02 17:48:13 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:13 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0210 (0.0210)	loss 0.0091 (0.0091)	grad_norm 0.0453 (0.0453)	mem 460MB
[2022-10-02 17:48:13 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:14 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0219 (0.0219)	loss 0.0159 (0.0159)	grad_norm 0.0550 (0.0550)	mem 460MB
[2022-10-02 17:48:14 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:14 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0219 (0.0219)	loss 0.0133 (0.0133)	grad_norm 0.0505 (0.0505)	mem 460MB
[2022-10-02 17:48:14 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:14 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0118 (0.0118)	loss 0.0117 (0.0117)	grad_norm 0.0478 (0.0478)	mem 460MB
[2022-10-02 17:48:14 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:14 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0216 (0.0216)	loss 0.0112 (0.0112)	grad_norm 0.0379 (0.0379)	mem 460MB
[2022-10-02 17:48:14 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:14 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0121 (0.0121)	loss 0.0120 (0.0120)	grad_norm 0.0478 (0.0478)	mem 460MB
[2022-10-02 17:48:14 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:14 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0210 (0.0210)	loss 0.0103 (0.0103)	grad_norm 0.0427 (0.0427)	mem 460MB
[2022-10-02 17:48:14 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:15 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0111 (0.0111)	loss 0.0130 (0.0130)	grad_norm 0.0355 (0.0355)	mem 460MB
[2022-10-02 17:48:15 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:15 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000041	time 0.0208 (0.0208)	loss 0.0106 (0.0106)	grad_norm 0.0432 (0.0432)	mem 460MB
[2022-10-02 17:48:15 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:15 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0122 (0.0122)	loss 0.0111 (0.0111)	grad_norm 0.0550 (0.0550)	mem 460MB
[2022-10-02 17:48:15 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:15 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0215 (0.0215)	loss 0.0169 (0.0169)	grad_norm 0.0564 (0.0564)	mem 460MB
[2022-10-02 17:48:15 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:15 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0139 (0.0139)	loss 0.0128 (0.0128)	grad_norm 0.0518 (0.0518)	mem 460MB
[2022-10-02 17:48:15 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:15 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0220 (0.0220)	loss 0.0162 (0.0162)	grad_norm 0.0401 (0.0401)	mem 460MB
[2022-10-02 17:48:15 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:16 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0159 (0.0159)	loss 0.0121 (0.0121)	grad_norm 0.0597 (0.0597)	mem 460MB
[2022-10-02 17:48:16 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:16 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0207 (0.0207)	loss 0.0114 (0.0114)	grad_norm 0.0610 (0.0610)	mem 460MB
[2022-10-02 17:48:16 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:16 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0166 (0.0166)	loss 0.0102 (0.0102)	grad_norm 0.0861 (0.0861)	mem 460MB
[2022-10-02 17:48:16 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:16 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0217 (0.0217)	loss 0.0096 (0.0096)	grad_norm 0.0325 (0.0325)	mem 460MB
[2022-10-02 17:48:16 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:16 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0185 (0.0185)	loss 0.0148 (0.0148)	grad_norm 0.0555 (0.0555)	mem 460MB
[2022-10-02 17:48:16 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:16 demo] (houston_program2.py 243): INFO Train: [28/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0097 (0.0097)	loss 0.0124 (0.0124)	grad_norm 0.0563 (0.0563)	mem 460MB
[2022-10-02 17:48:16 demo] (houston_program2.py 252): INFO EPOCH 28 training takes 0:00:00
[2022-10-02 17:48:17 demo] (houston_program2.py 333): INFO Train Ep: 28 	Loss1: 0.171849	Loss2: 0.149530	 Dis: 6.506796 Entropy: 5.287200 
[2022-10-02 17:48:17 demo] (houston_program2.py 335): INFO time_28_epoch:7.2145116329193115
[2022-10-02 17:48:17 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0118 (0.0118)	loss 0.0194 (0.0194)	grad_norm 0.0749 (0.0749)	mem 460MB
[2022-10-02 17:48:17 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:17 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0195 (0.0195)	loss 0.0157 (0.0157)	grad_norm 0.0543 (0.0543)	mem 460MB
[2022-10-02 17:48:17 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:17 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0109 (0.0109)	loss 0.0108 (0.0108)	grad_norm 0.0502 (0.0502)	mem 460MB
[2022-10-02 17:48:17 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:17 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0095 (0.0095)	loss 0.0113 (0.0113)	grad_norm 0.0492 (0.0492)	mem 460MB
[2022-10-02 17:48:17 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:17 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0081 (0.0081)	loss 0.0135 (0.0135)	grad_norm 0.0519 (0.0519)	mem 460MB
[2022-10-02 17:48:17 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:17 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0082 (0.0082)	loss 0.0131 (0.0131)	grad_norm 0.0578 (0.0578)	mem 460MB
[2022-10-02 17:48:17 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:18 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0091 (0.0091)	loss 0.0094 (0.0094)	grad_norm 0.0460 (0.0460)	mem 460MB
[2022-10-02 17:48:18 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:18 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0086 (0.0086)	loss 0.0101 (0.0101)	grad_norm 0.0801 (0.0801)	mem 460MB
[2022-10-02 17:48:18 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:18 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0082 (0.0082)	loss 0.0140 (0.0140)	grad_norm 0.0340 (0.0340)	mem 460MB
[2022-10-02 17:48:18 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:18 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0083 (0.0083)	loss 0.0114 (0.0114)	grad_norm 0.0713 (0.0713)	mem 460MB
[2022-10-02 17:48:18 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:18 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0133 (0.0133)	loss 0.0166 (0.0166)	grad_norm 0.0348 (0.0348)	mem 460MB
[2022-10-02 17:48:18 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:18 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0202 (0.0202)	loss 0.0098 (0.0098)	grad_norm 0.0330 (0.0330)	mem 460MB
[2022-10-02 17:48:18 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:18 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0276 (0.0276)	loss 0.0100 (0.0100)	grad_norm 0.0440 (0.0440)	mem 460MB
[2022-10-02 17:48:18 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:18 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0211 (0.0211)	loss 0.0110 (0.0110)	grad_norm 0.0502 (0.0502)	mem 460MB
[2022-10-02 17:48:18 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:19 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0120 (0.0120)	loss 0.0112 (0.0112)	grad_norm 0.0427 (0.0427)	mem 460MB
[2022-10-02 17:48:19 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:19 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0216 (0.0216)	loss 0.0118 (0.0118)	grad_norm 0.0425 (0.0425)	mem 460MB
[2022-10-02 17:48:19 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:19 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0116 (0.0116)	loss 0.0113 (0.0113)	grad_norm 0.0453 (0.0453)	mem 460MB
[2022-10-02 17:48:19 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:19 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0207 (0.0207)	loss 0.0109 (0.0109)	grad_norm 0.0695 (0.0695)	mem 460MB
[2022-10-02 17:48:19 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:19 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0119 (0.0119)	loss 0.0111 (0.0111)	grad_norm 0.0424 (0.0424)	mem 460MB
[2022-10-02 17:48:19 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:19 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0215 (0.0215)	loss 0.0108 (0.0108)	grad_norm 0.0436 (0.0436)	mem 460MB
[2022-10-02 17:48:19 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:20 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0150 (0.0150)	loss 0.0150 (0.0150)	grad_norm 0.0444 (0.0444)	mem 460MB
[2022-10-02 17:48:20 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:20 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0186 (0.0186)	loss 0.0142 (0.0142)	grad_norm 0.0389 (0.0389)	mem 460MB
[2022-10-02 17:48:20 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:20 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0166 (0.0166)	loss 0.0145 (0.0145)	grad_norm 0.0455 (0.0455)	mem 460MB
[2022-10-02 17:48:20 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:20 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0221 (0.0221)	loss 0.0135 (0.0135)	grad_norm 0.0435 (0.0435)	mem 460MB
[2022-10-02 17:48:20 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:20 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0198 (0.0198)	loss 0.0110 (0.0110)	grad_norm 0.0787 (0.0787)	mem 460MB
[2022-10-02 17:48:20 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:20 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0199 (0.0199)	loss 0.0142 (0.0142)	grad_norm 0.0480 (0.0480)	mem 460MB
[2022-10-02 17:48:21 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:21 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0200 (0.0200)	loss 0.0102 (0.0102)	grad_norm 0.0432 (0.0432)	mem 460MB
[2022-10-02 17:48:21 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:21 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0205 (0.0205)	loss 0.0125 (0.0125)	grad_norm 0.0654 (0.0654)	mem 460MB
[2022-10-02 17:48:21 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:21 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0206 (0.0206)	loss 0.0088 (0.0088)	grad_norm 0.0440 (0.0440)	mem 460MB
[2022-10-02 17:48:21 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:21 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0202 (0.0202)	loss 0.0119 (0.0119)	grad_norm 0.0606 (0.0606)	mem 460MB
[2022-10-02 17:48:21 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:21 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0202 (0.0202)	loss 0.0098 (0.0098)	grad_norm 0.0568 (0.0568)	mem 460MB
[2022-10-02 17:48:21 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:22 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0239 (0.0239)	loss 0.0136 (0.0136)	grad_norm 0.0461 (0.0461)	mem 460MB
[2022-10-02 17:48:22 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:22 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0232 (0.0232)	loss 0.0168 (0.0168)	grad_norm 0.0517 (0.0517)	mem 460MB
[2022-10-02 17:48:22 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:22 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0138 (0.0138)	loss 0.0117 (0.0117)	grad_norm 0.0714 (0.0714)	mem 460MB
[2022-10-02 17:48:22 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:22 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0215 (0.0215)	loss 0.0106 (0.0106)	grad_norm 0.0514 (0.0514)	mem 460MB
[2022-10-02 17:48:22 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:22 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0136 (0.0136)	loss 0.0122 (0.0122)	grad_norm 0.0574 (0.0574)	mem 460MB
[2022-10-02 17:48:22 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:23 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0184 (0.0184)	loss 0.0163 (0.0163)	grad_norm 0.0721 (0.0721)	mem 460MB
[2022-10-02 17:48:23 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:23 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0188 (0.0188)	loss 0.0100 (0.0100)	grad_norm 0.0696 (0.0696)	mem 460MB
[2022-10-02 17:48:23 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:23 demo] (houston_program2.py 243): INFO Train: [29/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0092 (0.0092)	loss 0.0111 (0.0111)	grad_norm 0.0566 (0.0566)	mem 460MB
[2022-10-02 17:48:23 demo] (houston_program2.py 252): INFO EPOCH 29 training takes 0:00:00
[2022-10-02 17:48:23 demo] (houston_program2.py 333): INFO Train Ep: 29 	Loss1: 0.165580	Loss2: 0.188895	 Dis: 5.346209 Entropy: 4.979459 
[2022-10-02 17:48:23 demo] (houston_program2.py 335): INFO time_29_epoch:6.294708728790283
[2022-10-02 17:48:23 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0116 (0.0116)	loss 0.0104 (0.0104)	grad_norm 0.0401 (0.0401)	mem 460MB
[2022-10-02 17:48:23 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:23 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0214 (0.0214)	loss 0.0145 (0.0145)	grad_norm 0.0689 (0.0689)	mem 460MB
[2022-10-02 17:48:23 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:24 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0199 (0.0199)	loss 0.0151 (0.0151)	grad_norm 0.0719 (0.0719)	mem 460MB
[2022-10-02 17:48:24 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:24 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0196 (0.0196)	loss 0.0132 (0.0132)	grad_norm 0.0574 (0.0574)	mem 460MB
[2022-10-02 17:48:24 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:24 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0214 (0.0214)	loss 0.0137 (0.0137)	grad_norm 0.0684 (0.0684)	mem 460MB
[2022-10-02 17:48:24 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:24 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0220 (0.0220)	loss 0.0116 (0.0116)	grad_norm 0.0615 (0.0615)	mem 460MB
[2022-10-02 17:48:24 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:24 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0217 (0.0217)	loss 0.0137 (0.0137)	grad_norm 0.0640 (0.0640)	mem 460MB
[2022-10-02 17:48:24 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:25 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0223 (0.0223)	loss 0.0137 (0.0137)	grad_norm 0.0601 (0.0601)	mem 460MB
[2022-10-02 17:48:25 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:25 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0202 (0.0202)	loss 0.0126 (0.0126)	grad_norm 0.0571 (0.0571)	mem 460MB
[2022-10-02 17:48:25 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:25 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0216 (0.0216)	loss 0.0107 (0.0107)	grad_norm 0.0659 (0.0659)	mem 460MB
[2022-10-02 17:48:25 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:25 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0194 (0.0194)	loss 0.0151 (0.0151)	grad_norm 0.0904 (0.0904)	mem 460MB
[2022-10-02 17:48:25 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:25 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0224 (0.0224)	loss 0.0102 (0.0102)	grad_norm 0.0545 (0.0545)	mem 460MB
[2022-10-02 17:48:25 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:25 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000040	time 0.0195 (0.0195)	loss 0.0129 (0.0129)	grad_norm 0.0593 (0.0593)	mem 460MB
[2022-10-02 17:48:25 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:26 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0231 (0.0231)	loss 0.0100 (0.0100)	grad_norm 0.0538 (0.0538)	mem 460MB
[2022-10-02 17:48:26 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:26 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0208 (0.0208)	loss 0.0145 (0.0145)	grad_norm 0.0465 (0.0465)	mem 460MB
[2022-10-02 17:48:26 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:26 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0137 (0.0137)	loss 0.0124 (0.0124)	grad_norm 0.0572 (0.0572)	mem 460MB
[2022-10-02 17:48:26 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:26 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0201 (0.0201)	loss 0.0126 (0.0126)	grad_norm 0.0406 (0.0406)	mem 460MB
[2022-10-02 17:48:26 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:26 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0116 (0.0116)	loss 0.0122 (0.0122)	grad_norm 0.0628 (0.0628)	mem 460MB
[2022-10-02 17:48:26 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:26 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0202 (0.0202)	loss 0.0090 (0.0090)	grad_norm 0.0514 (0.0514)	mem 460MB
[2022-10-02 17:48:27 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:27 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0139 (0.0139)	loss 0.0114 (0.0114)	grad_norm 0.0425 (0.0425)	mem 460MB
[2022-10-02 17:48:27 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:27 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0228 (0.0228)	loss 0.0113 (0.0113)	grad_norm 0.0635 (0.0635)	mem 460MB
[2022-10-02 17:48:27 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:27 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0162 (0.0162)	loss 0.0099 (0.0099)	grad_norm 0.0437 (0.0437)	mem 460MB
[2022-10-02 17:48:27 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:27 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0204 (0.0204)	loss 0.0110 (0.0110)	grad_norm 0.0375 (0.0375)	mem 460MB
[2022-10-02 17:48:27 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:27 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0226 (0.0226)	loss 0.0103 (0.0103)	grad_norm 0.0647 (0.0647)	mem 460MB
[2022-10-02 17:48:27 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:28 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0230 (0.0230)	loss 0.0086 (0.0086)	grad_norm 0.0510 (0.0510)	mem 460MB
[2022-10-02 17:48:28 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:28 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0220 (0.0220)	loss 0.0102 (0.0102)	grad_norm 0.0379 (0.0379)	mem 460MB
[2022-10-02 17:48:28 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:28 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0229 (0.0229)	loss 0.0105 (0.0105)	grad_norm 0.0434 (0.0434)	mem 460MB
[2022-10-02 17:48:28 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:28 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0239 (0.0239)	loss 0.0099 (0.0099)	grad_norm 0.0609 (0.0609)	mem 460MB
[2022-10-02 17:48:28 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:28 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0165 (0.0165)	loss 0.0087 (0.0087)	grad_norm 0.0510 (0.0510)	mem 460MB
[2022-10-02 17:48:28 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:29 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0247 (0.0247)	loss 0.0124 (0.0124)	grad_norm 0.0647 (0.0647)	mem 460MB
[2022-10-02 17:48:29 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:29 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0241 (0.0241)	loss 0.0110 (0.0110)	grad_norm 0.0464 (0.0464)	mem 460MB
[2022-10-02 17:48:29 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:29 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0185 (0.0185)	loss 0.0134 (0.0134)	grad_norm 0.0609 (0.0609)	mem 460MB
[2022-10-02 17:48:29 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:29 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0243 (0.0243)	loss 0.0113 (0.0113)	grad_norm 0.0487 (0.0487)	mem 460MB
[2022-10-02 17:48:29 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:29 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0177 (0.0177)	loss 0.0116 (0.0116)	grad_norm 0.0366 (0.0366)	mem 460MB
[2022-10-02 17:48:29 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:30 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0233 (0.0233)	loss 0.0126 (0.0126)	grad_norm 0.0636 (0.0636)	mem 460MB
[2022-10-02 17:48:30 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:30 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0172 (0.0172)	loss 0.0117 (0.0117)	grad_norm 0.0579 (0.0579)	mem 460MB
[2022-10-02 17:48:30 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:30 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0205 (0.0205)	loss 0.0166 (0.0166)	grad_norm 0.1350 (0.1350)	mem 460MB
[2022-10-02 17:48:30 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:30 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0167 (0.0167)	loss 0.0107 (0.0107)	grad_norm 0.0457 (0.0457)	mem 460MB
[2022-10-02 17:48:30 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:30 demo] (houston_program2.py 243): INFO Train: [30/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0208 (0.0208)	loss 0.0104 (0.0104)	grad_norm 0.0529 (0.0529)	mem 460MB
[2022-10-02 17:48:30 demo] (houston_program2.py 252): INFO EPOCH 30 training takes 0:00:00
[2022-10-02 17:48:30 demo] (houston_program2.py 333): INFO Train Ep: 30 	Loss1: 0.064638	Loss2: 0.057886	 Dis: 6.218487 Entropy: 5.357763 
[2022-10-02 17:48:30 demo] (houston_program2.py 335): INFO time_30_epoch:7.564958095550537
[2022-10-02 17:48:30 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 17:48:30 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 17:48:30 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 17:48:30 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 17:48:30 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 17:48:30 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 17:48:30 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 17:48:30 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 17:48:30 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 17:48:30 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 17:48:36 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.079535	Loss2: 0.102809	 Dis: 4.162098 Entropy: 4.828412 
[2022-10-02 17:48:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.000487	 E_lr_2:0.000487
[2022-10-02 17:48:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 17:48:43 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.094790	Loss2: 0.081680	 Dis: 5.567921 Entropy: 5.414617 
[2022-10-02 17:48:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.000987	 E_lr_2:0.000987
[2022-10-02 17:48:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 17:48:49 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.075431	Loss2: 0.077709	 Dis: 4.942156 Entropy: 4.592476 
[2022-10-02 17:48:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.001487	 E_lr_2:0.001487
[2022-10-02 17:48:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 17:48:55 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.065387	Loss2: 0.076083	 Dis: 2.686136 Entropy: 4.456724 
[2022-10-02 17:48:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.001987	 E_lr_2:0.001987
[2022-10-02 17:48:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 17:49:01 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.053824	Loss2: 0.065166	 Dis: 5.514931 Entropy: 4.625340 
[2022-10-02 17:49:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.002487	 E_lr_2:0.002487
[2022-10-02 17:49:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 17:49:07 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.053030	Loss2: 0.059002	 Dis: 2.682261 Entropy: 4.390011 
[2022-10-02 17:49:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.002987	 E_lr_2:0.002987
[2022-10-02 17:49:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 17:49:12 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.134368	Loss2: 0.113138	 Dis: 5.634436 Entropy: 4.448758 
[2022-10-02 17:49:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.003487	 E_lr_2:0.003487
[2022-10-02 17:49:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 17:49:19 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.231839	Loss2: 0.264744	 Dis: 4.488251 Entropy: 4.445734 
[2022-10-02 17:49:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.003987	 E_lr_2:0.003987
[2022-10-02 17:49:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 17:49:25 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.226342	Loss2: 0.234520	 Dis: 5.597178 Entropy: 4.798537 
[2022-10-02 17:49:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.004487	 E_lr_2:0.004487
[2022-10-02 17:49:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 17:49:31 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.189893	Loss2: 0.189907	 Dis: 5.400757 Entropy: 4.673862 
[2022-10-02 17:49:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.004987	 E_lr_2:0.004987
[2022-10-02 17:49:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000050	 C_lr_2:0.000050	 C_lr_3:0.000050	 C_lr_4:0.000050
[2022-10-02 17:49:36 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.149769	Loss2: 0.188179	 Dis: 5.206367 Entropy: 4.711387 
[2022-10-02 17:49:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.004853	 E_lr_2:0.004853
[2022-10-02 17:49:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000049	 C_lr_2:0.000049	 C_lr_3:0.000049	 C_lr_4:0.000049
[2022-10-02 17:49:42 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.082432	Loss2: 0.087764	 Dis: 5.239273 Entropy: 5.281527 
[2022-10-02 17:49:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.004825	 E_lr_2:0.004825
[2022-10-02 17:49:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 17:49:48 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.195194	Loss2: 0.166435	 Dis: 6.823994 Entropy: 5.146123 
[2022-10-02 17:49:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.004795	 E_lr_2:0.004795
[2022-10-02 17:49:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 17:49:54 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.576567	Loss2: 0.529770	 Dis: 6.523970 Entropy: 4.462716 
[2022-10-02 17:49:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.004763	 E_lr_2:0.004763
[2022-10-02 17:49:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 17:50:00 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.241677	Loss2: 0.202195	 Dis: 9.142429 Entropy: 6.316506 
[2022-10-02 17:50:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.004728	 E_lr_2:0.004728
[2022-10-02 17:50:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 17:50:06 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.236163	Loss2: 0.270522	 Dis: 7.270512 Entropy: 5.895311 
[2022-10-02 17:50:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.004692	 E_lr_2:0.004692
[2022-10-02 17:50:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 17:50:12 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.214210	Loss2: 0.175010	 Dis: 4.898645 Entropy: 4.514969 
[2022-10-02 17:50:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.004653	 E_lr_2:0.004653
[2022-10-02 17:50:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 17:50:19 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.340359	Loss2: 0.357524	 Dis: 8.997023 Entropy: 5.863372 
[2022-10-02 17:50:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.004612	 E_lr_2:0.004612
[2022-10-02 17:50:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 17:50:25 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.143672	Loss2: 0.130347	 Dis: 5.334541 Entropy: 4.756088 
[2022-10-02 17:50:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.004569	 E_lr_2:0.004569
[2022-10-02 17:50:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 17:50:31 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.137755	Loss2: 0.115967	 Dis: 3.945721 Entropy: 5.305158 
[2022-10-02 17:50:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.004524	 E_lr_2:0.004524
[2022-10-02 17:50:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 17:50:34 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.020282	Loss2: 0.021126	 Dis: 4.273777 Entropy: 5.292662 
[2022-10-02 17:50:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.004477	 E_lr_2:0.004477
[2022-10-02 17:50:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 17:50:40 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.104204	Loss2: 0.104024	 Dis: 4.248549 Entropy: 4.224505 
[2022-10-02 17:50:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.004428	 E_lr_2:0.004428
[2022-10-02 17:50:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 17:50:45 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.060155	Loss2: 0.062413	 Dis: 5.006699 Entropy: 5.586352 
[2022-10-02 17:50:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.004377	 E_lr_2:0.004377
[2022-10-02 17:50:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 17:50:51 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.221675	Loss2: 0.202726	 Dis: 7.770107 Entropy: 4.260195 
[2022-10-02 17:50:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.004324	 E_lr_2:0.004324
[2022-10-02 17:50:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 17:50:57 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.093445	Loss2: 0.103438	 Dis: 4.407515 Entropy: 5.166730 
[2022-10-02 17:50:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.004269	 E_lr_2:0.004269
[2022-10-02 17:50:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 17:51:02 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.073542	Loss2: 0.077961	 Dis: 4.715467 Entropy: 5.411708 
[2022-10-02 17:51:02 demo] (houston_program2.py 512): INFO E_lr_1: 0.004213	 E_lr_2:0.004213
[2022-10-02 17:51:02 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 17:51:08 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.104582	Loss2: 0.135171	 Dis: 4.569475 Entropy: 5.511022 
[2022-10-02 17:51:08 demo] (houston_program2.py 512): INFO E_lr_1: 0.004155	 E_lr_2:0.004155
[2022-10-02 17:51:08 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 17:51:14 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.202429	Loss2: 0.223288	 Dis: 5.380894 Entropy: 5.620574 
[2022-10-02 17:51:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.004095	 E_lr_2:0.004095
[2022-10-02 17:51:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000041	 C_lr_2:0.000041	 C_lr_3:0.000041	 C_lr_4:0.000041
[2022-10-02 17:51:20 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.078156	Loss2: 0.076986	 Dis: 4.666599 Entropy: 4.272391 
[2022-10-02 17:51:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.004034	 E_lr_2:0.004034
[2022-10-02 17:51:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 17:51:27 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.172395	Loss2: 0.158271	 Dis: 5.953747 Entropy: 5.192670 
[2022-10-02 17:51:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.003971	 E_lr_2:0.003971
[2022-10-02 17:51:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 17:51:33 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.191402	Loss2: 0.152937	 Dis: 2.621058 Entropy: 5.239913 
[2022-10-02 17:51:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.003907	 E_lr_2:0.003907
[2022-10-02 17:51:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000039	 C_lr_2:0.000039	 C_lr_3:0.000039	 C_lr_4:0.000039
[2022-10-02 17:51:39 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.105754	Loss2: 0.105108	 Dis: 4.276623 Entropy: 5.283715 
[2022-10-02 17:51:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.003841	 E_lr_2:0.003841
[2022-10-02 17:51:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 17:51:45 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.105662	Loss2: 0.097031	 Dis: 4.646835 Entropy: 5.732013 
[2022-10-02 17:51:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.003774	 E_lr_2:0.003774
[2022-10-02 17:51:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 17:51:51 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.056361	Loss2: 0.050265	 Dis: 3.583279 Entropy: 5.836915 
[2022-10-02 17:51:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.003706	 E_lr_2:0.003706
[2022-10-02 17:51:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000037	 C_lr_2:0.000037	 C_lr_3:0.000037	 C_lr_4:0.000037
[2022-10-02 17:51:57 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.098242	Loss2: 0.102248	 Dis: 5.883656 Entropy: 4.846868 
[2022-10-02 17:51:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.003637	 E_lr_2:0.003637
[2022-10-02 17:51:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 17:52:03 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.103311	Loss2: 0.102071	 Dis: 4.831549 Entropy: 6.478595 
[2022-10-02 17:52:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.003566	 E_lr_2:0.003566
[2022-10-02 17:52:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 17:52:09 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.313252	Loss2: 0.290635	 Dis: 3.388552 Entropy: 4.512483 
[2022-10-02 17:52:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.003495	 E_lr_2:0.003495
[2022-10-02 17:52:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 17:52:15 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.256463	Loss2: 0.268464	 Dis: 10.968496 Entropy: 4.544882 
[2022-10-02 17:52:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.003422	 E_lr_2:0.003422
[2022-10-02 17:52:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000034	 C_lr_2:0.000034	 C_lr_3:0.000034	 C_lr_4:0.000034
[2022-10-02 17:52:21 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.154462	Loss2: 0.149780	 Dis: 6.970295 Entropy: 5.619689 
[2022-10-02 17:52:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.003349	 E_lr_2:0.003349
[2022-10-02 17:52:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 17:52:28 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.147671	Loss2: 0.136681	 Dis: 4.671684 Entropy: 4.805744 
[2022-10-02 17:52:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.003274	 E_lr_2:0.003274
[2022-10-02 17:52:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 17:52:34 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.102441	Loss2: 0.127190	 Dis: 2.982435 Entropy: 4.585167 
[2022-10-02 17:52:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.003199	 E_lr_2:0.003199
[2022-10-02 17:52:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000032	 C_lr_2:0.000032	 C_lr_3:0.000032	 C_lr_4:0.000032
[2022-10-02 17:52:39 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.166601	Loss2: 0.143102	 Dis: 3.574282 Entropy: 5.357495 
[2022-10-02 17:52:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.003124	 E_lr_2:0.003124
[2022-10-02 17:52:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000031	 C_lr_2:0.000031	 C_lr_3:0.000031	 C_lr_4:0.000031
[2022-10-02 17:52:46 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.190106	Loss2: 0.208566	 Dis: 2.870859 Entropy: 4.595785 
[2022-10-02 17:52:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.003047	 E_lr_2:0.003047
[2022-10-02 17:52:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 17:52:52 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.082749	Loss2: 0.101951	 Dis: 4.643745 Entropy: 6.295215 
[2022-10-02 17:52:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.002970	 E_lr_2:0.002970
[2022-10-02 17:52:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 17:52:58 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.510688	Loss2: 0.505126	 Dis: 6.055023 Entropy: 7.308555 
[2022-10-02 17:52:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.002893	 E_lr_2:0.002893
[2022-10-02 17:52:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000029	 C_lr_2:0.000029	 C_lr_3:0.000029	 C_lr_4:0.000029
[2022-10-02 17:53:04 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.044599	Loss2: 0.041290	 Dis: 2.531908 Entropy: 5.706400 
[2022-10-02 17:53:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.002815	 E_lr_2:0.002815
[2022-10-02 17:53:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000028	 C_lr_2:0.000028	 C_lr_3:0.000028	 C_lr_4:0.000028
[2022-10-02 17:53:10 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.029958	Loss2: 0.024664	 Dis: 3.746782 Entropy: 4.909377 
[2022-10-02 17:53:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.002737	 E_lr_2:0.002737
[2022-10-02 17:53:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 17:53:16 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.119691	Loss2: 0.144494	 Dis: 3.214773 Entropy: 5.725348 
[2022-10-02 17:53:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.002659	 E_lr_2:0.002659
[2022-10-02 17:53:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 17:53:21 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.022622	Loss2: 0.031066	 Dis: 2.854317 Entropy: 5.108557 
[2022-10-02 17:53:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.002581	 E_lr_2:0.002581
[2022-10-02 17:53:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000026	 C_lr_2:0.000026	 C_lr_3:0.000026	 C_lr_4:0.000026
[2022-10-02 17:53:27 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.012036	Loss2: 0.010866	 Dis: 5.535589 Entropy: 4.656947 
[2022-10-02 17:53:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.002502	 E_lr_2:0.002502
[2022-10-02 17:53:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 17:53:33 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.013681	Loss2: 0.015819	 Dis: 4.097534 Entropy: 4.852046 
[2022-10-02 17:53:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.002423	 E_lr_2:0.002423
[2022-10-02 17:53:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000024	 C_lr_2:0.000024	 C_lr_3:0.000024	 C_lr_4:0.000024
[2022-10-02 17:53:40 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.094533	Loss2: 0.090860	 Dis: 2.622889 Entropy: 5.382175 
[2022-10-02 17:53:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.002345	 E_lr_2:0.002345
[2022-10-02 17:53:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 17:53:45 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.100104	Loss2: 0.078578	 Dis: 4.247398 Entropy: 4.659742 
[2022-10-02 17:53:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.002267	 E_lr_2:0.002267
[2022-10-02 17:53:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 17:53:51 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.055308	Loss2: 0.064435	 Dis: 2.148849 Entropy: 6.062302 
[2022-10-02 17:53:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.002189	 E_lr_2:0.002189
[2022-10-02 17:53:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000022	 C_lr_2:0.000022	 C_lr_3:0.000022	 C_lr_4:0.000022
[2022-10-02 17:53:57 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.010473	Loss2: 0.015572	 Dis: 4.208714 Entropy: 5.337987 
[2022-10-02 17:53:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.002111	 E_lr_2:0.002111
[2022-10-02 17:53:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000021	 C_lr_2:0.000021	 C_lr_3:0.000021	 C_lr_4:0.000021
[2022-10-02 17:54:03 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.044236	Loss2: 0.045511	 Dis: 2.559357 Entropy: 6.398561 
[2022-10-02 17:54:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.002034	 E_lr_2:0.002034
[2022-10-02 17:54:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 17:54:09 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.033164	Loss2: 0.037018	 Dis: 3.663864 Entropy: 4.820907 
[2022-10-02 17:54:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.001957	 E_lr_2:0.001957
[2022-10-02 17:54:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 17:54:15 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.038464	Loss2: 0.041929	 Dis: 3.957764 Entropy: 5.686556 
[2022-10-02 17:54:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.001880	 E_lr_2:0.001880
[2022-10-02 17:54:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000019	 C_lr_2:0.000019	 C_lr_3:0.000019	 C_lr_4:0.000019
[2022-10-02 17:54:21 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.054254	Loss2: 0.064218	 Dis: 2.489202 Entropy: 6.170521 
[2022-10-02 17:54:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.001804	 E_lr_2:0.001804
[2022-10-02 17:54:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000018	 C_lr_2:0.000018	 C_lr_3:0.000018	 C_lr_4:0.000018
[2022-10-02 17:54:27 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.019910	Loss2: 0.018666	 Dis: 2.863974 Entropy: 4.373775 
[2022-10-02 17:54:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.001729	 E_lr_2:0.001729
[2022-10-02 17:54:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 17:54:33 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.002473	Loss2: 0.002627	 Dis: 2.769920 Entropy: 4.376778 
[2022-10-02 17:54:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.001655	 E_lr_2:0.001655
[2022-10-02 17:54:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 17:54:39 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.010629	Loss2: 0.016949	 Dis: 4.130262 Entropy: 4.805991 
[2022-10-02 17:54:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.001582	 E_lr_2:0.001582
[2022-10-02 17:54:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000016	 C_lr_2:0.000016	 C_lr_3:0.000016	 C_lr_4:0.000016
[2022-10-02 17:54:45 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.004233	Loss2: 0.003985	 Dis: 3.032785 Entropy: 5.390788 
[2022-10-02 17:54:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.001509	 E_lr_2:0.001509
[2022-10-02 17:54:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 17:54:51 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.041441	Loss2: 0.058269	 Dis: 2.181671 Entropy: 6.329296 
[2022-10-02 17:54:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.001437	 E_lr_2:0.001437
[2022-10-02 17:54:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 17:54:58 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.009943	Loss2: 0.010045	 Dis: 1.749788 Entropy: 5.874006 
[2022-10-02 17:54:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.001367	 E_lr_2:0.001367
[2022-10-02 17:54:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 17:55:04 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.023591	Loss2: 0.031281	 Dis: 3.729704 Entropy: 5.318600 
[2022-10-02 17:55:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.001297	 E_lr_2:0.001297
[2022-10-02 17:55:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000013	 C_lr_2:0.000013	 C_lr_3:0.000013	 C_lr_4:0.000013
[2022-10-02 17:55:10 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.015535	Loss2: 0.014605	 Dis: 3.094618 Entropy: 4.931780 
[2022-10-02 17:55:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.001229	 E_lr_2:0.001229
[2022-10-02 17:55:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 17:55:16 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.028201	Loss2: 0.026105	 Dis: 2.296721 Entropy: 4.842085 
[2022-10-02 17:55:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.001162	 E_lr_2:0.001162
[2022-10-02 17:55:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 17:55:22 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.004468	Loss2: 0.005261	 Dis: 1.777945 Entropy: 4.992902 
[2022-10-02 17:55:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.001096	 E_lr_2:0.001096
[2022-10-02 17:55:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000011	 C_lr_2:0.000011	 C_lr_3:0.000011	 C_lr_4:0.000011
[2022-10-02 17:55:28 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.017698	Loss2: 0.018131	 Dis: 3.036133 Entropy: 4.747126 
[2022-10-02 17:55:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.001032	 E_lr_2:0.001032
[2022-10-02 17:55:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 17:55:32 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.013998	Loss2: 0.009964	 Dis: 1.827335 Entropy: 4.852332 
[2022-10-02 17:55:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.000969	 E_lr_2:0.000969
[2022-10-02 17:55:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 17:55:38 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.020274	Loss2: 0.017735	 Dis: 3.053703 Entropy: 5.149581 
[2022-10-02 17:55:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.000908	 E_lr_2:0.000908
[2022-10-02 17:55:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000009	 C_lr_2:0.000009	 C_lr_3:0.000009	 C_lr_4:0.000009
[2022-10-02 17:55:44 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.002810	Loss2: 0.002489	 Dis: 1.270042 Entropy: 4.555327 
[2022-10-02 17:55:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.000848	 E_lr_2:0.000848
[2022-10-02 17:55:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 17:55:51 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.002641	Loss2: 0.003147	 Dis: 1.755320 Entropy: 5.679679 
[2022-10-02 17:55:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.000790	 E_lr_2:0.000790
[2022-10-02 17:55:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 17:55:57 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.053562	Loss2: 0.057348	 Dis: 3.154625 Entropy: 4.760554 
[2022-10-02 17:55:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.000734	 E_lr_2:0.000734
[2022-10-02 17:55:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 17:56:03 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.003011	Loss2: 0.003631	 Dis: 1.212469 Entropy: 5.504602 
[2022-10-02 17:56:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.000679	 E_lr_2:0.000679
[2022-10-02 17:56:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 17:56:09 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.004798	Loss2: 0.005532	 Dis: 3.766670 Entropy: 4.540283 
[2022-10-02 17:56:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.000626	 E_lr_2:0.000626
[2022-10-02 17:56:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 17:56:15 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.017458	Loss2: 0.013811	 Dis: 2.353045 Entropy: 4.535761 
[2022-10-02 17:56:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.000575	 E_lr_2:0.000575
[2022-10-02 17:56:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 17:56:21 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.017641	Loss2: 0.021921	 Dis: 0.946623 Entropy: 5.143735 
[2022-10-02 17:56:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.000526	 E_lr_2:0.000526
[2022-10-02 17:56:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 17:56:26 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.009004	Loss2: 0.004457	 Dis: 1.713837 Entropy: 5.759758 
[2022-10-02 17:56:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.000479	 E_lr_2:0.000479
[2022-10-02 17:56:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 17:56:32 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.013421	Loss2: 0.006376	 Dis: 2.037046 Entropy: 5.735046 
[2022-10-02 17:56:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.000433	 E_lr_2:0.000433
[2022-10-02 17:56:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 17:56:37 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.023572	Loss2: 0.019914	 Dis: 3.131636 Entropy: 5.586114 
[2022-10-02 17:56:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.000390	 E_lr_2:0.000390
[2022-10-02 17:56:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 17:56:44 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.022525	Loss2: 0.017933	 Dis: 3.485931 Entropy: 4.842636 
[2022-10-02 17:56:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.000349	 E_lr_2:0.000349
[2022-10-02 17:56:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 17:56:50 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.000894	Loss2: 0.000993	 Dis: 3.768913 Entropy: 4.544750 
[2022-10-02 17:56:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.000310	 E_lr_2:0.000310
[2022-10-02 17:56:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 17:56:56 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.001753	Loss2: 0.001785	 Dis: 2.452862 Entropy: 4.959248 
[2022-10-02 17:56:56 demo] (houston_program2.py 512): INFO E_lr_1: 0.000273	 E_lr_2:0.000273
[2022-10-02 17:56:56 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 17:57:02 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.010518	Loss2: 0.007927	 Dis: 1.785875 Entropy: 4.276533 
[2022-10-02 17:57:02 demo] (houston_program2.py 512): INFO E_lr_1: 0.000239	 E_lr_2:0.000239
[2022-10-02 17:57:02 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 17:57:08 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.005708	Loss2: 0.008182	 Dis: 2.267345 Entropy: 4.718135 
[2022-10-02 17:57:08 demo] (houston_program2.py 512): INFO E_lr_1: 0.000206	 E_lr_2:0.000206
[2022-10-02 17:57:08 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 17:57:14 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.005514	Loss2: 0.005645	 Dis: 2.586250 Entropy: 4.778916 
[2022-10-02 17:57:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.000176	 E_lr_2:0.000176
[2022-10-02 17:57:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 17:57:19 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.007741	Loss2: 0.008123	 Dis: 2.425346 Entropy: 4.713683 
[2022-10-02 17:57:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.000148	 E_lr_2:0.000148
[2022-10-02 17:57:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:57:25 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.044430	Loss2: 0.039282	 Dis: 3.231554 Entropy: 4.354559 
[2022-10-02 17:57:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.000123	 E_lr_2:0.000123
[2022-10-02 17:57:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:57:30 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.002547	Loss2: 0.002157	 Dis: 3.532043 Entropy: 4.893888 
[2022-10-02 17:57:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.000100	 E_lr_2:0.000100
[2022-10-02 17:57:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:57:36 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.001578	Loss2: 0.001483	 Dis: 2.148409 Entropy: 4.387449 
[2022-10-02 17:57:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.000079	 E_lr_2:0.000079
[2022-10-02 17:57:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:57:42 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.003641	Loss2: 0.003865	 Dis: 2.123362 Entropy: 5.061621 
[2022-10-02 17:57:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.000061	 E_lr_2:0.000061
[2022-10-02 17:57:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 17:57:48 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.005271	Loss2: 0.002565	 Dis: 0.619694 Entropy: 5.854548 
[2022-10-02 17:57:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.000045	 E_lr_2:0.000045
[2022-10-02 17:57:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:57:54 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.003687	Loss2: 0.004307	 Dis: 1.735765 Entropy: 4.448256 
[2022-10-02 17:57:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.000031	 E_lr_2:0.000031
[2022-10-02 17:57:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:58:00 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.004898	Loss2: 0.004826	 Dis: 0.875561 Entropy: 4.630656 
[2022-10-02 17:58:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.000020	 E_lr_2:0.000020
[2022-10-02 17:58:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:58:06 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.004953	Loss2: 0.005381	 Dis: 1.095257 Entropy: 4.188223 
[2022-10-02 17:58:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.000011	 E_lr_2:0.000011
[2022-10-02 17:58:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:58:12 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.010372	Loss2: 0.009589	 Dis: 2.209412 Entropy: 4.451091 
[2022-10-02 17:58:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.000005	 E_lr_2:0.000005
[2022-10-02 17:58:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:58:18 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.002683	Loss2: 0.002834	 Dis: 1.358795 Entropy: 5.071639 
[2022-10-02 17:58:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.000001	 E_lr_2:0.000001
[2022-10-02 17:58:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:58:24 demo] (houston_program2.py 504): INFO Train Ep: 30 	Loss1: 0.015167	Loss2: 0.015201	 Dis: 1.963852 Entropy: 4.381489 
[2022-10-02 17:58:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.000000	 E_lr_2:0.000000
[2022-10-02 17:58:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 17:58:24 demo] (houston_program2.py 515): INFO time_30_epoch:593.5787765979767
[2022-10-02 17:58:32 demo] (houston_program2.py 673): INFO 	val_Accuracy: 31293/53200 (58.82%)	
[2022-10-02 17:58:32 demo] (utils.py 47): INFO outputs/demo/finetune_100/ckpt_epoch_30.pth saving......
[2022-10-02 17:58:32 demo] (utils.py 49): INFO outputs/demo/finetune_100/ckpt_epoch_30.pth saved !!!
[2022-10-02 17:58:32 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0112 (0.0112)	loss 0.0163 (0.0163)	grad_norm 0.0540 (0.0540)	mem 460MB
[2022-10-02 17:58:32 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:32 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0214 (0.0214)	loss 0.0115 (0.0115)	grad_norm 0.0512 (0.0512)	mem 460MB
[2022-10-02 17:58:32 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:32 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0094 (0.0094)	loss 0.0114 (0.0114)	grad_norm 0.0462 (0.0462)	mem 460MB
[2022-10-02 17:58:32 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:33 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0228 (0.0228)	loss 0.0104 (0.0104)	grad_norm 0.0609 (0.0609)	mem 460MB
[2022-10-02 17:58:33 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:33 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0125 (0.0125)	loss 0.0133 (0.0133)	grad_norm 0.0576 (0.0576)	mem 460MB
[2022-10-02 17:58:33 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:33 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0219 (0.0219)	loss 0.0103 (0.0103)	grad_norm 0.0526 (0.0526)	mem 460MB
[2022-10-02 17:58:33 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:33 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0163 (0.0163)	loss 0.0169 (0.0169)	grad_norm 0.0616 (0.0616)	mem 460MB
[2022-10-02 17:58:33 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:33 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0222 (0.0222)	loss 0.0112 (0.0112)	grad_norm 0.0531 (0.0531)	mem 460MB
[2022-10-02 17:58:33 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:34 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0145 (0.0145)	loss 0.0102 (0.0102)	grad_norm 0.0668 (0.0668)	mem 460MB
[2022-10-02 17:58:34 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:34 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0199 (0.0199)	loss 0.0113 (0.0113)	grad_norm 0.0652 (0.0652)	mem 460MB
[2022-10-02 17:58:34 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:34 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0204 (0.0204)	loss 0.0145 (0.0145)	grad_norm 0.0574 (0.0574)	mem 460MB
[2022-10-02 17:58:34 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:34 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0221 (0.0221)	loss 0.0108 (0.0108)	grad_norm 0.0709 (0.0709)	mem 460MB
[2022-10-02 17:58:34 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:34 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0205 (0.0205)	loss 0.0112 (0.0112)	grad_norm 0.0633 (0.0633)	mem 460MB
[2022-10-02 17:58:34 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:34 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0207 (0.0207)	loss 0.0136 (0.0136)	grad_norm 0.0971 (0.0971)	mem 460MB
[2022-10-02 17:58:34 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:35 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0188 (0.0188)	loss 0.0111 (0.0111)	grad_norm 0.0443 (0.0443)	mem 460MB
[2022-10-02 17:58:35 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:35 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0224 (0.0224)	loss 0.0127 (0.0127)	grad_norm 0.0602 (0.0602)	mem 460MB
[2022-10-02 17:58:35 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:35 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0218 (0.0218)	loss 0.0096 (0.0096)	grad_norm 0.0337 (0.0337)	mem 460MB
[2022-10-02 17:58:35 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:35 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0121 (0.0121)	loss 0.0113 (0.0113)	grad_norm 0.0703 (0.0703)	mem 460MB
[2022-10-02 17:58:35 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:35 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0219 (0.0219)	loss 0.0090 (0.0090)	grad_norm 0.0605 (0.0605)	mem 460MB
[2022-10-02 17:58:35 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:36 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0120 (0.0120)	loss 0.0120 (0.0120)	grad_norm 0.0577 (0.0577)	mem 460MB
[2022-10-02 17:58:36 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:36 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0204 (0.0204)	loss 0.0113 (0.0113)	grad_norm 0.0559 (0.0559)	mem 460MB
[2022-10-02 17:58:36 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:36 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0095 (0.0095)	loss 0.0163 (0.0163)	grad_norm 0.0747 (0.0747)	mem 460MB
[2022-10-02 17:58:36 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:36 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0196 (0.0196)	loss 0.0107 (0.0107)	grad_norm 0.0452 (0.0452)	mem 460MB
[2022-10-02 17:58:36 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:36 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0112 (0.0112)	loss 0.0127 (0.0127)	grad_norm 0.0733 (0.0733)	mem 460MB
[2022-10-02 17:58:36 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:36 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0209 (0.0209)	loss 0.0152 (0.0152)	grad_norm 0.0678 (0.0678)	mem 460MB
[2022-10-02 17:58:36 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:37 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0140 (0.0140)	loss 0.0094 (0.0094)	grad_norm 0.0584 (0.0584)	mem 460MB
[2022-10-02 17:58:37 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:37 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0216 (0.0216)	loss 0.0129 (0.0129)	grad_norm 0.0474 (0.0474)	mem 460MB
[2022-10-02 17:58:37 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:37 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0207 (0.0207)	loss 0.0120 (0.0120)	grad_norm 0.0673 (0.0673)	mem 460MB
[2022-10-02 17:58:37 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:37 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0201 (0.0201)	loss 0.0111 (0.0111)	grad_norm 0.0610 (0.0610)	mem 460MB
[2022-10-02 17:58:37 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:37 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0228 (0.0228)	loss 0.0111 (0.0111)	grad_norm 0.0481 (0.0481)	mem 460MB
[2022-10-02 17:58:37 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:38 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0148 (0.0148)	loss 0.0121 (0.0121)	grad_norm 0.0622 (0.0622)	mem 460MB
[2022-10-02 17:58:38 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:38 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0247 (0.0247)	loss 0.0097 (0.0097)	grad_norm 0.0543 (0.0543)	mem 460MB
[2022-10-02 17:58:38 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:38 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0154 (0.0154)	loss 0.0121 (0.0121)	grad_norm 0.0392 (0.0392)	mem 460MB
[2022-10-02 17:58:38 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:38 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000039	time 0.0247 (0.0247)	loss 0.0117 (0.0117)	grad_norm 0.0835 (0.0835)	mem 460MB
[2022-10-02 17:58:38 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:38 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0159 (0.0159)	loss 0.0095 (0.0095)	grad_norm 0.0463 (0.0463)	mem 460MB
[2022-10-02 17:58:38 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:38 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0263 (0.0263)	loss 0.0117 (0.0117)	grad_norm 0.0478 (0.0478)	mem 460MB
[2022-10-02 17:58:39 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:39 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0241 (0.0241)	loss 0.0132 (0.0132)	grad_norm 0.1280 (0.1280)	mem 460MB
[2022-10-02 17:58:39 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:39 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0178 (0.0178)	loss 0.0110 (0.0110)	grad_norm 0.0560 (0.0560)	mem 460MB
[2022-10-02 17:58:39 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:39 demo] (houston_program2.py 243): INFO Train: [31/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0247 (0.0247)	loss 0.0108 (0.0108)	grad_norm 0.0736 (0.0736)	mem 460MB
[2022-10-02 17:58:39 demo] (houston_program2.py 252): INFO EPOCH 31 training takes 0:00:00
[2022-10-02 17:58:39 demo] (houston_program2.py 333): INFO Train Ep: 31 	Loss1: 0.139491	Loss2: 0.136674	 Dis: 5.528776 Entropy: 4.549427 
[2022-10-02 17:58:39 demo] (houston_program2.py 335): INFO time_31_epoch:7.512455940246582
[2022-10-02 17:58:40 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0136 (0.0136)	loss 0.0131 (0.0131)	grad_norm 0.0848 (0.0848)	mem 460MB
[2022-10-02 17:58:40 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:40 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0210 (0.0210)	loss 0.0120 (0.0120)	grad_norm 0.0623 (0.0623)	mem 460MB
[2022-10-02 17:58:40 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:40 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0207 (0.0207)	loss 0.0132 (0.0132)	grad_norm 0.1352 (0.1352)	mem 460MB
[2022-10-02 17:58:40 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:40 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0211 (0.0211)	loss 0.0185 (0.0185)	grad_norm 0.0755 (0.0755)	mem 460MB
[2022-10-02 17:58:40 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:40 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0216 (0.0216)	loss 0.0164 (0.0164)	grad_norm 0.0427 (0.0427)	mem 460MB
[2022-10-02 17:58:40 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:41 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0211 (0.0211)	loss 0.0112 (0.0112)	grad_norm 0.0833 (0.0833)	mem 460MB
[2022-10-02 17:58:41 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:41 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0210 (0.0210)	loss 0.0105 (0.0105)	grad_norm 0.0978 (0.0978)	mem 460MB
[2022-10-02 17:58:41 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:41 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0210 (0.0210)	loss 0.0099 (0.0099)	grad_norm 0.0508 (0.0508)	mem 460MB
[2022-10-02 17:58:41 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:41 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0211 (0.0211)	loss 0.0140 (0.0140)	grad_norm 0.0759 (0.0759)	mem 460MB
[2022-10-02 17:58:41 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:41 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0237 (0.0237)	loss 0.0118 (0.0118)	grad_norm 0.0484 (0.0484)	mem 460MB
[2022-10-02 17:58:41 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:42 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0117 (0.0117)	loss 0.0122 (0.0122)	grad_norm 0.0745 (0.0745)	mem 460MB
[2022-10-02 17:58:42 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:42 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0267 (0.0267)	loss 0.0147 (0.0147)	grad_norm 0.0930 (0.0930)	mem 460MB
[2022-10-02 17:58:42 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:42 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0158 (0.0158)	loss 0.0107 (0.0107)	grad_norm 0.0596 (0.0596)	mem 460MB
[2022-10-02 17:58:42 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:42 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0223 (0.0223)	loss 0.0114 (0.0114)	grad_norm 0.0752 (0.0752)	mem 460MB
[2022-10-02 17:58:42 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:42 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0156 (0.0156)	loss 0.0153 (0.0153)	grad_norm 0.0976 (0.0976)	mem 460MB
[2022-10-02 17:58:42 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:43 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0222 (0.0222)	loss 0.0156 (0.0156)	grad_norm 0.0516 (0.0516)	mem 460MB
[2022-10-02 17:58:43 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:43 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0213 (0.0213)	loss 0.0151 (0.0151)	grad_norm 0.0652 (0.0652)	mem 460MB
[2022-10-02 17:58:43 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:43 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0200 (0.0200)	loss 0.0169 (0.0169)	grad_norm 0.0965 (0.0965)	mem 460MB
[2022-10-02 17:58:43 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:43 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0220 (0.0220)	loss 0.0129 (0.0129)	grad_norm 0.0491 (0.0491)	mem 460MB
[2022-10-02 17:58:43 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:43 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0252 (0.0252)	loss 0.0111 (0.0111)	grad_norm 0.0512 (0.0512)	mem 460MB
[2022-10-02 17:58:43 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:43 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0219 (0.0219)	loss 0.0144 (0.0144)	grad_norm 0.0631 (0.0631)	mem 460MB
[2022-10-02 17:58:43 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:44 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0122 (0.0122)	loss 0.0141 (0.0141)	grad_norm 0.0649 (0.0649)	mem 460MB
[2022-10-02 17:58:44 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:44 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0213 (0.0213)	loss 0.0093 (0.0093)	grad_norm 0.0668 (0.0668)	mem 460MB
[2022-10-02 17:58:44 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:44 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0193 (0.0193)	loss 0.0135 (0.0135)	grad_norm 0.0580 (0.0580)	mem 460MB
[2022-10-02 17:58:44 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:44 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0202 (0.0202)	loss 0.0116 (0.0116)	grad_norm 0.0658 (0.0658)	mem 460MB
[2022-10-02 17:58:44 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:44 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0155 (0.0155)	loss 0.0114 (0.0114)	grad_norm 0.0800 (0.0800)	mem 460MB
[2022-10-02 17:58:44 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:45 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0208 (0.0208)	loss 0.0145 (0.0145)	grad_norm 0.0699 (0.0699)	mem 460MB
[2022-10-02 17:58:45 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:45 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0207 (0.0207)	loss 0.0139 (0.0139)	grad_norm 0.0682 (0.0682)	mem 460MB
[2022-10-02 17:58:45 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:45 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0225 (0.0225)	loss 0.0092 (0.0092)	grad_norm 0.0882 (0.0882)	mem 460MB
[2022-10-02 17:58:45 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:45 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0193 (0.0193)	loss 0.0117 (0.0117)	grad_norm 0.0555 (0.0555)	mem 460MB
[2022-10-02 17:58:45 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:45 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0242 (0.0242)	loss 0.0133 (0.0133)	grad_norm 0.0773 (0.0773)	mem 460MB
[2022-10-02 17:58:45 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:45 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0215 (0.0215)	loss 0.0105 (0.0105)	grad_norm 0.0883 (0.0883)	mem 460MB
[2022-10-02 17:58:45 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:46 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0110 (0.0110)	loss 0.0116 (0.0116)	grad_norm 0.0673 (0.0673)	mem 460MB
[2022-10-02 17:58:46 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:46 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0219 (0.0219)	loss 0.0117 (0.0117)	grad_norm 0.1200 (0.1200)	mem 460MB
[2022-10-02 17:58:46 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:46 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0109 (0.0109)	loss 0.0110 (0.0110)	grad_norm 0.0648 (0.0648)	mem 460MB
[2022-10-02 17:58:46 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:46 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0208 (0.0208)	loss 0.0123 (0.0123)	grad_norm 0.0567 (0.0567)	mem 460MB
[2022-10-02 17:58:46 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:46 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0132 (0.0132)	loss 0.0105 (0.0105)	grad_norm 0.0545 (0.0545)	mem 460MB
[2022-10-02 17:58:46 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:46 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0206 (0.0206)	loss 0.0092 (0.0092)	grad_norm 0.0653 (0.0653)	mem 460MB
[2022-10-02 17:58:47 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:47 demo] (houston_program2.py 243): INFO Train: [32/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0172 (0.0172)	loss 0.0096 (0.0096)	grad_norm 0.0569 (0.0569)	mem 460MB
[2022-10-02 17:58:47 demo] (houston_program2.py 252): INFO EPOCH 32 training takes 0:00:00
[2022-10-02 17:58:47 demo] (houston_program2.py 333): INFO Train Ep: 32 	Loss1: 0.270722	Loss2: 0.271964	 Dis: 4.871807 Entropy: 5.091628 
[2022-10-02 17:58:47 demo] (houston_program2.py 335): INFO time_32_epoch:7.615872859954834
[2022-10-02 17:58:47 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0113 (0.0113)	loss 0.0125 (0.0125)	grad_norm 0.0639 (0.0639)	mem 460MB
[2022-10-02 17:58:47 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:47 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0218 (0.0218)	loss 0.0086 (0.0086)	grad_norm 0.0472 (0.0472)	mem 460MB
[2022-10-02 17:58:47 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:48 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0158 (0.0158)	loss 0.0116 (0.0116)	grad_norm 0.0738 (0.0738)	mem 460MB
[2022-10-02 17:58:48 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:48 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0201 (0.0201)	loss 0.0118 (0.0118)	grad_norm 0.0445 (0.0445)	mem 460MB
[2022-10-02 17:58:48 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:48 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0164 (0.0164)	loss 0.0125 (0.0125)	grad_norm 0.0547 (0.0547)	mem 460MB
[2022-10-02 17:58:48 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:48 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0220 (0.0220)	loss 0.0109 (0.0109)	grad_norm 0.0515 (0.0515)	mem 460MB
[2022-10-02 17:58:48 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:48 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0211 (0.0211)	loss 0.0104 (0.0104)	grad_norm 0.0511 (0.0511)	mem 460MB
[2022-10-02 17:58:48 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:49 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0220 (0.0220)	loss 0.0115 (0.0115)	grad_norm 0.0483 (0.0483)	mem 460MB
[2022-10-02 17:58:49 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:49 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0213 (0.0213)	loss 0.0122 (0.0122)	grad_norm 0.0513 (0.0513)	mem 460MB
[2022-10-02 17:58:49 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:49 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0258 (0.0258)	loss 0.0113 (0.0113)	grad_norm 0.0533 (0.0533)	mem 460MB
[2022-10-02 17:58:49 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:49 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0187 (0.0187)	loss 0.0140 (0.0140)	grad_norm 0.0508 (0.0508)	mem 460MB
[2022-10-02 17:58:49 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:49 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0109 (0.0109)	loss 0.0107 (0.0107)	grad_norm 0.0430 (0.0430)	mem 460MB
[2022-10-02 17:58:49 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:49 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0201 (0.0201)	loss 0.0161 (0.0161)	grad_norm 0.0815 (0.0815)	mem 460MB
[2022-10-02 17:58:49 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:50 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000038	time 0.0118 (0.0118)	loss 0.0125 (0.0125)	grad_norm 0.0509 (0.0509)	mem 460MB
[2022-10-02 17:58:50 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:50 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0212 (0.0212)	loss 0.0104 (0.0104)	grad_norm 0.0393 (0.0393)	mem 460MB
[2022-10-02 17:58:50 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:50 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0118 (0.0118)	loss 0.0105 (0.0105)	grad_norm 0.0695 (0.0695)	mem 460MB
[2022-10-02 17:58:50 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:50 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0221 (0.0221)	loss 0.0110 (0.0110)	grad_norm 0.0497 (0.0497)	mem 460MB
[2022-10-02 17:58:50 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:50 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0159 (0.0159)	loss 0.0094 (0.0094)	grad_norm 0.0498 (0.0498)	mem 460MB
[2022-10-02 17:58:50 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:51 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0217 (0.0217)	loss 0.0113 (0.0113)	grad_norm 0.0547 (0.0547)	mem 460MB
[2022-10-02 17:58:51 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:51 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0191 (0.0191)	loss 0.0106 (0.0106)	grad_norm 0.0369 (0.0369)	mem 460MB
[2022-10-02 17:58:51 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:51 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0204 (0.0204)	loss 0.0106 (0.0106)	grad_norm 0.0522 (0.0522)	mem 460MB
[2022-10-02 17:58:51 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:51 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0209 (0.0209)	loss 0.0131 (0.0131)	grad_norm 0.0519 (0.0519)	mem 460MB
[2022-10-02 17:58:51 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:51 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0219 (0.0219)	loss 0.0088 (0.0088)	grad_norm 0.0433 (0.0433)	mem 460MB
[2022-10-02 17:58:51 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:51 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0208 (0.0208)	loss 0.0090 (0.0090)	grad_norm 0.0428 (0.0428)	mem 460MB
[2022-10-02 17:58:51 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:52 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0377 (0.0377)	loss 0.0079 (0.0079)	grad_norm 0.0350 (0.0350)	mem 460MB
[2022-10-02 17:58:52 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:52 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0223 (0.0223)	loss 0.0095 (0.0095)	grad_norm 0.0614 (0.0614)	mem 460MB
[2022-10-02 17:58:52 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:52 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0108 (0.0108)	loss 0.0141 (0.0141)	grad_norm 0.0720 (0.0720)	mem 460MB
[2022-10-02 17:58:52 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:52 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0207 (0.0207)	loss 0.0106 (0.0106)	grad_norm 0.0486 (0.0486)	mem 460MB
[2022-10-02 17:58:52 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:52 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0194 (0.0194)	loss 0.0109 (0.0109)	grad_norm 0.0597 (0.0597)	mem 460MB
[2022-10-02 17:58:52 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:52 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0188 (0.0188)	loss 0.0104 (0.0104)	grad_norm 0.0575 (0.0575)	mem 460MB
[2022-10-02 17:58:53 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:53 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0150 (0.0150)	loss 0.0118 (0.0118)	grad_norm 0.0478 (0.0478)	mem 460MB
[2022-10-02 17:58:53 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:53 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0217 (0.0217)	loss 0.0084 (0.0084)	grad_norm 0.0480 (0.0480)	mem 460MB
[2022-10-02 17:58:53 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:53 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0205 (0.0205)	loss 0.0092 (0.0092)	grad_norm 0.0698 (0.0698)	mem 460MB
[2022-10-02 17:58:53 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:53 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0215 (0.0215)	loss 0.0164 (0.0164)	grad_norm 0.0542 (0.0542)	mem 460MB
[2022-10-02 17:58:53 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:53 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0240 (0.0240)	loss 0.0092 (0.0092)	grad_norm 0.0499 (0.0499)	mem 460MB
[2022-10-02 17:58:53 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:54 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0144 (0.0144)	loss 0.0102 (0.0102)	grad_norm 0.0633 (0.0633)	mem 460MB
[2022-10-02 17:58:54 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:54 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0217 (0.0217)	loss 0.0110 (0.0110)	grad_norm 0.0430 (0.0430)	mem 460MB
[2022-10-02 17:58:54 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:54 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0149 (0.0149)	loss 0.0118 (0.0118)	grad_norm 0.0590 (0.0590)	mem 460MB
[2022-10-02 17:58:54 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:54 demo] (houston_program2.py 243): INFO Train: [33/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0226 (0.0226)	loss 0.0101 (0.0101)	grad_norm 0.0603 (0.0603)	mem 460MB
[2022-10-02 17:58:54 demo] (houston_program2.py 252): INFO EPOCH 33 training takes 0:00:00
[2022-10-02 17:58:54 demo] (houston_program2.py 333): INFO Train Ep: 33 	Loss1: 0.288009	Loss2: 0.305834	 Dis: 4.492306 Entropy: 6.124558 
[2022-10-02 17:58:54 demo] (houston_program2.py 335): INFO time_33_epoch:7.4459052085876465
[2022-10-02 17:58:55 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0115 (0.0115)	loss 0.0122 (0.0122)	grad_norm 0.0532 (0.0532)	mem 460MB
[2022-10-02 17:58:55 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:55 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0205 (0.0205)	loss 0.0116 (0.0116)	grad_norm 0.0782 (0.0782)	mem 460MB
[2022-10-02 17:58:55 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:55 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0195 (0.0195)	loss 0.0126 (0.0126)	grad_norm 0.0440 (0.0440)	mem 460MB
[2022-10-02 17:58:55 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:55 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0197 (0.0197)	loss 0.0101 (0.0101)	grad_norm 0.0629 (0.0629)	mem 460MB
[2022-10-02 17:58:55 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:55 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0215 (0.0215)	loss 0.0113 (0.0113)	grad_norm 0.0783 (0.0783)	mem 460MB
[2022-10-02 17:58:56 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:56 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0224 (0.0224)	loss 0.0126 (0.0126)	grad_norm 0.0487 (0.0487)	mem 460MB
[2022-10-02 17:58:56 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:56 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0219 (0.0219)	loss 0.0111 (0.0111)	grad_norm 0.0599 (0.0599)	mem 460MB
[2022-10-02 17:58:56 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:56 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0218 (0.0218)	loss 0.0114 (0.0114)	grad_norm 0.0858 (0.0858)	mem 460MB
[2022-10-02 17:58:56 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:56 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0168 (0.0168)	loss 0.0114 (0.0114)	grad_norm 0.0404 (0.0404)	mem 460MB
[2022-10-02 17:58:56 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:56 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0224 (0.0224)	loss 0.0092 (0.0092)	grad_norm 0.0921 (0.0921)	mem 460MB
[2022-10-02 17:58:56 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:57 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0192 (0.0192)	loss 0.0095 (0.0095)	grad_norm 0.0494 (0.0494)	mem 460MB
[2022-10-02 17:58:57 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:57 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0213 (0.0213)	loss 0.0102 (0.0102)	grad_norm 0.0461 (0.0461)	mem 460MB
[2022-10-02 17:58:57 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:57 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0200 (0.0200)	loss 0.0123 (0.0123)	grad_norm 0.0650 (0.0650)	mem 460MB
[2022-10-02 17:58:57 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:57 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0218 (0.0218)	loss 0.0112 (0.0112)	grad_norm 0.0448 (0.0448)	mem 460MB
[2022-10-02 17:58:57 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:57 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0204 (0.0204)	loss 0.0138 (0.0138)	grad_norm 0.0560 (0.0560)	mem 460MB
[2022-10-02 17:58:57 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:58 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0266 (0.0266)	loss 0.0098 (0.0098)	grad_norm 0.0658 (0.0658)	mem 460MB
[2022-10-02 17:58:58 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:58 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0220 (0.0220)	loss 0.0076 (0.0076)	grad_norm 0.0455 (0.0455)	mem 460MB
[2022-10-02 17:58:58 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:58 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0092 (0.0092)	loss 0.0097 (0.0097)	grad_norm 0.0577 (0.0577)	mem 460MB
[2022-10-02 17:58:58 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:58 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0190 (0.0190)	loss 0.0102 (0.0102)	grad_norm 0.0669 (0.0669)	mem 460MB
[2022-10-02 17:58:58 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:58 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0113 (0.0113)	loss 0.0160 (0.0160)	grad_norm 0.0429 (0.0429)	mem 460MB
[2022-10-02 17:58:58 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:58 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0097 (0.0097)	loss 0.0112 (0.0112)	grad_norm 0.0777 (0.0777)	mem 460MB
[2022-10-02 17:58:58 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:58 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0084 (0.0084)	loss 0.0104 (0.0104)	grad_norm 0.0798 (0.0798)	mem 460MB
[2022-10-02 17:58:58 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:58 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0091 (0.0091)	loss 0.0108 (0.0108)	grad_norm 0.0503 (0.0503)	mem 460MB
[2022-10-02 17:58:58 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:59 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0143 (0.0143)	loss 0.0109 (0.0109)	grad_norm 0.0864 (0.0864)	mem 460MB
[2022-10-02 17:58:59 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:59 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0242 (0.0242)	loss 0.0080 (0.0080)	grad_norm 0.0679 (0.0679)	mem 460MB
[2022-10-02 17:58:59 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:59 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0176 (0.0176)	loss 0.0116 (0.0116)	grad_norm 0.0828 (0.0828)	mem 460MB
[2022-10-02 17:58:59 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:59 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0261 (0.0261)	loss 0.0123 (0.0123)	grad_norm 0.0723 (0.0723)	mem 460MB
[2022-10-02 17:58:59 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:58:59 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0272 (0.0272)	loss 0.0111 (0.0111)	grad_norm 0.0595 (0.0595)	mem 460MB
[2022-10-02 17:58:59 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:59:00 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0263 (0.0263)	loss 0.0153 (0.0153)	grad_norm 0.0546 (0.0546)	mem 460MB
[2022-10-02 17:59:00 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:59:00 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0205 (0.0205)	loss 0.0131 (0.0131)	grad_norm 0.0648 (0.0648)	mem 460MB
[2022-10-02 17:59:00 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:59:00 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0125 (0.0125)	loss 0.0112 (0.0112)	grad_norm 0.0667 (0.0667)	mem 460MB
[2022-10-02 17:59:00 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:59:00 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000037	time 0.0207 (0.0207)	loss 0.0109 (0.0109)	grad_norm 0.0445 (0.0445)	mem 460MB
[2022-10-02 17:59:00 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:59:00 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0099 (0.0099)	loss 0.0140 (0.0140)	grad_norm 0.0788 (0.0788)	mem 460MB
[2022-10-02 17:59:00 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:59:00 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0210 (0.0210)	loss 0.0093 (0.0093)	grad_norm 0.0659 (0.0659)	mem 460MB
[2022-10-02 17:59:00 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:59:01 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0111 (0.0111)	loss 0.0093 (0.0093)	grad_norm 0.0573 (0.0573)	mem 460MB
[2022-10-02 17:59:01 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:59:01 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0222 (0.0222)	loss 0.0119 (0.0119)	grad_norm 0.0629 (0.0629)	mem 460MB
[2022-10-02 17:59:01 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:59:01 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0125 (0.0125)	loss 0.0106 (0.0106)	grad_norm 0.0550 (0.0550)	mem 460MB
[2022-10-02 17:59:01 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:59:01 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0205 (0.0205)	loss 0.0174 (0.0174)	grad_norm 0.0658 (0.0658)	mem 460MB
[2022-10-02 17:59:01 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:59:01 demo] (houston_program2.py 243): INFO Train: [34/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0208 (0.0208)	loss 0.0121 (0.0121)	grad_norm 0.0683 (0.0683)	mem 460MB
[2022-10-02 17:59:01 demo] (houston_program2.py 252): INFO EPOCH 34 training takes 0:00:00
[2022-10-02 17:59:02 demo] (houston_program2.py 333): INFO Train Ep: 34 	Loss1: 0.160185	Loss2: 0.165555	 Dis: 3.928905 Entropy: 4.250115 
[2022-10-02 17:59:02 demo] (houston_program2.py 335): INFO time_34_epoch:7.119600534439087
[2022-10-02 17:59:02 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0117 (0.0117)	loss 0.0115 (0.0115)	grad_norm 0.0530 (0.0530)	mem 460MB
[2022-10-02 17:59:02 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:02 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0212 (0.0212)	loss 0.0150 (0.0150)	grad_norm 0.0668 (0.0668)	mem 460MB
[2022-10-02 17:59:02 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:02 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0226 (0.0226)	loss 0.0114 (0.0114)	grad_norm 0.0611 (0.0611)	mem 460MB
[2022-10-02 17:59:02 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:02 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0140 (0.0140)	loss 0.0090 (0.0090)	grad_norm 0.0297 (0.0297)	mem 460MB
[2022-10-02 17:59:02 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:03 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0213 (0.0213)	loss 0.0115 (0.0115)	grad_norm 0.0520 (0.0520)	mem 460MB
[2022-10-02 17:59:03 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:03 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0175 (0.0175)	loss 0.0170 (0.0170)	grad_norm 0.0777 (0.0777)	mem 460MB
[2022-10-02 17:59:03 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:03 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0207 (0.0207)	loss 0.0134 (0.0134)	grad_norm 0.0525 (0.0525)	mem 460MB
[2022-10-02 17:59:03 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:03 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0205 (0.0205)	loss 0.0126 (0.0126)	grad_norm 0.0666 (0.0666)	mem 460MB
[2022-10-02 17:59:03 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:03 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0230 (0.0230)	loss 0.0093 (0.0093)	grad_norm 0.0437 (0.0437)	mem 460MB
[2022-10-02 17:59:03 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:03 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0214 (0.0214)	loss 0.0104 (0.0104)	grad_norm 0.0558 (0.0558)	mem 460MB
[2022-10-02 17:59:04 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:04 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0201 (0.0201)	loss 0.0104 (0.0104)	grad_norm 0.0754 (0.0754)	mem 460MB
[2022-10-02 17:59:04 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:04 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0219 (0.0219)	loss 0.0101 (0.0101)	grad_norm 0.0355 (0.0355)	mem 460MB
[2022-10-02 17:59:04 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:04 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0203 (0.0203)	loss 0.0124 (0.0124)	grad_norm 0.0452 (0.0452)	mem 460MB
[2022-10-02 17:59:04 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:04 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0207 (0.0207)	loss 0.0109 (0.0109)	grad_norm 0.0450 (0.0450)	mem 460MB
[2022-10-02 17:59:04 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:04 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0092 (0.0092)	loss 0.0085 (0.0085)	grad_norm 0.0377 (0.0377)	mem 460MB
[2022-10-02 17:59:04 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:05 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0219 (0.0219)	loss 0.0099 (0.0099)	grad_norm 0.0582 (0.0582)	mem 460MB
[2022-10-02 17:59:05 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:05 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0117 (0.0117)	loss 0.0120 (0.0120)	grad_norm 0.0584 (0.0584)	mem 460MB
[2022-10-02 17:59:05 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:05 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0205 (0.0205)	loss 0.0130 (0.0130)	grad_norm 0.0418 (0.0418)	mem 460MB
[2022-10-02 17:59:05 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:05 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0124 (0.0124)	loss 0.0078 (0.0078)	grad_norm 0.0385 (0.0385)	mem 460MB
[2022-10-02 17:59:05 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:05 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0221 (0.0221)	loss 0.0105 (0.0105)	grad_norm 0.0489 (0.0489)	mem 460MB
[2022-10-02 17:59:05 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:05 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0144 (0.0144)	loss 0.0109 (0.0109)	grad_norm 0.0402 (0.0402)	mem 460MB
[2022-10-02 17:59:05 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:06 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0220 (0.0220)	loss 0.0111 (0.0111)	grad_norm 0.0506 (0.0506)	mem 460MB
[2022-10-02 17:59:06 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:06 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0197 (0.0197)	loss 0.0077 (0.0077)	grad_norm 0.0621 (0.0621)	mem 460MB
[2022-10-02 17:59:06 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:06 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0209 (0.0209)	loss 0.0128 (0.0128)	grad_norm 0.0506 (0.0506)	mem 460MB
[2022-10-02 17:59:06 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:06 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0195 (0.0195)	loss 0.0103 (0.0103)	grad_norm 0.0745 (0.0745)	mem 460MB
[2022-10-02 17:59:06 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:06 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0216 (0.0216)	loss 0.0109 (0.0109)	grad_norm 0.0519 (0.0519)	mem 460MB
[2022-10-02 17:59:06 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:06 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0207 (0.0207)	loss 0.0101 (0.0101)	grad_norm 0.0692 (0.0692)	mem 460MB
[2022-10-02 17:59:07 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:07 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0201 (0.0201)	loss 0.0127 (0.0127)	grad_norm 0.0744 (0.0744)	mem 460MB
[2022-10-02 17:59:07 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:07 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0200 (0.0200)	loss 0.0088 (0.0088)	grad_norm 0.0392 (0.0392)	mem 460MB
[2022-10-02 17:59:07 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:07 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0249 (0.0249)	loss 0.0111 (0.0111)	grad_norm 0.0718 (0.0718)	mem 460MB
[2022-10-02 17:59:07 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:07 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0217 (0.0217)	loss 0.0123 (0.0123)	grad_norm 0.0633 (0.0633)	mem 460MB
[2022-10-02 17:59:07 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:07 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0130 (0.0130)	loss 0.0111 (0.0111)	grad_norm 0.0581 (0.0581)	mem 460MB
[2022-10-02 17:59:07 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:08 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0215 (0.0215)	loss 0.0103 (0.0103)	grad_norm 0.0571 (0.0571)	mem 460MB
[2022-10-02 17:59:08 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:08 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0116 (0.0116)	loss 0.0099 (0.0099)	grad_norm 0.0585 (0.0585)	mem 460MB
[2022-10-02 17:59:08 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:08 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0219 (0.0219)	loss 0.0081 (0.0081)	grad_norm 0.0495 (0.0495)	mem 460MB
[2022-10-02 17:59:08 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:08 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0195 (0.0195)	loss 0.0144 (0.0144)	grad_norm 0.0555 (0.0555)	mem 460MB
[2022-10-02 17:59:08 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:08 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0220 (0.0220)	loss 0.0110 (0.0110)	grad_norm 0.0414 (0.0414)	mem 460MB
[2022-10-02 17:59:08 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:08 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0209 (0.0209)	loss 0.0080 (0.0080)	grad_norm 0.0402 (0.0402)	mem 460MB
[2022-10-02 17:59:08 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:09 demo] (houston_program2.py 243): INFO Train: [35/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0220 (0.0220)	loss 0.0088 (0.0088)	grad_norm 0.0515 (0.0515)	mem 460MB
[2022-10-02 17:59:09 demo] (houston_program2.py 252): INFO EPOCH 35 training takes 0:00:00
[2022-10-02 17:59:09 demo] (houston_program2.py 333): INFO Train Ep: 35 	Loss1: 0.073889	Loss2: 0.086787	 Dis: 3.667488 Entropy: 4.477178 
[2022-10-02 17:59:09 demo] (houston_program2.py 335): INFO time_35_epoch:7.422611474990845
[2022-10-02 17:59:09 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 17:59:09 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 17:59:09 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 17:59:09 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 17:59:09 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 17:59:09 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 17:59:09 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 17:59:09 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 17:59:09 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 17:59:09 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 17:59:15 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.037959	Loss2: 0.035505	 Dis: 5.204775 Entropy: 4.394261 
[2022-10-02 17:59:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.000487	 E_lr_2:0.000487
[2022-10-02 17:59:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 17:59:21 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.049906	Loss2: 0.062354	 Dis: 3.641447 Entropy: 5.114589 
[2022-10-02 17:59:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.000987	 E_lr_2:0.000987
[2022-10-02 17:59:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 17:59:27 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.048492	Loss2: 0.048807	 Dis: 3.389029 Entropy: 4.809923 
[2022-10-02 17:59:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.001487	 E_lr_2:0.001487
[2022-10-02 17:59:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 17:59:33 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.359462	Loss2: 0.355499	 Dis: 8.855549 Entropy: 4.972600 
[2022-10-02 17:59:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.001987	 E_lr_2:0.001987
[2022-10-02 17:59:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 17:59:39 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.785516	Loss2: 0.749415	 Dis: 9.944439 Entropy: 5.120567 
[2022-10-02 17:59:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.002487	 E_lr_2:0.002487
[2022-10-02 17:59:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 17:59:45 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.160985	Loss2: 0.159152	 Dis: 6.241440 Entropy: 5.191079 
[2022-10-02 17:59:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.002987	 E_lr_2:0.002987
[2022-10-02 17:59:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 17:59:51 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.112085	Loss2: 0.106730	 Dis: 7.243151 Entropy: 4.857516 
[2022-10-02 17:59:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.003487	 E_lr_2:0.003487
[2022-10-02 17:59:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 17:59:57 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.159053	Loss2: 0.143636	 Dis: 6.244267 Entropy: 5.165111 
[2022-10-02 17:59:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.003987	 E_lr_2:0.003987
[2022-10-02 17:59:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 18:00:03 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.122525	Loss2: 0.125679	 Dis: 7.318134 Entropy: 5.314099 
[2022-10-02 18:00:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.004487	 E_lr_2:0.004487
[2022-10-02 18:00:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 18:00:10 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.073605	Loss2: 0.102492	 Dis: 5.528917 Entropy: 4.807425 
[2022-10-02 18:00:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.004987	 E_lr_2:0.004987
[2022-10-02 18:00:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000050	 C_lr_2:0.000050	 C_lr_3:0.000050	 C_lr_4:0.000050
[2022-10-02 18:00:16 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.163296	Loss2: 0.164038	 Dis: 5.968578 Entropy: 4.586349 
[2022-10-02 18:00:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.004853	 E_lr_2:0.004853
[2022-10-02 18:00:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000049	 C_lr_2:0.000049	 C_lr_3:0.000049	 C_lr_4:0.000049
[2022-10-02 18:00:22 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.136377	Loss2: 0.115489	 Dis: 7.912094 Entropy: 6.078953 
[2022-10-02 18:00:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.004825	 E_lr_2:0.004825
[2022-10-02 18:00:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 18:00:28 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.137368	Loss2: 0.122794	 Dis: 6.944759 Entropy: 4.752941 
[2022-10-02 18:00:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.004795	 E_lr_2:0.004795
[2022-10-02 18:00:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 18:00:34 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.183160	Loss2: 0.183422	 Dis: 6.738998 Entropy: 4.860329 
[2022-10-02 18:00:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.004763	 E_lr_2:0.004763
[2022-10-02 18:00:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 18:00:40 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.129301	Loss2: 0.140574	 Dis: 5.149729 Entropy: 5.038110 
[2022-10-02 18:00:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.004728	 E_lr_2:0.004728
[2022-10-02 18:00:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 18:00:45 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.214740	Loss2: 0.210957	 Dis: 6.135895 Entropy: 4.817183 
[2022-10-02 18:00:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.004692	 E_lr_2:0.004692
[2022-10-02 18:00:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 18:00:51 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.110388	Loss2: 0.114333	 Dis: 3.854963 Entropy: 4.853530 
[2022-10-02 18:00:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.004653	 E_lr_2:0.004653
[2022-10-02 18:00:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 18:00:57 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.170111	Loss2: 0.179571	 Dis: 9.205986 Entropy: 4.318854 
[2022-10-02 18:00:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.004612	 E_lr_2:0.004612
[2022-10-02 18:00:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 18:01:03 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.135201	Loss2: 0.116595	 Dis: 6.505056 Entropy: 5.068959 
[2022-10-02 18:01:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.004569	 E_lr_2:0.004569
[2022-10-02 18:01:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 18:01:09 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.096050	Loss2: 0.102054	 Dis: 5.119238 Entropy: 5.445296 
[2022-10-02 18:01:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.004524	 E_lr_2:0.004524
[2022-10-02 18:01:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 18:01:15 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.092623	Loss2: 0.081210	 Dis: 5.523685 Entropy: 5.602225 
[2022-10-02 18:01:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.004477	 E_lr_2:0.004477
[2022-10-02 18:01:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 18:01:21 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.179282	Loss2: 0.206055	 Dis: 6.618593 Entropy: 4.495391 
[2022-10-02 18:01:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.004428	 E_lr_2:0.004428
[2022-10-02 18:01:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 18:01:27 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.140656	Loss2: 0.117678	 Dis: 5.012987 Entropy: 4.457310 
[2022-10-02 18:01:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.004377	 E_lr_2:0.004377
[2022-10-02 18:01:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 18:01:33 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.139270	Loss2: 0.170808	 Dis: 4.265123 Entropy: 4.961023 
[2022-10-02 18:01:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.004324	 E_lr_2:0.004324
[2022-10-02 18:01:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 18:01:39 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.251218	Loss2: 0.224150	 Dis: 5.211058 Entropy: 4.475206 
[2022-10-02 18:01:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.004269	 E_lr_2:0.004269
[2022-10-02 18:01:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 18:01:45 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.179243	Loss2: 0.164188	 Dis: 5.517399 Entropy: 5.665626 
[2022-10-02 18:01:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.004213	 E_lr_2:0.004213
[2022-10-02 18:01:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 18:01:51 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.039121	Loss2: 0.034973	 Dis: 7.231655 Entropy: 6.740234 
[2022-10-02 18:01:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.004155	 E_lr_2:0.004155
[2022-10-02 18:01:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 18:01:57 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.065937	Loss2: 0.058592	 Dis: 4.939581 Entropy: 5.444047 
[2022-10-02 18:01:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.004095	 E_lr_2:0.004095
[2022-10-02 18:01:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000041	 C_lr_2:0.000041	 C_lr_3:0.000041	 C_lr_4:0.000041
[2022-10-02 18:02:03 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.118020	Loss2: 0.101749	 Dis: 4.221355 Entropy: 4.842694 
[2022-10-02 18:02:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.004034	 E_lr_2:0.004034
[2022-10-02 18:02:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 18:02:09 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.146728	Loss2: 0.192848	 Dis: 4.808611 Entropy: 4.706093 
[2022-10-02 18:02:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.003971	 E_lr_2:0.003971
[2022-10-02 18:02:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 18:02:15 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.104909	Loss2: 0.113497	 Dis: 4.619402 Entropy: 5.415743 
[2022-10-02 18:02:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.003907	 E_lr_2:0.003907
[2022-10-02 18:02:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000039	 C_lr_2:0.000039	 C_lr_3:0.000039	 C_lr_4:0.000039
[2022-10-02 18:02:21 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.047452	Loss2: 0.043045	 Dis: 4.035837 Entropy: 4.514521 
[2022-10-02 18:02:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.003841	 E_lr_2:0.003841
[2022-10-02 18:02:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 18:02:27 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.119354	Loss2: 0.119786	 Dis: 5.287975 Entropy: 5.446418 
[2022-10-02 18:02:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.003774	 E_lr_2:0.003774
[2022-10-02 18:02:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 18:02:32 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.102946	Loss2: 0.097173	 Dis: 4.341343 Entropy: 6.313692 
[2022-10-02 18:02:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.003706	 E_lr_2:0.003706
[2022-10-02 18:02:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000037	 C_lr_2:0.000037	 C_lr_3:0.000037	 C_lr_4:0.000037
[2022-10-02 18:02:38 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.011450	Loss2: 0.013226	 Dis: 5.822086 Entropy: 4.338853 
[2022-10-02 18:02:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.003637	 E_lr_2:0.003637
[2022-10-02 18:02:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 18:02:44 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.161039	Loss2: 0.161563	 Dis: 3.971968 Entropy: 4.520305 
[2022-10-02 18:02:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.003566	 E_lr_2:0.003566
[2022-10-02 18:02:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 18:02:50 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.114092	Loss2: 0.086119	 Dis: 4.085556 Entropy: 4.681732 
[2022-10-02 18:02:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.003495	 E_lr_2:0.003495
[2022-10-02 18:02:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 18:02:56 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.053484	Loss2: 0.052067	 Dis: 3.747791 Entropy: 4.685154 
[2022-10-02 18:02:56 demo] (houston_program2.py 512): INFO E_lr_1: 0.003422	 E_lr_2:0.003422
[2022-10-02 18:02:56 demo] (houston_program2.py 513): INFO C_lr_1: 0.000034	 C_lr_2:0.000034	 C_lr_3:0.000034	 C_lr_4:0.000034
[2022-10-02 18:03:02 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.022785	Loss2: 0.034966	 Dis: 4.050776 Entropy: 4.507829 
[2022-10-02 18:03:02 demo] (houston_program2.py 512): INFO E_lr_1: 0.003349	 E_lr_2:0.003349
[2022-10-02 18:03:02 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 18:03:08 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.049009	Loss2: 0.051236	 Dis: 5.537128 Entropy: 5.318057 
[2022-10-02 18:03:08 demo] (houston_program2.py 512): INFO E_lr_1: 0.003274	 E_lr_2:0.003274
[2022-10-02 18:03:08 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 18:03:12 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.195292	Loss2: 0.157471	 Dis: 3.032021 Entropy: 4.625759 
[2022-10-02 18:03:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.003199	 E_lr_2:0.003199
[2022-10-02 18:03:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000032	 C_lr_2:0.000032	 C_lr_3:0.000032	 C_lr_4:0.000032
[2022-10-02 18:03:19 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.060014	Loss2: 0.068894	 Dis: 5.251057 Entropy: 5.270214 
[2022-10-02 18:03:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.003124	 E_lr_2:0.003124
[2022-10-02 18:03:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000031	 C_lr_2:0.000031	 C_lr_3:0.000031	 C_lr_4:0.000031
[2022-10-02 18:03:25 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.085480	Loss2: 0.090865	 Dis: 3.209942 Entropy: 5.199818 
[2022-10-02 18:03:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.003047	 E_lr_2:0.003047
[2022-10-02 18:03:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 18:03:31 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.016281	Loss2: 0.015786	 Dis: 4.680910 Entropy: 5.281812 
[2022-10-02 18:03:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.002970	 E_lr_2:0.002970
[2022-10-02 18:03:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 18:03:37 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.157690	Loss2: 0.154361	 Dis: 2.481419 Entropy: 4.434185 
[2022-10-02 18:03:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.002893	 E_lr_2:0.002893
[2022-10-02 18:03:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000029	 C_lr_2:0.000029	 C_lr_3:0.000029	 C_lr_4:0.000029
[2022-10-02 18:03:43 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.036556	Loss2: 0.040279	 Dis: 5.034096 Entropy: 5.651785 
[2022-10-02 18:03:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.002815	 E_lr_2:0.002815
[2022-10-02 18:03:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000028	 C_lr_2:0.000028	 C_lr_3:0.000028	 C_lr_4:0.000028
[2022-10-02 18:03:49 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.087878	Loss2: 0.099433	 Dis: 2.780897 Entropy: 4.361716 
[2022-10-02 18:03:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.002737	 E_lr_2:0.002737
[2022-10-02 18:03:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 18:03:55 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.130652	Loss2: 0.101998	 Dis: 3.730740 Entropy: 5.972825 
[2022-10-02 18:03:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.002659	 E_lr_2:0.002659
[2022-10-02 18:03:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 18:04:01 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.126829	Loss2: 0.147605	 Dis: 2.786596 Entropy: 5.310867 
[2022-10-02 18:04:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.002581	 E_lr_2:0.002581
[2022-10-02 18:04:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000026	 C_lr_2:0.000026	 C_lr_3:0.000026	 C_lr_4:0.000026
[2022-10-02 18:04:07 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.023682	Loss2: 0.020608	 Dis: 3.074091 Entropy: 5.518994 
[2022-10-02 18:04:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.002502	 E_lr_2:0.002502
[2022-10-02 18:04:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 18:04:13 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.089948	Loss2: 0.101700	 Dis: 3.459867 Entropy: 5.253401 
[2022-10-02 18:04:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.002423	 E_lr_2:0.002423
[2022-10-02 18:04:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000024	 C_lr_2:0.000024	 C_lr_3:0.000024	 C_lr_4:0.000024
[2022-10-02 18:04:19 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.126104	Loss2: 0.113792	 Dis: 4.950912 Entropy: 4.497653 
[2022-10-02 18:04:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.002345	 E_lr_2:0.002345
[2022-10-02 18:04:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 18:04:25 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.138171	Loss2: 0.097441	 Dis: 4.389448 Entropy: 5.661249 
[2022-10-02 18:04:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.002267	 E_lr_2:0.002267
[2022-10-02 18:04:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 18:04:30 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.075362	Loss2: 0.063390	 Dis: 4.439552 Entropy: 4.525971 
[2022-10-02 18:04:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.002189	 E_lr_2:0.002189
[2022-10-02 18:04:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000022	 C_lr_2:0.000022	 C_lr_3:0.000022	 C_lr_4:0.000022
[2022-10-02 18:04:36 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.115585	Loss2: 0.121659	 Dis: 5.590014 Entropy: 6.071467 
[2022-10-02 18:04:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.002111	 E_lr_2:0.002111
[2022-10-02 18:04:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000021	 C_lr_2:0.000021	 C_lr_3:0.000021	 C_lr_4:0.000021
[2022-10-02 18:04:42 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.062669	Loss2: 0.071816	 Dis: 3.159187 Entropy: 5.360198 
[2022-10-02 18:04:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.002034	 E_lr_2:0.002034
[2022-10-02 18:04:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 18:04:49 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.357724	Loss2: 0.340651	 Dis: 4.466393 Entropy: 5.548260 
[2022-10-02 18:04:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.001957	 E_lr_2:0.001957
[2022-10-02 18:04:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 18:04:55 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.105821	Loss2: 0.097002	 Dis: 3.176857 Entropy: 4.735439 
[2022-10-02 18:04:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.001880	 E_lr_2:0.001880
[2022-10-02 18:04:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000019	 C_lr_2:0.000019	 C_lr_3:0.000019	 C_lr_4:0.000019
[2022-10-02 18:05:01 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.028287	Loss2: 0.031519	 Dis: 2.430147 Entropy: 5.251155 
[2022-10-02 18:05:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.001804	 E_lr_2:0.001804
[2022-10-02 18:05:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000018	 C_lr_2:0.000018	 C_lr_3:0.000018	 C_lr_4:0.000018
[2022-10-02 18:05:07 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.033758	Loss2: 0.036752	 Dis: 4.436432 Entropy: 4.525781 
[2022-10-02 18:05:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.001729	 E_lr_2:0.001729
[2022-10-02 18:05:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 18:05:13 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.064904	Loss2: 0.065095	 Dis: 3.006945 Entropy: 4.857841 
[2022-10-02 18:05:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.001655	 E_lr_2:0.001655
[2022-10-02 18:05:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 18:05:19 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.056497	Loss2: 0.066806	 Dis: 2.973282 Entropy: 5.096752 
[2022-10-02 18:05:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.001582	 E_lr_2:0.001582
[2022-10-02 18:05:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000016	 C_lr_2:0.000016	 C_lr_3:0.000016	 C_lr_4:0.000016
[2022-10-02 18:05:25 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.014865	Loss2: 0.018764	 Dis: 2.701447 Entropy: 5.805331 
[2022-10-02 18:05:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.001509	 E_lr_2:0.001509
[2022-10-02 18:05:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 18:05:31 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.105850	Loss2: 0.080339	 Dis: 3.973358 Entropy: 4.917349 
[2022-10-02 18:05:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.001437	 E_lr_2:0.001437
[2022-10-02 18:05:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 18:05:37 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.153556	Loss2: 0.139788	 Dis: 3.845732 Entropy: 4.559693 
[2022-10-02 18:05:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.001367	 E_lr_2:0.001367
[2022-10-02 18:05:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 18:05:43 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.053643	Loss2: 0.033247	 Dis: 1.424114 Entropy: 4.317493 
[2022-10-02 18:05:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.001297	 E_lr_2:0.001297
[2022-10-02 18:05:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000013	 C_lr_2:0.000013	 C_lr_3:0.000013	 C_lr_4:0.000013
[2022-10-02 18:05:49 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.014618	Loss2: 0.015371	 Dis: 1.735142 Entropy: 4.540652 
[2022-10-02 18:05:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.001229	 E_lr_2:0.001229
[2022-10-02 18:05:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 18:05:56 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.011832	Loss2: 0.014985	 Dis: 1.935251 Entropy: 5.700478 
[2022-10-02 18:05:56 demo] (houston_program2.py 512): INFO E_lr_1: 0.001162	 E_lr_2:0.001162
[2022-10-02 18:05:56 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 18:06:02 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.032120	Loss2: 0.034944	 Dis: 2.190022 Entropy: 5.449713 
[2022-10-02 18:06:02 demo] (houston_program2.py 512): INFO E_lr_1: 0.001096	 E_lr_2:0.001096
[2022-10-02 18:06:02 demo] (houston_program2.py 513): INFO C_lr_1: 0.000011	 C_lr_2:0.000011	 C_lr_3:0.000011	 C_lr_4:0.000011
[2022-10-02 18:06:08 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.017093	Loss2: 0.021978	 Dis: 2.160130 Entropy: 4.813585 
[2022-10-02 18:06:08 demo] (houston_program2.py 512): INFO E_lr_1: 0.001032	 E_lr_2:0.001032
[2022-10-02 18:06:08 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 18:06:12 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.014199	Loss2: 0.017065	 Dis: 2.419909 Entropy: 4.212604 
[2022-10-02 18:06:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.000969	 E_lr_2:0.000969
[2022-10-02 18:06:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 18:06:18 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.021130	Loss2: 0.019959	 Dis: 2.734798 Entropy: 4.285167 
[2022-10-02 18:06:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.000908	 E_lr_2:0.000908
[2022-10-02 18:06:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000009	 C_lr_2:0.000009	 C_lr_3:0.000009	 C_lr_4:0.000009
[2022-10-02 18:06:24 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.097428	Loss2: 0.073731	 Dis: 2.326525 Entropy: 4.939535 
[2022-10-02 18:06:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.000848	 E_lr_2:0.000848
[2022-10-02 18:06:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 18:06:30 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.016884	Loss2: 0.019783	 Dis: 2.125713 Entropy: 4.846810 
[2022-10-02 18:06:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.000790	 E_lr_2:0.000790
[2022-10-02 18:06:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 18:06:36 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.400506	Loss2: 0.342477	 Dis: 3.330551 Entropy: 4.834268 
[2022-10-02 18:06:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.000734	 E_lr_2:0.000734
[2022-10-02 18:06:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 18:06:43 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.152174	Loss2: 0.166154	 Dis: 2.537060 Entropy: 4.544055 
[2022-10-02 18:06:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.000679	 E_lr_2:0.000679
[2022-10-02 18:06:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 18:06:48 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.013401	Loss2: 0.009252	 Dis: 1.857723 Entropy: 4.373016 
[2022-10-02 18:06:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.000626	 E_lr_2:0.000626
[2022-10-02 18:06:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 18:06:54 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.009334	Loss2: 0.009995	 Dis: 1.844162 Entropy: 4.452027 
[2022-10-02 18:06:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.000575	 E_lr_2:0.000575
[2022-10-02 18:06:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 18:07:00 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.008605	Loss2: 0.008490	 Dis: 1.897093 Entropy: 5.255645 
[2022-10-02 18:07:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.000526	 E_lr_2:0.000526
[2022-10-02 18:07:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 18:07:07 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.010115	Loss2: 0.012130	 Dis: 3.420893 Entropy: 4.592928 
[2022-10-02 18:07:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.000479	 E_lr_2:0.000479
[2022-10-02 18:07:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 18:07:12 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.070389	Loss2: 0.045592	 Dis: 2.890572 Entropy: 5.700975 
[2022-10-02 18:07:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.000433	 E_lr_2:0.000433
[2022-10-02 18:07:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 18:07:18 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.005537	Loss2: 0.004545	 Dis: 1.803799 Entropy: 5.401362 
[2022-10-02 18:07:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.000390	 E_lr_2:0.000390
[2022-10-02 18:07:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 18:07:24 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.112716	Loss2: 0.119659	 Dis: 2.220161 Entropy: 4.516318 
[2022-10-02 18:07:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.000349	 E_lr_2:0.000349
[2022-10-02 18:07:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 18:07:30 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.008081	Loss2: 0.012041	 Dis: 3.424004 Entropy: 5.696639 
[2022-10-02 18:07:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.000310	 E_lr_2:0.000310
[2022-10-02 18:07:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 18:07:36 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.008532	Loss2: 0.012343	 Dis: 1.831717 Entropy: 4.579954 
[2022-10-02 18:07:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.000273	 E_lr_2:0.000273
[2022-10-02 18:07:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 18:07:42 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.004627	Loss2: 0.005040	 Dis: 2.293701 Entropy: 4.669068 
[2022-10-02 18:07:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.000239	 E_lr_2:0.000239
[2022-10-02 18:07:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 18:07:49 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.008399	Loss2: 0.006964	 Dis: 3.392471 Entropy: 4.844903 
[2022-10-02 18:07:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.000206	 E_lr_2:0.000206
[2022-10-02 18:07:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 18:07:55 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.022870	Loss2: 0.023237	 Dis: 1.233696 Entropy: 5.447061 
[2022-10-02 18:07:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.000176	 E_lr_2:0.000176
[2022-10-02 18:07:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 18:08:01 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.005140	Loss2: 0.004869	 Dis: 2.399462 Entropy: 6.482177 
[2022-10-02 18:08:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.000148	 E_lr_2:0.000148
[2022-10-02 18:08:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 18:08:06 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.015640	Loss2: 0.006940	 Dis: 1.212013 Entropy: 4.372310 
[2022-10-02 18:08:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.000123	 E_lr_2:0.000123
[2022-10-02 18:08:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 18:08:12 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.020276	Loss2: 0.020260	 Dis: 2.829851 Entropy: 4.321235 
[2022-10-02 18:08:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.000100	 E_lr_2:0.000100
[2022-10-02 18:08:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 18:08:18 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.007894	Loss2: 0.007843	 Dis: 2.288588 Entropy: 5.189799 
[2022-10-02 18:08:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.000079	 E_lr_2:0.000079
[2022-10-02 18:08:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 18:08:24 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.005220	Loss2: 0.004624	 Dis: 2.980795 Entropy: 5.088943 
[2022-10-02 18:08:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.000061	 E_lr_2:0.000061
[2022-10-02 18:08:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 18:08:30 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.024067	Loss2: 0.021741	 Dis: 2.107803 Entropy: 4.587735 
[2022-10-02 18:08:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.000045	 E_lr_2:0.000045
[2022-10-02 18:08:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:08:36 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.012181	Loss2: 0.014814	 Dis: 2.055023 Entropy: 5.786915 
[2022-10-02 18:08:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.000031	 E_lr_2:0.000031
[2022-10-02 18:08:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:08:42 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.008239	Loss2: 0.008183	 Dis: 2.988022 Entropy: 4.709791 
[2022-10-02 18:08:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.000020	 E_lr_2:0.000020
[2022-10-02 18:08:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:08:48 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.022561	Loss2: 0.018244	 Dis: 2.862833 Entropy: 4.595566 
[2022-10-02 18:08:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.000011	 E_lr_2:0.000011
[2022-10-02 18:08:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:08:54 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.009083	Loss2: 0.008289	 Dis: 2.529888 Entropy: 6.109630 
[2022-10-02 18:08:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.000005	 E_lr_2:0.000005
[2022-10-02 18:08:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:09:00 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.006765	Loss2: 0.004707	 Dis: 2.026501 Entropy: 5.270290 
[2022-10-02 18:09:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.000001	 E_lr_2:0.000001
[2022-10-02 18:09:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:09:06 demo] (houston_program2.py 504): INFO Train Ep: 35 	Loss1: 0.009996	Loss2: 0.007536	 Dis: 1.839710 Entropy: 4.544837 
[2022-10-02 18:09:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.000000	 E_lr_2:0.000000
[2022-10-02 18:09:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:09:06 demo] (houston_program2.py 515): INFO time_35_epoch:597.2962906360626
[2022-10-02 18:09:14 demo] (houston_program2.py 673): INFO 	val_Accuracy: 31369/53200 (58.96%)	
[2022-10-02 18:09:14 demo] (utils.py 47): INFO outputs/demo/finetune_100/ckpt_epoch_35.pth saving......
[2022-10-02 18:09:14 demo] (utils.py 49): INFO outputs/demo/finetune_100/ckpt_epoch_35.pth saved !!!
[2022-10-02 18:09:15 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0118 (0.0118)	loss 0.0082 (0.0082)	grad_norm 0.0510 (0.0510)	mem 460MB
[2022-10-02 18:09:15 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:15 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0177 (0.0177)	loss 0.0118 (0.0118)	grad_norm 0.0424 (0.0424)	mem 460MB
[2022-10-02 18:09:15 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:15 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0221 (0.0221)	loss 0.0176 (0.0176)	grad_norm 0.0774 (0.0774)	mem 460MB
[2022-10-02 18:09:15 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:15 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0197 (0.0197)	loss 0.0149 (0.0149)	grad_norm 0.0414 (0.0414)	mem 460MB
[2022-10-02 18:09:15 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:16 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0242 (0.0242)	loss 0.0128 (0.0128)	grad_norm 0.0617 (0.0617)	mem 460MB
[2022-10-02 18:09:16 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:16 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0215 (0.0215)	loss 0.0102 (0.0102)	grad_norm 0.0649 (0.0649)	mem 460MB
[2022-10-02 18:09:16 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:16 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0113 (0.0113)	loss 0.0129 (0.0129)	grad_norm 0.0506 (0.0506)	mem 460MB
[2022-10-02 18:09:16 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:16 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0210 (0.0210)	loss 0.0117 (0.0117)	grad_norm 0.0820 (0.0820)	mem 460MB
[2022-10-02 18:09:16 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:16 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000036	time 0.0144 (0.0144)	loss 0.0107 (0.0107)	grad_norm 0.0699 (0.0699)	mem 460MB
[2022-10-02 18:09:16 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:16 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0211 (0.0211)	loss 0.0103 (0.0103)	grad_norm 0.0451 (0.0451)	mem 460MB
[2022-10-02 18:09:16 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:17 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0199 (0.0199)	loss 0.0142 (0.0142)	grad_norm 0.0707 (0.0707)	mem 460MB
[2022-10-02 18:09:17 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:17 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0202 (0.0202)	loss 0.0104 (0.0104)	grad_norm 0.0615 (0.0615)	mem 460MB
[2022-10-02 18:09:17 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:17 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0188 (0.0188)	loss 0.0121 (0.0121)	grad_norm 0.0440 (0.0440)	mem 460MB
[2022-10-02 18:09:17 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:17 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0220 (0.0220)	loss 0.0103 (0.0103)	grad_norm 0.0638 (0.0638)	mem 460MB
[2022-10-02 18:09:17 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:17 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0184 (0.0184)	loss 0.0079 (0.0079)	grad_norm 0.0459 (0.0459)	mem 460MB
[2022-10-02 18:09:17 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:17 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0211 (0.0211)	loss 0.0091 (0.0091)	grad_norm 0.0573 (0.0573)	mem 460MB
[2022-10-02 18:09:17 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:18 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0210 (0.0210)	loss 0.0130 (0.0130)	grad_norm 0.0512 (0.0512)	mem 460MB
[2022-10-02 18:09:18 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:18 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0218 (0.0218)	loss 0.0076 (0.0076)	grad_norm 0.0459 (0.0459)	mem 460MB
[2022-10-02 18:09:18 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:18 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0210 (0.0210)	loss 0.0100 (0.0100)	grad_norm 0.0509 (0.0509)	mem 460MB
[2022-10-02 18:09:18 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:18 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0180 (0.0180)	loss 0.0093 (0.0093)	grad_norm 0.0464 (0.0464)	mem 460MB
[2022-10-02 18:09:18 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:18 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0220 (0.0220)	loss 0.0089 (0.0089)	grad_norm 0.0425 (0.0425)	mem 460MB
[2022-10-02 18:09:18 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:19 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0098 (0.0098)	loss 0.0102 (0.0102)	grad_norm 0.0748 (0.0748)	mem 460MB
[2022-10-02 18:09:19 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:19 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0211 (0.0211)	loss 0.0100 (0.0100)	grad_norm 0.0476 (0.0476)	mem 460MB
[2022-10-02 18:09:19 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:19 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0106 (0.0106)	loss 0.0113 (0.0113)	grad_norm 0.0444 (0.0444)	mem 460MB
[2022-10-02 18:09:19 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:19 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0218 (0.0218)	loss 0.0105 (0.0105)	grad_norm 0.0317 (0.0317)	mem 460MB
[2022-10-02 18:09:19 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:19 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0087 (0.0087)	loss 0.0101 (0.0101)	grad_norm 0.0382 (0.0382)	mem 460MB
[2022-10-02 18:09:19 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:19 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0085 (0.0085)	loss 0.0113 (0.0113)	grad_norm 0.0448 (0.0448)	mem 460MB
[2022-10-02 18:09:19 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:19 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0084 (0.0084)	loss 0.0141 (0.0141)	grad_norm 0.0548 (0.0548)	mem 460MB
[2022-10-02 18:09:19 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:19 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0086 (0.0086)	loss 0.0106 (0.0106)	grad_norm 0.0368 (0.0368)	mem 460MB
[2022-10-02 18:09:19 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:20 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0146 (0.0146)	loss 0.0116 (0.0116)	grad_norm 0.0375 (0.0375)	mem 460MB
[2022-10-02 18:09:20 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:20 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0206 (0.0206)	loss 0.0101 (0.0101)	grad_norm 0.0355 (0.0355)	mem 460MB
[2022-10-02 18:09:20 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:20 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0204 (0.0204)	loss 0.0114 (0.0114)	grad_norm 0.0452 (0.0452)	mem 460MB
[2022-10-02 18:09:20 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:20 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0200 (0.0200)	loss 0.0119 (0.0119)	grad_norm 0.0622 (0.0622)	mem 460MB
[2022-10-02 18:09:20 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:20 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0212 (0.0212)	loss 0.0134 (0.0134)	grad_norm 0.0462 (0.0462)	mem 460MB
[2022-10-02 18:09:20 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:20 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0211 (0.0211)	loss 0.0104 (0.0104)	grad_norm 0.0609 (0.0609)	mem 460MB
[2022-10-02 18:09:20 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:21 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0247 (0.0247)	loss 0.0096 (0.0096)	grad_norm 0.0463 (0.0463)	mem 460MB
[2022-10-02 18:09:21 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:21 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0214 (0.0214)	loss 0.0078 (0.0078)	grad_norm 0.0446 (0.0446)	mem 460MB
[2022-10-02 18:09:21 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:21 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0111 (0.0111)	loss 0.0089 (0.0089)	grad_norm 0.0473 (0.0473)	mem 460MB
[2022-10-02 18:09:21 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:21 demo] (houston_program2.py 243): INFO Train: [36/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0220 (0.0220)	loss 0.0119 (0.0119)	grad_norm 0.0542 (0.0542)	mem 460MB
[2022-10-02 18:09:21 demo] (houston_program2.py 252): INFO EPOCH 36 training takes 0:00:00
[2022-10-02 18:09:21 demo] (houston_program2.py 333): INFO Train Ep: 36 	Loss1: 0.153130	Loss2: 0.172682	 Dis: 5.652422 Entropy: 5.605234 
[2022-10-02 18:09:21 demo] (houston_program2.py 335): INFO time_36_epoch:6.994088172912598
[2022-10-02 18:09:22 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0116 (0.0116)	loss 0.0106 (0.0106)	grad_norm 0.0464 (0.0464)	mem 460MB
[2022-10-02 18:09:22 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:22 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0227 (0.0227)	loss 0.0136 (0.0136)	grad_norm 0.0518 (0.0518)	mem 460MB
[2022-10-02 18:09:22 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:22 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0229 (0.0229)	loss 0.0096 (0.0096)	grad_norm 0.0500 (0.0500)	mem 460MB
[2022-10-02 18:09:22 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:22 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0183 (0.0183)	loss 0.0086 (0.0086)	grad_norm 0.0490 (0.0490)	mem 460MB
[2022-10-02 18:09:22 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:22 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0217 (0.0217)	loss 0.0137 (0.0137)	grad_norm 0.0608 (0.0608)	mem 460MB
[2022-10-02 18:09:23 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:23 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0123 (0.0123)	loss 0.0118 (0.0118)	grad_norm 0.0568 (0.0568)	mem 460MB
[2022-10-02 18:09:23 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:23 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0224 (0.0224)	loss 0.0107 (0.0107)	grad_norm 0.0412 (0.0412)	mem 460MB
[2022-10-02 18:09:23 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:23 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0146 (0.0146)	loss 0.0084 (0.0084)	grad_norm 0.0791 (0.0791)	mem 460MB
[2022-10-02 18:09:23 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:23 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0223 (0.0223)	loss 0.0111 (0.0111)	grad_norm 0.0469 (0.0469)	mem 460MB
[2022-10-02 18:09:23 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:23 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0138 (0.0138)	loss 0.0111 (0.0111)	grad_norm 0.0691 (0.0691)	mem 460MB
[2022-10-02 18:09:23 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:24 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0221 (0.0221)	loss 0.0092 (0.0092)	grad_norm 0.0604 (0.0604)	mem 460MB
[2022-10-02 18:09:24 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:24 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0172 (0.0172)	loss 0.0103 (0.0103)	grad_norm 0.0525 (0.0525)	mem 460MB
[2022-10-02 18:09:24 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:24 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0218 (0.0218)	loss 0.0096 (0.0096)	grad_norm 0.0465 (0.0465)	mem 460MB
[2022-10-02 18:09:24 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:24 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0197 (0.0197)	loss 0.0109 (0.0109)	grad_norm 0.0759 (0.0759)	mem 460MB
[2022-10-02 18:09:24 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:24 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0204 (0.0204)	loss 0.0094 (0.0094)	grad_norm 0.0523 (0.0523)	mem 460MB
[2022-10-02 18:09:24 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:24 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0206 (0.0206)	loss 0.0100 (0.0100)	grad_norm 0.0438 (0.0438)	mem 460MB
[2022-10-02 18:09:24 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:25 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0217 (0.0217)	loss 0.0112 (0.0112)	grad_norm 0.0645 (0.0645)	mem 460MB
[2022-10-02 18:09:25 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:25 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0208 (0.0208)	loss 0.0089 (0.0089)	grad_norm 0.0549 (0.0549)	mem 460MB
[2022-10-02 18:09:25 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:25 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0206 (0.0206)	loss 0.0136 (0.0136)	grad_norm 0.0617 (0.0617)	mem 460MB
[2022-10-02 18:09:25 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:25 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0201 (0.0201)	loss 0.0108 (0.0108)	grad_norm 0.0671 (0.0671)	mem 460MB
[2022-10-02 18:09:25 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:25 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0245 (0.0245)	loss 0.0078 (0.0078)	grad_norm 0.0478 (0.0478)	mem 460MB
[2022-10-02 18:09:25 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:26 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0236 (0.0236)	loss 0.0112 (0.0112)	grad_norm 0.0737 (0.0737)	mem 460MB
[2022-10-02 18:09:26 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:26 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0199 (0.0199)	loss 0.0106 (0.0106)	grad_norm 0.0594 (0.0594)	mem 460MB
[2022-10-02 18:09:26 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:26 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000035	time 0.0242 (0.0242)	loss 0.0138 (0.0138)	grad_norm 0.0455 (0.0455)	mem 460MB
[2022-10-02 18:09:26 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:26 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0222 (0.0222)	loss 0.0100 (0.0100)	grad_norm 0.0696 (0.0696)	mem 460MB
[2022-10-02 18:09:26 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:26 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0239 (0.0239)	loss 0.0145 (0.0145)	grad_norm 0.0609 (0.0609)	mem 460MB
[2022-10-02 18:09:26 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:26 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0256 (0.0256)	loss 0.0118 (0.0118)	grad_norm 0.0511 (0.0511)	mem 460MB
[2022-10-02 18:09:26 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:27 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0111 (0.0111)	loss 0.0095 (0.0095)	grad_norm 0.0595 (0.0595)	mem 460MB
[2022-10-02 18:09:27 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:27 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0206 (0.0206)	loss 0.0134 (0.0134)	grad_norm 0.0703 (0.0703)	mem 460MB
[2022-10-02 18:09:27 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:27 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0105 (0.0105)	loss 0.0116 (0.0116)	grad_norm 0.0429 (0.0429)	mem 460MB
[2022-10-02 18:09:27 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:27 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0217 (0.0217)	loss 0.0173 (0.0173)	grad_norm 0.1067 (0.1067)	mem 460MB
[2022-10-02 18:09:27 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:27 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0102 (0.0102)	loss 0.0117 (0.0117)	grad_norm 0.0820 (0.0820)	mem 460MB
[2022-10-02 18:09:27 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:28 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0207 (0.0207)	loss 0.0097 (0.0097)	grad_norm 0.0395 (0.0395)	mem 460MB
[2022-10-02 18:09:28 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:28 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0116 (0.0116)	loss 0.0162 (0.0162)	grad_norm 0.0951 (0.0951)	mem 460MB
[2022-10-02 18:09:28 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:28 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0202 (0.0202)	loss 0.0101 (0.0101)	grad_norm 0.0567 (0.0567)	mem 460MB
[2022-10-02 18:09:28 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:28 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0119 (0.0119)	loss 0.0119 (0.0119)	grad_norm 0.0372 (0.0372)	mem 460MB
[2022-10-02 18:09:28 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:28 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0219 (0.0219)	loss 0.0126 (0.0126)	grad_norm 0.0616 (0.0616)	mem 460MB
[2022-10-02 18:09:28 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:28 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0123 (0.0123)	loss 0.0138 (0.0138)	grad_norm 0.0838 (0.0838)	mem 460MB
[2022-10-02 18:09:28 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:29 demo] (houston_program2.py 243): INFO Train: [37/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0207 (0.0207)	loss 0.0102 (0.0102)	grad_norm 0.0350 (0.0350)	mem 460MB
[2022-10-02 18:09:29 demo] (houston_program2.py 252): INFO EPOCH 37 training takes 0:00:00
[2022-10-02 18:09:29 demo] (houston_program2.py 333): INFO Train Ep: 37 	Loss1: 0.247960	Loss2: 0.297174	 Dis: 4.886778 Entropy: 4.721737 
[2022-10-02 18:09:29 demo] (houston_program2.py 335): INFO time_37_epoch:7.337005376815796
[2022-10-02 18:09:29 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0116 (0.0116)	loss 0.0089 (0.0089)	grad_norm 0.0928 (0.0928)	mem 460MB
[2022-10-02 18:09:29 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:29 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0206 (0.0206)	loss 0.0081 (0.0081)	grad_norm 0.0609 (0.0609)	mem 460MB
[2022-10-02 18:09:29 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:29 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0220 (0.0220)	loss 0.0130 (0.0130)	grad_norm 0.0373 (0.0373)	mem 460MB
[2022-10-02 18:09:30 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:30 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0191 (0.0191)	loss 0.0156 (0.0156)	grad_norm 0.0919 (0.0919)	mem 460MB
[2022-10-02 18:09:30 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:30 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0210 (0.0210)	loss 0.0151 (0.0151)	grad_norm 0.0691 (0.0691)	mem 460MB
[2022-10-02 18:09:30 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:30 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0170 (0.0170)	loss 0.0102 (0.0102)	grad_norm 0.0495 (0.0495)	mem 460MB
[2022-10-02 18:09:30 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:30 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0212 (0.0212)	loss 0.0105 (0.0105)	grad_norm 0.0663 (0.0663)	mem 460MB
[2022-10-02 18:09:30 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:30 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0202 (0.0202)	loss 0.0101 (0.0101)	grad_norm 0.0565 (0.0565)	mem 460MB
[2022-10-02 18:09:30 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:31 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0214 (0.0214)	loss 0.0107 (0.0107)	grad_norm 0.0321 (0.0321)	mem 460MB
[2022-10-02 18:09:31 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:31 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0207 (0.0207)	loss 0.0120 (0.0120)	grad_norm 0.0731 (0.0731)	mem 460MB
[2022-10-02 18:09:31 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:31 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0212 (0.0212)	loss 0.0146 (0.0146)	grad_norm 0.0718 (0.0718)	mem 460MB
[2022-10-02 18:09:31 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:31 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0212 (0.0212)	loss 0.0110 (0.0110)	grad_norm 0.0383 (0.0383)	mem 460MB
[2022-10-02 18:09:31 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:31 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0151 (0.0151)	loss 0.0085 (0.0085)	grad_norm 0.0722 (0.0722)	mem 460MB
[2022-10-02 18:09:31 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:31 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0090 (0.0090)	loss 0.0121 (0.0121)	grad_norm 0.0455 (0.0455)	mem 460MB
[2022-10-02 18:09:31 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:31 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0086 (0.0086)	loss 0.0086 (0.0086)	grad_norm 0.0702 (0.0702)	mem 460MB
[2022-10-02 18:09:31 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:32 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0096 (0.0096)	loss 0.0144 (0.0144)	grad_norm 0.0885 (0.0885)	mem 460MB
[2022-10-02 18:09:32 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:32 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0209 (0.0209)	loss 0.0114 (0.0114)	grad_norm 0.0555 (0.0555)	mem 460MB
[2022-10-02 18:09:32 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:32 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0212 (0.0212)	loss 0.0105 (0.0105)	grad_norm 0.0601 (0.0601)	mem 460MB
[2022-10-02 18:09:32 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:32 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0135 (0.0135)	loss 0.0143 (0.0143)	grad_norm 0.0952 (0.0952)	mem 460MB
[2022-10-02 18:09:32 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:32 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0214 (0.0214)	loss 0.0106 (0.0106)	grad_norm 0.0508 (0.0508)	mem 460MB
[2022-10-02 18:09:32 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:32 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0120 (0.0120)	loss 0.0096 (0.0096)	grad_norm 0.0610 (0.0610)	mem 460MB
[2022-10-02 18:09:32 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:33 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0206 (0.0206)	loss 0.0100 (0.0100)	grad_norm 0.0540 (0.0540)	mem 460MB
[2022-10-02 18:09:33 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:33 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0191 (0.0191)	loss 0.0111 (0.0111)	grad_norm 0.0681 (0.0681)	mem 460MB
[2022-10-02 18:09:33 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:33 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0206 (0.0206)	loss 0.0095 (0.0095)	grad_norm 0.0398 (0.0398)	mem 460MB
[2022-10-02 18:09:33 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:33 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0184 (0.0184)	loss 0.0080 (0.0080)	grad_norm 0.0722 (0.0722)	mem 460MB
[2022-10-02 18:09:33 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:33 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0214 (0.0214)	loss 0.0139 (0.0139)	grad_norm 0.0610 (0.0610)	mem 460MB
[2022-10-02 18:09:33 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:33 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0191 (0.0191)	loss 0.0139 (0.0139)	grad_norm 0.0564 (0.0564)	mem 460MB
[2022-10-02 18:09:33 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:34 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0207 (0.0207)	loss 0.0090 (0.0090)	grad_norm 0.0599 (0.0599)	mem 460MB
[2022-10-02 18:09:34 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:34 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0202 (0.0202)	loss 0.0098 (0.0098)	grad_norm 0.0783 (0.0783)	mem 460MB
[2022-10-02 18:09:34 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:34 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0245 (0.0245)	loss 0.0098 (0.0098)	grad_norm 0.0458 (0.0458)	mem 460MB
[2022-10-02 18:09:34 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:34 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0211 (0.0211)	loss 0.0128 (0.0128)	grad_norm 0.0584 (0.0584)	mem 460MB
[2022-10-02 18:09:34 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:34 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0105 (0.0105)	loss 0.0118 (0.0118)	grad_norm 0.0496 (0.0496)	mem 460MB
[2022-10-02 18:09:34 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:34 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0200 (0.0200)	loss 0.0093 (0.0093)	grad_norm 0.0863 (0.0863)	mem 460MB
[2022-10-02 18:09:35 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:35 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0105 (0.0105)	loss 0.0136 (0.0136)	grad_norm 0.0819 (0.0819)	mem 460MB
[2022-10-02 18:09:35 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:35 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0204 (0.0204)	loss 0.0102 (0.0102)	grad_norm 0.0517 (0.0517)	mem 460MB
[2022-10-02 18:09:35 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:35 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0116 (0.0116)	loss 0.0142 (0.0142)	grad_norm 0.0798 (0.0798)	mem 460MB
[2022-10-02 18:09:35 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:35 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0217 (0.0217)	loss 0.0094 (0.0094)	grad_norm 0.0939 (0.0939)	mem 460MB
[2022-10-02 18:09:35 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:35 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0134 (0.0134)	loss 0.0094 (0.0094)	grad_norm 0.0329 (0.0329)	mem 460MB
[2022-10-02 18:09:35 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:36 demo] (houston_program2.py 243): INFO Train: [38/100][0/2]	eta 0:00:00 lr 0.000034	time 0.0200 (0.0200)	loss 0.0079 (0.0079)	grad_norm 0.0889 (0.0889)	mem 460MB
[2022-10-02 18:09:36 demo] (houston_program2.py 252): INFO EPOCH 38 training takes 0:00:00
[2022-10-02 18:09:36 demo] (houston_program2.py 333): INFO Train Ep: 38 	Loss1: 0.062443	Loss2: 0.056752	 Dis: 2.626623 Entropy: 5.697666 
[2022-10-02 18:09:36 demo] (houston_program2.py 335): INFO time_38_epoch:7.030559062957764
[2022-10-02 18:09:36 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0118 (0.0118)	loss 0.0110 (0.0110)	grad_norm 0.0749 (0.0749)	mem 460MB
[2022-10-02 18:09:36 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:36 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0215 (0.0215)	loss 0.0124 (0.0124)	grad_norm 0.0452 (0.0452)	mem 460MB
[2022-10-02 18:09:36 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:37 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0221 (0.0221)	loss 0.0094 (0.0094)	grad_norm 0.0438 (0.0438)	mem 460MB
[2022-10-02 18:09:37 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:37 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0188 (0.0188)	loss 0.0111 (0.0111)	grad_norm 0.0394 (0.0394)	mem 460MB
[2022-10-02 18:09:37 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:37 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0216 (0.0216)	loss 0.0131 (0.0131)	grad_norm 0.0556 (0.0556)	mem 460MB
[2022-10-02 18:09:37 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:37 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0211 (0.0211)	loss 0.0090 (0.0090)	grad_norm 0.0398 (0.0398)	mem 460MB
[2022-10-02 18:09:37 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:37 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0217 (0.0217)	loss 0.0066 (0.0066)	grad_norm 0.0505 (0.0505)	mem 460MB
[2022-10-02 18:09:37 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:37 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0220 (0.0220)	loss 0.0095 (0.0095)	grad_norm 0.0431 (0.0431)	mem 460MB
[2022-10-02 18:09:37 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:38 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0239 (0.0239)	loss 0.0121 (0.0121)	grad_norm 0.0575 (0.0575)	mem 460MB
[2022-10-02 18:09:38 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:38 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0189 (0.0189)	loss 0.0102 (0.0102)	grad_norm 0.0334 (0.0334)	mem 460MB
[2022-10-02 18:09:38 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:38 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0238 (0.0238)	loss 0.0080 (0.0080)	grad_norm 0.0375 (0.0375)	mem 460MB
[2022-10-02 18:09:38 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:38 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0220 (0.0220)	loss 0.0091 (0.0091)	grad_norm 0.0392 (0.0392)	mem 460MB
[2022-10-02 18:09:38 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:38 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0133 (0.0133)	loss 0.0109 (0.0109)	grad_norm 0.0477 (0.0477)	mem 460MB
[2022-10-02 18:09:38 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:39 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0208 (0.0208)	loss 0.0116 (0.0116)	grad_norm 0.0477 (0.0477)	mem 460MB
[2022-10-02 18:09:39 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:39 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0135 (0.0135)	loss 0.0100 (0.0100)	grad_norm 0.0364 (0.0364)	mem 460MB
[2022-10-02 18:09:39 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:39 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0219 (0.0219)	loss 0.0096 (0.0096)	grad_norm 0.0608 (0.0608)	mem 460MB
[2022-10-02 18:09:39 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:39 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0153 (0.0153)	loss 0.0105 (0.0105)	grad_norm 0.0456 (0.0456)	mem 460MB
[2022-10-02 18:09:39 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:39 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0243 (0.0243)	loss 0.0090 (0.0090)	grad_norm 0.0428 (0.0428)	mem 460MB
[2022-10-02 18:09:39 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:39 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0210 (0.0210)	loss 0.0105 (0.0105)	grad_norm 0.0803 (0.0803)	mem 460MB
[2022-10-02 18:09:39 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:40 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0208 (0.0208)	loss 0.0151 (0.0151)	grad_norm 0.1007 (0.1007)	mem 460MB
[2022-10-02 18:09:40 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:40 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0202 (0.0202)	loss 0.0138 (0.0138)	grad_norm 0.0449 (0.0449)	mem 460MB
[2022-10-02 18:09:40 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:40 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0194 (0.0194)	loss 0.0115 (0.0115)	grad_norm 0.0713 (0.0713)	mem 460MB
[2022-10-02 18:09:40 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:40 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0215 (0.0215)	loss 0.0121 (0.0121)	grad_norm 0.0924 (0.0924)	mem 460MB
[2022-10-02 18:09:40 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:40 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0113 (0.0113)	loss 0.0081 (0.0081)	grad_norm 0.0419 (0.0419)	mem 460MB
[2022-10-02 18:09:40 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:41 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0207 (0.0207)	loss 0.0107 (0.0107)	grad_norm 0.0585 (0.0585)	mem 460MB
[2022-10-02 18:09:41 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:41 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0132 (0.0132)	loss 0.0135 (0.0135)	grad_norm 0.0534 (0.0534)	mem 460MB
[2022-10-02 18:09:41 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:41 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0193 (0.0193)	loss 0.0089 (0.0089)	grad_norm 0.0336 (0.0336)	mem 460MB
[2022-10-02 18:09:41 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:41 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0202 (0.0202)	loss 0.0103 (0.0103)	grad_norm 0.0672 (0.0672)	mem 460MB
[2022-10-02 18:09:41 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:41 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0196 (0.0196)	loss 0.0115 (0.0115)	grad_norm 0.0457 (0.0457)	mem 460MB
[2022-10-02 18:09:41 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:41 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0205 (0.0205)	loss 0.0087 (0.0087)	grad_norm 0.0533 (0.0533)	mem 460MB
[2022-10-02 18:09:41 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:42 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0206 (0.0206)	loss 0.0114 (0.0114)	grad_norm 0.0383 (0.0383)	mem 460MB
[2022-10-02 18:09:42 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:42 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0121 (0.0121)	loss 0.0117 (0.0117)	grad_norm 0.0748 (0.0748)	mem 460MB
[2022-10-02 18:09:42 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:42 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0264 (0.0264)	loss 0.0089 (0.0089)	grad_norm 0.0515 (0.0515)	mem 460MB
[2022-10-02 18:09:42 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:42 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0208 (0.0208)	loss 0.0137 (0.0137)	grad_norm 0.0506 (0.0506)	mem 460MB
[2022-10-02 18:09:42 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:42 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0265 (0.0265)	loss 0.0105 (0.0105)	grad_norm 0.0949 (0.0949)	mem 460MB
[2022-10-02 18:09:42 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:43 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0229 (0.0229)	loss 0.0095 (0.0095)	grad_norm 0.0549 (0.0549)	mem 460MB
[2022-10-02 18:09:43 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:43 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0127 (0.0127)	loss 0.0100 (0.0100)	grad_norm 0.0465 (0.0465)	mem 460MB
[2022-10-02 18:09:43 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:43 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0248 (0.0248)	loss 0.0114 (0.0114)	grad_norm 0.0774 (0.0774)	mem 460MB
[2022-10-02 18:09:43 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:43 demo] (houston_program2.py 243): INFO Train: [39/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0151 (0.0151)	loss 0.0093 (0.0093)	grad_norm 0.0625 (0.0625)	mem 460MB
[2022-10-02 18:09:43 demo] (houston_program2.py 252): INFO EPOCH 39 training takes 0:00:00
[2022-10-02 18:09:43 demo] (houston_program2.py 333): INFO Train Ep: 39 	Loss1: 0.122841	Loss2: 0.129618	 Dis: 3.064035 Entropy: 6.036219 
[2022-10-02 18:09:43 demo] (houston_program2.py 335): INFO time_39_epoch:7.605364561080933
[2022-10-02 18:09:44 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0126 (0.0126)	loss 0.0117 (0.0117)	grad_norm 0.0734 (0.0734)	mem 460MB
[2022-10-02 18:09:44 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:44 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0195 (0.0195)	loss 0.0146 (0.0146)	grad_norm 0.0864 (0.0864)	mem 460MB
[2022-10-02 18:09:44 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:44 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0202 (0.0202)	loss 0.0081 (0.0081)	grad_norm 0.0564 (0.0564)	mem 460MB
[2022-10-02 18:09:44 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:44 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0222 (0.0222)	loss 0.0106 (0.0106)	grad_norm 0.0443 (0.0443)	mem 460MB
[2022-10-02 18:09:44 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:45 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0227 (0.0227)	loss 0.0119 (0.0119)	grad_norm 0.1121 (0.1121)	mem 460MB
[2022-10-02 18:09:45 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:45 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0223 (0.0223)	loss 0.0086 (0.0086)	grad_norm 0.0500 (0.0500)	mem 460MB
[2022-10-02 18:09:45 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:45 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0221 (0.0221)	loss 0.0096 (0.0096)	grad_norm 0.0684 (0.0684)	mem 460MB
[2022-10-02 18:09:45 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:45 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0132 (0.0132)	loss 0.0131 (0.0131)	grad_norm 0.0823 (0.0823)	mem 460MB
[2022-10-02 18:09:45 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:45 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0216 (0.0216)	loss 0.0145 (0.0145)	grad_norm 0.0824 (0.0824)	mem 460MB
[2022-10-02 18:09:45 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:45 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0159 (0.0159)	loss 0.0097 (0.0097)	grad_norm 0.0463 (0.0463)	mem 460MB
[2022-10-02 18:09:45 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:46 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0203 (0.0203)	loss 0.0116 (0.0116)	grad_norm 0.0923 (0.0923)	mem 460MB
[2022-10-02 18:09:46 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:46 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0207 (0.0207)	loss 0.0165 (0.0165)	grad_norm 0.1041 (0.1041)	mem 460MB
[2022-10-02 18:09:46 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:46 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000033	time 0.0223 (0.0223)	loss 0.0081 (0.0081)	grad_norm 0.0428 (0.0428)	mem 460MB
[2022-10-02 18:09:46 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:46 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0215 (0.0215)	loss 0.0092 (0.0092)	grad_norm 0.0856 (0.0856)	mem 460MB
[2022-10-02 18:09:46 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:46 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0265 (0.0265)	loss 0.0121 (0.0121)	grad_norm 0.0853 (0.0853)	mem 460MB
[2022-10-02 18:09:46 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:47 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0227 (0.0227)	loss 0.0138 (0.0138)	grad_norm 0.0554 (0.0554)	mem 460MB
[2022-10-02 18:09:47 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:47 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0104 (0.0104)	loss 0.0082 (0.0082)	grad_norm 0.0695 (0.0695)	mem 460MB
[2022-10-02 18:09:47 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:47 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0220 (0.0220)	loss 0.0148 (0.0148)	grad_norm 0.0928 (0.0928)	mem 460MB
[2022-10-02 18:09:47 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:47 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0117 (0.0117)	loss 0.0112 (0.0112)	grad_norm 0.0635 (0.0635)	mem 460MB
[2022-10-02 18:09:47 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:47 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0223 (0.0223)	loss 0.0117 (0.0117)	grad_norm 0.0387 (0.0387)	mem 460MB
[2022-10-02 18:09:47 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:47 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0164 (0.0164)	loss 0.0097 (0.0097)	grad_norm 0.0645 (0.0645)	mem 460MB
[2022-10-02 18:09:47 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:48 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0221 (0.0221)	loss 0.0131 (0.0131)	grad_norm 0.0860 (0.0860)	mem 460MB
[2022-10-02 18:09:48 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:48 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0211 (0.0211)	loss 0.0108 (0.0108)	grad_norm 0.0542 (0.0542)	mem 460MB
[2022-10-02 18:09:48 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:48 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0210 (0.0210)	loss 0.0100 (0.0100)	grad_norm 0.0656 (0.0656)	mem 460MB
[2022-10-02 18:09:48 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:48 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0217 (0.0217)	loss 0.0085 (0.0085)	grad_norm 0.0693 (0.0693)	mem 460MB
[2022-10-02 18:09:48 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:48 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0205 (0.0205)	loss 0.0129 (0.0129)	grad_norm 0.0659 (0.0659)	mem 460MB
[2022-10-02 18:09:48 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:49 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0220 (0.0220)	loss 0.0111 (0.0111)	grad_norm 0.0535 (0.0535)	mem 460MB
[2022-10-02 18:09:49 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:49 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0134 (0.0134)	loss 0.0089 (0.0089)	grad_norm 0.0856 (0.0856)	mem 460MB
[2022-10-02 18:09:49 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:49 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0211 (0.0211)	loss 0.0101 (0.0101)	grad_norm 0.0610 (0.0610)	mem 460MB
[2022-10-02 18:09:49 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:49 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0133 (0.0133)	loss 0.0098 (0.0098)	grad_norm 0.0824 (0.0824)	mem 460MB
[2022-10-02 18:09:49 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:49 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0218 (0.0218)	loss 0.0089 (0.0089)	grad_norm 0.0576 (0.0576)	mem 460MB
[2022-10-02 18:09:49 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:49 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0157 (0.0157)	loss 0.0111 (0.0111)	grad_norm 0.0533 (0.0533)	mem 460MB
[2022-10-02 18:09:49 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:50 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0204 (0.0204)	loss 0.0111 (0.0111)	grad_norm 0.0793 (0.0793)	mem 460MB
[2022-10-02 18:09:50 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:50 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0167 (0.0167)	loss 0.0113 (0.0113)	grad_norm 0.0466 (0.0466)	mem 460MB
[2022-10-02 18:09:50 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:50 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0219 (0.0219)	loss 0.0164 (0.0164)	grad_norm 0.0613 (0.0613)	mem 460MB
[2022-10-02 18:09:50 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:50 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0194 (0.0194)	loss 0.0131 (0.0131)	grad_norm 0.0452 (0.0452)	mem 460MB
[2022-10-02 18:09:50 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:50 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0218 (0.0218)	loss 0.0132 (0.0132)	grad_norm 0.1022 (0.1022)	mem 460MB
[2022-10-02 18:09:50 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:50 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0089 (0.0089)	loss 0.0129 (0.0129)	grad_norm 0.0563 (0.0563)	mem 460MB
[2022-10-02 18:09:50 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:51 demo] (houston_program2.py 243): INFO Train: [40/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0082 (0.0082)	loss 0.0126 (0.0126)	grad_norm 0.0840 (0.0840)	mem 460MB
[2022-10-02 18:09:51 demo] (houston_program2.py 252): INFO EPOCH 40 training takes 0:00:00
[2022-10-02 18:09:51 demo] (houston_program2.py 333): INFO Train Ep: 40 	Loss1: 0.190475	Loss2: 0.175845	 Dis: 3.085045 Entropy: 5.106525 
[2022-10-02 18:09:51 demo] (houston_program2.py 335): INFO time_40_epoch:7.284220218658447
[2022-10-02 18:09:51 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 18:09:51 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 18:09:51 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 18:09:51 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 18:09:51 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 18:09:51 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 18:09:51 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 18:09:51 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 18:09:51 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 18:09:51 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 18:09:57 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.035366	Loss2: 0.044073	 Dis: 2.068375 Entropy: 5.996286 
[2022-10-02 18:09:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.000487	 E_lr_2:0.000487
[2022-10-02 18:09:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 18:10:03 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.017229	Loss2: 0.014285	 Dis: 2.252274 Entropy: 5.299406 
[2022-10-02 18:10:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.000987	 E_lr_2:0.000987
[2022-10-02 18:10:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 18:10:09 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.099677	Loss2: 0.085487	 Dis: 4.629034 Entropy: 6.067145 
[2022-10-02 18:10:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.001487	 E_lr_2:0.001487
[2022-10-02 18:10:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 18:10:15 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.121430	Loss2: 0.097907	 Dis: 4.992105 Entropy: 4.857020 
[2022-10-02 18:10:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.001987	 E_lr_2:0.001987
[2022-10-02 18:10:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 18:10:21 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.144209	Loss2: 0.139958	 Dis: 4.806433 Entropy: 4.550228 
[2022-10-02 18:10:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.002487	 E_lr_2:0.002487
[2022-10-02 18:10:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 18:10:27 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.191539	Loss2: 0.158502	 Dis: 4.181789 Entropy: 4.725083 
[2022-10-02 18:10:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.002987	 E_lr_2:0.002987
[2022-10-02 18:10:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 18:10:33 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.161793	Loss2: 0.180004	 Dis: 2.810490 Entropy: 4.479983 
[2022-10-02 18:10:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.003487	 E_lr_2:0.003487
[2022-10-02 18:10:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 18:10:39 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.107378	Loss2: 0.102346	 Dis: 7.242304 Entropy: 4.522017 
[2022-10-02 18:10:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.003987	 E_lr_2:0.003987
[2022-10-02 18:10:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 18:10:45 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.104363	Loss2: 0.105553	 Dis: 6.136467 Entropy: 4.694374 
[2022-10-02 18:10:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.004487	 E_lr_2:0.004487
[2022-10-02 18:10:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 18:10:51 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.049871	Loss2: 0.050887	 Dis: 3.639843 Entropy: 5.071203 
[2022-10-02 18:10:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.004987	 E_lr_2:0.004987
[2022-10-02 18:10:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000050	 C_lr_2:0.000050	 C_lr_3:0.000050	 C_lr_4:0.000050
[2022-10-02 18:10:57 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.175048	Loss2: 0.143926	 Dis: 3.918015 Entropy: 5.559332 
[2022-10-02 18:10:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.004853	 E_lr_2:0.004853
[2022-10-02 18:10:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000049	 C_lr_2:0.000049	 C_lr_3:0.000049	 C_lr_4:0.000049
[2022-10-02 18:11:04 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.085067	Loss2: 0.065540	 Dis: 7.351297 Entropy: 4.884871 
[2022-10-02 18:11:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.004825	 E_lr_2:0.004825
[2022-10-02 18:11:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 18:11:09 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.217341	Loss2: 0.235631	 Dis: 4.951763 Entropy: 4.490661 
[2022-10-02 18:11:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.004795	 E_lr_2:0.004795
[2022-10-02 18:11:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 18:11:15 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.028754	Loss2: 0.037351	 Dis: 3.674730 Entropy: 6.672021 
[2022-10-02 18:11:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.004763	 E_lr_2:0.004763
[2022-10-02 18:11:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 18:11:21 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.205001	Loss2: 0.194132	 Dis: 5.705589 Entropy: 5.186774 
[2022-10-02 18:11:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.004728	 E_lr_2:0.004728
[2022-10-02 18:11:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 18:11:28 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.091044	Loss2: 0.075448	 Dis: 2.766586 Entropy: 4.376672 
[2022-10-02 18:11:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.004692	 E_lr_2:0.004692
[2022-10-02 18:11:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 18:11:34 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.052384	Loss2: 0.073968	 Dis: 4.426579 Entropy: 4.627364 
[2022-10-02 18:11:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.004653	 E_lr_2:0.004653
[2022-10-02 18:11:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 18:11:40 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.067529	Loss2: 0.060429	 Dis: 4.021982 Entropy: 4.767799 
[2022-10-02 18:11:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.004612	 E_lr_2:0.004612
[2022-10-02 18:11:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 18:11:46 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.046294	Loss2: 0.048160	 Dis: 4.380295 Entropy: 4.860655 
[2022-10-02 18:11:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.004569	 E_lr_2:0.004569
[2022-10-02 18:11:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 18:11:51 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.090530	Loss2: 0.090333	 Dis: 1.522726 Entropy: 4.548354 
[2022-10-02 18:11:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.004524	 E_lr_2:0.004524
[2022-10-02 18:11:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 18:11:57 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.040946	Loss2: 0.037446	 Dis: 3.355316 Entropy: 4.418110 
[2022-10-02 18:11:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.004477	 E_lr_2:0.004477
[2022-10-02 18:11:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 18:12:03 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.173770	Loss2: 0.125420	 Dis: 3.566570 Entropy: 5.146568 
[2022-10-02 18:12:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.004428	 E_lr_2:0.004428
[2022-10-02 18:12:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 18:12:09 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.070159	Loss2: 0.073404	 Dis: 2.806461 Entropy: 5.256141 
[2022-10-02 18:12:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.004377	 E_lr_2:0.004377
[2022-10-02 18:12:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 18:12:15 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.054591	Loss2: 0.064207	 Dis: 2.840921 Entropy: 6.002160 
[2022-10-02 18:12:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.004324	 E_lr_2:0.004324
[2022-10-02 18:12:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 18:12:21 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.014890	Loss2: 0.014961	 Dis: 4.687897 Entropy: 5.450515 
[2022-10-02 18:12:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.004269	 E_lr_2:0.004269
[2022-10-02 18:12:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 18:12:28 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.015508	Loss2: 0.021220	 Dis: 2.423225 Entropy: 4.541930 
[2022-10-02 18:12:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.004213	 E_lr_2:0.004213
[2022-10-02 18:12:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 18:12:33 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.044046	Loss2: 0.056351	 Dis: 3.606634 Entropy: 4.672191 
[2022-10-02 18:12:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.004155	 E_lr_2:0.004155
[2022-10-02 18:12:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 18:12:39 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.225539	Loss2: 0.225203	 Dis: 3.732603 Entropy: 4.766319 
[2022-10-02 18:12:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.004095	 E_lr_2:0.004095
[2022-10-02 18:12:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000041	 C_lr_2:0.000041	 C_lr_3:0.000041	 C_lr_4:0.000041
[2022-10-02 18:12:45 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.020967	Loss2: 0.023110	 Dis: 3.787291 Entropy: 4.504673 
[2022-10-02 18:12:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.004034	 E_lr_2:0.004034
[2022-10-02 18:12:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 18:12:51 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.035775	Loss2: 0.033316	 Dis: 3.687681 Entropy: 4.629342 
[2022-10-02 18:12:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.003971	 E_lr_2:0.003971
[2022-10-02 18:12:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 18:12:57 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.054181	Loss2: 0.029403	 Dis: 5.010857 Entropy: 4.409268 
[2022-10-02 18:12:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.003907	 E_lr_2:0.003907
[2022-10-02 18:12:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000039	 C_lr_2:0.000039	 C_lr_3:0.000039	 C_lr_4:0.000039
[2022-10-02 18:13:03 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.111185	Loss2: 0.115855	 Dis: 4.362303 Entropy: 4.431270 
[2022-10-02 18:13:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.003841	 E_lr_2:0.003841
[2022-10-02 18:13:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 18:13:09 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.107511	Loss2: 0.115087	 Dis: 2.603693 Entropy: 6.567478 
[2022-10-02 18:13:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.003774	 E_lr_2:0.003774
[2022-10-02 18:13:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 18:13:15 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.012988	Loss2: 0.017746	 Dis: 1.412838 Entropy: 5.794734 
[2022-10-02 18:13:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.003706	 E_lr_2:0.003706
[2022-10-02 18:13:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000037	 C_lr_2:0.000037	 C_lr_3:0.000037	 C_lr_4:0.000037
[2022-10-02 18:13:21 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.199998	Loss2: 0.122622	 Dis: 4.256077 Entropy: 4.723461 
[2022-10-02 18:13:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.003637	 E_lr_2:0.003637
[2022-10-02 18:13:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 18:13:27 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.166201	Loss2: 0.186320	 Dis: 3.541710 Entropy: 5.272969 
[2022-10-02 18:13:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.003566	 E_lr_2:0.003566
[2022-10-02 18:13:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 18:13:33 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.012585	Loss2: 0.012557	 Dis: 4.902706 Entropy: 5.260226 
[2022-10-02 18:13:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.003495	 E_lr_2:0.003495
[2022-10-02 18:13:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 18:13:40 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.041255	Loss2: 0.054669	 Dis: 5.057766 Entropy: 4.775958 
[2022-10-02 18:13:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.003422	 E_lr_2:0.003422
[2022-10-02 18:13:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000034	 C_lr_2:0.000034	 C_lr_3:0.000034	 C_lr_4:0.000034
[2022-10-02 18:13:45 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.014791	Loss2: 0.013543	 Dis: 3.603910 Entropy: 4.682777 
[2022-10-02 18:13:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.003349	 E_lr_2:0.003349
[2022-10-02 18:13:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 18:13:52 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.059233	Loss2: 0.101707	 Dis: 5.260439 Entropy: 4.546901 
[2022-10-02 18:13:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.003274	 E_lr_2:0.003274
[2022-10-02 18:13:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 18:13:58 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.045262	Loss2: 0.068800	 Dis: 3.780823 Entropy: 4.250981 
[2022-10-02 18:13:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.003199	 E_lr_2:0.003199
[2022-10-02 18:13:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000032	 C_lr_2:0.000032	 C_lr_3:0.000032	 C_lr_4:0.000032
[2022-10-02 18:14:04 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.053307	Loss2: 0.082513	 Dis: 1.938293 Entropy: 4.559912 
[2022-10-02 18:14:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.003124	 E_lr_2:0.003124
[2022-10-02 18:14:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000031	 C_lr_2:0.000031	 C_lr_3:0.000031	 C_lr_4:0.000031
[2022-10-02 18:14:10 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.044386	Loss2: 0.044142	 Dis: 2.211998 Entropy: 5.021387 
[2022-10-02 18:14:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.003047	 E_lr_2:0.003047
[2022-10-02 18:14:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 18:14:16 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.051847	Loss2: 0.044597	 Dis: 3.675560 Entropy: 4.249265 
[2022-10-02 18:14:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.002970	 E_lr_2:0.002970
[2022-10-02 18:14:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 18:14:23 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.028550	Loss2: 0.023362	 Dis: 1.985224 Entropy: 5.429439 
[2022-10-02 18:14:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.002893	 E_lr_2:0.002893
[2022-10-02 18:14:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000029	 C_lr_2:0.000029	 C_lr_3:0.000029	 C_lr_4:0.000029
[2022-10-02 18:14:29 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.079500	Loss2: 0.089449	 Dis: 5.578470 Entropy: 4.852900 
[2022-10-02 18:14:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.002815	 E_lr_2:0.002815
[2022-10-02 18:14:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000028	 C_lr_2:0.000028	 C_lr_3:0.000028	 C_lr_4:0.000028
[2022-10-02 18:14:35 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.030139	Loss2: 0.033208	 Dis: 2.118961 Entropy: 4.691277 
[2022-10-02 18:14:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.002737	 E_lr_2:0.002737
[2022-10-02 18:14:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 18:14:42 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.218466	Loss2: 0.225165	 Dis: 7.105249 Entropy: 5.114380 
[2022-10-02 18:14:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.002659	 E_lr_2:0.002659
[2022-10-02 18:14:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 18:14:48 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.085958	Loss2: 0.107828	 Dis: 5.138058 Entropy: 4.383417 
[2022-10-02 18:14:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.002581	 E_lr_2:0.002581
[2022-10-02 18:14:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000026	 C_lr_2:0.000026	 C_lr_3:0.000026	 C_lr_4:0.000026
[2022-10-02 18:14:54 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.014733	Loss2: 0.013209	 Dis: 3.842197 Entropy: 4.705991 
[2022-10-02 18:14:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.002502	 E_lr_2:0.002502
[2022-10-02 18:14:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 18:15:00 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.149386	Loss2: 0.156138	 Dis: 2.741905 Entropy: 5.264066 
[2022-10-02 18:15:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.002423	 E_lr_2:0.002423
[2022-10-02 18:15:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000024	 C_lr_2:0.000024	 C_lr_3:0.000024	 C_lr_4:0.000024
[2022-10-02 18:15:06 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.345097	Loss2: 0.342236	 Dis: 3.284073 Entropy: 4.776353 
[2022-10-02 18:15:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.002345	 E_lr_2:0.002345
[2022-10-02 18:15:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 18:15:12 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.061429	Loss2: 0.060900	 Dis: 4.337238 Entropy: 5.536036 
[2022-10-02 18:15:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.002267	 E_lr_2:0.002267
[2022-10-02 18:15:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 18:15:18 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.083826	Loss2: 0.100486	 Dis: 3.631466 Entropy: 4.310619 
[2022-10-02 18:15:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.002189	 E_lr_2:0.002189
[2022-10-02 18:15:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000022	 C_lr_2:0.000022	 C_lr_3:0.000022	 C_lr_4:0.000022
[2022-10-02 18:15:25 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.015395	Loss2: 0.013017	 Dis: 3.365654 Entropy: 4.170789 
[2022-10-02 18:15:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.002111	 E_lr_2:0.002111
[2022-10-02 18:15:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000021	 C_lr_2:0.000021	 C_lr_3:0.000021	 C_lr_4:0.000021
[2022-10-02 18:15:30 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.016182	Loss2: 0.020502	 Dis: 2.808903 Entropy: 4.039546 
[2022-10-02 18:15:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.002034	 E_lr_2:0.002034
[2022-10-02 18:15:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 18:15:36 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.014275	Loss2: 0.011481	 Dis: 2.369741 Entropy: 5.530985 
[2022-10-02 18:15:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.001957	 E_lr_2:0.001957
[2022-10-02 18:15:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 18:15:42 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.052695	Loss2: 0.048507	 Dis: 2.931925 Entropy: 4.471816 
[2022-10-02 18:15:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.001880	 E_lr_2:0.001880
[2022-10-02 18:15:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000019	 C_lr_2:0.000019	 C_lr_3:0.000019	 C_lr_4:0.000019
[2022-10-02 18:15:48 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.145407	Loss2: 0.147290	 Dis: 4.707066 Entropy: 4.990026 
[2022-10-02 18:15:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.001804	 E_lr_2:0.001804
[2022-10-02 18:15:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000018	 C_lr_2:0.000018	 C_lr_3:0.000018	 C_lr_4:0.000018
[2022-10-02 18:15:55 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.010798	Loss2: 0.012764	 Dis: 4.259018 Entropy: 4.617913 
[2022-10-02 18:15:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.001729	 E_lr_2:0.001729
[2022-10-02 18:15:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 18:16:01 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.026437	Loss2: 0.026505	 Dis: 3.299038 Entropy: 4.413514 
[2022-10-02 18:16:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.001655	 E_lr_2:0.001655
[2022-10-02 18:16:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 18:16:07 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.024080	Loss2: 0.028424	 Dis: 2.414349 Entropy: 4.465177 
[2022-10-02 18:16:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.001582	 E_lr_2:0.001582
[2022-10-02 18:16:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000016	 C_lr_2:0.000016	 C_lr_3:0.000016	 C_lr_4:0.000016
[2022-10-02 18:16:13 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.074754	Loss2: 0.071138	 Dis: 2.490490 Entropy: 4.663210 
[2022-10-02 18:16:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.001509	 E_lr_2:0.001509
[2022-10-02 18:16:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 18:16:19 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.016517	Loss2: 0.019429	 Dis: 2.801762 Entropy: 4.442657 
[2022-10-02 18:16:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.001437	 E_lr_2:0.001437
[2022-10-02 18:16:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 18:16:25 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.018312	Loss2: 0.013972	 Dis: 1.559185 Entropy: 4.453660 
[2022-10-02 18:16:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.001367	 E_lr_2:0.001367
[2022-10-02 18:16:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 18:16:30 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.005974	Loss2: 0.016067	 Dis: 1.876528 Entropy: 5.352932 
[2022-10-02 18:16:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.001297	 E_lr_2:0.001297
[2022-10-02 18:16:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000013	 C_lr_2:0.000013	 C_lr_3:0.000013	 C_lr_4:0.000013
[2022-10-02 18:16:36 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.012707	Loss2: 0.011580	 Dis: 3.504150 Entropy: 4.284567 
[2022-10-02 18:16:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.001229	 E_lr_2:0.001229
[2022-10-02 18:16:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 18:16:42 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.036385	Loss2: 0.052553	 Dis: 2.561996 Entropy: 5.360318 
[2022-10-02 18:16:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.001162	 E_lr_2:0.001162
[2022-10-02 18:16:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 18:16:49 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.010574	Loss2: 0.006667	 Dis: 1.317047 Entropy: 6.509963 
[2022-10-02 18:16:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.001096	 E_lr_2:0.001096
[2022-10-02 18:16:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000011	 C_lr_2:0.000011	 C_lr_3:0.000011	 C_lr_4:0.000011
[2022-10-02 18:16:55 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.090706	Loss2: 0.086484	 Dis: 3.228725 Entropy: 4.587916 
[2022-10-02 18:16:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.001032	 E_lr_2:0.001032
[2022-10-02 18:16:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 18:17:01 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.036435	Loss2: 0.027223	 Dis: 2.279297 Entropy: 4.606846 
[2022-10-02 18:17:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.000969	 E_lr_2:0.000969
[2022-10-02 18:17:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 18:17:07 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.053275	Loss2: 0.075109	 Dis: 3.194347 Entropy: 4.591262 
[2022-10-02 18:17:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.000908	 E_lr_2:0.000908
[2022-10-02 18:17:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000009	 C_lr_2:0.000009	 C_lr_3:0.000009	 C_lr_4:0.000009
[2022-10-02 18:17:14 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.009894	Loss2: 0.012351	 Dis: 1.370075 Entropy: 6.730397 
[2022-10-02 18:17:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.000848	 E_lr_2:0.000848
[2022-10-02 18:17:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 18:17:20 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.021730	Loss2: 0.021898	 Dis: 2.893881 Entropy: 4.493711 
[2022-10-02 18:17:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.000790	 E_lr_2:0.000790
[2022-10-02 18:17:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 18:17:26 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.015887	Loss2: 0.014917	 Dis: 2.242199 Entropy: 4.260077 
[2022-10-02 18:17:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.000734	 E_lr_2:0.000734
[2022-10-02 18:17:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 18:17:32 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.012774	Loss2: 0.016481	 Dis: 2.269928 Entropy: 4.546384 
[2022-10-02 18:17:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.000679	 E_lr_2:0.000679
[2022-10-02 18:17:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 18:17:38 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.002882	Loss2: 0.002353	 Dis: 1.346077 Entropy: 4.626657 
[2022-10-02 18:17:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.000626	 E_lr_2:0.000626
[2022-10-02 18:17:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 18:17:44 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.008652	Loss2: 0.010633	 Dis: 2.465618 Entropy: 5.946000 
[2022-10-02 18:17:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.000575	 E_lr_2:0.000575
[2022-10-02 18:17:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 18:17:50 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.008212	Loss2: 0.010856	 Dis: 1.720722 Entropy: 5.551678 
[2022-10-02 18:17:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.000526	 E_lr_2:0.000526
[2022-10-02 18:17:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 18:17:56 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.009213	Loss2: 0.006118	 Dis: 1.736359 Entropy: 4.341949 
[2022-10-02 18:17:56 demo] (houston_program2.py 512): INFO E_lr_1: 0.000479	 E_lr_2:0.000479
[2022-10-02 18:17:56 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 18:18:02 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.010765	Loss2: 0.009337	 Dis: 2.391008 Entropy: 4.464601 
[2022-10-02 18:18:02 demo] (houston_program2.py 512): INFO E_lr_1: 0.000433	 E_lr_2:0.000433
[2022-10-02 18:18:02 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 18:18:08 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.008026	Loss2: 0.007063	 Dis: 1.741695 Entropy: 5.413929 
[2022-10-02 18:18:08 demo] (houston_program2.py 512): INFO E_lr_1: 0.000390	 E_lr_2:0.000390
[2022-10-02 18:18:08 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 18:18:14 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.005642	Loss2: 0.003978	 Dis: 1.097113 Entropy: 4.787495 
[2022-10-02 18:18:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.000349	 E_lr_2:0.000349
[2022-10-02 18:18:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 18:18:19 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.013388	Loss2: 0.012372	 Dis: 2.233528 Entropy: 5.360850 
[2022-10-02 18:18:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.000310	 E_lr_2:0.000310
[2022-10-02 18:18:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 18:18:25 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.003862	Loss2: 0.004977	 Dis: 2.380630 Entropy: 5.232662 
[2022-10-02 18:18:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.000273	 E_lr_2:0.000273
[2022-10-02 18:18:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 18:18:31 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.002553	Loss2: 0.003279	 Dis: 1.437010 Entropy: 5.356582 
[2022-10-02 18:18:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.000239	 E_lr_2:0.000239
[2022-10-02 18:18:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 18:18:37 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.015089	Loss2: 0.013731	 Dis: 3.041628 Entropy: 4.324548 
[2022-10-02 18:18:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.000206	 E_lr_2:0.000206
[2022-10-02 18:18:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 18:18:43 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.005462	Loss2: 0.005229	 Dis: 1.530174 Entropy: 4.461979 
[2022-10-02 18:18:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.000176	 E_lr_2:0.000176
[2022-10-02 18:18:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 18:18:49 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.008742	Loss2: 0.011690	 Dis: 2.507578 Entropy: 5.585192 
[2022-10-02 18:18:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.000148	 E_lr_2:0.000148
[2022-10-02 18:18:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 18:18:55 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.004608	Loss2: 0.004462	 Dis: 1.389664 Entropy: 4.194841 
[2022-10-02 18:18:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.000123	 E_lr_2:0.000123
[2022-10-02 18:18:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 18:18:58 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.006392	Loss2: 0.009077	 Dis: 2.447414 Entropy: 4.127025 
[2022-10-02 18:18:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.000100	 E_lr_2:0.000100
[2022-10-02 18:18:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 18:19:04 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.018339	Loss2: 0.021376	 Dis: 2.076141 Entropy: 4.780029 
[2022-10-02 18:19:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.000079	 E_lr_2:0.000079
[2022-10-02 18:19:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 18:19:10 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.004032	Loss2: 0.004090	 Dis: 2.236000 Entropy: 4.199995 
[2022-10-02 18:19:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.000061	 E_lr_2:0.000061
[2022-10-02 18:19:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 18:19:16 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.014579	Loss2: 0.012242	 Dis: 3.101635 Entropy: 4.657518 
[2022-10-02 18:19:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.000045	 E_lr_2:0.000045
[2022-10-02 18:19:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:19:21 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.002326	Loss2: 0.002224	 Dis: 1.060122 Entropy: 6.162179 
[2022-10-02 18:19:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.000031	 E_lr_2:0.000031
[2022-10-02 18:19:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:19:27 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.001242	Loss2: 0.001649	 Dis: 2.146156 Entropy: 5.207366 
[2022-10-02 18:19:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.000020	 E_lr_2:0.000020
[2022-10-02 18:19:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:19:33 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.001662	Loss2: 0.001448	 Dis: 2.249329 Entropy: 4.546092 
[2022-10-02 18:19:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.000011	 E_lr_2:0.000011
[2022-10-02 18:19:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:19:39 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.025656	Loss2: 0.035852	 Dis: 2.612055 Entropy: 4.216323 
[2022-10-02 18:19:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.000005	 E_lr_2:0.000005
[2022-10-02 18:19:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:19:45 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.002431	Loss2: 0.003257	 Dis: 0.773048 Entropy: 6.087760 
[2022-10-02 18:19:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.000001	 E_lr_2:0.000001
[2022-10-02 18:19:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:19:51 demo] (houston_program2.py 504): INFO Train Ep: 40 	Loss1: 0.004088	Loss2: 0.005569	 Dis: 1.477621 Entropy: 4.086641 
[2022-10-02 18:19:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.000000	 E_lr_2:0.000000
[2022-10-02 18:19:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:19:51 demo] (houston_program2.py 515): INFO time_40_epoch:600.6963357925415
[2022-10-02 18:19:59 demo] (houston_program2.py 673): INFO 	val_Accuracy: 32357/53200 (60.82%)	
[2022-10-02 18:19:59 demo] (utils.py 47): INFO outputs/demo/finetune_100/ckpt_epoch_40.pth saving......
[2022-10-02 18:19:59 demo] (utils.py 49): INFO outputs/demo/finetune_100/ckpt_epoch_40.pth saved !!!
[2022-10-02 18:20:00 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0170 (0.0170)	loss 0.0121 (0.0121)	grad_norm 0.0692 (0.0692)	mem 460MB
[2022-10-02 18:20:00 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:00 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0234 (0.0234)	loss 0.0139 (0.0139)	grad_norm 0.0700 (0.0700)	mem 460MB
[2022-10-02 18:20:00 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:00 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0244 (0.0244)	loss 0.0127 (0.0127)	grad_norm 0.0716 (0.0716)	mem 460MB
[2022-10-02 18:20:00 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:00 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0230 (0.0230)	loss 0.0132 (0.0132)	grad_norm 0.0704 (0.0704)	mem 460MB
[2022-10-02 18:20:00 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:00 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0258 (0.0258)	loss 0.0114 (0.0114)	grad_norm 0.1552 (0.1552)	mem 460MB
[2022-10-02 18:20:00 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:01 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0269 (0.0269)	loss 0.0117 (0.0117)	grad_norm 0.0777 (0.0777)	mem 460MB
[2022-10-02 18:20:01 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:01 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0268 (0.0268)	loss 0.0103 (0.0103)	grad_norm 0.0727 (0.0727)	mem 460MB
[2022-10-02 18:20:01 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:01 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0225 (0.0225)	loss 0.0100 (0.0100)	grad_norm 0.0432 (0.0432)	mem 460MB
[2022-10-02 18:20:01 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:01 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0216 (0.0216)	loss 0.0094 (0.0094)	grad_norm 0.0558 (0.0558)	mem 460MB
[2022-10-02 18:20:01 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:01 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0241 (0.0241)	loss 0.0101 (0.0101)	grad_norm 0.0493 (0.0493)	mem 460MB
[2022-10-02 18:20:01 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:02 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0223 (0.0223)	loss 0.0082 (0.0082)	grad_norm 0.0414 (0.0414)	mem 460MB
[2022-10-02 18:20:02 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:02 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0155 (0.0155)	loss 0.0108 (0.0108)	grad_norm 0.0636 (0.0636)	mem 460MB
[2022-10-02 18:20:02 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:02 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0198 (0.0198)	loss 0.0101 (0.0101)	grad_norm 0.0564 (0.0564)	mem 460MB
[2022-10-02 18:20:02 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:02 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0207 (0.0207)	loss 0.0090 (0.0090)	grad_norm 0.0545 (0.0545)	mem 460MB
[2022-10-02 18:20:02 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:02 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0231 (0.0231)	loss 0.0127 (0.0127)	grad_norm 0.0565 (0.0565)	mem 460MB
[2022-10-02 18:20:02 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:03 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0232 (0.0232)	loss 0.0091 (0.0091)	grad_norm 0.0481 (0.0481)	mem 460MB
[2022-10-02 18:20:03 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:03 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0117 (0.0117)	loss 0.0113 (0.0113)	grad_norm 0.0362 (0.0362)	mem 460MB
[2022-10-02 18:20:03 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:03 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0244 (0.0244)	loss 0.0097 (0.0097)	grad_norm 0.0748 (0.0748)	mem 460MB
[2022-10-02 18:20:03 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:03 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0214 (0.0214)	loss 0.0121 (0.0121)	grad_norm 0.0664 (0.0664)	mem 460MB
[2022-10-02 18:20:03 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:03 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0242 (0.0242)	loss 0.0121 (0.0121)	grad_norm 0.0336 (0.0336)	mem 460MB
[2022-10-02 18:20:03 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:04 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0243 (0.0243)	loss 0.0086 (0.0086)	grad_norm 0.0527 (0.0527)	mem 460MB
[2022-10-02 18:20:04 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:04 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0115 (0.0115)	loss 0.0088 (0.0088)	grad_norm 0.0418 (0.0418)	mem 460MB
[2022-10-02 18:20:04 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:04 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0218 (0.0218)	loss 0.0111 (0.0111)	grad_norm 0.0651 (0.0651)	mem 460MB
[2022-10-02 18:20:04 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:04 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0104 (0.0104)	loss 0.0115 (0.0115)	grad_norm 0.0469 (0.0469)	mem 460MB
[2022-10-02 18:20:04 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:04 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0085 (0.0085)	loss 0.0090 (0.0090)	grad_norm 0.0530 (0.0530)	mem 460MB
[2022-10-02 18:20:04 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:04 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000032	time 0.0088 (0.0088)	loss 0.0105 (0.0105)	grad_norm 0.0419 (0.0419)	mem 460MB
[2022-10-02 18:20:04 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:04 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0082 (0.0082)	loss 0.0095 (0.0095)	grad_norm 0.0450 (0.0450)	mem 460MB
[2022-10-02 18:20:04 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:04 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0113 (0.0113)	loss 0.0103 (0.0103)	grad_norm 0.0647 (0.0647)	mem 460MB
[2022-10-02 18:20:04 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:05 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0228 (0.0228)	loss 0.0079 (0.0079)	grad_norm 0.0468 (0.0468)	mem 460MB
[2022-10-02 18:20:05 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:05 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0120 (0.0120)	loss 0.0138 (0.0138)	grad_norm 0.0461 (0.0461)	mem 460MB
[2022-10-02 18:20:05 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:05 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0220 (0.0220)	loss 0.0118 (0.0118)	grad_norm 0.0685 (0.0685)	mem 460MB
[2022-10-02 18:20:05 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:05 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0139 (0.0139)	loss 0.0090 (0.0090)	grad_norm 0.0341 (0.0341)	mem 460MB
[2022-10-02 18:20:05 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:05 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0206 (0.0206)	loss 0.0104 (0.0104)	grad_norm 0.0487 (0.0487)	mem 460MB
[2022-10-02 18:20:05 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:06 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0213 (0.0213)	loss 0.0105 (0.0105)	grad_norm 0.0400 (0.0400)	mem 460MB
[2022-10-02 18:20:06 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:06 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0208 (0.0208)	loss 0.0101 (0.0101)	grad_norm 0.0423 (0.0423)	mem 460MB
[2022-10-02 18:20:06 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:06 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0215 (0.0215)	loss 0.0102 (0.0102)	grad_norm 0.0475 (0.0475)	mem 460MB
[2022-10-02 18:20:06 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:06 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0202 (0.0202)	loss 0.0125 (0.0125)	grad_norm 0.0387 (0.0387)	mem 460MB
[2022-10-02 18:20:06 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:06 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0247 (0.0247)	loss 0.0098 (0.0098)	grad_norm 0.0410 (0.0410)	mem 460MB
[2022-10-02 18:20:06 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:06 demo] (houston_program2.py 243): INFO Train: [41/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0261 (0.0261)	loss 0.0106 (0.0106)	grad_norm 0.0438 (0.0438)	mem 460MB
[2022-10-02 18:20:06 demo] (houston_program2.py 252): INFO EPOCH 41 training takes 0:00:00
[2022-10-02 18:20:07 demo] (houston_program2.py 333): INFO Train Ep: 41 	Loss1: 0.081714	Loss2: 0.108180	 Dis: 2.940277 Entropy: 5.287542 
[2022-10-02 18:20:07 demo] (houston_program2.py 335): INFO time_41_epoch:7.394332408905029
[2022-10-02 18:20:07 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0146 (0.0146)	loss 0.0098 (0.0098)	grad_norm 0.0429 (0.0429)	mem 460MB
[2022-10-02 18:20:07 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:07 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0238 (0.0238)	loss 0.0117 (0.0117)	grad_norm 0.0366 (0.0366)	mem 460MB
[2022-10-02 18:20:07 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:07 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0204 (0.0204)	loss 0.0090 (0.0090)	grad_norm 0.0392 (0.0392)	mem 460MB
[2022-10-02 18:20:07 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:08 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0230 (0.0230)	loss 0.0161 (0.0161)	grad_norm 0.0436 (0.0436)	mem 460MB
[2022-10-02 18:20:08 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:08 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0226 (0.0226)	loss 0.0093 (0.0093)	grad_norm 0.0424 (0.0424)	mem 460MB
[2022-10-02 18:20:08 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:08 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0265 (0.0265)	loss 0.0096 (0.0096)	grad_norm 0.0427 (0.0427)	mem 460MB
[2022-10-02 18:20:08 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:08 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0223 (0.0223)	loss 0.0109 (0.0109)	grad_norm 0.0330 (0.0330)	mem 460MB
[2022-10-02 18:20:08 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:08 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0265 (0.0265)	loss 0.0083 (0.0083)	grad_norm 0.0379 (0.0379)	mem 460MB
[2022-10-02 18:20:08 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:09 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0220 (0.0220)	loss 0.0083 (0.0083)	grad_norm 0.0587 (0.0587)	mem 460MB
[2022-10-02 18:20:09 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:09 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0157 (0.0157)	loss 0.0132 (0.0132)	grad_norm 0.0413 (0.0413)	mem 460MB
[2022-10-02 18:20:09 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:09 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0211 (0.0211)	loss 0.0108 (0.0108)	grad_norm 0.0370 (0.0370)	mem 460MB
[2022-10-02 18:20:09 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:09 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0119 (0.0119)	loss 0.0157 (0.0157)	grad_norm 0.0713 (0.0713)	mem 460MB
[2022-10-02 18:20:09 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:09 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0221 (0.0221)	loss 0.0152 (0.0152)	grad_norm 0.0361 (0.0361)	mem 460MB
[2022-10-02 18:20:09 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:10 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0235 (0.0235)	loss 0.0088 (0.0088)	grad_norm 0.0549 (0.0549)	mem 460MB
[2022-10-02 18:20:10 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:10 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0244 (0.0244)	loss 0.0105 (0.0105)	grad_norm 0.0410 (0.0410)	mem 460MB
[2022-10-02 18:20:10 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:10 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0258 (0.0258)	loss 0.0176 (0.0176)	grad_norm 0.0437 (0.0437)	mem 460MB
[2022-10-02 18:20:10 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:10 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0133 (0.0133)	loss 0.0136 (0.0136)	grad_norm 0.0385 (0.0385)	mem 460MB
[2022-10-02 18:20:10 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:10 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0228 (0.0228)	loss 0.0084 (0.0084)	grad_norm 0.0583 (0.0583)	mem 460MB
[2022-10-02 18:20:10 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:11 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0249 (0.0249)	loss 0.0084 (0.0084)	grad_norm 0.0680 (0.0680)	mem 460MB
[2022-10-02 18:20:11 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:11 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0254 (0.0254)	loss 0.0100 (0.0100)	grad_norm 0.0350 (0.0350)	mem 460MB
[2022-10-02 18:20:11 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:11 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0234 (0.0234)	loss 0.0096 (0.0096)	grad_norm 0.0762 (0.0762)	mem 460MB
[2022-10-02 18:20:11 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:11 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0186 (0.0186)	loss 0.0085 (0.0085)	grad_norm 0.0532 (0.0532)	mem 460MB
[2022-10-02 18:20:11 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:11 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0238 (0.0238)	loss 0.0094 (0.0094)	grad_norm 0.0584 (0.0584)	mem 460MB
[2022-10-02 18:20:11 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:12 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0243 (0.0243)	loss 0.0100 (0.0100)	grad_norm 0.0674 (0.0674)	mem 460MB
[2022-10-02 18:20:12 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:12 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0109 (0.0109)	loss 0.0097 (0.0097)	grad_norm 0.0543 (0.0543)	mem 460MB
[2022-10-02 18:20:12 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:12 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0246 (0.0246)	loss 0.0087 (0.0087)	grad_norm 0.0602 (0.0602)	mem 460MB
[2022-10-02 18:20:12 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:12 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0224 (0.0224)	loss 0.0108 (0.0108)	grad_norm 0.0737 (0.0737)	mem 460MB
[2022-10-02 18:20:12 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:12 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0220 (0.0220)	loss 0.0119 (0.0119)	grad_norm 0.0623 (0.0623)	mem 460MB
[2022-10-02 18:20:12 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:13 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0201 (0.0201)	loss 0.0119 (0.0119)	grad_norm 0.0928 (0.0928)	mem 460MB
[2022-10-02 18:20:13 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:13 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0235 (0.0235)	loss 0.0108 (0.0108)	grad_norm 0.0476 (0.0476)	mem 460MB
[2022-10-02 18:20:13 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:13 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0197 (0.0197)	loss 0.0118 (0.0118)	grad_norm 0.0631 (0.0631)	mem 460MB
[2022-10-02 18:20:13 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:13 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0180 (0.0180)	loss 0.0138 (0.0138)	grad_norm 0.0532 (0.0532)	mem 460MB
[2022-10-02 18:20:13 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:13 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0199 (0.0199)	loss 0.0090 (0.0090)	grad_norm 0.0676 (0.0676)	mem 460MB
[2022-10-02 18:20:13 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:13 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0173 (0.0173)	loss 0.0090 (0.0090)	grad_norm 0.0478 (0.0478)	mem 460MB
[2022-10-02 18:20:13 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:14 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0197 (0.0197)	loss 0.0097 (0.0097)	grad_norm 0.0701 (0.0701)	mem 460MB
[2022-10-02 18:20:14 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:14 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0113 (0.0113)	loss 0.0117 (0.0117)	grad_norm 0.0725 (0.0725)	mem 460MB
[2022-10-02 18:20:14 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:14 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0208 (0.0208)	loss 0.0097 (0.0097)	grad_norm 0.0646 (0.0646)	mem 460MB
[2022-10-02 18:20:14 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:14 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000031	time 0.0107 (0.0107)	loss 0.0089 (0.0089)	grad_norm 0.0855 (0.0855)	mem 460MB
[2022-10-02 18:20:14 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:14 demo] (houston_program2.py 243): INFO Train: [42/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0211 (0.0211)	loss 0.0104 (0.0104)	grad_norm 0.0657 (0.0657)	mem 460MB
[2022-10-02 18:20:14 demo] (houston_program2.py 252): INFO EPOCH 42 training takes 0:00:00
[2022-10-02 18:20:15 demo] (houston_program2.py 333): INFO Train Ep: 42 	Loss1: 0.623439	Loss2: 0.604166	 Dis: 8.529459 Entropy: 4.572910 
[2022-10-02 18:20:15 demo] (houston_program2.py 335): INFO time_42_epoch:7.889719486236572
[2022-10-02 18:20:15 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0118 (0.0118)	loss 0.0100 (0.0100)	grad_norm 0.0796 (0.0796)	mem 460MB
[2022-10-02 18:20:15 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:15 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0235 (0.0235)	loss 0.0124 (0.0124)	grad_norm 0.0793 (0.0793)	mem 460MB
[2022-10-02 18:20:15 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:15 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0221 (0.0221)	loss 0.0141 (0.0141)	grad_norm 0.0597 (0.0597)	mem 460MB
[2022-10-02 18:20:15 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:15 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0154 (0.0154)	loss 0.0157 (0.0157)	grad_norm 0.0555 (0.0555)	mem 460MB
[2022-10-02 18:20:15 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:16 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0215 (0.0215)	loss 0.0107 (0.0107)	grad_norm 0.0757 (0.0757)	mem 460MB
[2022-10-02 18:20:16 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:16 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0133 (0.0133)	loss 0.0125 (0.0125)	grad_norm 0.0495 (0.0495)	mem 460MB
[2022-10-02 18:20:16 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:16 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0223 (0.0223)	loss 0.0115 (0.0115)	grad_norm 0.0720 (0.0720)	mem 460MB
[2022-10-02 18:20:16 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:16 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0182 (0.0182)	loss 0.0120 (0.0120)	grad_norm 0.0812 (0.0812)	mem 460MB
[2022-10-02 18:20:16 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:16 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0217 (0.0217)	loss 0.0095 (0.0095)	grad_norm 0.0543 (0.0543)	mem 460MB
[2022-10-02 18:20:16 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:17 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0215 (0.0215)	loss 0.0098 (0.0098)	grad_norm 0.0788 (0.0788)	mem 460MB
[2022-10-02 18:20:17 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:17 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0250 (0.0250)	loss 0.0134 (0.0134)	grad_norm 0.0984 (0.0984)	mem 460MB
[2022-10-02 18:20:17 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:17 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0206 (0.0206)	loss 0.0097 (0.0097)	grad_norm 0.0523 (0.0523)	mem 460MB
[2022-10-02 18:20:17 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:17 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0108 (0.0108)	loss 0.0099 (0.0099)	grad_norm 0.0832 (0.0832)	mem 460MB
[2022-10-02 18:20:17 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:17 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0208 (0.0208)	loss 0.0094 (0.0094)	grad_norm 0.0807 (0.0807)	mem 460MB
[2022-10-02 18:20:17 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:17 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0110 (0.0110)	loss 0.0147 (0.0147)	grad_norm 0.0607 (0.0607)	mem 460MB
[2022-10-02 18:20:18 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:18 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0208 (0.0208)	loss 0.0144 (0.0144)	grad_norm 0.0839 (0.0839)	mem 460MB
[2022-10-02 18:20:18 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:18 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0101 (0.0101)	loss 0.0112 (0.0112)	grad_norm 0.0786 (0.0786)	mem 460MB
[2022-10-02 18:20:18 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:18 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0214 (0.0214)	loss 0.0103 (0.0103)	grad_norm 0.0526 (0.0526)	mem 460MB
[2022-10-02 18:20:18 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:18 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0103 (0.0103)	loss 0.0101 (0.0101)	grad_norm 0.0713 (0.0713)	mem 460MB
[2022-10-02 18:20:18 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:18 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0202 (0.0202)	loss 0.0116 (0.0116)	grad_norm 0.0584 (0.0584)	mem 460MB
[2022-10-02 18:20:18 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:19 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0150 (0.0150)	loss 0.0134 (0.0134)	grad_norm 0.0624 (0.0624)	mem 460MB
[2022-10-02 18:20:19 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:19 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0183 (0.0183)	loss 0.0103 (0.0103)	grad_norm 0.0535 (0.0535)	mem 460MB
[2022-10-02 18:20:19 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:19 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0206 (0.0206)	loss 0.0102 (0.0102)	grad_norm 0.0485 (0.0485)	mem 460MB
[2022-10-02 18:20:19 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:19 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0208 (0.0208)	loss 0.0076 (0.0076)	grad_norm 0.0515 (0.0515)	mem 460MB
[2022-10-02 18:20:19 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:19 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0209 (0.0209)	loss 0.0118 (0.0118)	grad_norm 0.0456 (0.0456)	mem 460MB
[2022-10-02 18:20:19 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:19 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0216 (0.0216)	loss 0.0081 (0.0081)	grad_norm 0.0437 (0.0437)	mem 460MB
[2022-10-02 18:20:19 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:20 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0110 (0.0110)	loss 0.0122 (0.0122)	grad_norm 0.0580 (0.0580)	mem 460MB
[2022-10-02 18:20:20 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:20 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0238 (0.0238)	loss 0.0120 (0.0120)	grad_norm 0.0577 (0.0577)	mem 460MB
[2022-10-02 18:20:20 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:20 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0238 (0.0238)	loss 0.0121 (0.0121)	grad_norm 0.0423 (0.0423)	mem 460MB
[2022-10-02 18:20:20 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:20 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0267 (0.0267)	loss 0.0109 (0.0109)	grad_norm 0.0579 (0.0579)	mem 460MB
[2022-10-02 18:20:20 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:20 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0189 (0.0189)	loss 0.0115 (0.0115)	grad_norm 0.0655 (0.0655)	mem 460MB
[2022-10-02 18:20:20 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:21 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0147 (0.0147)	loss 0.0110 (0.0110)	grad_norm 0.0948 (0.0948)	mem 460MB
[2022-10-02 18:20:21 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:21 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0208 (0.0208)	loss 0.0126 (0.0126)	grad_norm 0.0629 (0.0629)	mem 460MB
[2022-10-02 18:20:21 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:21 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0120 (0.0120)	loss 0.0088 (0.0088)	grad_norm 0.0917 (0.0917)	mem 460MB
[2022-10-02 18:20:21 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:21 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0217 (0.0217)	loss 0.0108 (0.0108)	grad_norm 0.1361 (0.1361)	mem 460MB
[2022-10-02 18:20:21 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:21 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0120 (0.0120)	loss 0.0079 (0.0079)	grad_norm 0.0447 (0.0447)	mem 460MB
[2022-10-02 18:20:21 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:21 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0201 (0.0201)	loss 0.0132 (0.0132)	grad_norm 0.0450 (0.0450)	mem 460MB
[2022-10-02 18:20:21 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:22 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0103 (0.0103)	loss 0.0134 (0.0134)	grad_norm 0.0635 (0.0635)	mem 460MB
[2022-10-02 18:20:22 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:22 demo] (houston_program2.py 243): INFO Train: [43/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0208 (0.0208)	loss 0.0118 (0.0118)	grad_norm 0.0912 (0.0912)	mem 460MB
[2022-10-02 18:20:22 demo] (houston_program2.py 252): INFO EPOCH 43 training takes 0:00:00
[2022-10-02 18:20:22 demo] (houston_program2.py 333): INFO Train Ep: 43 	Loss1: 0.239086	Loss2: 0.247164	 Dis: 8.074297 Entropy: 4.916964 
[2022-10-02 18:20:22 demo] (houston_program2.py 335): INFO time_43_epoch:7.440524101257324
[2022-10-02 18:20:22 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0114 (0.0114)	loss 0.0104 (0.0104)	grad_norm 0.0420 (0.0420)	mem 460MB
[2022-10-02 18:20:22 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:23 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0245 (0.0245)	loss 0.0111 (0.0111)	grad_norm 0.0776 (0.0776)	mem 460MB
[2022-10-02 18:20:23 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:23 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0216 (0.0216)	loss 0.0111 (0.0111)	grad_norm 0.0659 (0.0659)	mem 460MB
[2022-10-02 18:20:23 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:23 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0222 (0.0222)	loss 0.0076 (0.0076)	grad_norm 0.0518 (0.0518)	mem 460MB
[2022-10-02 18:20:23 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:23 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0214 (0.0214)	loss 0.0085 (0.0085)	grad_norm 0.0576 (0.0576)	mem 460MB
[2022-10-02 18:20:23 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:23 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0124 (0.0124)	loss 0.0130 (0.0130)	grad_norm 0.0578 (0.0578)	mem 460MB
[2022-10-02 18:20:23 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:23 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0221 (0.0221)	loss 0.0082 (0.0082)	grad_norm 0.0714 (0.0714)	mem 460MB
[2022-10-02 18:20:23 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:24 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0130 (0.0130)	loss 0.0084 (0.0084)	grad_norm 0.0406 (0.0406)	mem 460MB
[2022-10-02 18:20:24 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:24 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0221 (0.0221)	loss 0.0125 (0.0125)	grad_norm 0.0936 (0.0936)	mem 460MB
[2022-10-02 18:20:24 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:24 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0162 (0.0162)	loss 0.0107 (0.0107)	grad_norm 0.0513 (0.0513)	mem 460MB
[2022-10-02 18:20:24 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:24 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000030	time 0.0215 (0.0215)	loss 0.0117 (0.0117)	grad_norm 0.0686 (0.0686)	mem 460MB
[2022-10-02 18:20:24 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:24 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0209 (0.0209)	loss 0.0098 (0.0098)	grad_norm 0.0421 (0.0421)	mem 460MB
[2022-10-02 18:20:24 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:25 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0207 (0.0207)	loss 0.0107 (0.0107)	grad_norm 0.0567 (0.0567)	mem 460MB
[2022-10-02 18:20:25 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:25 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0166 (0.0166)	loss 0.0085 (0.0085)	grad_norm 0.0380 (0.0380)	mem 460MB
[2022-10-02 18:20:25 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:25 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0220 (0.0220)	loss 0.0135 (0.0135)	grad_norm 0.0450 (0.0450)	mem 460MB
[2022-10-02 18:20:25 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:25 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0186 (0.0186)	loss 0.0101 (0.0101)	grad_norm 0.0405 (0.0405)	mem 460MB
[2022-10-02 18:20:25 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:25 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0220 (0.0220)	loss 0.0156 (0.0156)	grad_norm 0.0600 (0.0600)	mem 460MB
[2022-10-02 18:20:25 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:25 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0216 (0.0216)	loss 0.0096 (0.0096)	grad_norm 0.0501 (0.0501)	mem 460MB
[2022-10-02 18:20:25 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:26 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0114 (0.0114)	loss 0.0133 (0.0133)	grad_norm 0.0569 (0.0569)	mem 460MB
[2022-10-02 18:20:26 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:26 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0215 (0.0215)	loss 0.0136 (0.0136)	grad_norm 0.0383 (0.0383)	mem 460MB
[2022-10-02 18:20:26 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:26 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0095 (0.0095)	loss 0.0144 (0.0144)	grad_norm 0.0467 (0.0467)	mem 460MB
[2022-10-02 18:20:26 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:26 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0214 (0.0214)	loss 0.0095 (0.0095)	grad_norm 0.0468 (0.0468)	mem 460MB
[2022-10-02 18:20:26 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:26 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0117 (0.0117)	loss 0.0129 (0.0129)	grad_norm 0.0447 (0.0447)	mem 460MB
[2022-10-02 18:20:26 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:26 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0217 (0.0217)	loss 0.0103 (0.0103)	grad_norm 0.0512 (0.0512)	mem 460MB
[2022-10-02 18:20:26 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:27 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0131 (0.0131)	loss 0.0132 (0.0132)	grad_norm 0.0501 (0.0501)	mem 460MB
[2022-10-02 18:20:27 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:27 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0212 (0.0212)	loss 0.0105 (0.0105)	grad_norm 0.0512 (0.0512)	mem 460MB
[2022-10-02 18:20:27 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:27 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0132 (0.0132)	loss 0.0137 (0.0137)	grad_norm 0.0481 (0.0481)	mem 460MB
[2022-10-02 18:20:27 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:27 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0218 (0.0218)	loss 0.0116 (0.0116)	grad_norm 0.0472 (0.0472)	mem 460MB
[2022-10-02 18:20:27 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:27 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0146 (0.0146)	loss 0.0093 (0.0093)	grad_norm 0.0360 (0.0360)	mem 460MB
[2022-10-02 18:20:27 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:28 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0219 (0.0219)	loss 0.0148 (0.0148)	grad_norm 0.0593 (0.0593)	mem 460MB
[2022-10-02 18:20:28 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:28 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0204 (0.0204)	loss 0.0113 (0.0113)	grad_norm 0.0745 (0.0745)	mem 460MB
[2022-10-02 18:20:28 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:28 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0219 (0.0219)	loss 0.0126 (0.0126)	grad_norm 0.0890 (0.0890)	mem 460MB
[2022-10-02 18:20:28 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:28 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0184 (0.0184)	loss 0.0080 (0.0080)	grad_norm 0.0520 (0.0520)	mem 460MB
[2022-10-02 18:20:28 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:28 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0209 (0.0209)	loss 0.0089 (0.0089)	grad_norm 0.0636 (0.0636)	mem 460MB
[2022-10-02 18:20:28 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:28 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0195 (0.0195)	loss 0.0082 (0.0082)	grad_norm 0.0429 (0.0429)	mem 460MB
[2022-10-02 18:20:28 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:29 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0219 (0.0219)	loss 0.0086 (0.0086)	grad_norm 0.0441 (0.0441)	mem 460MB
[2022-10-02 18:20:29 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:29 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0202 (0.0202)	loss 0.0110 (0.0110)	grad_norm 0.0542 (0.0542)	mem 460MB
[2022-10-02 18:20:29 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:29 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0254 (0.0254)	loss 0.0089 (0.0089)	grad_norm 0.0543 (0.0543)	mem 460MB
[2022-10-02 18:20:29 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:29 demo] (houston_program2.py 243): INFO Train: [44/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0204 (0.0204)	loss 0.0089 (0.0089)	grad_norm 0.0660 (0.0660)	mem 460MB
[2022-10-02 18:20:29 demo] (houston_program2.py 252): INFO EPOCH 44 training takes 0:00:00
[2022-10-02 18:20:29 demo] (houston_program2.py 333): INFO Train Ep: 44 	Loss1: 0.188491	Loss2: 0.207442	 Dis: 8.032593 Entropy: 4.762045 
[2022-10-02 18:20:29 demo] (houston_program2.py 335): INFO time_44_epoch:7.331048011779785
[2022-10-02 18:20:30 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0111 (0.0111)	loss 0.0099 (0.0099)	grad_norm 0.0630 (0.0630)	mem 460MB
[2022-10-02 18:20:30 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:30 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0222 (0.0222)	loss 0.0103 (0.0103)	grad_norm 0.0568 (0.0568)	mem 460MB
[2022-10-02 18:20:30 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:30 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0229 (0.0229)	loss 0.0107 (0.0107)	grad_norm 0.0975 (0.0975)	mem 460MB
[2022-10-02 18:20:30 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:30 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0199 (0.0199)	loss 0.0086 (0.0086)	grad_norm 0.0674 (0.0674)	mem 460MB
[2022-10-02 18:20:30 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:30 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0211 (0.0211)	loss 0.0104 (0.0104)	grad_norm 0.0587 (0.0587)	mem 460MB
[2022-10-02 18:20:30 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:31 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0126 (0.0126)	loss 0.0090 (0.0090)	grad_norm 0.0530 (0.0530)	mem 460MB
[2022-10-02 18:20:31 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:31 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0223 (0.0223)	loss 0.0114 (0.0114)	grad_norm 0.0890 (0.0890)	mem 460MB
[2022-10-02 18:20:31 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:31 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0156 (0.0156)	loss 0.0122 (0.0122)	grad_norm 0.0672 (0.0672)	mem 460MB
[2022-10-02 18:20:31 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:31 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0208 (0.0208)	loss 0.0111 (0.0111)	grad_norm 0.0403 (0.0403)	mem 460MB
[2022-10-02 18:20:31 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:31 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0166 (0.0166)	loss 0.0113 (0.0113)	grad_norm 0.0822 (0.0822)	mem 460MB
[2022-10-02 18:20:31 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:32 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0220 (0.0220)	loss 0.0104 (0.0104)	grad_norm 0.0476 (0.0476)	mem 460MB
[2022-10-02 18:20:32 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:32 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0236 (0.0236)	loss 0.0090 (0.0090)	grad_norm 0.0386 (0.0386)	mem 460MB
[2022-10-02 18:20:32 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:32 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0215 (0.0215)	loss 0.0121 (0.0121)	grad_norm 0.0763 (0.0763)	mem 460MB
[2022-10-02 18:20:32 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:32 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0216 (0.0216)	loss 0.0125 (0.0125)	grad_norm 0.0644 (0.0644)	mem 460MB
[2022-10-02 18:20:32 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:32 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0236 (0.0236)	loss 0.0121 (0.0121)	grad_norm 0.0543 (0.0543)	mem 460MB
[2022-10-02 18:20:32 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:32 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0211 (0.0211)	loss 0.0137 (0.0137)	grad_norm 0.0576 (0.0576)	mem 460MB
[2022-10-02 18:20:32 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:33 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0102 (0.0102)	loss 0.0101 (0.0101)	grad_norm 0.0624 (0.0624)	mem 460MB
[2022-10-02 18:20:33 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:33 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0218 (0.0218)	loss 0.0096 (0.0096)	grad_norm 0.0577 (0.0577)	mem 460MB
[2022-10-02 18:20:33 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:33 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0113 (0.0113)	loss 0.0081 (0.0081)	grad_norm 0.0790 (0.0790)	mem 460MB
[2022-10-02 18:20:33 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:33 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0203 (0.0203)	loss 0.0148 (0.0148)	grad_norm 0.0634 (0.0634)	mem 460MB
[2022-10-02 18:20:33 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:33 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0150 (0.0150)	loss 0.0115 (0.0115)	grad_norm 0.0511 (0.0511)	mem 460MB
[2022-10-02 18:20:33 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:34 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000029	time 0.0195 (0.0195)	loss 0.0091 (0.0091)	grad_norm 0.0663 (0.0663)	mem 460MB
[2022-10-02 18:20:34 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:34 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0182 (0.0182)	loss 0.0115 (0.0115)	grad_norm 0.0603 (0.0603)	mem 460MB
[2022-10-02 18:20:34 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:34 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0207 (0.0207)	loss 0.0099 (0.0099)	grad_norm 0.0554 (0.0554)	mem 460MB
[2022-10-02 18:20:34 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:34 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0218 (0.0218)	loss 0.0101 (0.0101)	grad_norm 0.0603 (0.0603)	mem 460MB
[2022-10-02 18:20:34 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:34 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0218 (0.0218)	loss 0.0121 (0.0121)	grad_norm 0.0578 (0.0578)	mem 460MB
[2022-10-02 18:20:34 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:34 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0217 (0.0217)	loss 0.0091 (0.0091)	grad_norm 0.0473 (0.0473)	mem 460MB
[2022-10-02 18:20:34 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:35 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0255 (0.0255)	loss 0.0122 (0.0122)	grad_norm 0.0751 (0.0751)	mem 460MB
[2022-10-02 18:20:35 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:35 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0216 (0.0216)	loss 0.0106 (0.0106)	grad_norm 0.0722 (0.0722)	mem 460MB
[2022-10-02 18:20:35 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:35 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0126 (0.0126)	loss 0.0103 (0.0103)	grad_norm 0.0978 (0.0978)	mem 460MB
[2022-10-02 18:20:35 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:35 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0221 (0.0221)	loss 0.0141 (0.0141)	grad_norm 0.0788 (0.0788)	mem 460MB
[2022-10-02 18:20:35 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:35 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0196 (0.0196)	loss 0.0101 (0.0101)	grad_norm 0.0602 (0.0602)	mem 460MB
[2022-10-02 18:20:35 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:36 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0202 (0.0202)	loss 0.0103 (0.0103)	grad_norm 0.0816 (0.0816)	mem 460MB
[2022-10-02 18:20:36 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:36 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0221 (0.0221)	loss 0.0136 (0.0136)	grad_norm 0.1104 (0.1104)	mem 460MB
[2022-10-02 18:20:36 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:36 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0227 (0.0227)	loss 0.0090 (0.0090)	grad_norm 0.0565 (0.0565)	mem 460MB
[2022-10-02 18:20:36 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:36 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0266 (0.0266)	loss 0.0105 (0.0105)	grad_norm 0.0825 (0.0825)	mem 460MB
[2022-10-02 18:20:36 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:36 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0248 (0.0248)	loss 0.0082 (0.0082)	grad_norm 0.0758 (0.0758)	mem 460MB
[2022-10-02 18:20:36 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:36 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0166 (0.0166)	loss 0.0160 (0.0160)	grad_norm 0.1252 (0.1252)	mem 460MB
[2022-10-02 18:20:36 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:37 demo] (houston_program2.py 243): INFO Train: [45/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0148 (0.0148)	loss 0.0102 (0.0102)	grad_norm 0.0739 (0.0739)	mem 460MB
[2022-10-02 18:20:37 demo] (houston_program2.py 252): INFO EPOCH 45 training takes 0:00:00
[2022-10-02 18:20:37 demo] (houston_program2.py 333): INFO Train Ep: 45 	Loss1: 0.327910	Loss2: 0.382871	 Dis: 5.927252 Entropy: 4.957355 
[2022-10-02 18:20:37 demo] (houston_program2.py 335): INFO time_45_epoch:7.585711479187012
[2022-10-02 18:20:37 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 18:20:37 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 18:20:37 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 18:20:37 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 18:20:37 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 18:20:37 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 18:20:37 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 18:20:37 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 18:20:37 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 18:20:37 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 18:20:43 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.137842	Loss2: 0.153227	 Dis: 7.294474 Entropy: 4.407647 
[2022-10-02 18:20:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.000487	 E_lr_2:0.000487
[2022-10-02 18:20:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 18:20:49 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.094874	Loss2: 0.101475	 Dis: 7.771255 Entropy: 5.302489 
[2022-10-02 18:20:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.000987	 E_lr_2:0.000987
[2022-10-02 18:20:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 18:20:55 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.104416	Loss2: 0.108649	 Dis: 4.778566 Entropy: 4.786480 
[2022-10-02 18:20:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.001487	 E_lr_2:0.001487
[2022-10-02 18:20:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 18:21:01 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.247057	Loss2: 0.230761	 Dis: 5.596104 Entropy: 4.507304 
[2022-10-02 18:21:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.001987	 E_lr_2:0.001987
[2022-10-02 18:21:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 18:21:07 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.094842	Loss2: 0.079703	 Dis: 3.499168 Entropy: 5.786178 
[2022-10-02 18:21:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.002487	 E_lr_2:0.002487
[2022-10-02 18:21:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 18:21:13 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.280801	Loss2: 0.280257	 Dis: 4.320877 Entropy: 4.808878 
[2022-10-02 18:21:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.002987	 E_lr_2:0.002987
[2022-10-02 18:21:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 18:21:19 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.701372	Loss2: 0.779223	 Dis: 12.321234 Entropy: 4.404293 
[2022-10-02 18:21:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.003487	 E_lr_2:0.003487
[2022-10-02 18:21:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 18:21:25 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.416139	Loss2: 0.454549	 Dis: 9.390553 Entropy: 4.269371 
[2022-10-02 18:21:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.003987	 E_lr_2:0.003987
[2022-10-02 18:21:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 18:21:31 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.230927	Loss2: 0.228604	 Dis: 7.912214 Entropy: 4.898923 
[2022-10-02 18:21:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.004487	 E_lr_2:0.004487
[2022-10-02 18:21:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 18:21:37 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.154282	Loss2: 0.146697	 Dis: 7.753420 Entropy: 4.919233 
[2022-10-02 18:21:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.004987	 E_lr_2:0.004987
[2022-10-02 18:21:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000050	 C_lr_2:0.000050	 C_lr_3:0.000050	 C_lr_4:0.000050
[2022-10-02 18:21:43 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.337673	Loss2: 0.334510	 Dis: 6.180038 Entropy: 4.338578 
[2022-10-02 18:21:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.004853	 E_lr_2:0.004853
[2022-10-02 18:21:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000049	 C_lr_2:0.000049	 C_lr_3:0.000049	 C_lr_4:0.000049
[2022-10-02 18:21:50 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.155658	Loss2: 0.211560	 Dis: 4.962320 Entropy: 5.238266 
[2022-10-02 18:21:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.004825	 E_lr_2:0.004825
[2022-10-02 18:21:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 18:21:56 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.272061	Loss2: 0.289555	 Dis: 4.308355 Entropy: 5.694572 
[2022-10-02 18:21:56 demo] (houston_program2.py 512): INFO E_lr_1: 0.004795	 E_lr_2:0.004795
[2022-10-02 18:21:56 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 18:22:00 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.090533	Loss2: 0.104260	 Dis: 6.530893 Entropy: 5.099437 
[2022-10-02 18:22:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.004763	 E_lr_2:0.004763
[2022-10-02 18:22:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 18:22:06 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.108495	Loss2: 0.107221	 Dis: 5.929556 Entropy: 5.445239 
[2022-10-02 18:22:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.004728	 E_lr_2:0.004728
[2022-10-02 18:22:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 18:22:12 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 1.403168	Loss2: 1.325586	 Dis: 7.356113 Entropy: 4.218025 
[2022-10-02 18:22:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.004692	 E_lr_2:0.004692
[2022-10-02 18:22:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 18:22:17 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.197090	Loss2: 0.196396	 Dis: 8.334070 Entropy: 4.470276 
[2022-10-02 18:22:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.004653	 E_lr_2:0.004653
[2022-10-02 18:22:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 18:22:24 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.345273	Loss2: 0.359212	 Dis: 4.663021 Entropy: 4.371943 
[2022-10-02 18:22:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.004612	 E_lr_2:0.004612
[2022-10-02 18:22:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 18:22:30 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.205382	Loss2: 0.214365	 Dis: 6.465080 Entropy: 5.469695 
[2022-10-02 18:22:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.004569	 E_lr_2:0.004569
[2022-10-02 18:22:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 18:22:36 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.214415	Loss2: 0.251664	 Dis: 6.521927 Entropy: 4.883051 
[2022-10-02 18:22:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.004524	 E_lr_2:0.004524
[2022-10-02 18:22:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 18:22:42 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.129333	Loss2: 0.113402	 Dis: 5.652676 Entropy: 5.254605 
[2022-10-02 18:22:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.004477	 E_lr_2:0.004477
[2022-10-02 18:22:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 18:22:48 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.296273	Loss2: 0.249457	 Dis: 5.010399 Entropy: 4.528392 
[2022-10-02 18:22:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.004428	 E_lr_2:0.004428
[2022-10-02 18:22:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 18:22:54 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.256918	Loss2: 0.301868	 Dis: 6.329357 Entropy: 5.797629 
[2022-10-02 18:22:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.004377	 E_lr_2:0.004377
[2022-10-02 18:22:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 18:23:00 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.184418	Loss2: 0.155975	 Dis: 6.878086 Entropy: 4.452482 
[2022-10-02 18:23:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.004324	 E_lr_2:0.004324
[2022-10-02 18:23:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 18:23:06 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.116315	Loss2: 0.150727	 Dis: 4.729979 Entropy: 4.772376 
[2022-10-02 18:23:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.004269	 E_lr_2:0.004269
[2022-10-02 18:23:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 18:23:12 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.084734	Loss2: 0.077627	 Dis: 4.663744 Entropy: 5.387221 
[2022-10-02 18:23:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.004213	 E_lr_2:0.004213
[2022-10-02 18:23:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 18:23:18 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.092578	Loss2: 0.092315	 Dis: 4.435907 Entropy: 4.956882 
[2022-10-02 18:23:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.004155	 E_lr_2:0.004155
[2022-10-02 18:23:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 18:23:24 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.064031	Loss2: 0.065215	 Dis: 4.283495 Entropy: 4.433289 
[2022-10-02 18:23:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.004095	 E_lr_2:0.004095
[2022-10-02 18:23:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000041	 C_lr_2:0.000041	 C_lr_3:0.000041	 C_lr_4:0.000041
[2022-10-02 18:23:30 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.058029	Loss2: 0.059941	 Dis: 3.511572 Entropy: 4.636999 
[2022-10-02 18:23:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.004034	 E_lr_2:0.004034
[2022-10-02 18:23:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 18:23:36 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.119975	Loss2: 0.103611	 Dis: 4.001059 Entropy: 4.394396 
[2022-10-02 18:23:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.003971	 E_lr_2:0.003971
[2022-10-02 18:23:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 18:23:42 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.072754	Loss2: 0.056610	 Dis: 5.315462 Entropy: 4.158863 
[2022-10-02 18:23:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.003907	 E_lr_2:0.003907
[2022-10-02 18:23:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000039	 C_lr_2:0.000039	 C_lr_3:0.000039	 C_lr_4:0.000039
[2022-10-02 18:23:48 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.232649	Loss2: 0.261435	 Dis: 3.013741 Entropy: 4.575321 
[2022-10-02 18:23:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.003841	 E_lr_2:0.003841
[2022-10-02 18:23:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 18:23:54 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.172487	Loss2: 0.181495	 Dis: 6.779459 Entropy: 4.657874 
[2022-10-02 18:23:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.003774	 E_lr_2:0.003774
[2022-10-02 18:23:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 18:24:00 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.145859	Loss2: 0.130746	 Dis: 3.488270 Entropy: 5.635563 
[2022-10-02 18:24:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.003706	 E_lr_2:0.003706
[2022-10-02 18:24:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000037	 C_lr_2:0.000037	 C_lr_3:0.000037	 C_lr_4:0.000037
[2022-10-02 18:24:06 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.036792	Loss2: 0.034453	 Dis: 3.901020 Entropy: 5.042360 
[2022-10-02 18:24:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.003637	 E_lr_2:0.003637
[2022-10-02 18:24:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 18:24:12 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.331810	Loss2: 0.352465	 Dis: 8.876123 Entropy: 5.147116 
[2022-10-02 18:24:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.003566	 E_lr_2:0.003566
[2022-10-02 18:24:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 18:24:18 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.239477	Loss2: 0.198870	 Dis: 7.532059 Entropy: 5.103870 
[2022-10-02 18:24:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.003495	 E_lr_2:0.003495
[2022-10-02 18:24:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 18:24:24 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.120916	Loss2: 0.141138	 Dis: 5.202784 Entropy: 5.133138 
[2022-10-02 18:24:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.003422	 E_lr_2:0.003422
[2022-10-02 18:24:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000034	 C_lr_2:0.000034	 C_lr_3:0.000034	 C_lr_4:0.000034
[2022-10-02 18:24:29 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.089045	Loss2: 0.103096	 Dis: 4.280451 Entropy: 4.452466 
[2022-10-02 18:24:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.003349	 E_lr_2:0.003349
[2022-10-02 18:24:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 18:24:35 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.165487	Loss2: 0.194291	 Dis: 5.154392 Entropy: 5.362840 
[2022-10-02 18:24:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.003274	 E_lr_2:0.003274
[2022-10-02 18:24:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 18:24:42 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.145299	Loss2: 0.144385	 Dis: 4.991877 Entropy: 4.444450 
[2022-10-02 18:24:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.003199	 E_lr_2:0.003199
[2022-10-02 18:24:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000032	 C_lr_2:0.000032	 C_lr_3:0.000032	 C_lr_4:0.000032
[2022-10-02 18:24:47 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.116212	Loss2: 0.112850	 Dis: 3.451843 Entropy: 4.784435 
[2022-10-02 18:24:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.003124	 E_lr_2:0.003124
[2022-10-02 18:24:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000031	 C_lr_2:0.000031	 C_lr_3:0.000031	 C_lr_4:0.000031
[2022-10-02 18:24:53 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.273389	Loss2: 0.276222	 Dis: 8.025719 Entropy: 4.465512 
[2022-10-02 18:24:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.003047	 E_lr_2:0.003047
[2022-10-02 18:24:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 18:24:59 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.165095	Loss2: 0.169093	 Dis: 5.192732 Entropy: 4.565730 
[2022-10-02 18:24:59 demo] (houston_program2.py 512): INFO E_lr_1: 0.002970	 E_lr_2:0.002970
[2022-10-02 18:24:59 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 18:25:06 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.131334	Loss2: 0.176024	 Dis: 5.392216 Entropy: 4.477214 
[2022-10-02 18:25:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.002893	 E_lr_2:0.002893
[2022-10-02 18:25:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000029	 C_lr_2:0.000029	 C_lr_3:0.000029	 C_lr_4:0.000029
[2022-10-02 18:25:12 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.085825	Loss2: 0.088168	 Dis: 4.144653 Entropy: 4.704011 
[2022-10-02 18:25:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.002815	 E_lr_2:0.002815
[2022-10-02 18:25:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000028	 C_lr_2:0.000028	 C_lr_3:0.000028	 C_lr_4:0.000028
[2022-10-02 18:25:18 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.049093	Loss2: 0.055086	 Dis: 4.942959 Entropy: 4.592607 
[2022-10-02 18:25:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.002737	 E_lr_2:0.002737
[2022-10-02 18:25:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 18:25:24 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.146435	Loss2: 0.177093	 Dis: 3.266136 Entropy: 6.279974 
[2022-10-02 18:25:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.002659	 E_lr_2:0.002659
[2022-10-02 18:25:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 18:25:30 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.072317	Loss2: 0.054749	 Dis: 3.607687 Entropy: 4.727986 
[2022-10-02 18:25:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.002581	 E_lr_2:0.002581
[2022-10-02 18:25:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000026	 C_lr_2:0.000026	 C_lr_3:0.000026	 C_lr_4:0.000026
[2022-10-02 18:25:36 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.060331	Loss2: 0.050521	 Dis: 3.441166 Entropy: 4.607114 
[2022-10-02 18:25:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.002502	 E_lr_2:0.002502
[2022-10-02 18:25:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 18:25:41 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.194596	Loss2: 0.217619	 Dis: 3.806845 Entropy: 5.791975 
[2022-10-02 18:25:41 demo] (houston_program2.py 512): INFO E_lr_1: 0.002423	 E_lr_2:0.002423
[2022-10-02 18:25:41 demo] (houston_program2.py 513): INFO C_lr_1: 0.000024	 C_lr_2:0.000024	 C_lr_3:0.000024	 C_lr_4:0.000024
[2022-10-02 18:25:47 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.040404	Loss2: 0.048500	 Dis: 1.989855 Entropy: 5.372738 
[2022-10-02 18:25:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.002345	 E_lr_2:0.002345
[2022-10-02 18:25:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 18:25:52 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.095614	Loss2: 0.072465	 Dis: 1.882286 Entropy: 4.942111 
[2022-10-02 18:25:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.002267	 E_lr_2:0.002267
[2022-10-02 18:25:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 18:25:58 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.082048	Loss2: 0.035645	 Dis: 3.622709 Entropy: 5.010937 
[2022-10-02 18:25:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.002189	 E_lr_2:0.002189
[2022-10-02 18:25:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000022	 C_lr_2:0.000022	 C_lr_3:0.000022	 C_lr_4:0.000022
[2022-10-02 18:26:03 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.028463	Loss2: 0.029575	 Dis: 2.771593 Entropy: 5.678195 
[2022-10-02 18:26:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.002111	 E_lr_2:0.002111
[2022-10-02 18:26:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000021	 C_lr_2:0.000021	 C_lr_3:0.000021	 C_lr_4:0.000021
[2022-10-02 18:26:09 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.073995	Loss2: 0.043259	 Dis: 3.080397 Entropy: 5.639708 
[2022-10-02 18:26:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.002034	 E_lr_2:0.002034
[2022-10-02 18:26:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 18:26:15 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.036346	Loss2: 0.040346	 Dis: 2.571823 Entropy: 5.137318 
[2022-10-02 18:26:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.001957	 E_lr_2:0.001957
[2022-10-02 18:26:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 18:26:21 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.081231	Loss2: 0.066847	 Dis: 2.617916 Entropy: 4.616615 
[2022-10-02 18:26:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.001880	 E_lr_2:0.001880
[2022-10-02 18:26:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000019	 C_lr_2:0.000019	 C_lr_3:0.000019	 C_lr_4:0.000019
[2022-10-02 18:26:27 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.064893	Loss2: 0.053812	 Dis: 3.156841 Entropy: 6.449187 
[2022-10-02 18:26:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.001804	 E_lr_2:0.001804
[2022-10-02 18:26:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000018	 C_lr_2:0.000018	 C_lr_3:0.000018	 C_lr_4:0.000018
[2022-10-02 18:26:33 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.026818	Loss2: 0.028004	 Dis: 4.505878 Entropy: 4.556561 
[2022-10-02 18:26:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.001729	 E_lr_2:0.001729
[2022-10-02 18:26:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 18:26:39 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.072113	Loss2: 0.056905	 Dis: 4.123077 Entropy: 4.840347 
[2022-10-02 18:26:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.001655	 E_lr_2:0.001655
[2022-10-02 18:26:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 18:26:45 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.040032	Loss2: 0.040260	 Dis: 1.842342 Entropy: 5.246347 
[2022-10-02 18:26:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.001582	 E_lr_2:0.001582
[2022-10-02 18:26:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000016	 C_lr_2:0.000016	 C_lr_3:0.000016	 C_lr_4:0.000016
[2022-10-02 18:26:51 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.029113	Loss2: 0.031610	 Dis: 2.122683 Entropy: 4.728276 
[2022-10-02 18:26:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.001509	 E_lr_2:0.001509
[2022-10-02 18:26:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 18:26:57 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.074583	Loss2: 0.053850	 Dis: 1.886219 Entropy: 6.223439 
[2022-10-02 18:26:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.001437	 E_lr_2:0.001437
[2022-10-02 18:26:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 18:27:03 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.071649	Loss2: 0.033410	 Dis: 1.664402 Entropy: 5.626727 
[2022-10-02 18:27:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.001367	 E_lr_2:0.001367
[2022-10-02 18:27:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 18:27:09 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.015241	Loss2: 0.015681	 Dis: 1.772554 Entropy: 5.883213 
[2022-10-02 18:27:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.001297	 E_lr_2:0.001297
[2022-10-02 18:27:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000013	 C_lr_2:0.000013	 C_lr_3:0.000013	 C_lr_4:0.000013
[2022-10-02 18:27:16 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.131016	Loss2: 0.140370	 Dis: 3.650566 Entropy: 4.447543 
[2022-10-02 18:27:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.001229	 E_lr_2:0.001229
[2022-10-02 18:27:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 18:27:21 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.008801	Loss2: 0.006316	 Dis: 1.894285 Entropy: 5.339830 
[2022-10-02 18:27:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.001162	 E_lr_2:0.001162
[2022-10-02 18:27:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 18:27:27 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.019924	Loss2: 0.022533	 Dis: 1.700064 Entropy: 4.570070 
[2022-10-02 18:27:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.001096	 E_lr_2:0.001096
[2022-10-02 18:27:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000011	 C_lr_2:0.000011	 C_lr_3:0.000011	 C_lr_4:0.000011
[2022-10-02 18:27:33 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.005910	Loss2: 0.005974	 Dis: 2.031601 Entropy: 6.014075 
[2022-10-02 18:27:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.001032	 E_lr_2:0.001032
[2022-10-02 18:27:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 18:27:39 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.084952	Loss2: 0.073864	 Dis: 2.315868 Entropy: 4.699823 
[2022-10-02 18:27:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.000969	 E_lr_2:0.000969
[2022-10-02 18:27:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 18:27:45 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.034491	Loss2: 0.027272	 Dis: 2.444511 Entropy: 4.310700 
[2022-10-02 18:27:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.000908	 E_lr_2:0.000908
[2022-10-02 18:27:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000009	 C_lr_2:0.000009	 C_lr_3:0.000009	 C_lr_4:0.000009
[2022-10-02 18:27:51 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.006486	Loss2: 0.005278	 Dis: 2.493116 Entropy: 5.061440 
[2022-10-02 18:27:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.000848	 E_lr_2:0.000848
[2022-10-02 18:27:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 18:27:58 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.011075	Loss2: 0.011224	 Dis: 3.997862 Entropy: 4.730006 
[2022-10-02 18:27:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.000790	 E_lr_2:0.000790
[2022-10-02 18:27:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 18:28:04 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.013285	Loss2: 0.014214	 Dis: 1.664536 Entropy: 5.105994 
[2022-10-02 18:28:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.000734	 E_lr_2:0.000734
[2022-10-02 18:28:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 18:28:10 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.007780	Loss2: 0.010646	 Dis: 0.623268 Entropy: 6.209750 
[2022-10-02 18:28:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.000679	 E_lr_2:0.000679
[2022-10-02 18:28:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 18:28:16 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.025522	Loss2: 0.023619	 Dis: 1.275793 Entropy: 6.946887 
[2022-10-02 18:28:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.000626	 E_lr_2:0.000626
[2022-10-02 18:28:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 18:28:21 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.004604	Loss2: 0.005947	 Dis: 2.079479 Entropy: 4.343602 
[2022-10-02 18:28:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.000575	 E_lr_2:0.000575
[2022-10-02 18:28:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 18:28:27 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.014390	Loss2: 0.016755	 Dis: 1.960890 Entropy: 5.619040 
[2022-10-02 18:28:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.000526	 E_lr_2:0.000526
[2022-10-02 18:28:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 18:28:33 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.020726	Loss2: 0.030512	 Dis: 3.721693 Entropy: 4.037811 
[2022-10-02 18:28:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.000479	 E_lr_2:0.000479
[2022-10-02 18:28:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 18:28:39 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.013303	Loss2: 0.018522	 Dis: 3.449051 Entropy: 4.875107 
[2022-10-02 18:28:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.000433	 E_lr_2:0.000433
[2022-10-02 18:28:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 18:28:45 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.015977	Loss2: 0.020687	 Dis: 1.514185 Entropy: 5.606112 
[2022-10-02 18:28:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.000390	 E_lr_2:0.000390
[2022-10-02 18:28:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 18:28:52 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.014027	Loss2: 0.010858	 Dis: 3.187418 Entropy: 5.576669 
[2022-10-02 18:28:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.000349	 E_lr_2:0.000349
[2022-10-02 18:28:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 18:28:58 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.002621	Loss2: 0.002602	 Dis: 1.973358 Entropy: 4.225087 
[2022-10-02 18:28:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.000310	 E_lr_2:0.000310
[2022-10-02 18:28:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 18:29:03 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.005855	Loss2: 0.006448	 Dis: 2.654964 Entropy: 4.765251 
[2022-10-02 18:29:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.000273	 E_lr_2:0.000273
[2022-10-02 18:29:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 18:29:09 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.008362	Loss2: 0.005749	 Dis: 1.861458 Entropy: 5.408557 
[2022-10-02 18:29:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.000239	 E_lr_2:0.000239
[2022-10-02 18:29:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 18:29:15 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.006340	Loss2: 0.008157	 Dis: 2.249533 Entropy: 5.305833 
[2022-10-02 18:29:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.000206	 E_lr_2:0.000206
[2022-10-02 18:29:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 18:29:21 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.010109	Loss2: 0.006675	 Dis: 1.907990 Entropy: 5.444809 
[2022-10-02 18:29:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.000176	 E_lr_2:0.000176
[2022-10-02 18:29:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 18:29:27 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.046556	Loss2: 0.033923	 Dis: 1.546520 Entropy: 4.166552 
[2022-10-02 18:29:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.000148	 E_lr_2:0.000148
[2022-10-02 18:29:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 18:29:33 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.020764	Loss2: 0.018168	 Dis: 1.314392 Entropy: 4.437716 
[2022-10-02 18:29:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.000123	 E_lr_2:0.000123
[2022-10-02 18:29:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 18:29:39 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.003945	Loss2: 0.004202	 Dis: 1.630144 Entropy: 5.603306 
[2022-10-02 18:29:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.000100	 E_lr_2:0.000100
[2022-10-02 18:29:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 18:29:45 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.014439	Loss2: 0.018011	 Dis: 1.473995 Entropy: 4.488982 
[2022-10-02 18:29:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.000079	 E_lr_2:0.000079
[2022-10-02 18:29:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 18:29:51 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.003023	Loss2: 0.003129	 Dis: 3.312155 Entropy: 5.513215 
[2022-10-02 18:29:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.000061	 E_lr_2:0.000061
[2022-10-02 18:29:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 18:29:57 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.002467	Loss2: 0.002964	 Dis: 2.461353 Entropy: 4.538185 
[2022-10-02 18:29:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.000045	 E_lr_2:0.000045
[2022-10-02 18:29:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:30:03 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.009977	Loss2: 0.013347	 Dis: 2.578392 Entropy: 4.350099 
[2022-10-02 18:30:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.000031	 E_lr_2:0.000031
[2022-10-02 18:30:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:30:09 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.001853	Loss2: 0.001815	 Dis: 1.433809 Entropy: 4.659508 
[2022-10-02 18:30:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.000020	 E_lr_2:0.000020
[2022-10-02 18:30:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:30:16 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.002892	Loss2: 0.003471	 Dis: 1.647169 Entropy: 4.683782 
[2022-10-02 18:30:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.000011	 E_lr_2:0.000011
[2022-10-02 18:30:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:30:22 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.006757	Loss2: 0.006697	 Dis: 2.099064 Entropy: 5.050752 
[2022-10-02 18:30:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.000005	 E_lr_2:0.000005
[2022-10-02 18:30:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:30:28 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.003610	Loss2: 0.003501	 Dis: 2.379221 Entropy: 6.614346 
[2022-10-02 18:30:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.000001	 E_lr_2:0.000001
[2022-10-02 18:30:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:30:34 demo] (houston_program2.py 504): INFO Train Ep: 45 	Loss1: 0.049454	Loss2: 0.050899	 Dis: 2.490593 Entropy: 4.604143 
[2022-10-02 18:30:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.000000	 E_lr_2:0.000000
[2022-10-02 18:30:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:30:34 demo] (houston_program2.py 515): INFO time_45_epoch:597.4580702781677
[2022-10-02 18:30:43 demo] (houston_program2.py 673): INFO 	val_Accuracy: 33130/53200 (62.27%)	
[2022-10-02 18:30:43 demo] (utils.py 47): INFO outputs/demo/finetune_100/ckpt_epoch_45.pth saving......
[2022-10-02 18:30:43 demo] (utils.py 49): INFO outputs/demo/finetune_100/ckpt_epoch_45.pth saved !!!
[2022-10-02 18:30:43 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0164 (0.0164)	loss 0.0097 (0.0097)	grad_norm 0.1062 (0.1062)	mem 460MB
[2022-10-02 18:30:43 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:43 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0214 (0.0214)	loss 0.0095 (0.0095)	grad_norm 0.0615 (0.0615)	mem 460MB
[2022-10-02 18:30:43 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:44 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0233 (0.0233)	loss 0.0084 (0.0084)	grad_norm 0.0514 (0.0514)	mem 460MB
[2022-10-02 18:30:44 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:44 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0204 (0.0204)	loss 0.0103 (0.0103)	grad_norm 0.0719 (0.0719)	mem 460MB
[2022-10-02 18:30:44 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:44 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0098 (0.0098)	loss 0.0105 (0.0105)	grad_norm 0.0498 (0.0498)	mem 460MB
[2022-10-02 18:30:44 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:44 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0227 (0.0227)	loss 0.0082 (0.0082)	grad_norm 0.0656 (0.0656)	mem 460MB
[2022-10-02 18:30:44 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:44 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0134 (0.0134)	loss 0.0096 (0.0096)	grad_norm 0.0692 (0.0692)	mem 460MB
[2022-10-02 18:30:44 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:44 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0191 (0.0191)	loss 0.0087 (0.0087)	grad_norm 0.0441 (0.0441)	mem 460MB
[2022-10-02 18:30:44 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:45 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0128 (0.0128)	loss 0.0082 (0.0082)	grad_norm 0.0582 (0.0582)	mem 460MB
[2022-10-02 18:30:45 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:45 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0212 (0.0212)	loss 0.0091 (0.0091)	grad_norm 0.0763 (0.0763)	mem 460MB
[2022-10-02 18:30:45 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:45 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0115 (0.0115)	loss 0.0116 (0.0116)	grad_norm 0.0431 (0.0431)	mem 460MB
[2022-10-02 18:30:45 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:45 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0212 (0.0212)	loss 0.0111 (0.0111)	grad_norm 0.0408 (0.0408)	mem 460MB
[2022-10-02 18:30:45 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:45 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0135 (0.0135)	loss 0.0103 (0.0103)	grad_norm 0.0643 (0.0643)	mem 460MB
[2022-10-02 18:30:45 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:45 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0216 (0.0216)	loss 0.0103 (0.0103)	grad_norm 0.0536 (0.0536)	mem 460MB
[2022-10-02 18:30:45 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:46 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0209 (0.0209)	loss 0.0106 (0.0106)	grad_norm 0.0517 (0.0517)	mem 460MB
[2022-10-02 18:30:46 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:46 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0210 (0.0210)	loss 0.0089 (0.0089)	grad_norm 0.0682 (0.0682)	mem 460MB
[2022-10-02 18:30:46 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:46 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0211 (0.0211)	loss 0.0113 (0.0113)	grad_norm 0.0746 (0.0746)	mem 460MB
[2022-10-02 18:30:46 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:46 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0204 (0.0204)	loss 0.0077 (0.0077)	grad_norm 0.0379 (0.0379)	mem 460MB
[2022-10-02 18:30:46 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:46 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0205 (0.0205)	loss 0.0097 (0.0097)	grad_norm 0.0597 (0.0597)	mem 460MB
[2022-10-02 18:30:46 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:47 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0248 (0.0248)	loss 0.0095 (0.0095)	grad_norm 0.0842 (0.0842)	mem 460MB
[2022-10-02 18:30:47 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:47 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0203 (0.0203)	loss 0.0101 (0.0101)	grad_norm 0.1028 (0.1028)	mem 460MB
[2022-10-02 18:30:47 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:47 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0105 (0.0105)	loss 0.0107 (0.0107)	grad_norm 0.0777 (0.0777)	mem 460MB
[2022-10-02 18:30:47 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:47 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0204 (0.0204)	loss 0.0080 (0.0080)	grad_norm 0.0458 (0.0458)	mem 460MB
[2022-10-02 18:30:47 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:47 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0136 (0.0136)	loss 0.0126 (0.0126)	grad_norm 0.0746 (0.0746)	mem 460MB
[2022-10-02 18:30:47 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:47 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0205 (0.0205)	loss 0.0099 (0.0099)	grad_norm 0.0401 (0.0401)	mem 460MB
[2022-10-02 18:30:47 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:48 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0202 (0.0202)	loss 0.0122 (0.0122)	grad_norm 0.0590 (0.0590)	mem 460MB
[2022-10-02 18:30:48 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:48 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0218 (0.0218)	loss 0.0121 (0.0121)	grad_norm 0.0499 (0.0499)	mem 460MB
[2022-10-02 18:30:48 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:48 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0195 (0.0195)	loss 0.0099 (0.0099)	grad_norm 0.0479 (0.0479)	mem 460MB
[2022-10-02 18:30:48 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:48 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0201 (0.0201)	loss 0.0154 (0.0154)	grad_norm 0.0691 (0.0691)	mem 460MB
[2022-10-02 18:30:48 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:48 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0198 (0.0198)	loss 0.0100 (0.0100)	grad_norm 0.0560 (0.0560)	mem 460MB
[2022-10-02 18:30:48 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:48 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0217 (0.0217)	loss 0.0099 (0.0099)	grad_norm 0.0438 (0.0438)	mem 460MB
[2022-10-02 18:30:49 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:49 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0211 (0.0211)	loss 0.0088 (0.0088)	grad_norm 0.0505 (0.0505)	mem 460MB
[2022-10-02 18:30:49 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:49 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000028	time 0.0217 (0.0217)	loss 0.0158 (0.0158)	grad_norm 0.1068 (0.1068)	mem 460MB
[2022-10-02 18:30:49 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:49 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0210 (0.0210)	loss 0.0114 (0.0114)	grad_norm 0.0569 (0.0569)	mem 460MB
[2022-10-02 18:30:49 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:49 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0270 (0.0270)	loss 0.0108 (0.0108)	grad_norm 0.0569 (0.0569)	mem 460MB
[2022-10-02 18:30:49 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:49 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0267 (0.0267)	loss 0.0082 (0.0082)	grad_norm 0.0381 (0.0381)	mem 460MB
[2022-10-02 18:30:49 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:50 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0157 (0.0157)	loss 0.0164 (0.0164)	grad_norm 0.0546 (0.0546)	mem 460MB
[2022-10-02 18:30:50 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:50 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0266 (0.0266)	loss 0.0110 (0.0110)	grad_norm 0.0540 (0.0540)	mem 460MB
[2022-10-02 18:30:50 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:50 demo] (houston_program2.py 243): INFO Train: [46/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0228 (0.0228)	loss 0.0087 (0.0087)	grad_norm 0.0563 (0.0563)	mem 460MB
[2022-10-02 18:30:50 demo] (houston_program2.py 252): INFO EPOCH 46 training takes 0:00:00
[2022-10-02 18:30:50 demo] (houston_program2.py 333): INFO Train Ep: 46 	Loss1: 0.325362	Loss2: 0.325840	 Dis: 5.377453 Entropy: 4.467455 
[2022-10-02 18:30:50 demo] (houston_program2.py 335): INFO time_46_epoch:7.423437118530273
[2022-10-02 18:30:51 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0116 (0.0116)	loss 0.0094 (0.0094)	grad_norm 0.0565 (0.0565)	mem 460MB
[2022-10-02 18:30:51 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:51 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0221 (0.0221)	loss 0.0096 (0.0096)	grad_norm 0.0467 (0.0467)	mem 460MB
[2022-10-02 18:30:51 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:51 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0228 (0.0228)	loss 0.0125 (0.0125)	grad_norm 0.0669 (0.0669)	mem 460MB
[2022-10-02 18:30:51 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:51 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0194 (0.0194)	loss 0.0092 (0.0092)	grad_norm 0.0355 (0.0355)	mem 460MB
[2022-10-02 18:30:51 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:51 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0218 (0.0218)	loss 0.0098 (0.0098)	grad_norm 0.0422 (0.0422)	mem 460MB
[2022-10-02 18:30:51 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:52 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0215 (0.0215)	loss 0.0106 (0.0106)	grad_norm 0.0464 (0.0464)	mem 460MB
[2022-10-02 18:30:52 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:52 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0216 (0.0216)	loss 0.0088 (0.0088)	grad_norm 0.0475 (0.0475)	mem 460MB
[2022-10-02 18:30:52 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:52 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0150 (0.0150)	loss 0.0099 (0.0099)	grad_norm 0.0398 (0.0398)	mem 460MB
[2022-10-02 18:30:52 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:52 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0211 (0.0211)	loss 0.0083 (0.0083)	grad_norm 0.0532 (0.0532)	mem 460MB
[2022-10-02 18:30:52 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:52 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0123 (0.0123)	loss 0.0106 (0.0106)	grad_norm 0.0645 (0.0645)	mem 460MB
[2022-10-02 18:30:52 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:52 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0188 (0.0188)	loss 0.0076 (0.0076)	grad_norm 0.0420 (0.0420)	mem 460MB
[2022-10-02 18:30:52 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:53 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0156 (0.0156)	loss 0.0101 (0.0101)	grad_norm 0.0381 (0.0381)	mem 460MB
[2022-10-02 18:30:53 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:53 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0191 (0.0191)	loss 0.0097 (0.0097)	grad_norm 0.0980 (0.0980)	mem 460MB
[2022-10-02 18:30:53 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:53 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0222 (0.0222)	loss 0.0100 (0.0100)	grad_norm 0.0707 (0.0707)	mem 460MB
[2022-10-02 18:30:53 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:53 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0199 (0.0199)	loss 0.0103 (0.0103)	grad_norm 0.0455 (0.0455)	mem 460MB
[2022-10-02 18:30:53 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:53 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0212 (0.0212)	loss 0.0088 (0.0088)	grad_norm 0.0450 (0.0450)	mem 460MB
[2022-10-02 18:30:53 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:54 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0217 (0.0217)	loss 0.0119 (0.0119)	grad_norm 0.0496 (0.0496)	mem 460MB
[2022-10-02 18:30:54 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:54 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0196 (0.0196)	loss 0.0108 (0.0108)	grad_norm 0.0367 (0.0367)	mem 460MB
[2022-10-02 18:30:54 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:54 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0279 (0.0279)	loss 0.0105 (0.0105)	grad_norm 0.0406 (0.0406)	mem 460MB
[2022-10-02 18:30:54 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:54 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0209 (0.0209)	loss 0.0106 (0.0106)	grad_norm 0.0511 (0.0511)	mem 460MB
[2022-10-02 18:30:54 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:54 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0138 (0.0138)	loss 0.0099 (0.0099)	grad_norm 0.0549 (0.0549)	mem 460MB
[2022-10-02 18:30:54 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:54 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0208 (0.0208)	loss 0.0113 (0.0113)	grad_norm 0.1413 (0.1413)	mem 460MB
[2022-10-02 18:30:54 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:55 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0209 (0.0209)	loss 0.0097 (0.0097)	grad_norm 0.0469 (0.0469)	mem 460MB
[2022-10-02 18:30:55 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:55 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0210 (0.0210)	loss 0.0121 (0.0121)	grad_norm 0.0733 (0.0733)	mem 460MB
[2022-10-02 18:30:55 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:55 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0209 (0.0209)	loss 0.0117 (0.0117)	grad_norm 0.0521 (0.0521)	mem 460MB
[2022-10-02 18:30:55 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:55 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0204 (0.0204)	loss 0.0083 (0.0083)	grad_norm 0.0533 (0.0533)	mem 460MB
[2022-10-02 18:30:55 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:55 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0150 (0.0150)	loss 0.0091 (0.0091)	grad_norm 0.0599 (0.0599)	mem 460MB
[2022-10-02 18:30:55 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:56 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0196 (0.0196)	loss 0.0078 (0.0078)	grad_norm 0.0360 (0.0360)	mem 460MB
[2022-10-02 18:30:56 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:56 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0205 (0.0205)	loss 0.0118 (0.0118)	grad_norm 0.0722 (0.0722)	mem 460MB
[2022-10-02 18:30:56 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:56 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0186 (0.0186)	loss 0.0085 (0.0085)	grad_norm 0.0504 (0.0504)	mem 460MB
[2022-10-02 18:30:56 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:56 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0213 (0.0213)	loss 0.0111 (0.0111)	grad_norm 0.0459 (0.0459)	mem 460MB
[2022-10-02 18:30:56 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:56 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0201 (0.0201)	loss 0.0083 (0.0083)	grad_norm 0.0546 (0.0546)	mem 460MB
[2022-10-02 18:30:56 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:56 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0189 (0.0189)	loss 0.0083 (0.0083)	grad_norm 0.0404 (0.0404)	mem 460MB
[2022-10-02 18:30:56 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:57 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0257 (0.0257)	loss 0.0108 (0.0108)	grad_norm 0.0528 (0.0528)	mem 460MB
[2022-10-02 18:30:57 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:57 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0205 (0.0205)	loss 0.0165 (0.0165)	grad_norm 0.0616 (0.0616)	mem 460MB
[2022-10-02 18:30:57 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:57 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0178 (0.0178)	loss 0.0117 (0.0117)	grad_norm 0.0530 (0.0530)	mem 460MB
[2022-10-02 18:30:57 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:57 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0220 (0.0220)	loss 0.0120 (0.0120)	grad_norm 0.0632 (0.0632)	mem 460MB
[2022-10-02 18:30:57 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:57 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0134 (0.0134)	loss 0.0112 (0.0112)	grad_norm 0.0610 (0.0610)	mem 460MB
[2022-10-02 18:30:57 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:58 demo] (houston_program2.py 243): INFO Train: [47/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0205 (0.0205)	loss 0.0114 (0.0114)	grad_norm 0.0424 (0.0424)	mem 460MB
[2022-10-02 18:30:58 demo] (houston_program2.py 252): INFO EPOCH 47 training takes 0:00:00
[2022-10-02 18:30:58 demo] (houston_program2.py 333): INFO Train Ep: 47 	Loss1: 0.161114	Loss2: 0.171389	 Dis: 6.266674 Entropy: 4.953684 
[2022-10-02 18:30:58 demo] (houston_program2.py 335): INFO time_47_epoch:7.525424003601074
[2022-10-02 18:30:58 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0114 (0.0114)	loss 0.0098 (0.0098)	grad_norm 0.0504 (0.0504)	mem 460MB
[2022-10-02 18:30:58 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:30:58 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0221 (0.0221)	loss 0.0101 (0.0101)	grad_norm 0.0652 (0.0652)	mem 460MB
[2022-10-02 18:30:58 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:30:58 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0199 (0.0199)	loss 0.0087 (0.0087)	grad_norm 0.0336 (0.0336)	mem 460MB
[2022-10-02 18:30:59 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:30:59 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0158 (0.0158)	loss 0.0068 (0.0068)	grad_norm 0.0525 (0.0525)	mem 460MB
[2022-10-02 18:30:59 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:30:59 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000027	time 0.0220 (0.0220)	loss 0.0113 (0.0113)	grad_norm 0.0728 (0.0728)	mem 460MB
[2022-10-02 18:30:59 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:30:59 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0217 (0.0217)	loss 0.0114 (0.0114)	grad_norm 0.0474 (0.0474)	mem 460MB
[2022-10-02 18:30:59 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:30:59 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0217 (0.0217)	loss 0.0114 (0.0114)	grad_norm 0.0624 (0.0624)	mem 460MB
[2022-10-02 18:30:59 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:30:59 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0193 (0.0193)	loss 0.0091 (0.0091)	grad_norm 0.0446 (0.0446)	mem 460MB
[2022-10-02 18:30:59 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:00 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0216 (0.0216)	loss 0.0096 (0.0096)	grad_norm 0.0510 (0.0510)	mem 460MB
[2022-10-02 18:31:00 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:00 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0171 (0.0171)	loss 0.0091 (0.0091)	grad_norm 0.0679 (0.0679)	mem 460MB
[2022-10-02 18:31:00 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:00 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0180 (0.0180)	loss 0.0076 (0.0076)	grad_norm 0.0540 (0.0540)	mem 460MB
[2022-10-02 18:31:00 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:00 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0209 (0.0209)	loss 0.0109 (0.0109)	grad_norm 0.0546 (0.0546)	mem 460MB
[2022-10-02 18:31:00 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:00 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0270 (0.0270)	loss 0.0111 (0.0111)	grad_norm 0.0654 (0.0654)	mem 460MB
[2022-10-02 18:31:00 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:01 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0218 (0.0218)	loss 0.0102 (0.0102)	grad_norm 0.0404 (0.0404)	mem 460MB
[2022-10-02 18:31:01 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:01 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0110 (0.0110)	loss 0.0125 (0.0125)	grad_norm 0.0414 (0.0414)	mem 460MB
[2022-10-02 18:31:01 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:01 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0202 (0.0202)	loss 0.0087 (0.0087)	grad_norm 0.0640 (0.0640)	mem 460MB
[2022-10-02 18:31:01 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:01 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0124 (0.0124)	loss 0.0099 (0.0099)	grad_norm 0.0671 (0.0671)	mem 460MB
[2022-10-02 18:31:01 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:01 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0222 (0.0222)	loss 0.0151 (0.0151)	grad_norm 0.0555 (0.0555)	mem 460MB
[2022-10-02 18:31:01 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:01 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0151 (0.0151)	loss 0.0095 (0.0095)	grad_norm 0.0647 (0.0647)	mem 460MB
[2022-10-02 18:31:01 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:02 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0218 (0.0218)	loss 0.0087 (0.0087)	grad_norm 0.0705 (0.0705)	mem 460MB
[2022-10-02 18:31:02 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:02 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0262 (0.0262)	loss 0.0107 (0.0107)	grad_norm 0.0485 (0.0485)	mem 460MB
[2022-10-02 18:31:02 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:02 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0213 (0.0213)	loss 0.0081 (0.0081)	grad_norm 0.0517 (0.0517)	mem 460MB
[2022-10-02 18:31:02 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:02 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0219 (0.0219)	loss 0.0087 (0.0087)	grad_norm 0.0520 (0.0520)	mem 460MB
[2022-10-02 18:31:02 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:02 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0206 (0.0206)	loss 0.0081 (0.0081)	grad_norm 0.0326 (0.0326)	mem 460MB
[2022-10-02 18:31:02 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:02 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0197 (0.0197)	loss 0.0098 (0.0098)	grad_norm 0.0678 (0.0678)	mem 460MB
[2022-10-02 18:31:03 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:03 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0263 (0.0263)	loss 0.0098 (0.0098)	grad_norm 0.0493 (0.0493)	mem 460MB
[2022-10-02 18:31:03 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:03 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0217 (0.0217)	loss 0.0087 (0.0087)	grad_norm 0.0588 (0.0588)	mem 460MB
[2022-10-02 18:31:03 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:03 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0194 (0.0194)	loss 0.0090 (0.0090)	grad_norm 0.0779 (0.0779)	mem 460MB
[2022-10-02 18:31:03 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:03 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0205 (0.0205)	loss 0.0117 (0.0117)	grad_norm 0.0430 (0.0430)	mem 460MB
[2022-10-02 18:31:03 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:03 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0106 (0.0106)	loss 0.0070 (0.0070)	grad_norm 0.0383 (0.0383)	mem 460MB
[2022-10-02 18:31:03 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:04 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0223 (0.0223)	loss 0.0100 (0.0100)	grad_norm 0.0335 (0.0335)	mem 460MB
[2022-10-02 18:31:04 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:04 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0132 (0.0132)	loss 0.0113 (0.0113)	grad_norm 0.0474 (0.0474)	mem 460MB
[2022-10-02 18:31:04 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:04 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0221 (0.0221)	loss 0.0139 (0.0139)	grad_norm 0.0554 (0.0554)	mem 460MB
[2022-10-02 18:31:04 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:04 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0129 (0.0129)	loss 0.0147 (0.0147)	grad_norm 0.0911 (0.0911)	mem 460MB
[2022-10-02 18:31:04 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:04 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0223 (0.0223)	loss 0.0096 (0.0096)	grad_norm 0.0481 (0.0481)	mem 460MB
[2022-10-02 18:31:04 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:04 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0138 (0.0138)	loss 0.0099 (0.0099)	grad_norm 0.0486 (0.0486)	mem 460MB
[2022-10-02 18:31:04 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:05 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0215 (0.0215)	loss 0.0100 (0.0100)	grad_norm 0.0571 (0.0571)	mem 460MB
[2022-10-02 18:31:05 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:05 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0203 (0.0203)	loss 0.0087 (0.0087)	grad_norm 0.0279 (0.0279)	mem 460MB
[2022-10-02 18:31:05 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:05 demo] (houston_program2.py 243): INFO Train: [48/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0204 (0.0204)	loss 0.0128 (0.0128)	grad_norm 0.0513 (0.0513)	mem 460MB
[2022-10-02 18:31:05 demo] (houston_program2.py 252): INFO EPOCH 48 training takes 0:00:00
[2022-10-02 18:31:05 demo] (houston_program2.py 333): INFO Train Ep: 48 	Loss1: 0.117622	Loss2: 0.109046	 Dis: 6.699486 Entropy: 4.664420 
[2022-10-02 18:31:05 demo] (houston_program2.py 335): INFO time_48_epoch:7.3925652503967285
[2022-10-02 18:31:06 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0115 (0.0115)	loss 0.0094 (0.0094)	grad_norm 0.0448 (0.0448)	mem 460MB
[2022-10-02 18:31:06 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:06 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0237 (0.0237)	loss 0.0113 (0.0113)	grad_norm 0.0498 (0.0498)	mem 460MB
[2022-10-02 18:31:06 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:06 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0240 (0.0240)	loss 0.0098 (0.0098)	grad_norm 0.0509 (0.0509)	mem 460MB
[2022-10-02 18:31:06 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:06 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0214 (0.0214)	loss 0.0085 (0.0085)	grad_norm 0.0406 (0.0406)	mem 460MB
[2022-10-02 18:31:06 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:06 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0251 (0.0251)	loss 0.0123 (0.0123)	grad_norm 0.0921 (0.0921)	mem 460MB
[2022-10-02 18:31:06 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:07 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0240 (0.0240)	loss 0.0108 (0.0108)	grad_norm 0.0361 (0.0361)	mem 460MB
[2022-10-02 18:31:07 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:07 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0215 (0.0215)	loss 0.0085 (0.0085)	grad_norm 0.0466 (0.0466)	mem 460MB
[2022-10-02 18:31:07 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:07 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0243 (0.0243)	loss 0.0090 (0.0090)	grad_norm 0.0490 (0.0490)	mem 460MB
[2022-10-02 18:31:07 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:07 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0244 (0.0244)	loss 0.0100 (0.0100)	grad_norm 0.0565 (0.0565)	mem 460MB
[2022-10-02 18:31:07 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:07 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0175 (0.0175)	loss 0.0087 (0.0087)	grad_norm 0.0480 (0.0480)	mem 460MB
[2022-10-02 18:31:07 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:08 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0185 (0.0185)	loss 0.0084 (0.0084)	grad_norm 0.0473 (0.0473)	mem 460MB
[2022-10-02 18:31:08 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:08 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0210 (0.0210)	loss 0.0106 (0.0106)	grad_norm 0.0460 (0.0460)	mem 460MB
[2022-10-02 18:31:08 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:08 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0204 (0.0204)	loss 0.0104 (0.0104)	grad_norm 0.0474 (0.0474)	mem 460MB
[2022-10-02 18:31:08 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:08 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0168 (0.0168)	loss 0.0090 (0.0090)	grad_norm 0.0351 (0.0351)	mem 460MB
[2022-10-02 18:31:08 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:08 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0208 (0.0208)	loss 0.0114 (0.0114)	grad_norm 0.0510 (0.0510)	mem 460MB
[2022-10-02 18:31:08 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:09 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000026	time 0.0217 (0.0217)	loss 0.0119 (0.0119)	grad_norm 0.0444 (0.0444)	mem 460MB
[2022-10-02 18:31:09 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:09 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0205 (0.0205)	loss 0.0100 (0.0100)	grad_norm 0.0375 (0.0375)	mem 460MB
[2022-10-02 18:31:09 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:09 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0220 (0.0220)	loss 0.0089 (0.0089)	grad_norm 0.0513 (0.0513)	mem 460MB
[2022-10-02 18:31:09 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:09 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0106 (0.0106)	loss 0.0088 (0.0088)	grad_norm 0.0628 (0.0628)	mem 460MB
[2022-10-02 18:31:09 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:09 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0218 (0.0218)	loss 0.0093 (0.0093)	grad_norm 0.0308 (0.0308)	mem 460MB
[2022-10-02 18:31:09 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:09 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0099 (0.0099)	loss 0.0092 (0.0092)	grad_norm 0.0446 (0.0446)	mem 460MB
[2022-10-02 18:31:09 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:10 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0207 (0.0207)	loss 0.0099 (0.0099)	grad_norm 0.0477 (0.0477)	mem 460MB
[2022-10-02 18:31:10 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:10 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0176 (0.0176)	loss 0.0099 (0.0099)	grad_norm 0.0579 (0.0579)	mem 460MB
[2022-10-02 18:31:10 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:10 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0218 (0.0218)	loss 0.0095 (0.0095)	grad_norm 0.0551 (0.0551)	mem 460MB
[2022-10-02 18:31:10 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:10 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0210 (0.0210)	loss 0.0111 (0.0111)	grad_norm 0.0567 (0.0567)	mem 460MB
[2022-10-02 18:31:10 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:10 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0205 (0.0205)	loss 0.0080 (0.0080)	grad_norm 0.0380 (0.0380)	mem 460MB
[2022-10-02 18:31:10 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:11 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0219 (0.0219)	loss 0.0080 (0.0080)	grad_norm 0.0509 (0.0509)	mem 460MB
[2022-10-02 18:31:11 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:11 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0226 (0.0226)	loss 0.0113 (0.0113)	grad_norm 0.0579 (0.0579)	mem 460MB
[2022-10-02 18:31:11 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:11 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0255 (0.0255)	loss 0.0088 (0.0088)	grad_norm 0.0573 (0.0573)	mem 460MB
[2022-10-02 18:31:11 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:11 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0131 (0.0131)	loss 0.0073 (0.0073)	grad_norm 0.0446 (0.0446)	mem 460MB
[2022-10-02 18:31:11 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:11 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0240 (0.0240)	loss 0.0104 (0.0104)	grad_norm 0.0668 (0.0668)	mem 460MB
[2022-10-02 18:31:11 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:11 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0176 (0.0176)	loss 0.0090 (0.0090)	grad_norm 0.0778 (0.0778)	mem 460MB
[2022-10-02 18:31:11 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:12 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0203 (0.0203)	loss 0.0130 (0.0130)	grad_norm 0.0693 (0.0693)	mem 460MB
[2022-10-02 18:31:12 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:12 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0208 (0.0208)	loss 0.0110 (0.0110)	grad_norm 0.0662 (0.0662)	mem 460MB
[2022-10-02 18:31:12 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:12 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0190 (0.0190)	loss 0.0095 (0.0095)	grad_norm 0.0713 (0.0713)	mem 460MB
[2022-10-02 18:31:12 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:12 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0176 (0.0176)	loss 0.0127 (0.0127)	grad_norm 0.0822 (0.0822)	mem 460MB
[2022-10-02 18:31:12 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:12 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0228 (0.0228)	loss 0.0104 (0.0104)	grad_norm 0.0414 (0.0414)	mem 460MB
[2022-10-02 18:31:12 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:13 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0215 (0.0215)	loss 0.0094 (0.0094)	grad_norm 0.0387 (0.0387)	mem 460MB
[2022-10-02 18:31:13 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:13 demo] (houston_program2.py 243): INFO Train: [49/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0210 (0.0210)	loss 0.0094 (0.0094)	grad_norm 0.0558 (0.0558)	mem 460MB
[2022-10-02 18:31:13 demo] (houston_program2.py 252): INFO EPOCH 49 training takes 0:00:00
[2022-10-02 18:31:13 demo] (houston_program2.py 333): INFO Train Ep: 49 	Loss1: 0.300687	Loss2: 0.296516	 Dis: 4.702959 Entropy: 5.882633 
[2022-10-02 18:31:13 demo] (houston_program2.py 335): INFO time_49_epoch:7.736151456832886
[2022-10-02 18:31:13 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0116 (0.0116)	loss 0.0094 (0.0094)	grad_norm 0.0406 (0.0406)	mem 460MB
[2022-10-02 18:31:13 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:13 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0159 (0.0159)	loss 0.0077 (0.0077)	grad_norm 0.0608 (0.0608)	mem 460MB
[2022-10-02 18:31:13 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:14 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0214 (0.0214)	loss 0.0077 (0.0077)	grad_norm 0.0633 (0.0633)	mem 460MB
[2022-10-02 18:31:14 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:14 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0194 (0.0194)	loss 0.0136 (0.0136)	grad_norm 0.0903 (0.0903)	mem 460MB
[2022-10-02 18:31:14 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:14 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0209 (0.0209)	loss 0.0103 (0.0103)	grad_norm 0.0735 (0.0735)	mem 460MB
[2022-10-02 18:31:14 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:14 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0202 (0.0202)	loss 0.0093 (0.0093)	grad_norm 0.0442 (0.0442)	mem 460MB
[2022-10-02 18:31:14 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:14 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0210 (0.0210)	loss 0.0093 (0.0093)	grad_norm 0.0708 (0.0708)	mem 460MB
[2022-10-02 18:31:14 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:15 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0210 (0.0210)	loss 0.0086 (0.0086)	grad_norm 0.0838 (0.0838)	mem 460MB
[2022-10-02 18:31:15 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:15 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0241 (0.0241)	loss 0.0084 (0.0084)	grad_norm 0.0584 (0.0584)	mem 460MB
[2022-10-02 18:31:15 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:15 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0217 (0.0217)	loss 0.0094 (0.0094)	grad_norm 0.0489 (0.0489)	mem 460MB
[2022-10-02 18:31:15 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:15 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0100 (0.0100)	loss 0.0113 (0.0113)	grad_norm 0.0934 (0.0934)	mem 460MB
[2022-10-02 18:31:15 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:15 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0199 (0.0199)	loss 0.0108 (0.0108)	grad_norm 0.0796 (0.0796)	mem 460MB
[2022-10-02 18:31:15 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:15 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0127 (0.0127)	loss 0.0110 (0.0110)	grad_norm 0.0491 (0.0491)	mem 460MB
[2022-10-02 18:31:15 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:16 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0209 (0.0209)	loss 0.0115 (0.0115)	grad_norm 0.0680 (0.0680)	mem 460MB
[2022-10-02 18:31:16 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:16 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0205 (0.0205)	loss 0.0119 (0.0119)	grad_norm 0.0948 (0.0948)	mem 460MB
[2022-10-02 18:31:16 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:16 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0201 (0.0201)	loss 0.0091 (0.0091)	grad_norm 0.0440 (0.0440)	mem 460MB
[2022-10-02 18:31:16 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:16 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0204 (0.0204)	loss 0.0116 (0.0116)	grad_norm 0.0580 (0.0580)	mem 460MB
[2022-10-02 18:31:16 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:16 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0210 (0.0210)	loss 0.0103 (0.0103)	grad_norm 0.0870 (0.0870)	mem 460MB
[2022-10-02 18:31:16 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:17 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0152 (0.0152)	loss 0.0134 (0.0134)	grad_norm 0.0584 (0.0584)	mem 460MB
[2022-10-02 18:31:17 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:17 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0210 (0.0210)	loss 0.0110 (0.0110)	grad_norm 0.0716 (0.0716)	mem 460MB
[2022-10-02 18:31:17 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:17 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0152 (0.0152)	loss 0.0108 (0.0108)	grad_norm 0.0705 (0.0705)	mem 460MB
[2022-10-02 18:31:17 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:17 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0209 (0.0209)	loss 0.0093 (0.0093)	grad_norm 0.0649 (0.0649)	mem 460MB
[2022-10-02 18:31:17 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:17 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0204 (0.0204)	loss 0.0115 (0.0115)	grad_norm 0.0395 (0.0395)	mem 460MB
[2022-10-02 18:31:17 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:17 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0218 (0.0218)	loss 0.0092 (0.0092)	grad_norm 0.0764 (0.0764)	mem 460MB
[2022-10-02 18:31:17 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:18 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0235 (0.0235)	loss 0.0086 (0.0086)	grad_norm 0.0761 (0.0761)	mem 460MB
[2022-10-02 18:31:18 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:18 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000025	time 0.0157 (0.0157)	loss 0.0096 (0.0096)	grad_norm 0.0324 (0.0324)	mem 460MB
[2022-10-02 18:31:18 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:18 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0251 (0.0251)	loss 0.0122 (0.0122)	grad_norm 0.0826 (0.0826)	mem 460MB
[2022-10-02 18:31:18 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:18 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0192 (0.0192)	loss 0.0080 (0.0080)	grad_norm 0.0587 (0.0587)	mem 460MB
[2022-10-02 18:31:18 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:19 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0257 (0.0257)	loss 0.0115 (0.0115)	grad_norm 0.0976 (0.0976)	mem 460MB
[2022-10-02 18:31:19 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:19 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0246 (0.0246)	loss 0.0101 (0.0101)	grad_norm 0.0582 (0.0582)	mem 460MB
[2022-10-02 18:31:19 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:19 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0152 (0.0152)	loss 0.0108 (0.0108)	grad_norm 0.0733 (0.0733)	mem 460MB
[2022-10-02 18:31:19 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:19 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0233 (0.0233)	loss 0.0096 (0.0096)	grad_norm 0.0627 (0.0627)	mem 460MB
[2022-10-02 18:31:19 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:19 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0164 (0.0164)	loss 0.0105 (0.0105)	grad_norm 0.0763 (0.0763)	mem 460MB
[2022-10-02 18:31:19 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:20 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0246 (0.0246)	loss 0.0115 (0.0115)	grad_norm 0.0792 (0.0792)	mem 460MB
[2022-10-02 18:31:20 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:20 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0246 (0.0246)	loss 0.0101 (0.0101)	grad_norm 0.0568 (0.0568)	mem 460MB
[2022-10-02 18:31:20 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:20 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0160 (0.0160)	loss 0.0106 (0.0106)	grad_norm 0.0704 (0.0704)	mem 460MB
[2022-10-02 18:31:20 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:20 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0204 (0.0204)	loss 0.0086 (0.0086)	grad_norm 0.0542 (0.0542)	mem 460MB
[2022-10-02 18:31:20 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:20 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0187 (0.0187)	loss 0.0084 (0.0084)	grad_norm 0.0433 (0.0433)	mem 460MB
[2022-10-02 18:31:20 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:20 demo] (houston_program2.py 243): INFO Train: [50/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0222 (0.0222)	loss 0.0102 (0.0102)	grad_norm 0.0889 (0.0889)	mem 460MB
[2022-10-02 18:31:20 demo] (houston_program2.py 252): INFO EPOCH 50 training takes 0:00:00
[2022-10-02 18:31:21 demo] (houston_program2.py 333): INFO Train Ep: 50 	Loss1: 0.019257	Loss2: 0.021332	 Dis: 5.172981 Entropy: 4.620416 
[2022-10-02 18:31:21 demo] (houston_program2.py 335): INFO time_50_epoch:7.769167423248291
[2022-10-02 18:31:21 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 18:31:21 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 18:31:21 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 18:31:21 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 18:31:21 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 18:31:21 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 18:31:21 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 18:31:21 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 18:31:21 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 18:31:21 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 18:31:27 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.086332	Loss2: 0.071149	 Dis: 3.934565 Entropy: 5.389261 
[2022-10-02 18:31:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.000487	 E_lr_2:0.000487
[2022-10-02 18:31:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 18:31:33 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.048977	Loss2: 0.039707	 Dis: 4.617130 Entropy: 5.319952 
[2022-10-02 18:31:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.000987	 E_lr_2:0.000987
[2022-10-02 18:31:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 18:31:38 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.098919	Loss2: 0.083016	 Dis: 4.347610 Entropy: 5.244411 
[2022-10-02 18:31:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.001487	 E_lr_2:0.001487
[2022-10-02 18:31:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 18:31:44 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.038188	Loss2: 0.037986	 Dis: 4.053743 Entropy: 4.418562 
[2022-10-02 18:31:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.001987	 E_lr_2:0.001987
[2022-10-02 18:31:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 18:31:50 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.083472	Loss2: 0.080277	 Dis: 5.448576 Entropy: 6.364509 
[2022-10-02 18:31:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.002487	 E_lr_2:0.002487
[2022-10-02 18:31:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 18:31:57 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.127424	Loss2: 0.117727	 Dis: 3.893559 Entropy: 4.379211 
[2022-10-02 18:31:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.002987	 E_lr_2:0.002987
[2022-10-02 18:31:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 18:32:00 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.070535	Loss2: 0.067986	 Dis: 4.944126 Entropy: 6.528233 
[2022-10-02 18:32:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.003487	 E_lr_2:0.003487
[2022-10-02 18:32:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 18:32:07 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.093891	Loss2: 0.093661	 Dis: 4.345354 Entropy: 6.485196 
[2022-10-02 18:32:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.003987	 E_lr_2:0.003987
[2022-10-02 18:32:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 18:32:13 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.470590	Loss2: 0.439074	 Dis: 5.336224 Entropy: 4.958785 
[2022-10-02 18:32:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.004487	 E_lr_2:0.004487
[2022-10-02 18:32:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 18:32:19 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.252007	Loss2: 0.274492	 Dis: 5.662558 Entropy: 4.775286 
[2022-10-02 18:32:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.004987	 E_lr_2:0.004987
[2022-10-02 18:32:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000050	 C_lr_2:0.000050	 C_lr_3:0.000050	 C_lr_4:0.000050
[2022-10-02 18:32:25 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.255807	Loss2: 0.281748	 Dis: 5.563616 Entropy: 6.238636 
[2022-10-02 18:32:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.004853	 E_lr_2:0.004853
[2022-10-02 18:32:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000049	 C_lr_2:0.000049	 C_lr_3:0.000049	 C_lr_4:0.000049
[2022-10-02 18:32:31 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.074888	Loss2: 0.081714	 Dis: 4.792610 Entropy: 5.430493 
[2022-10-02 18:32:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.004825	 E_lr_2:0.004825
[2022-10-02 18:32:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 18:32:37 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.108833	Loss2: 0.086508	 Dis: 4.736835 Entropy: 5.771915 
[2022-10-02 18:32:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.004795	 E_lr_2:0.004795
[2022-10-02 18:32:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 18:32:43 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.101598	Loss2: 0.095115	 Dis: 4.099033 Entropy: 5.619831 
[2022-10-02 18:32:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.004763	 E_lr_2:0.004763
[2022-10-02 18:32:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 18:32:49 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.069458	Loss2: 0.076613	 Dis: 4.844986 Entropy: 5.393391 
[2022-10-02 18:32:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.004728	 E_lr_2:0.004728
[2022-10-02 18:32:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 18:32:55 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.140099	Loss2: 0.101925	 Dis: 3.920103 Entropy: 4.852080 
[2022-10-02 18:32:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.004692	 E_lr_2:0.004692
[2022-10-02 18:32:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 18:33:01 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.204321	Loss2: 0.209506	 Dis: 5.584061 Entropy: 4.931118 
[2022-10-02 18:33:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.004653	 E_lr_2:0.004653
[2022-10-02 18:33:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 18:33:07 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.093146	Loss2: 0.104887	 Dis: 5.291342 Entropy: 4.440208 
[2022-10-02 18:33:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.004612	 E_lr_2:0.004612
[2022-10-02 18:33:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 18:33:13 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.195460	Loss2: 0.170452	 Dis: 3.059845 Entropy: 4.481453 
[2022-10-02 18:33:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.004569	 E_lr_2:0.004569
[2022-10-02 18:33:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 18:33:19 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.234145	Loss2: 0.260365	 Dis: 4.776600 Entropy: 4.609941 
[2022-10-02 18:33:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.004524	 E_lr_2:0.004524
[2022-10-02 18:33:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 18:33:25 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.114415	Loss2: 0.097972	 Dis: 5.591105 Entropy: 5.752996 
[2022-10-02 18:33:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.004477	 E_lr_2:0.004477
[2022-10-02 18:33:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 18:33:31 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.148717	Loss2: 0.174453	 Dis: 4.782005 Entropy: 4.374454 
[2022-10-02 18:33:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.004428	 E_lr_2:0.004428
[2022-10-02 18:33:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 18:33:38 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.073636	Loss2: 0.066576	 Dis: 3.362125 Entropy: 5.786768 
[2022-10-02 18:33:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.004377	 E_lr_2:0.004377
[2022-10-02 18:33:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 18:33:44 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.285644	Loss2: 0.278881	 Dis: 6.634510 Entropy: 4.859422 
[2022-10-02 18:33:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.004324	 E_lr_2:0.004324
[2022-10-02 18:33:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 18:33:50 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.036588	Loss2: 0.027012	 Dis: 4.055273 Entropy: 5.195188 
[2022-10-02 18:33:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.004269	 E_lr_2:0.004269
[2022-10-02 18:33:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 18:33:56 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.020507	Loss2: 0.021944	 Dis: 4.705566 Entropy: 4.891651 
[2022-10-02 18:33:56 demo] (houston_program2.py 512): INFO E_lr_1: 0.004213	 E_lr_2:0.004213
[2022-10-02 18:33:56 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 18:34:02 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.213487	Loss2: 0.160582	 Dis: 2.448360 Entropy: 5.470411 
[2022-10-02 18:34:02 demo] (houston_program2.py 512): INFO E_lr_1: 0.004155	 E_lr_2:0.004155
[2022-10-02 18:34:02 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 18:34:08 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.139557	Loss2: 0.153445	 Dis: 4.214504 Entropy: 5.172684 
[2022-10-02 18:34:08 demo] (houston_program2.py 512): INFO E_lr_1: 0.004095	 E_lr_2:0.004095
[2022-10-02 18:34:08 demo] (houston_program2.py 513): INFO C_lr_1: 0.000041	 C_lr_2:0.000041	 C_lr_3:0.000041	 C_lr_4:0.000041
[2022-10-02 18:34:14 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.038984	Loss2: 0.045947	 Dis: 6.754799 Entropy: 5.069281 
[2022-10-02 18:34:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.004034	 E_lr_2:0.004034
[2022-10-02 18:34:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 18:34:18 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 2.747874	Loss2: 2.590610	 Dis: 10.946953 Entropy: 4.446482 
[2022-10-02 18:34:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.003971	 E_lr_2:0.003971
[2022-10-02 18:34:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 18:34:24 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.200131	Loss2: 0.186481	 Dis: 8.305368 Entropy: 4.586552 
[2022-10-02 18:34:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.003907	 E_lr_2:0.003907
[2022-10-02 18:34:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000039	 C_lr_2:0.000039	 C_lr_3:0.000039	 C_lr_4:0.000039
[2022-10-02 18:34:30 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.072312	Loss2: 0.078717	 Dis: 7.274557 Entropy: 4.847550 
[2022-10-02 18:34:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.003841	 E_lr_2:0.003841
[2022-10-02 18:34:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 18:34:36 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.271807	Loss2: 0.284984	 Dis: 4.248215 Entropy: 4.727931 
[2022-10-02 18:34:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.003774	 E_lr_2:0.003774
[2022-10-02 18:34:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 18:34:42 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.237254	Loss2: 0.221681	 Dis: 5.681633 Entropy: 4.889512 
[2022-10-02 18:34:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.003706	 E_lr_2:0.003706
[2022-10-02 18:34:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000037	 C_lr_2:0.000037	 C_lr_3:0.000037	 C_lr_4:0.000037
[2022-10-02 18:34:49 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.274434	Loss2: 0.260251	 Dis: 5.714729 Entropy: 4.529799 
[2022-10-02 18:34:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.003637	 E_lr_2:0.003637
[2022-10-02 18:34:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 18:34:55 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.083132	Loss2: 0.092019	 Dis: 5.939125 Entropy: 4.820890 
[2022-10-02 18:34:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.003566	 E_lr_2:0.003566
[2022-10-02 18:34:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 18:35:01 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.037406	Loss2: 0.048462	 Dis: 6.161680 Entropy: 4.373638 
[2022-10-02 18:35:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.003495	 E_lr_2:0.003495
[2022-10-02 18:35:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 18:35:07 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.602352	Loss2: 0.692713	 Dis: 6.888670 Entropy: 4.398737 
[2022-10-02 18:35:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.003422	 E_lr_2:0.003422
[2022-10-02 18:35:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000034	 C_lr_2:0.000034	 C_lr_3:0.000034	 C_lr_4:0.000034
[2022-10-02 18:35:13 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.232774	Loss2: 0.220334	 Dis: 5.652733 Entropy: 5.135046 
[2022-10-02 18:35:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.003349	 E_lr_2:0.003349
[2022-10-02 18:35:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 18:35:19 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.096330	Loss2: 0.081266	 Dis: 4.603954 Entropy: 4.874755 
[2022-10-02 18:35:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.003274	 E_lr_2:0.003274
[2022-10-02 18:35:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 18:35:25 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.154283	Loss2: 0.148306	 Dis: 5.403795 Entropy: 4.814384 
[2022-10-02 18:35:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.003199	 E_lr_2:0.003199
[2022-10-02 18:35:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000032	 C_lr_2:0.000032	 C_lr_3:0.000032	 C_lr_4:0.000032
[2022-10-02 18:35:31 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.120777	Loss2: 0.114938	 Dis: 5.700027 Entropy: 4.541978 
[2022-10-02 18:35:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.003124	 E_lr_2:0.003124
[2022-10-02 18:35:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000031	 C_lr_2:0.000031	 C_lr_3:0.000031	 C_lr_4:0.000031
[2022-10-02 18:35:37 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.105148	Loss2: 0.097113	 Dis: 3.744776 Entropy: 5.672190 
[2022-10-02 18:35:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.003047	 E_lr_2:0.003047
[2022-10-02 18:35:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 18:35:43 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.222459	Loss2: 0.254439	 Dis: 5.382782 Entropy: 4.440139 
[2022-10-02 18:35:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.002970	 E_lr_2:0.002970
[2022-10-02 18:35:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 18:35:49 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.181398	Loss2: 0.183930	 Dis: 4.763359 Entropy: 4.579363 
[2022-10-02 18:35:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.002893	 E_lr_2:0.002893
[2022-10-02 18:35:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000029	 C_lr_2:0.000029	 C_lr_3:0.000029	 C_lr_4:0.000029
[2022-10-02 18:35:55 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.184658	Loss2: 0.173315	 Dis: 4.840105 Entropy: 4.588959 
[2022-10-02 18:35:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.002815	 E_lr_2:0.002815
[2022-10-02 18:35:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000028	 C_lr_2:0.000028	 C_lr_3:0.000028	 C_lr_4:0.000028
[2022-10-02 18:36:02 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.313536	Loss2: 0.290607	 Dis: 5.214857 Entropy: 4.635826 
[2022-10-02 18:36:02 demo] (houston_program2.py 512): INFO E_lr_1: 0.002737	 E_lr_2:0.002737
[2022-10-02 18:36:02 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 18:36:08 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.155679	Loss2: 0.185283	 Dis: 5.797377 Entropy: 5.346930 
[2022-10-02 18:36:08 demo] (houston_program2.py 512): INFO E_lr_1: 0.002659	 E_lr_2:0.002659
[2022-10-02 18:36:08 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 18:36:14 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.043099	Loss2: 0.046571	 Dis: 5.450039 Entropy: 4.819416 
[2022-10-02 18:36:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.002581	 E_lr_2:0.002581
[2022-10-02 18:36:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000026	 C_lr_2:0.000026	 C_lr_3:0.000026	 C_lr_4:0.000026
[2022-10-02 18:36:20 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.112564	Loss2: 0.099604	 Dis: 4.601973 Entropy: 4.814306 
[2022-10-02 18:36:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.002502	 E_lr_2:0.002502
[2022-10-02 18:36:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 18:36:26 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.995079	Loss2: 0.851738	 Dis: 11.061480 Entropy: 4.370810 
[2022-10-02 18:36:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.002423	 E_lr_2:0.002423
[2022-10-02 18:36:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000024	 C_lr_2:0.000024	 C_lr_3:0.000024	 C_lr_4:0.000024
[2022-10-02 18:36:32 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.203631	Loss2: 0.184261	 Dis: 11.542183 Entropy: 5.270222 
[2022-10-02 18:36:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.002345	 E_lr_2:0.002345
[2022-10-02 18:36:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 18:36:38 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.154571	Loss2: 0.152988	 Dis: 7.549730 Entropy: 5.621288 
[2022-10-02 18:36:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.002267	 E_lr_2:0.002267
[2022-10-02 18:36:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 18:36:44 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.151434	Loss2: 0.166238	 Dis: 6.631245 Entropy: 4.575958 
[2022-10-02 18:36:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.002189	 E_lr_2:0.002189
[2022-10-02 18:36:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000022	 C_lr_2:0.000022	 C_lr_3:0.000022	 C_lr_4:0.000022
[2022-10-02 18:36:50 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.048819	Loss2: 0.052864	 Dis: 4.015732 Entropy: 4.768203 
[2022-10-02 18:36:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.002111	 E_lr_2:0.002111
[2022-10-02 18:36:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000021	 C_lr_2:0.000021	 C_lr_3:0.000021	 C_lr_4:0.000021
[2022-10-02 18:36:56 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.111792	Loss2: 0.104512	 Dis: 4.766777 Entropy: 4.690081 
[2022-10-02 18:36:56 demo] (houston_program2.py 512): INFO E_lr_1: 0.002034	 E_lr_2:0.002034
[2022-10-02 18:36:56 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 18:37:02 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.138362	Loss2: 0.155237	 Dis: 4.823795 Entropy: 5.084375 
[2022-10-02 18:37:02 demo] (houston_program2.py 512): INFO E_lr_1: 0.001957	 E_lr_2:0.001957
[2022-10-02 18:37:02 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 18:37:08 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.042493	Loss2: 0.050735	 Dis: 5.053856 Entropy: 5.322035 
[2022-10-02 18:37:08 demo] (houston_program2.py 512): INFO E_lr_1: 0.001880	 E_lr_2:0.001880
[2022-10-02 18:37:08 demo] (houston_program2.py 513): INFO C_lr_1: 0.000019	 C_lr_2:0.000019	 C_lr_3:0.000019	 C_lr_4:0.000019
[2022-10-02 18:37:14 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.057620	Loss2: 0.045641	 Dis: 5.926342 Entropy: 4.440825 
[2022-10-02 18:37:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.001804	 E_lr_2:0.001804
[2022-10-02 18:37:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000018	 C_lr_2:0.000018	 C_lr_3:0.000018	 C_lr_4:0.000018
[2022-10-02 18:37:20 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.020295	Loss2: 0.017571	 Dis: 3.626551 Entropy: 5.890012 
[2022-10-02 18:37:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.001729	 E_lr_2:0.001729
[2022-10-02 18:37:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 18:37:26 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.058297	Loss2: 0.053696	 Dis: 2.957676 Entropy: 5.178322 
[2022-10-02 18:37:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.001655	 E_lr_2:0.001655
[2022-10-02 18:37:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 18:37:32 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.080042	Loss2: 0.049782	 Dis: 3.944271 Entropy: 5.964782 
[2022-10-02 18:37:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.001582	 E_lr_2:0.001582
[2022-10-02 18:37:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000016	 C_lr_2:0.000016	 C_lr_3:0.000016	 C_lr_4:0.000016
[2022-10-02 18:37:38 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.074895	Loss2: 0.106548	 Dis: 4.487686 Entropy: 5.229409 
[2022-10-02 18:37:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.001509	 E_lr_2:0.001509
[2022-10-02 18:37:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 18:37:45 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.208307	Loss2: 0.206137	 Dis: 4.067457 Entropy: 5.422441 
[2022-10-02 18:37:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.001437	 E_lr_2:0.001437
[2022-10-02 18:37:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 18:37:50 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.011114	Loss2: 0.013123	 Dis: 1.990616 Entropy: 5.285824 
[2022-10-02 18:37:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.001367	 E_lr_2:0.001367
[2022-10-02 18:37:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 18:37:56 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.029021	Loss2: 0.023901	 Dis: 2.584436 Entropy: 4.785879 
[2022-10-02 18:37:56 demo] (houston_program2.py 512): INFO E_lr_1: 0.001297	 E_lr_2:0.001297
[2022-10-02 18:37:56 demo] (houston_program2.py 513): INFO C_lr_1: 0.000013	 C_lr_2:0.000013	 C_lr_3:0.000013	 C_lr_4:0.000013
[2022-10-02 18:38:03 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.029995	Loss2: 0.034041	 Dis: 3.540592 Entropy: 4.651296 
[2022-10-02 18:38:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.001229	 E_lr_2:0.001229
[2022-10-02 18:38:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 18:38:08 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.073081	Loss2: 0.071849	 Dis: 3.190683 Entropy: 4.739353 
[2022-10-02 18:38:08 demo] (houston_program2.py 512): INFO E_lr_1: 0.001162	 E_lr_2:0.001162
[2022-10-02 18:38:08 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 18:38:14 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.040189	Loss2: 0.069449	 Dis: 3.526270 Entropy: 5.954485 
[2022-10-02 18:38:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.001096	 E_lr_2:0.001096
[2022-10-02 18:38:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000011	 C_lr_2:0.000011	 C_lr_3:0.000011	 C_lr_4:0.000011
[2022-10-02 18:38:20 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.066794	Loss2: 0.065290	 Dis: 2.233652 Entropy: 4.685228 
[2022-10-02 18:38:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.001032	 E_lr_2:0.001032
[2022-10-02 18:38:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 18:38:26 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.008467	Loss2: 0.010061	 Dis: 3.850672 Entropy: 4.437449 
[2022-10-02 18:38:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.000969	 E_lr_2:0.000969
[2022-10-02 18:38:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 18:38:33 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.019502	Loss2: 0.006963	 Dis: 1.648064 Entropy: 4.459601 
[2022-10-02 18:38:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.000908	 E_lr_2:0.000908
[2022-10-02 18:38:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000009	 C_lr_2:0.000009	 C_lr_3:0.000009	 C_lr_4:0.000009
[2022-10-02 18:38:39 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.005531	Loss2: 0.005822	 Dis: 2.317360 Entropy: 5.779515 
[2022-10-02 18:38:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.000848	 E_lr_2:0.000848
[2022-10-02 18:38:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 18:38:45 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.014790	Loss2: 0.020653	 Dis: 2.169117 Entropy: 4.235786 
[2022-10-02 18:38:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.000790	 E_lr_2:0.000790
[2022-10-02 18:38:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 18:38:51 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.023193	Loss2: 0.020070	 Dis: 2.816658 Entropy: 4.786490 
[2022-10-02 18:38:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.000734	 E_lr_2:0.000734
[2022-10-02 18:38:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 18:38:57 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.062410	Loss2: 0.036367	 Dis: 2.284006 Entropy: 4.727439 
[2022-10-02 18:38:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.000679	 E_lr_2:0.000679
[2022-10-02 18:38:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 18:39:03 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.009070	Loss2: 0.013319	 Dis: 2.486496 Entropy: 4.713894 
[2022-10-02 18:39:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.000626	 E_lr_2:0.000626
[2022-10-02 18:39:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 18:39:09 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.004870	Loss2: 0.008458	 Dis: 2.072645 Entropy: 4.799254 
[2022-10-02 18:39:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.000575	 E_lr_2:0.000575
[2022-10-02 18:39:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 18:39:15 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.007958	Loss2: 0.010912	 Dis: 2.881519 Entropy: 4.515769 
[2022-10-02 18:39:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.000526	 E_lr_2:0.000526
[2022-10-02 18:39:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 18:39:22 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.026489	Loss2: 0.023298	 Dis: 2.969719 Entropy: 5.188076 
[2022-10-02 18:39:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.000479	 E_lr_2:0.000479
[2022-10-02 18:39:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 18:39:28 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.045455	Loss2: 0.044821	 Dis: 3.944885 Entropy: 4.548535 
[2022-10-02 18:39:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.000433	 E_lr_2:0.000433
[2022-10-02 18:39:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 18:39:34 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.004277	Loss2: 0.003448	 Dis: 2.028006 Entropy: 4.470592 
[2022-10-02 18:39:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.000390	 E_lr_2:0.000390
[2022-10-02 18:39:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 18:39:40 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.003600	Loss2: 0.004328	 Dis: 2.599174 Entropy: 4.701040 
[2022-10-02 18:39:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.000349	 E_lr_2:0.000349
[2022-10-02 18:39:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 18:39:46 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.005857	Loss2: 0.005787	 Dis: 3.418879 Entropy: 4.213564 
[2022-10-02 18:39:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.000310	 E_lr_2:0.000310
[2022-10-02 18:39:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 18:39:53 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.004082	Loss2: 0.004850	 Dis: 2.944529 Entropy: 4.761085 
[2022-10-02 18:39:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.000273	 E_lr_2:0.000273
[2022-10-02 18:39:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 18:39:59 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.002285	Loss2: 0.002232	 Dis: 1.417465 Entropy: 4.449394 
[2022-10-02 18:39:59 demo] (houston_program2.py 512): INFO E_lr_1: 0.000239	 E_lr_2:0.000239
[2022-10-02 18:39:59 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 18:40:04 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.002883	Loss2: 0.002895	 Dis: 1.224651 Entropy: 5.836459 
[2022-10-02 18:40:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.000206	 E_lr_2:0.000206
[2022-10-02 18:40:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 18:40:10 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.004741	Loss2: 0.003323	 Dis: 2.238094 Entropy: 4.664167 
[2022-10-02 18:40:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.000176	 E_lr_2:0.000176
[2022-10-02 18:40:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 18:40:16 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.004407	Loss2: 0.003919	 Dis: 2.241447 Entropy: 4.715178 
[2022-10-02 18:40:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.000148	 E_lr_2:0.000148
[2022-10-02 18:40:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 18:40:22 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.003702	Loss2: 0.003750	 Dis: 1.910374 Entropy: 4.747894 
[2022-10-02 18:40:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.000123	 E_lr_2:0.000123
[2022-10-02 18:40:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 18:40:29 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.005523	Loss2: 0.004759	 Dis: 3.442717 Entropy: 4.616926 
[2022-10-02 18:40:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.000100	 E_lr_2:0.000100
[2022-10-02 18:40:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 18:40:35 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.029505	Loss2: 0.016940	 Dis: 1.288860 Entropy: 4.679233 
[2022-10-02 18:40:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.000079	 E_lr_2:0.000079
[2022-10-02 18:40:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 18:40:41 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.004431	Loss2: 0.004949	 Dis: 2.271851 Entropy: 4.866929 
[2022-10-02 18:40:41 demo] (houston_program2.py 512): INFO E_lr_1: 0.000061	 E_lr_2:0.000061
[2022-10-02 18:40:41 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 18:40:47 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.012078	Loss2: 0.009658	 Dis: 1.057945 Entropy: 6.333086 
[2022-10-02 18:40:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.000045	 E_lr_2:0.000045
[2022-10-02 18:40:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:40:54 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.007201	Loss2: 0.007497	 Dis: 1.949282 Entropy: 5.188394 
[2022-10-02 18:40:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.000031	 E_lr_2:0.000031
[2022-10-02 18:40:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:41:00 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.002366	Loss2: 0.002576	 Dis: 2.637184 Entropy: 4.559043 
[2022-10-02 18:41:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.000020	 E_lr_2:0.000020
[2022-10-02 18:41:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:41:06 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.005113	Loss2: 0.005300	 Dis: 2.625050 Entropy: 5.408308 
[2022-10-02 18:41:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.000011	 E_lr_2:0.000011
[2022-10-02 18:41:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:41:12 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.003703	Loss2: 0.003707	 Dis: 3.919983 Entropy: 4.861475 
[2022-10-02 18:41:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.000005	 E_lr_2:0.000005
[2022-10-02 18:41:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:41:18 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.005452	Loss2: 0.005524	 Dis: 3.170883 Entropy: 4.432257 
[2022-10-02 18:41:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.000001	 E_lr_2:0.000001
[2022-10-02 18:41:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:41:24 demo] (houston_program2.py 504): INFO Train Ep: 50 	Loss1: 0.003688	Loss2: 0.002668	 Dis: 1.836102 Entropy: 4.876521 
[2022-10-02 18:41:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.000000	 E_lr_2:0.000000
[2022-10-02 18:41:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:41:24 demo] (houston_program2.py 515): INFO time_50_epoch:603.7532548904419
[2022-10-02 18:41:33 demo] (houston_program2.py 673): INFO 	val_Accuracy: 33820/53200 (63.57%)	
[2022-10-02 18:41:33 demo] (utils.py 47): INFO outputs/demo/finetune_100/ckpt_epoch_50.pth saving......
[2022-10-02 18:41:33 demo] (utils.py 49): INFO outputs/demo/finetune_100/ckpt_epoch_50.pth saved !!!
[2022-10-02 18:41:33 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0224 (0.0224)	loss 0.0080 (0.0080)	grad_norm 0.0528 (0.0528)	mem 460MB
[2022-10-02 18:41:33 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:34 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0216 (0.0216)	loss 0.0084 (0.0084)	grad_norm 0.0542 (0.0542)	mem 460MB
[2022-10-02 18:41:34 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:34 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0235 (0.0235)	loss 0.0092 (0.0092)	grad_norm 0.0456 (0.0456)	mem 460MB
[2022-10-02 18:41:34 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:34 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0223 (0.0223)	loss 0.0111 (0.0111)	grad_norm 0.0603 (0.0603)	mem 460MB
[2022-10-02 18:41:34 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:34 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0230 (0.0230)	loss 0.0092 (0.0092)	grad_norm 0.0701 (0.0701)	mem 460MB
[2022-10-02 18:41:34 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:34 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0232 (0.0232)	loss 0.0104 (0.0104)	grad_norm 0.0537 (0.0537)	mem 460MB
[2022-10-02 18:41:34 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:35 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0204 (0.0204)	loss 0.0085 (0.0085)	grad_norm 0.0793 (0.0793)	mem 460MB
[2022-10-02 18:41:35 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:35 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0231 (0.0231)	loss 0.0085 (0.0085)	grad_norm 0.0614 (0.0614)	mem 460MB
[2022-10-02 18:41:35 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:35 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0232 (0.0232)	loss 0.0131 (0.0131)	grad_norm 0.0365 (0.0365)	mem 460MB
[2022-10-02 18:41:35 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:35 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0294 (0.0294)	loss 0.0089 (0.0089)	grad_norm 0.0601 (0.0601)	mem 460MB
[2022-10-02 18:41:35 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:36 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0211 (0.0211)	loss 0.0084 (0.0084)	grad_norm 0.0699 (0.0699)	mem 460MB
[2022-10-02 18:41:36 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:36 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0141 (0.0141)	loss 0.0095 (0.0095)	grad_norm 0.0611 (0.0611)	mem 460MB
[2022-10-02 18:41:36 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:36 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0229 (0.0229)	loss 0.0081 (0.0081)	grad_norm 0.0366 (0.0366)	mem 460MB
[2022-10-02 18:41:36 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:36 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0213 (0.0213)	loss 0.0101 (0.0101)	grad_norm 0.0574 (0.0574)	mem 460MB
[2022-10-02 18:41:36 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:36 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0198 (0.0198)	loss 0.0108 (0.0108)	grad_norm 0.0580 (0.0580)	mem 460MB
[2022-10-02 18:41:36 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:36 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0241 (0.0241)	loss 0.0108 (0.0108)	grad_norm 0.0506 (0.0506)	mem 460MB
[2022-10-02 18:41:36 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:37 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0202 (0.0202)	loss 0.0088 (0.0088)	grad_norm 0.0644 (0.0644)	mem 460MB
[2022-10-02 18:41:37 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:37 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0240 (0.0240)	loss 0.0117 (0.0117)	grad_norm 0.0858 (0.0858)	mem 460MB
[2022-10-02 18:41:37 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:37 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0131 (0.0131)	loss 0.0109 (0.0109)	grad_norm 0.0437 (0.0437)	mem 460MB
[2022-10-02 18:41:37 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:37 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0190 (0.0190)	loss 0.0107 (0.0107)	grad_norm 0.0584 (0.0584)	mem 460MB
[2022-10-02 18:41:37 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:37 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0099 (0.0099)	loss 0.0084 (0.0084)	grad_norm 0.0775 (0.0775)	mem 460MB
[2022-10-02 18:41:37 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:37 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0094 (0.0094)	loss 0.0139 (0.0139)	grad_norm 0.0424 (0.0424)	mem 460MB
[2022-10-02 18:41:37 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:38 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0183 (0.0183)	loss 0.0080 (0.0080)	grad_norm 0.0486 (0.0486)	mem 460MB
[2022-10-02 18:41:38 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:38 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0213 (0.0213)	loss 0.0124 (0.0124)	grad_norm 0.0549 (0.0549)	mem 460MB
[2022-10-02 18:41:38 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:38 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0237 (0.0237)	loss 0.0092 (0.0092)	grad_norm 0.0526 (0.0526)	mem 460MB
[2022-10-02 18:41:38 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:38 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0206 (0.0206)	loss 0.0084 (0.0084)	grad_norm 0.0363 (0.0363)	mem 460MB
[2022-10-02 18:41:38 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:38 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0222 (0.0222)	loss 0.0091 (0.0091)	grad_norm 0.0600 (0.0600)	mem 460MB
[2022-10-02 18:41:38 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:38 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0215 (0.0215)	loss 0.0096 (0.0096)	grad_norm 0.0590 (0.0590)	mem 460MB
[2022-10-02 18:41:39 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:39 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0233 (0.0233)	loss 0.0134 (0.0134)	grad_norm 0.0448 (0.0448)	mem 460MB
[2022-10-02 18:41:39 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:39 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0220 (0.0220)	loss 0.0086 (0.0086)	grad_norm 0.0681 (0.0681)	mem 460MB
[2022-10-02 18:41:39 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:39 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0114 (0.0114)	loss 0.0090 (0.0090)	grad_norm 0.0834 (0.0834)	mem 460MB
[2022-10-02 18:41:39 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:39 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0220 (0.0220)	loss 0.0084 (0.0084)	grad_norm 0.0388 (0.0388)	mem 460MB
[2022-10-02 18:41:39 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:39 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0136 (0.0136)	loss 0.0102 (0.0102)	grad_norm 0.0526 (0.0526)	mem 460MB
[2022-10-02 18:41:39 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:40 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0211 (0.0211)	loss 0.0096 (0.0096)	grad_norm 0.0768 (0.0768)	mem 460MB
[2022-10-02 18:41:40 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:40 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0168 (0.0168)	loss 0.0128 (0.0128)	grad_norm 0.0811 (0.0811)	mem 460MB
[2022-10-02 18:41:40 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:40 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0208 (0.0208)	loss 0.0105 (0.0105)	grad_norm 0.0514 (0.0514)	mem 460MB
[2022-10-02 18:41:40 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:40 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000024	time 0.0190 (0.0190)	loss 0.0091 (0.0091)	grad_norm 0.0905 (0.0905)	mem 460MB
[2022-10-02 18:41:40 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:40 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0211 (0.0211)	loss 0.0130 (0.0130)	grad_norm 0.0692 (0.0692)	mem 460MB
[2022-10-02 18:41:40 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:40 demo] (houston_program2.py 243): INFO Train: [51/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0217 (0.0217)	loss 0.0130 (0.0130)	grad_norm 0.0941 (0.0941)	mem 460MB
[2022-10-02 18:41:40 demo] (houston_program2.py 252): INFO EPOCH 51 training takes 0:00:00
[2022-10-02 18:41:41 demo] (houston_program2.py 333): INFO Train Ep: 51 	Loss1: 0.102765	Loss2: 0.109221	 Dis: 3.284002 Entropy: 4.913693 
[2022-10-02 18:41:41 demo] (houston_program2.py 335): INFO time_51_epoch:7.6489036083221436
[2022-10-02 18:41:41 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0114 (0.0114)	loss 0.0088 (0.0088)	grad_norm 0.0761 (0.0761)	mem 460MB
[2022-10-02 18:41:41 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:41 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0206 (0.0206)	loss 0.0095 (0.0095)	grad_norm 0.0741 (0.0741)	mem 460MB
[2022-10-02 18:41:41 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:41 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0246 (0.0246)	loss 0.0104 (0.0104)	grad_norm 0.0512 (0.0512)	mem 460MB
[2022-10-02 18:41:41 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:42 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0220 (0.0220)	loss 0.0115 (0.0115)	grad_norm 0.0826 (0.0826)	mem 460MB
[2022-10-02 18:41:42 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:42 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0243 (0.0243)	loss 0.0083 (0.0083)	grad_norm 0.0735 (0.0735)	mem 460MB
[2022-10-02 18:41:42 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:42 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0220 (0.0220)	loss 0.0085 (0.0085)	grad_norm 0.0430 (0.0430)	mem 460MB
[2022-10-02 18:41:42 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:42 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0239 (0.0239)	loss 0.0099 (0.0099)	grad_norm 0.0717 (0.0717)	mem 460MB
[2022-10-02 18:41:42 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:42 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0217 (0.0217)	loss 0.0096 (0.0096)	grad_norm 0.0815 (0.0815)	mem 460MB
[2022-10-02 18:41:42 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:43 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0244 (0.0244)	loss 0.0122 (0.0122)	grad_norm 0.0499 (0.0499)	mem 460MB
[2022-10-02 18:41:43 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:43 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0231 (0.0231)	loss 0.0112 (0.0112)	grad_norm 0.0374 (0.0374)	mem 460MB
[2022-10-02 18:41:43 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:43 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0129 (0.0129)	loss 0.0089 (0.0089)	grad_norm 0.0861 (0.0861)	mem 460MB
[2022-10-02 18:41:43 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:43 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0216 (0.0216)	loss 0.0079 (0.0079)	grad_norm 0.0463 (0.0463)	mem 460MB
[2022-10-02 18:41:43 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:43 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0129 (0.0129)	loss 0.0155 (0.0155)	grad_norm 0.0512 (0.0512)	mem 460MB
[2022-10-02 18:41:43 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:44 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0111 (0.0111)	loss 0.0096 (0.0096)	grad_norm 0.0835 (0.0835)	mem 460MB
[2022-10-02 18:41:44 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:44 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0116 (0.0116)	loss 0.0104 (0.0104)	grad_norm 0.0718 (0.0718)	mem 460MB
[2022-10-02 18:41:44 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:44 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0177 (0.0177)	loss 0.0111 (0.0111)	grad_norm 0.0692 (0.0692)	mem 460MB
[2022-10-02 18:41:44 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:44 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0246 (0.0246)	loss 0.0118 (0.0118)	grad_norm 0.0607 (0.0607)	mem 460MB
[2022-10-02 18:41:44 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:44 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0111 (0.0111)	loss 0.0095 (0.0095)	grad_norm 0.0947 (0.0947)	mem 460MB
[2022-10-02 18:41:44 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:44 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0111 (0.0111)	loss 0.0080 (0.0080)	grad_norm 0.0468 (0.0468)	mem 460MB
[2022-10-02 18:41:44 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:44 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0110 (0.0110)	loss 0.0079 (0.0079)	grad_norm 0.0578 (0.0578)	mem 460MB
[2022-10-02 18:41:44 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:44 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0218 (0.0218)	loss 0.0075 (0.0075)	grad_norm 0.0798 (0.0798)	mem 460MB
[2022-10-02 18:41:44 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:45 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0247 (0.0247)	loss 0.0106 (0.0106)	grad_norm 0.0497 (0.0497)	mem 460MB
[2022-10-02 18:41:45 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:45 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0250 (0.0250)	loss 0.0093 (0.0093)	grad_norm 0.0385 (0.0385)	mem 460MB
[2022-10-02 18:41:45 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:45 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0209 (0.0209)	loss 0.0136 (0.0136)	grad_norm 0.0575 (0.0575)	mem 460MB
[2022-10-02 18:41:45 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:45 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0252 (0.0252)	loss 0.0097 (0.0097)	grad_norm 0.0643 (0.0643)	mem 460MB
[2022-10-02 18:41:45 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:46 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0242 (0.0242)	loss 0.0109 (0.0109)	grad_norm 0.0600 (0.0600)	mem 460MB
[2022-10-02 18:41:46 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:46 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0152 (0.0152)	loss 0.0088 (0.0088)	grad_norm 0.0298 (0.0298)	mem 460MB
[2022-10-02 18:41:46 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:46 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0242 (0.0242)	loss 0.0076 (0.0076)	grad_norm 0.0497 (0.0497)	mem 460MB
[2022-10-02 18:41:46 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:46 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0238 (0.0238)	loss 0.0126 (0.0126)	grad_norm 0.0930 (0.0930)	mem 460MB
[2022-10-02 18:41:46 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:46 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0240 (0.0240)	loss 0.0088 (0.0088)	grad_norm 0.0376 (0.0376)	mem 460MB
[2022-10-02 18:41:46 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:47 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0273 (0.0273)	loss 0.0102 (0.0102)	grad_norm 0.0425 (0.0425)	mem 460MB
[2022-10-02 18:41:47 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:47 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0161 (0.0161)	loss 0.0133 (0.0133)	grad_norm 0.1007 (0.1007)	mem 460MB
[2022-10-02 18:41:47 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:47 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0257 (0.0257)	loss 0.0097 (0.0097)	grad_norm 0.0678 (0.0678)	mem 460MB
[2022-10-02 18:41:47 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:47 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0220 (0.0220)	loss 0.0110 (0.0110)	grad_norm 0.0487 (0.0487)	mem 460MB
[2022-10-02 18:41:47 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:47 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0240 (0.0240)	loss 0.0089 (0.0089)	grad_norm 0.0465 (0.0465)	mem 460MB
[2022-10-02 18:41:47 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:48 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0213 (0.0213)	loss 0.0093 (0.0093)	grad_norm 0.0907 (0.0907)	mem 460MB
[2022-10-02 18:41:48 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:48 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0184 (0.0184)	loss 0.0080 (0.0080)	grad_norm 0.0370 (0.0370)	mem 460MB
[2022-10-02 18:41:48 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:48 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0211 (0.0211)	loss 0.0084 (0.0084)	grad_norm 0.0522 (0.0522)	mem 460MB
[2022-10-02 18:41:48 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:48 demo] (houston_program2.py 243): INFO Train: [52/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0120 (0.0120)	loss 0.0096 (0.0096)	grad_norm 0.0478 (0.0478)	mem 460MB
[2022-10-02 18:41:48 demo] (houston_program2.py 252): INFO EPOCH 52 training takes 0:00:00
[2022-10-02 18:41:48 demo] (houston_program2.py 333): INFO Train Ep: 52 	Loss1: 0.177893	Loss2: 0.186368	 Dis: 4.765106 Entropy: 4.762368 
[2022-10-02 18:41:48 demo] (houston_program2.py 335): INFO time_52_epoch:7.5037219524383545
[2022-10-02 18:41:49 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0135 (0.0135)	loss 0.0107 (0.0107)	grad_norm 0.0372 (0.0372)	mem 460MB
[2022-10-02 18:41:49 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:49 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0251 (0.0251)	loss 0.0088 (0.0088)	grad_norm 0.0401 (0.0401)	mem 460MB
[2022-10-02 18:41:49 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:49 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0252 (0.0252)	loss 0.0127 (0.0127)	grad_norm 0.0481 (0.0481)	mem 460MB
[2022-10-02 18:41:49 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:49 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0226 (0.0226)	loss 0.0102 (0.0102)	grad_norm 0.0377 (0.0377)	mem 460MB
[2022-10-02 18:41:49 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:49 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0147 (0.0147)	loss 0.0075 (0.0075)	grad_norm 0.0390 (0.0390)	mem 460MB
[2022-10-02 18:41:49 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:50 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0237 (0.0237)	loss 0.0098 (0.0098)	grad_norm 0.0432 (0.0432)	mem 460MB
[2022-10-02 18:41:50 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:50 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0229 (0.0229)	loss 0.0108 (0.0108)	grad_norm 0.0469 (0.0469)	mem 460MB
[2022-10-02 18:41:50 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:50 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0229 (0.0229)	loss 0.0107 (0.0107)	grad_norm 0.0510 (0.0510)	mem 460MB
[2022-10-02 18:41:50 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:50 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000023	time 0.0214 (0.0214)	loss 0.0118 (0.0118)	grad_norm 0.0347 (0.0347)	mem 460MB
[2022-10-02 18:41:50 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:50 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0159 (0.0159)	loss 0.0098 (0.0098)	grad_norm 0.0391 (0.0391)	mem 460MB
[2022-10-02 18:41:50 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:51 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0209 (0.0209)	loss 0.0098 (0.0098)	grad_norm 0.0432 (0.0432)	mem 460MB
[2022-10-02 18:41:51 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:51 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0132 (0.0132)	loss 0.0097 (0.0097)	grad_norm 0.0497 (0.0497)	mem 460MB
[2022-10-02 18:41:51 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:51 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0209 (0.0209)	loss 0.0079 (0.0079)	grad_norm 0.0433 (0.0433)	mem 460MB
[2022-10-02 18:41:51 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:51 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0151 (0.0151)	loss 0.0126 (0.0126)	grad_norm 0.0741 (0.0741)	mem 460MB
[2022-10-02 18:41:51 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:51 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0207 (0.0207)	loss 0.0081 (0.0081)	grad_norm 0.0413 (0.0413)	mem 460MB
[2022-10-02 18:41:51 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:51 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0170 (0.0170)	loss 0.0107 (0.0107)	grad_norm 0.0473 (0.0473)	mem 460MB
[2022-10-02 18:41:51 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:52 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0201 (0.0201)	loss 0.0099 (0.0099)	grad_norm 0.0573 (0.0573)	mem 460MB
[2022-10-02 18:41:52 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:52 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0226 (0.0226)	loss 0.0098 (0.0098)	grad_norm 0.0569 (0.0569)	mem 460MB
[2022-10-02 18:41:52 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:52 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0222 (0.0222)	loss 0.0100 (0.0100)	grad_norm 0.0552 (0.0552)	mem 460MB
[2022-10-02 18:41:52 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:52 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0218 (0.0218)	loss 0.0091 (0.0091)	grad_norm 0.0367 (0.0367)	mem 460MB
[2022-10-02 18:41:52 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:52 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0225 (0.0225)	loss 0.0093 (0.0093)	grad_norm 0.0527 (0.0527)	mem 460MB
[2022-10-02 18:41:52 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:53 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0206 (0.0206)	loss 0.0070 (0.0070)	grad_norm 0.0383 (0.0383)	mem 460MB
[2022-10-02 18:41:53 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:53 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0245 (0.0245)	loss 0.0089 (0.0089)	grad_norm 0.0331 (0.0331)	mem 460MB
[2022-10-02 18:41:53 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:53 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0204 (0.0204)	loss 0.0089 (0.0089)	grad_norm 0.0287 (0.0287)	mem 460MB
[2022-10-02 18:41:53 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:53 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0198 (0.0198)	loss 0.0094 (0.0094)	grad_norm 0.0350 (0.0350)	mem 460MB
[2022-10-02 18:41:53 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:53 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0207 (0.0207)	loss 0.0085 (0.0085)	grad_norm 0.0407 (0.0407)	mem 460MB
[2022-10-02 18:41:53 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:53 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0205 (0.0205)	loss 0.0081 (0.0081)	grad_norm 0.0395 (0.0395)	mem 460MB
[2022-10-02 18:41:53 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:54 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0214 (0.0214)	loss 0.0093 (0.0093)	grad_norm 0.0508 (0.0508)	mem 460MB
[2022-10-02 18:41:54 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:54 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0132 (0.0132)	loss 0.0113 (0.0113)	grad_norm 0.0361 (0.0361)	mem 460MB
[2022-10-02 18:41:54 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:54 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0215 (0.0215)	loss 0.0086 (0.0086)	grad_norm 0.0548 (0.0548)	mem 460MB
[2022-10-02 18:41:54 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:54 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0133 (0.0133)	loss 0.0095 (0.0095)	grad_norm 0.0405 (0.0405)	mem 460MB
[2022-10-02 18:41:54 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:54 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0219 (0.0219)	loss 0.0105 (0.0105)	grad_norm 0.0394 (0.0394)	mem 460MB
[2022-10-02 18:41:54 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:55 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0162 (0.0162)	loss 0.0068 (0.0068)	grad_norm 0.0390 (0.0390)	mem 460MB
[2022-10-02 18:41:55 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:55 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0225 (0.0225)	loss 0.0114 (0.0114)	grad_norm 0.0756 (0.0756)	mem 460MB
[2022-10-02 18:41:55 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:55 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0207 (0.0207)	loss 0.0100 (0.0100)	grad_norm 0.0445 (0.0445)	mem 460MB
[2022-10-02 18:41:55 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:55 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0215 (0.0215)	loss 0.0124 (0.0124)	grad_norm 0.0612 (0.0612)	mem 460MB
[2022-10-02 18:41:55 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:55 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0204 (0.0204)	loss 0.0111 (0.0111)	grad_norm 0.0406 (0.0406)	mem 460MB
[2022-10-02 18:41:55 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:56 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0235 (0.0235)	loss 0.0110 (0.0110)	grad_norm 0.0356 (0.0356)	mem 460MB
[2022-10-02 18:41:56 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:56 demo] (houston_program2.py 243): INFO Train: [53/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0207 (0.0207)	loss 0.0083 (0.0083)	grad_norm 0.0578 (0.0578)	mem 460MB
[2022-10-02 18:41:56 demo] (houston_program2.py 252): INFO EPOCH 53 training takes 0:00:00
[2022-10-02 18:41:56 demo] (houston_program2.py 333): INFO Train Ep: 53 	Loss1: 0.164632	Loss2: 0.164844	 Dis: 4.055805 Entropy: 5.098811 
[2022-10-02 18:41:56 demo] (houston_program2.py 335): INFO time_53_epoch:7.685682773590088
[2022-10-02 18:41:56 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0123 (0.0123)	loss 0.0092 (0.0092)	grad_norm 0.0510 (0.0510)	mem 460MB
[2022-10-02 18:41:56 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:41:56 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0189 (0.0189)	loss 0.0100 (0.0100)	grad_norm 0.0452 (0.0452)	mem 460MB
[2022-10-02 18:41:56 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:41:57 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0186 (0.0186)	loss 0.0135 (0.0135)	grad_norm 0.0394 (0.0394)	mem 460MB
[2022-10-02 18:41:57 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:41:57 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0213 (0.0213)	loss 0.0091 (0.0091)	grad_norm 0.0610 (0.0610)	mem 460MB
[2022-10-02 18:41:57 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:41:57 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0170 (0.0170)	loss 0.0091 (0.0091)	grad_norm 0.0398 (0.0398)	mem 460MB
[2022-10-02 18:41:57 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:41:57 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0259 (0.0259)	loss 0.0089 (0.0089)	grad_norm 0.0590 (0.0590)	mem 460MB
[2022-10-02 18:41:57 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:41:57 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0227 (0.0227)	loss 0.0081 (0.0081)	grad_norm 0.0439 (0.0439)	mem 460MB
[2022-10-02 18:41:57 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:41:58 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0098 (0.0098)	loss 0.0087 (0.0087)	grad_norm 0.0553 (0.0553)	mem 460MB
[2022-10-02 18:41:58 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:41:58 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0214 (0.0214)	loss 0.0101 (0.0101)	grad_norm 0.0445 (0.0445)	mem 460MB
[2022-10-02 18:41:58 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:41:58 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0166 (0.0166)	loss 0.0074 (0.0074)	grad_norm 0.0555 (0.0555)	mem 460MB
[2022-10-02 18:41:58 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:41:58 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0216 (0.0216)	loss 0.0073 (0.0073)	grad_norm 0.0577 (0.0577)	mem 460MB
[2022-10-02 18:41:58 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:41:58 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0128 (0.0128)	loss 0.0134 (0.0134)	grad_norm 0.0872 (0.0872)	mem 460MB
[2022-10-02 18:41:58 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:41:58 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0201 (0.0201)	loss 0.0073 (0.0073)	grad_norm 0.0392 (0.0392)	mem 460MB
[2022-10-02 18:41:58 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:41:59 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0212 (0.0212)	loss 0.0084 (0.0084)	grad_norm 0.0400 (0.0400)	mem 460MB
[2022-10-02 18:41:59 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:41:59 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0201 (0.0201)	loss 0.0081 (0.0081)	grad_norm 0.0399 (0.0399)	mem 460MB
[2022-10-02 18:41:59 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:41:59 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0202 (0.0202)	loss 0.0084 (0.0084)	grad_norm 0.0470 (0.0470)	mem 460MB
[2022-10-02 18:41:59 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:41:59 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0211 (0.0211)	loss 0.0107 (0.0107)	grad_norm 0.0320 (0.0320)	mem 460MB
[2022-10-02 18:41:59 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:41:59 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0225 (0.0225)	loss 0.0107 (0.0107)	grad_norm 0.1019 (0.1019)	mem 460MB
[2022-10-02 18:41:59 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:42:00 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0224 (0.0224)	loss 0.0115 (0.0115)	grad_norm 0.0927 (0.0927)	mem 460MB
[2022-10-02 18:42:00 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:42:00 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000022	time 0.0225 (0.0225)	loss 0.0132 (0.0132)	grad_norm 0.0553 (0.0553)	mem 460MB
[2022-10-02 18:42:00 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:42:00 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0227 (0.0227)	loss 0.0106 (0.0106)	grad_norm 0.0763 (0.0763)	mem 460MB
[2022-10-02 18:42:00 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:42:00 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0192 (0.0192)	loss 0.0100 (0.0100)	grad_norm 0.0734 (0.0734)	mem 460MB
[2022-10-02 18:42:00 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:42:00 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0142 (0.0142)	loss 0.0096 (0.0096)	grad_norm 0.0475 (0.0475)	mem 460MB
[2022-10-02 18:42:00 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:42:00 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0218 (0.0218)	loss 0.0110 (0.0110)	grad_norm 0.0480 (0.0480)	mem 460MB
[2022-10-02 18:42:00 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:42:01 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0144 (0.0144)	loss 0.0101 (0.0101)	grad_norm 0.0645 (0.0645)	mem 460MB
[2022-10-02 18:42:01 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:42:01 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0202 (0.0202)	loss 0.0063 (0.0063)	grad_norm 0.0629 (0.0629)	mem 460MB
[2022-10-02 18:42:01 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:42:01 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0149 (0.0149)	loss 0.0086 (0.0086)	grad_norm 0.0388 (0.0388)	mem 460MB
[2022-10-02 18:42:01 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:42:01 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0206 (0.0206)	loss 0.0126 (0.0126)	grad_norm 0.0544 (0.0544)	mem 460MB
[2022-10-02 18:42:01 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:42:01 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0212 (0.0212)	loss 0.0093 (0.0093)	grad_norm 0.0513 (0.0513)	mem 460MB
[2022-10-02 18:42:01 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:42:02 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0209 (0.0209)	loss 0.0149 (0.0149)	grad_norm 0.0730 (0.0730)	mem 460MB
[2022-10-02 18:42:02 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:42:02 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0204 (0.0204)	loss 0.0093 (0.0093)	grad_norm 0.0743 (0.0743)	mem 460MB
[2022-10-02 18:42:02 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:42:02 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0210 (0.0210)	loss 0.0091 (0.0091)	grad_norm 0.0331 (0.0331)	mem 460MB
[2022-10-02 18:42:02 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:42:02 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0216 (0.0216)	loss 0.0080 (0.0080)	grad_norm 0.0374 (0.0374)	mem 460MB
[2022-10-02 18:42:02 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:42:02 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0212 (0.0212)	loss 0.0098 (0.0098)	grad_norm 0.0586 (0.0586)	mem 460MB
[2022-10-02 18:42:02 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:42:02 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0205 (0.0205)	loss 0.0123 (0.0123)	grad_norm 0.0462 (0.0462)	mem 460MB
[2022-10-02 18:42:02 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:42:03 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0258 (0.0258)	loss 0.0105 (0.0105)	grad_norm 0.0497 (0.0497)	mem 460MB
[2022-10-02 18:42:03 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:42:03 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0209 (0.0209)	loss 0.0100 (0.0100)	grad_norm 0.0297 (0.0297)	mem 460MB
[2022-10-02 18:42:03 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:42:03 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0131 (0.0131)	loss 0.0076 (0.0076)	grad_norm 0.0275 (0.0275)	mem 460MB
[2022-10-02 18:42:03 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:42:03 demo] (houston_program2.py 243): INFO Train: [54/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0204 (0.0204)	loss 0.0085 (0.0085)	grad_norm 0.0784 (0.0784)	mem 460MB
[2022-10-02 18:42:03 demo] (houston_program2.py 252): INFO EPOCH 54 training takes 0:00:00
[2022-10-02 18:42:03 demo] (houston_program2.py 333): INFO Train Ep: 54 	Loss1: 0.115428	Loss2: 0.121928	 Dis: 3.594873 Entropy: 4.356971 
[2022-10-02 18:42:03 demo] (houston_program2.py 335): INFO time_54_epoch:7.453521013259888
[2022-10-02 18:42:04 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0119 (0.0119)	loss 0.0079 (0.0079)	grad_norm 0.0424 (0.0424)	mem 460MB
[2022-10-02 18:42:04 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:04 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0203 (0.0203)	loss 0.0071 (0.0071)	grad_norm 0.0327 (0.0327)	mem 460MB
[2022-10-02 18:42:04 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:04 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0209 (0.0209)	loss 0.0107 (0.0107)	grad_norm 0.0487 (0.0487)	mem 460MB
[2022-10-02 18:42:04 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:04 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0115 (0.0115)	loss 0.0071 (0.0071)	grad_norm 0.0342 (0.0342)	mem 460MB
[2022-10-02 18:42:04 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:04 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0228 (0.0228)	loss 0.0130 (0.0130)	grad_norm 0.0491 (0.0491)	mem 460MB
[2022-10-02 18:42:04 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:05 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0134 (0.0134)	loss 0.0096 (0.0096)	grad_norm 0.0352 (0.0352)	mem 460MB
[2022-10-02 18:42:05 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:05 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0225 (0.0225)	loss 0.0094 (0.0094)	grad_norm 0.0342 (0.0342)	mem 460MB
[2022-10-02 18:42:05 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:05 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0153 (0.0153)	loss 0.0100 (0.0100)	grad_norm 0.0438 (0.0438)	mem 460MB
[2022-10-02 18:42:05 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:05 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0218 (0.0218)	loss 0.0095 (0.0095)	grad_norm 0.0685 (0.0685)	mem 460MB
[2022-10-02 18:42:05 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:05 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0164 (0.0164)	loss 0.0091 (0.0091)	grad_norm 0.0373 (0.0373)	mem 460MB
[2022-10-02 18:42:05 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:06 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0208 (0.0208)	loss 0.0095 (0.0095)	grad_norm 0.0682 (0.0682)	mem 460MB
[2022-10-02 18:42:06 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:06 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0182 (0.0182)	loss 0.0075 (0.0075)	grad_norm 0.0407 (0.0407)	mem 460MB
[2022-10-02 18:42:06 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:06 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0218 (0.0218)	loss 0.0107 (0.0107)	grad_norm 0.0465 (0.0465)	mem 460MB
[2022-10-02 18:42:06 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:06 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0217 (0.0217)	loss 0.0108 (0.0108)	grad_norm 0.0513 (0.0513)	mem 460MB
[2022-10-02 18:42:06 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:06 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0217 (0.0217)	loss 0.0104 (0.0104)	grad_norm 0.0568 (0.0568)	mem 460MB
[2022-10-02 18:42:06 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:06 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0207 (0.0207)	loss 0.0099 (0.0099)	grad_norm 0.0419 (0.0419)	mem 460MB
[2022-10-02 18:42:06 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:07 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0244 (0.0244)	loss 0.0095 (0.0095)	grad_norm 0.0481 (0.0481)	mem 460MB
[2022-10-02 18:42:07 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:07 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0254 (0.0254)	loss 0.0098 (0.0098)	grad_norm 0.0653 (0.0653)	mem 460MB
[2022-10-02 18:42:07 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:07 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0194 (0.0194)	loss 0.0110 (0.0110)	grad_norm 0.0556 (0.0556)	mem 460MB
[2022-10-02 18:42:07 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:07 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0246 (0.0246)	loss 0.0081 (0.0081)	grad_norm 0.0561 (0.0561)	mem 460MB
[2022-10-02 18:42:07 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:07 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0255 (0.0255)	loss 0.0104 (0.0104)	grad_norm 0.0742 (0.0742)	mem 460MB
[2022-10-02 18:42:07 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:08 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0127 (0.0127)	loss 0.0077 (0.0077)	grad_norm 0.0387 (0.0387)	mem 460MB
[2022-10-02 18:42:08 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:08 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0252 (0.0252)	loss 0.0076 (0.0076)	grad_norm 0.0540 (0.0540)	mem 460MB
[2022-10-02 18:42:08 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:08 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0266 (0.0266)	loss 0.0083 (0.0083)	grad_norm 0.0502 (0.0502)	mem 460MB
[2022-10-02 18:42:08 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:08 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0242 (0.0242)	loss 0.0100 (0.0100)	grad_norm 0.0414 (0.0414)	mem 460MB
[2022-10-02 18:42:08 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:08 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0239 (0.0239)	loss 0.0083 (0.0083)	grad_norm 0.0419 (0.0419)	mem 460MB
[2022-10-02 18:42:08 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:09 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0136 (0.0136)	loss 0.0103 (0.0103)	grad_norm 0.0705 (0.0705)	mem 460MB
[2022-10-02 18:42:09 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:09 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0207 (0.0207)	loss 0.0097 (0.0097)	grad_norm 0.0531 (0.0531)	mem 460MB
[2022-10-02 18:42:09 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:09 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0215 (0.0215)	loss 0.0123 (0.0123)	grad_norm 0.0495 (0.0495)	mem 460MB
[2022-10-02 18:42:09 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:09 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0261 (0.0261)	loss 0.0098 (0.0098)	grad_norm 0.0870 (0.0870)	mem 460MB
[2022-10-02 18:42:09 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:09 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000021	time 0.0221 (0.0221)	loss 0.0091 (0.0091)	grad_norm 0.0583 (0.0583)	mem 460MB
[2022-10-02 18:42:09 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:10 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0228 (0.0228)	loss 0.0087 (0.0087)	grad_norm 0.0450 (0.0450)	mem 460MB
[2022-10-02 18:42:10 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:10 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0215 (0.0215)	loss 0.0094 (0.0094)	grad_norm 0.0666 (0.0666)	mem 460MB
[2022-10-02 18:42:10 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:10 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0173 (0.0173)	loss 0.0098 (0.0098)	grad_norm 0.0835 (0.0835)	mem 460MB
[2022-10-02 18:42:10 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:10 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0218 (0.0218)	loss 0.0099 (0.0099)	grad_norm 0.0354 (0.0354)	mem 460MB
[2022-10-02 18:42:10 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:10 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0200 (0.0200)	loss 0.0077 (0.0077)	grad_norm 0.0719 (0.0719)	mem 460MB
[2022-10-02 18:42:10 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:11 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0221 (0.0221)	loss 0.0092 (0.0092)	grad_norm 0.0658 (0.0658)	mem 460MB
[2022-10-02 18:42:11 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:11 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0210 (0.0210)	loss 0.0094 (0.0094)	grad_norm 0.0490 (0.0490)	mem 460MB
[2022-10-02 18:42:11 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:11 demo] (houston_program2.py 243): INFO Train: [55/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0242 (0.0242)	loss 0.0104 (0.0104)	grad_norm 0.0528 (0.0528)	mem 460MB
[2022-10-02 18:42:11 demo] (houston_program2.py 252): INFO EPOCH 55 training takes 0:00:00
[2022-10-02 18:42:11 demo] (houston_program2.py 333): INFO Train Ep: 55 	Loss1: 0.167026	Loss2: 0.139628	 Dis: 4.902954 Entropy: 5.297225 
[2022-10-02 18:42:11 demo] (houston_program2.py 335): INFO time_55_epoch:7.7746734619140625
[2022-10-02 18:42:11 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 18:42:11 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 18:42:11 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 18:42:11 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 18:42:11 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 18:42:11 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 18:42:11 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 18:42:11 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 18:42:11 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 18:42:11 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 18:42:17 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.049526	Loss2: 0.037726	 Dis: 4.041447 Entropy: 4.852991 
[2022-10-02 18:42:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.000487	 E_lr_2:0.000487
[2022-10-02 18:42:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 18:42:23 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.028384	Loss2: 0.033075	 Dis: 4.118336 Entropy: 4.575893 
[2022-10-02 18:42:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.000987	 E_lr_2:0.000987
[2022-10-02 18:42:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 18:42:29 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.035192	Loss2: 0.042107	 Dis: 2.903942 Entropy: 4.365121 
[2022-10-02 18:42:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.001487	 E_lr_2:0.001487
[2022-10-02 18:42:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 18:42:35 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.019685	Loss2: 0.027849	 Dis: 2.335846 Entropy: 4.639448 
[2022-10-02 18:42:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.001987	 E_lr_2:0.001987
[2022-10-02 18:42:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 18:42:41 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.053750	Loss2: 0.063741	 Dis: 3.827179 Entropy: 5.123549 
[2022-10-02 18:42:41 demo] (houston_program2.py 512): INFO E_lr_1: 0.002487	 E_lr_2:0.002487
[2022-10-02 18:42:41 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 18:42:47 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.063393	Loss2: 0.077244	 Dis: 4.255955 Entropy: 4.837551 
[2022-10-02 18:42:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.002987	 E_lr_2:0.002987
[2022-10-02 18:42:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 18:42:53 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.056790	Loss2: 0.050035	 Dis: 6.034157 Entropy: 4.419086 
[2022-10-02 18:42:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.003487	 E_lr_2:0.003487
[2022-10-02 18:42:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 18:43:00 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.075968	Loss2: 0.086378	 Dis: 5.468445 Entropy: 4.648699 
[2022-10-02 18:43:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.003987	 E_lr_2:0.003987
[2022-10-02 18:43:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 18:43:05 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.075088	Loss2: 0.080474	 Dis: 3.793173 Entropy: 5.572491 
[2022-10-02 18:43:05 demo] (houston_program2.py 512): INFO E_lr_1: 0.004487	 E_lr_2:0.004487
[2022-10-02 18:43:05 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 18:43:12 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.220023	Loss2: 0.196203	 Dis: 4.182087 Entropy: 4.628437 
[2022-10-02 18:43:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.004987	 E_lr_2:0.004987
[2022-10-02 18:43:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000050	 C_lr_2:0.000050	 C_lr_3:0.000050	 C_lr_4:0.000050
[2022-10-02 18:43:18 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.323682	Loss2: 0.257641	 Dis: 5.165552 Entropy: 4.212387 
[2022-10-02 18:43:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.004853	 E_lr_2:0.004853
[2022-10-02 18:43:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000049	 C_lr_2:0.000049	 C_lr_3:0.000049	 C_lr_4:0.000049
[2022-10-02 18:43:24 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.057223	Loss2: 0.059738	 Dis: 3.421572 Entropy: 5.222272 
[2022-10-02 18:43:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.004825	 E_lr_2:0.004825
[2022-10-02 18:43:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 18:43:29 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.023461	Loss2: 0.027026	 Dis: 5.025721 Entropy: 4.536185 
[2022-10-02 18:43:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.004795	 E_lr_2:0.004795
[2022-10-02 18:43:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 18:43:35 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.049725	Loss2: 0.056043	 Dis: 5.538942 Entropy: 5.733191 
[2022-10-02 18:43:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.004763	 E_lr_2:0.004763
[2022-10-02 18:43:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 18:43:41 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.078134	Loss2: 0.096969	 Dis: 3.370140 Entropy: 6.253319 
[2022-10-02 18:43:41 demo] (houston_program2.py 512): INFO E_lr_1: 0.004728	 E_lr_2:0.004728
[2022-10-02 18:43:41 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 18:43:47 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.085807	Loss2: 0.078125	 Dis: 4.301405 Entropy: 5.449279 
[2022-10-02 18:43:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.004692	 E_lr_2:0.004692
[2022-10-02 18:43:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 18:43:53 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.243915	Loss2: 0.209031	 Dis: 4.245914 Entropy: 4.854661 
[2022-10-02 18:43:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.004653	 E_lr_2:0.004653
[2022-10-02 18:43:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 18:43:59 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.066880	Loss2: 0.066188	 Dis: 3.775707 Entropy: 4.745086 
[2022-10-02 18:43:59 demo] (houston_program2.py 512): INFO E_lr_1: 0.004612	 E_lr_2:0.004612
[2022-10-02 18:43:59 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 18:44:05 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.169197	Loss2: 0.202956	 Dis: 5.952055 Entropy: 4.951663 
[2022-10-02 18:44:05 demo] (houston_program2.py 512): INFO E_lr_1: 0.004569	 E_lr_2:0.004569
[2022-10-02 18:44:05 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 18:44:12 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.200531	Loss2: 0.209141	 Dis: 4.542961 Entropy: 4.783510 
[2022-10-02 18:44:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.004524	 E_lr_2:0.004524
[2022-10-02 18:44:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 18:44:18 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.054790	Loss2: 0.057684	 Dis: 2.984304 Entropy: 5.904772 
[2022-10-02 18:44:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.004477	 E_lr_2:0.004477
[2022-10-02 18:44:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 18:44:24 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.111958	Loss2: 0.102615	 Dis: 5.347492 Entropy: 4.896541 
[2022-10-02 18:44:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.004428	 E_lr_2:0.004428
[2022-10-02 18:44:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 18:44:30 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.073858	Loss2: 0.084645	 Dis: 4.344679 Entropy: 4.338479 
[2022-10-02 18:44:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.004377	 E_lr_2:0.004377
[2022-10-02 18:44:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 18:44:37 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.051322	Loss2: 0.061694	 Dis: 3.285070 Entropy: 5.725730 
[2022-10-02 18:44:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.004324	 E_lr_2:0.004324
[2022-10-02 18:44:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 18:44:43 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.105165	Loss2: 0.119660	 Dis: 3.874897 Entropy: 5.382156 
[2022-10-02 18:44:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.004269	 E_lr_2:0.004269
[2022-10-02 18:44:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 18:44:49 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.085196	Loss2: 0.093251	 Dis: 3.957415 Entropy: 6.178252 
[2022-10-02 18:44:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.004213	 E_lr_2:0.004213
[2022-10-02 18:44:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 18:44:56 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.257757	Loss2: 0.333202	 Dis: 4.171490 Entropy: 4.245037 
[2022-10-02 18:44:56 demo] (houston_program2.py 512): INFO E_lr_1: 0.004155	 E_lr_2:0.004155
[2022-10-02 18:44:56 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 18:45:02 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.035585	Loss2: 0.027381	 Dis: 2.845730 Entropy: 4.511831 
[2022-10-02 18:45:02 demo] (houston_program2.py 512): INFO E_lr_1: 0.004095	 E_lr_2:0.004095
[2022-10-02 18:45:02 demo] (houston_program2.py 513): INFO C_lr_1: 0.000041	 C_lr_2:0.000041	 C_lr_3:0.000041	 C_lr_4:0.000041
[2022-10-02 18:45:09 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.041589	Loss2: 0.060336	 Dis: 5.198988 Entropy: 5.000779 
[2022-10-02 18:45:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.004034	 E_lr_2:0.004034
[2022-10-02 18:45:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 18:45:14 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.110203	Loss2: 0.122154	 Dis: 3.422611 Entropy: 4.415998 
[2022-10-02 18:45:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.003971	 E_lr_2:0.003971
[2022-10-02 18:45:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 18:45:21 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.169504	Loss2: 0.178271	 Dis: 4.245358 Entropy: 4.544284 
[2022-10-02 18:45:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.003907	 E_lr_2:0.003907
[2022-10-02 18:45:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000039	 C_lr_2:0.000039	 C_lr_3:0.000039	 C_lr_4:0.000039
[2022-10-02 18:45:26 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.055618	Loss2: 0.090422	 Dis: 3.783924 Entropy: 5.389599 
[2022-10-02 18:45:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.003841	 E_lr_2:0.003841
[2022-10-02 18:45:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 18:45:33 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.076337	Loss2: 0.056304	 Dis: 4.554167 Entropy: 4.294070 
[2022-10-02 18:45:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.003774	 E_lr_2:0.003774
[2022-10-02 18:45:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 18:45:39 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.038079	Loss2: 0.048065	 Dis: 3.325558 Entropy: 5.159473 
[2022-10-02 18:45:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.003706	 E_lr_2:0.003706
[2022-10-02 18:45:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000037	 C_lr_2:0.000037	 C_lr_3:0.000037	 C_lr_4:0.000037
[2022-10-02 18:45:45 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.064941	Loss2: 0.054950	 Dis: 4.005497 Entropy: 4.798005 
[2022-10-02 18:45:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.003637	 E_lr_2:0.003637
[2022-10-02 18:45:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 18:45:51 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.083500	Loss2: 0.069547	 Dis: 3.956078 Entropy: 4.661012 
[2022-10-02 18:45:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.003566	 E_lr_2:0.003566
[2022-10-02 18:45:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 18:45:58 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.024265	Loss2: 0.024136	 Dis: 3.458897 Entropy: 4.747743 
[2022-10-02 18:45:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.003495	 E_lr_2:0.003495
[2022-10-02 18:45:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 18:46:05 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.051128	Loss2: 0.053591	 Dis: 3.023508 Entropy: 4.396111 
[2022-10-02 18:46:05 demo] (houston_program2.py 512): INFO E_lr_1: 0.003422	 E_lr_2:0.003422
[2022-10-02 18:46:05 demo] (houston_program2.py 513): INFO C_lr_1: 0.000034	 C_lr_2:0.000034	 C_lr_3:0.000034	 C_lr_4:0.000034
[2022-10-02 18:46:11 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.023519	Loss2: 0.018577	 Dis: 3.436886 Entropy: 4.731775 
[2022-10-02 18:46:11 demo] (houston_program2.py 512): INFO E_lr_1: 0.003349	 E_lr_2:0.003349
[2022-10-02 18:46:11 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 18:46:17 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.022828	Loss2: 0.027029	 Dis: 2.723303 Entropy: 5.339048 
[2022-10-02 18:46:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.003274	 E_lr_2:0.003274
[2022-10-02 18:46:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 18:46:23 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.047329	Loss2: 0.032932	 Dis: 2.738464 Entropy: 4.548254 
[2022-10-02 18:46:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.003199	 E_lr_2:0.003199
[2022-10-02 18:46:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000032	 C_lr_2:0.000032	 C_lr_3:0.000032	 C_lr_4:0.000032
[2022-10-02 18:46:29 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.030739	Loss2: 0.025934	 Dis: 3.100471 Entropy: 4.684956 
[2022-10-02 18:46:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.003124	 E_lr_2:0.003124
[2022-10-02 18:46:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000031	 C_lr_2:0.000031	 C_lr_3:0.000031	 C_lr_4:0.000031
[2022-10-02 18:46:35 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.271229	Loss2: 0.243990	 Dis: 9.212219 Entropy: 4.865101 
[2022-10-02 18:46:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.003047	 E_lr_2:0.003047
[2022-10-02 18:46:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 18:46:42 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.148822	Loss2: 0.147888	 Dis: 4.833408 Entropy: 4.497416 
[2022-10-02 18:46:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.002970	 E_lr_2:0.002970
[2022-10-02 18:46:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 18:46:48 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.146302	Loss2: 0.155184	 Dis: 5.012121 Entropy: 6.284621 
[2022-10-02 18:46:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.002893	 E_lr_2:0.002893
[2022-10-02 18:46:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000029	 C_lr_2:0.000029	 C_lr_3:0.000029	 C_lr_4:0.000029
[2022-10-02 18:46:54 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.142016	Loss2: 0.156008	 Dis: 4.096434 Entropy: 4.551094 
[2022-10-02 18:46:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.002815	 E_lr_2:0.002815
[2022-10-02 18:46:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000028	 C_lr_2:0.000028	 C_lr_3:0.000028	 C_lr_4:0.000028
[2022-10-02 18:46:59 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.093679	Loss2: 0.078423	 Dis: 5.082602 Entropy: 4.973911 
[2022-10-02 18:46:59 demo] (houston_program2.py 512): INFO E_lr_1: 0.002737	 E_lr_2:0.002737
[2022-10-02 18:46:59 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 18:47:05 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.039793	Loss2: 0.034146	 Dis: 3.060720 Entropy: 4.839814 
[2022-10-02 18:47:05 demo] (houston_program2.py 512): INFO E_lr_1: 0.002659	 E_lr_2:0.002659
[2022-10-02 18:47:05 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 18:47:11 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.026553	Loss2: 0.030232	 Dis: 5.230495 Entropy: 4.560594 
[2022-10-02 18:47:11 demo] (houston_program2.py 512): INFO E_lr_1: 0.002581	 E_lr_2:0.002581
[2022-10-02 18:47:11 demo] (houston_program2.py 513): INFO C_lr_1: 0.000026	 C_lr_2:0.000026	 C_lr_3:0.000026	 C_lr_4:0.000026
[2022-10-02 18:47:15 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.075601	Loss2: 0.064063	 Dis: 3.817337 Entropy: 5.821330 
[2022-10-02 18:47:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.002502	 E_lr_2:0.002502
[2022-10-02 18:47:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 18:47:19 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.039897	Loss2: 0.038424	 Dis: 3.018797 Entropy: 5.821264 
[2022-10-02 18:47:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.002423	 E_lr_2:0.002423
[2022-10-02 18:47:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000024	 C_lr_2:0.000024	 C_lr_3:0.000024	 C_lr_4:0.000024
[2022-10-02 18:47:25 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.144421	Loss2: 0.126881	 Dis: 5.538681 Entropy: 5.068457 
[2022-10-02 18:47:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.002345	 E_lr_2:0.002345
[2022-10-02 18:47:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 18:47:30 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.015845	Loss2: 0.015084	 Dis: 2.624804 Entropy: 6.431568 
[2022-10-02 18:47:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.002267	 E_lr_2:0.002267
[2022-10-02 18:47:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 18:47:35 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.040137	Loss2: 0.035053	 Dis: 3.339945 Entropy: 4.992315 
[2022-10-02 18:47:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.002189	 E_lr_2:0.002189
[2022-10-02 18:47:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000022	 C_lr_2:0.000022	 C_lr_3:0.000022	 C_lr_4:0.000022
[2022-10-02 18:47:41 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.012690	Loss2: 0.014142	 Dis: 1.985094 Entropy: 4.597769 
[2022-10-02 18:47:41 demo] (houston_program2.py 512): INFO E_lr_1: 0.002111	 E_lr_2:0.002111
[2022-10-02 18:47:41 demo] (houston_program2.py 513): INFO C_lr_1: 0.000021	 C_lr_2:0.000021	 C_lr_3:0.000021	 C_lr_4:0.000021
[2022-10-02 18:47:46 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.108968	Loss2: 0.132359	 Dis: 2.802279 Entropy: 4.624134 
[2022-10-02 18:47:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.002034	 E_lr_2:0.002034
[2022-10-02 18:47:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 18:47:53 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.053654	Loss2: 0.062490	 Dis: 2.347239 Entropy: 5.306866 
[2022-10-02 18:47:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.001957	 E_lr_2:0.001957
[2022-10-02 18:47:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 18:47:58 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.098704	Loss2: 0.117232	 Dis: 2.547352 Entropy: 4.282557 
[2022-10-02 18:47:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.001880	 E_lr_2:0.001880
[2022-10-02 18:47:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000019	 C_lr_2:0.000019	 C_lr_3:0.000019	 C_lr_4:0.000019
[2022-10-02 18:48:05 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.013836	Loss2: 0.017056	 Dis: 2.964718 Entropy: 4.206027 
[2022-10-02 18:48:05 demo] (houston_program2.py 512): INFO E_lr_1: 0.001804	 E_lr_2:0.001804
[2022-10-02 18:48:05 demo] (houston_program2.py 513): INFO C_lr_1: 0.000018	 C_lr_2:0.000018	 C_lr_3:0.000018	 C_lr_4:0.000018
[2022-10-02 18:48:11 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.014354	Loss2: 0.018041	 Dis: 2.174309 Entropy: 5.670219 
[2022-10-02 18:48:11 demo] (houston_program2.py 512): INFO E_lr_1: 0.001729	 E_lr_2:0.001729
[2022-10-02 18:48:11 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 18:48:17 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.058866	Loss2: 0.060824	 Dis: 3.595425 Entropy: 4.698709 
[2022-10-02 18:48:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.001655	 E_lr_2:0.001655
[2022-10-02 18:48:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 18:48:23 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.035071	Loss2: 0.035827	 Dis: 2.198805 Entropy: 4.533917 
[2022-10-02 18:48:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.001582	 E_lr_2:0.001582
[2022-10-02 18:48:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000016	 C_lr_2:0.000016	 C_lr_3:0.000016	 C_lr_4:0.000016
[2022-10-02 18:48:29 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.019381	Loss2: 0.022852	 Dis: 2.450878 Entropy: 5.256807 
[2022-10-02 18:48:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.001509	 E_lr_2:0.001509
[2022-10-02 18:48:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 18:48:35 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.012106	Loss2: 0.018248	 Dis: 2.482616 Entropy: 5.409393 
[2022-10-02 18:48:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.001437	 E_lr_2:0.001437
[2022-10-02 18:48:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 18:48:41 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.029892	Loss2: 0.025435	 Dis: 2.096672 Entropy: 4.450122 
[2022-10-02 18:48:41 demo] (houston_program2.py 512): INFO E_lr_1: 0.001367	 E_lr_2:0.001367
[2022-10-02 18:48:41 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 18:48:47 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.166982	Loss2: 0.157754	 Dis: 3.396812 Entropy: 4.662179 
[2022-10-02 18:48:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.001297	 E_lr_2:0.001297
[2022-10-02 18:48:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000013	 C_lr_2:0.000013	 C_lr_3:0.000013	 C_lr_4:0.000013
[2022-10-02 18:48:53 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.074905	Loss2: 0.066165	 Dis: 3.372700 Entropy: 4.395450 
[2022-10-02 18:48:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.001229	 E_lr_2:0.001229
[2022-10-02 18:48:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 18:49:00 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.050795	Loss2: 0.047358	 Dis: 2.701817 Entropy: 4.806371 
[2022-10-02 18:49:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.001162	 E_lr_2:0.001162
[2022-10-02 18:49:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 18:49:06 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.056534	Loss2: 0.050596	 Dis: 2.976511 Entropy: 4.133451 
[2022-10-02 18:49:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.001096	 E_lr_2:0.001096
[2022-10-02 18:49:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000011	 C_lr_2:0.000011	 C_lr_3:0.000011	 C_lr_4:0.000011
[2022-10-02 18:49:12 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.029690	Loss2: 0.035207	 Dis: 3.289547 Entropy: 4.457185 
[2022-10-02 18:49:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.001032	 E_lr_2:0.001032
[2022-10-02 18:49:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 18:49:18 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.010629	Loss2: 0.008093	 Dis: 3.081074 Entropy: 5.788464 
[2022-10-02 18:49:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.000969	 E_lr_2:0.000969
[2022-10-02 18:49:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 18:49:24 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.030252	Loss2: 0.029256	 Dis: 2.974079 Entropy: 4.409266 
[2022-10-02 18:49:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.000908	 E_lr_2:0.000908
[2022-10-02 18:49:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000009	 C_lr_2:0.000009	 C_lr_3:0.000009	 C_lr_4:0.000009
[2022-10-02 18:49:30 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.009706	Loss2: 0.009798	 Dis: 3.786009 Entropy: 4.798048 
[2022-10-02 18:49:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.000848	 E_lr_2:0.000848
[2022-10-02 18:49:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 18:49:36 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.003269	Loss2: 0.005949	 Dis: 2.694527 Entropy: 4.847728 
[2022-10-02 18:49:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.000790	 E_lr_2:0.000790
[2022-10-02 18:49:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 18:49:42 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.011252	Loss2: 0.011046	 Dis: 2.322870 Entropy: 5.255046 
[2022-10-02 18:49:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.000734	 E_lr_2:0.000734
[2022-10-02 18:49:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 18:49:47 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.005882	Loss2: 0.007648	 Dis: 1.456228 Entropy: 4.655617 
[2022-10-02 18:49:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.000679	 E_lr_2:0.000679
[2022-10-02 18:49:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 18:49:53 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.014969	Loss2: 0.014099	 Dis: 1.366701 Entropy: 4.564896 
[2022-10-02 18:49:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.000626	 E_lr_2:0.000626
[2022-10-02 18:49:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 18:50:00 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.023551	Loss2: 0.023582	 Dis: 2.121710 Entropy: 4.301121 
[2022-10-02 18:50:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.000575	 E_lr_2:0.000575
[2022-10-02 18:50:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 18:50:04 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.003148	Loss2: 0.003062	 Dis: 1.676538 Entropy: 4.234944 
[2022-10-02 18:50:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.000526	 E_lr_2:0.000526
[2022-10-02 18:50:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 18:50:10 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.003975	Loss2: 0.003585	 Dis: 2.684782 Entropy: 4.387986 
[2022-10-02 18:50:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.000479	 E_lr_2:0.000479
[2022-10-02 18:50:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 18:50:16 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.009431	Loss2: 0.011551	 Dis: 1.745871 Entropy: 4.690365 
[2022-10-02 18:50:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.000433	 E_lr_2:0.000433
[2022-10-02 18:50:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 18:50:22 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.005692	Loss2: 0.006887	 Dis: 3.956627 Entropy: 5.557862 
[2022-10-02 18:50:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.000390	 E_lr_2:0.000390
[2022-10-02 18:50:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 18:50:28 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.015192	Loss2: 0.006543	 Dis: 2.310591 Entropy: 5.540802 
[2022-10-02 18:50:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.000349	 E_lr_2:0.000349
[2022-10-02 18:50:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 18:50:34 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.003165	Loss2: 0.003936	 Dis: 2.441788 Entropy: 4.716856 
[2022-10-02 18:50:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.000310	 E_lr_2:0.000310
[2022-10-02 18:50:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 18:50:40 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.010632	Loss2: 0.009486	 Dis: 2.348808 Entropy: 4.289616 
[2022-10-02 18:50:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.000273	 E_lr_2:0.000273
[2022-10-02 18:50:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 18:50:46 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.023264	Loss2: 0.013433	 Dis: 2.324816 Entropy: 5.043529 
[2022-10-02 18:50:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.000239	 E_lr_2:0.000239
[2022-10-02 18:50:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 18:50:52 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.020302	Loss2: 0.024641	 Dis: 2.515007 Entropy: 4.679399 
[2022-10-02 18:50:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.000206	 E_lr_2:0.000206
[2022-10-02 18:50:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 18:50:59 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.003002	Loss2: 0.003077	 Dis: 2.853315 Entropy: 4.610799 
[2022-10-02 18:50:59 demo] (houston_program2.py 512): INFO E_lr_1: 0.000176	 E_lr_2:0.000176
[2022-10-02 18:50:59 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 18:51:05 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.007074	Loss2: 0.007105	 Dis: 0.894283 Entropy: 4.638364 
[2022-10-02 18:51:05 demo] (houston_program2.py 512): INFO E_lr_1: 0.000148	 E_lr_2:0.000148
[2022-10-02 18:51:05 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 18:51:11 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.005924	Loss2: 0.006748	 Dis: 1.732830 Entropy: 4.556869 
[2022-10-02 18:51:11 demo] (houston_program2.py 512): INFO E_lr_1: 0.000123	 E_lr_2:0.000123
[2022-10-02 18:51:11 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 18:51:17 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.003989	Loss2: 0.002985	 Dis: 1.099131 Entropy: 4.213593 
[2022-10-02 18:51:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.000100	 E_lr_2:0.000100
[2022-10-02 18:51:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 18:51:23 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.004126	Loss2: 0.004095	 Dis: 1.401524 Entropy: 4.976985 
[2022-10-02 18:51:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.000079	 E_lr_2:0.000079
[2022-10-02 18:51:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 18:51:29 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.007251	Loss2: 0.002927	 Dis: 1.449905 Entropy: 4.259540 
[2022-10-02 18:51:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.000061	 E_lr_2:0.000061
[2022-10-02 18:51:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 18:51:35 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.003107	Loss2: 0.003056	 Dis: 2.186892 Entropy: 4.319666 
[2022-10-02 18:51:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.000045	 E_lr_2:0.000045
[2022-10-02 18:51:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:51:40 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.004691	Loss2: 0.006694	 Dis: 1.294422 Entropy: 4.707518 
[2022-10-02 18:51:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.000031	 E_lr_2:0.000031
[2022-10-02 18:51:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:51:46 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.003899	Loss2: 0.004264	 Dis: 3.303282 Entropy: 4.450245 
[2022-10-02 18:51:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.000020	 E_lr_2:0.000020
[2022-10-02 18:51:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:51:52 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.015526	Loss2: 0.018006	 Dis: 1.209785 Entropy: 4.331844 
[2022-10-02 18:51:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.000011	 E_lr_2:0.000011
[2022-10-02 18:51:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:51:59 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.003365	Loss2: 0.004302	 Dis: 2.559256 Entropy: 5.227358 
[2022-10-02 18:51:59 demo] (houston_program2.py 512): INFO E_lr_1: 0.000005	 E_lr_2:0.000005
[2022-10-02 18:51:59 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:52:05 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.003531	Loss2: 0.002958	 Dis: 2.937002 Entropy: 4.166835 
[2022-10-02 18:52:05 demo] (houston_program2.py 512): INFO E_lr_1: 0.000001	 E_lr_2:0.000001
[2022-10-02 18:52:05 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:52:10 demo] (houston_program2.py 504): INFO Train Ep: 55 	Loss1: 0.015378	Loss2: 0.015119	 Dis: 0.958078 Entropy: 4.430855 
[2022-10-02 18:52:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.000000	 E_lr_2:0.000000
[2022-10-02 18:52:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 18:52:10 demo] (houston_program2.py 515): INFO time_55_epoch:598.6265947818756
[2022-10-02 18:52:17 demo] (houston_program2.py 673): INFO 	val_Accuracy: 27559/53200 (51.80%)	
[2022-10-02 18:52:17 demo] (utils.py 47): INFO outputs/demo/finetune_100/ckpt_epoch_55.pth saving......
[2022-10-02 18:52:18 demo] (utils.py 49): INFO outputs/demo/finetune_100/ckpt_epoch_55.pth saved !!!
[2022-10-02 18:52:18 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0116 (0.0116)	loss 0.0110 (0.0110)	grad_norm 0.0615 (0.0615)	mem 460MB
[2022-10-02 18:52:18 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:18 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0204 (0.0204)	loss 0.0102 (0.0102)	grad_norm 0.0567 (0.0567)	mem 460MB
[2022-10-02 18:52:18 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:18 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0223 (0.0223)	loss 0.0103 (0.0103)	grad_norm 0.0462 (0.0462)	mem 460MB
[2022-10-02 18:52:18 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:19 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0118 (0.0118)	loss 0.0103 (0.0103)	grad_norm 0.0723 (0.0723)	mem 460MB
[2022-10-02 18:52:19 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:19 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0222 (0.0222)	loss 0.0096 (0.0096)	grad_norm 0.0782 (0.0782)	mem 460MB
[2022-10-02 18:52:19 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:19 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0170 (0.0170)	loss 0.0092 (0.0092)	grad_norm 0.0619 (0.0619)	mem 460MB
[2022-10-02 18:52:19 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:19 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0217 (0.0217)	loss 0.0088 (0.0088)	grad_norm 0.0453 (0.0453)	mem 460MB
[2022-10-02 18:52:19 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:19 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0185 (0.0185)	loss 0.0123 (0.0123)	grad_norm 0.0675 (0.0675)	mem 460MB
[2022-10-02 18:52:19 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:19 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0219 (0.0219)	loss 0.0121 (0.0121)	grad_norm 0.0502 (0.0502)	mem 460MB
[2022-10-02 18:52:19 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:20 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0212 (0.0212)	loss 0.0096 (0.0096)	grad_norm 0.0594 (0.0594)	mem 460MB
[2022-10-02 18:52:20 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:20 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0201 (0.0201)	loss 0.0121 (0.0121)	grad_norm 0.0660 (0.0660)	mem 460MB
[2022-10-02 18:52:20 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:20 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0207 (0.0207)	loss 0.0084 (0.0084)	grad_norm 0.0602 (0.0602)	mem 460MB
[2022-10-02 18:52:20 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:20 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0205 (0.0205)	loss 0.0136 (0.0136)	grad_norm 0.0546 (0.0546)	mem 460MB
[2022-10-02 18:52:20 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:20 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0215 (0.0215)	loss 0.0103 (0.0103)	grad_norm 0.0395 (0.0395)	mem 460MB
[2022-10-02 18:52:20 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:21 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0196 (0.0196)	loss 0.0100 (0.0100)	grad_norm 0.0795 (0.0795)	mem 460MB
[2022-10-02 18:52:21 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:21 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0203 (0.0203)	loss 0.0105 (0.0105)	grad_norm 0.0600 (0.0600)	mem 460MB
[2022-10-02 18:52:21 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:21 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0253 (0.0253)	loss 0.0086 (0.0086)	grad_norm 0.0380 (0.0380)	mem 460MB
[2022-10-02 18:52:21 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:21 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0217 (0.0217)	loss 0.0080 (0.0080)	grad_norm 0.0803 (0.0803)	mem 460MB
[2022-10-02 18:52:21 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:21 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0132 (0.0132)	loss 0.0099 (0.0099)	grad_norm 0.0714 (0.0714)	mem 460MB
[2022-10-02 18:52:21 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:21 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0220 (0.0220)	loss 0.0122 (0.0122)	grad_norm 0.0863 (0.0863)	mem 460MB
[2022-10-02 18:52:21 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:22 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0129 (0.0129)	loss 0.0109 (0.0109)	grad_norm 0.0556 (0.0556)	mem 460MB
[2022-10-02 18:52:22 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:22 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0214 (0.0214)	loss 0.0091 (0.0091)	grad_norm 0.0489 (0.0489)	mem 460MB
[2022-10-02 18:52:22 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:22 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0155 (0.0155)	loss 0.0077 (0.0077)	grad_norm 0.0276 (0.0276)	mem 460MB
[2022-10-02 18:52:22 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:22 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0208 (0.0208)	loss 0.0098 (0.0098)	grad_norm 0.0428 (0.0428)	mem 460MB
[2022-10-02 18:52:22 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:22 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0154 (0.0154)	loss 0.0084 (0.0084)	grad_norm 0.0355 (0.0355)	mem 460MB
[2022-10-02 18:52:22 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:23 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0204 (0.0204)	loss 0.0081 (0.0081)	grad_norm 0.0504 (0.0504)	mem 460MB
[2022-10-02 18:52:23 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:23 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0199 (0.0199)	loss 0.0088 (0.0088)	grad_norm 0.0383 (0.0383)	mem 460MB
[2022-10-02 18:52:23 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:23 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0201 (0.0201)	loss 0.0137 (0.0137)	grad_norm 0.0574 (0.0574)	mem 460MB
[2022-10-02 18:52:23 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:23 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0235 (0.0235)	loss 0.0085 (0.0085)	grad_norm 0.0397 (0.0397)	mem 460MB
[2022-10-02 18:52:23 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:23 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0239 (0.0239)	loss 0.0099 (0.0099)	grad_norm 0.0580 (0.0580)	mem 460MB
[2022-10-02 18:52:23 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:23 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0226 (0.0226)	loss 0.0096 (0.0096)	grad_norm 0.0457 (0.0457)	mem 460MB
[2022-10-02 18:52:24 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:24 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0216 (0.0216)	loss 0.0151 (0.0151)	grad_norm 0.0862 (0.0862)	mem 460MB
[2022-10-02 18:52:24 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:24 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0245 (0.0245)	loss 0.0096 (0.0096)	grad_norm 0.0383 (0.0383)	mem 460MB
[2022-10-02 18:52:24 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:24 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0205 (0.0205)	loss 0.0119 (0.0119)	grad_norm 0.0405 (0.0405)	mem 460MB
[2022-10-02 18:52:24 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:24 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0207 (0.0207)	loss 0.0097 (0.0097)	grad_norm 0.0713 (0.0713)	mem 460MB
[2022-10-02 18:52:24 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:24 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0085 (0.0085)	loss 0.0111 (0.0111)	grad_norm 0.0315 (0.0315)	mem 460MB
[2022-10-02 18:52:24 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:24 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0096 (0.0096)	loss 0.0068 (0.0068)	grad_norm 0.0536 (0.0536)	mem 460MB
[2022-10-02 18:52:24 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:25 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0086 (0.0086)	loss 0.0077 (0.0077)	grad_norm 0.0496 (0.0496)	mem 460MB
[2022-10-02 18:52:25 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:25 demo] (houston_program2.py 243): INFO Train: [56/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0084 (0.0084)	loss 0.0109 (0.0109)	grad_norm 0.0529 (0.0529)	mem 460MB
[2022-10-02 18:52:25 demo] (houston_program2.py 252): INFO EPOCH 56 training takes 0:00:00
[2022-10-02 18:52:25 demo] (houston_program2.py 333): INFO Train Ep: 56 	Loss1: 0.058318	Loss2: 0.083604	 Dis: 2.892687 Entropy: 4.577502 
[2022-10-02 18:52:25 demo] (houston_program2.py 335): INFO time_56_epoch:7.072140216827393
[2022-10-02 18:52:25 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0120 (0.0120)	loss 0.0094 (0.0094)	grad_norm 0.0357 (0.0357)	mem 460MB
[2022-10-02 18:52:25 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:25 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0214 (0.0214)	loss 0.0106 (0.0106)	grad_norm 0.0540 (0.0540)	mem 460MB
[2022-10-02 18:52:25 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:25 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0216 (0.0216)	loss 0.0083 (0.0083)	grad_norm 0.0518 (0.0518)	mem 460MB
[2022-10-02 18:52:26 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:26 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000020	time 0.0203 (0.0203)	loss 0.0095 (0.0095)	grad_norm 0.0709 (0.0709)	mem 460MB
[2022-10-02 18:52:26 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:26 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0214 (0.0214)	loss 0.0104 (0.0104)	grad_norm 0.0718 (0.0718)	mem 460MB
[2022-10-02 18:52:26 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:26 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0216 (0.0216)	loss 0.0091 (0.0091)	grad_norm 0.0553 (0.0553)	mem 460MB
[2022-10-02 18:52:26 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:26 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0225 (0.0225)	loss 0.0106 (0.0106)	grad_norm 0.0328 (0.0328)	mem 460MB
[2022-10-02 18:52:26 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:26 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0202 (0.0202)	loss 0.0085 (0.0085)	grad_norm 0.0417 (0.0417)	mem 460MB
[2022-10-02 18:52:26 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:27 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0230 (0.0230)	loss 0.0099 (0.0099)	grad_norm 0.0401 (0.0401)	mem 460MB
[2022-10-02 18:52:27 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:27 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0219 (0.0219)	loss 0.0093 (0.0093)	grad_norm 0.0496 (0.0496)	mem 460MB
[2022-10-02 18:52:27 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:27 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0197 (0.0197)	loss 0.0144 (0.0144)	grad_norm 0.0560 (0.0560)	mem 460MB
[2022-10-02 18:52:27 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:27 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0206 (0.0206)	loss 0.0080 (0.0080)	grad_norm 0.0574 (0.0574)	mem 460MB
[2022-10-02 18:52:27 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:27 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0248 (0.0248)	loss 0.0076 (0.0076)	grad_norm 0.0476 (0.0476)	mem 460MB
[2022-10-02 18:52:27 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:27 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0217 (0.0217)	loss 0.0169 (0.0169)	grad_norm 0.0669 (0.0669)	mem 460MB
[2022-10-02 18:52:27 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:28 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0119 (0.0119)	loss 0.0132 (0.0132)	grad_norm 0.0534 (0.0534)	mem 460MB
[2022-10-02 18:52:28 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:28 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0227 (0.0227)	loss 0.0071 (0.0071)	grad_norm 0.0922 (0.0922)	mem 460MB
[2022-10-02 18:52:28 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:28 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0110 (0.0110)	loss 0.0080 (0.0080)	grad_norm 0.0549 (0.0549)	mem 460MB
[2022-10-02 18:52:28 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:28 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0218 (0.0218)	loss 0.0094 (0.0094)	grad_norm 0.0396 (0.0396)	mem 460MB
[2022-10-02 18:52:28 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:28 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0149 (0.0149)	loss 0.0183 (0.0183)	grad_norm 0.0931 (0.0931)	mem 460MB
[2022-10-02 18:52:28 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:29 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0203 (0.0203)	loss 0.0086 (0.0086)	grad_norm 0.0457 (0.0457)	mem 460MB
[2022-10-02 18:52:29 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:29 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0205 (0.0205)	loss 0.0084 (0.0084)	grad_norm 0.0567 (0.0567)	mem 460MB
[2022-10-02 18:52:29 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:29 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0202 (0.0202)	loss 0.0097 (0.0097)	grad_norm 0.0614 (0.0614)	mem 460MB
[2022-10-02 18:52:29 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:29 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0223 (0.0223)	loss 0.0113 (0.0113)	grad_norm 0.0804 (0.0804)	mem 460MB
[2022-10-02 18:52:29 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:29 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0291 (0.0291)	loss 0.0100 (0.0100)	grad_norm 0.0338 (0.0338)	mem 460MB
[2022-10-02 18:52:29 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:29 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0226 (0.0226)	loss 0.0106 (0.0106)	grad_norm 0.0548 (0.0548)	mem 460MB
[2022-10-02 18:52:30 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:30 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0149 (0.0149)	loss 0.0098 (0.0098)	grad_norm 0.0893 (0.0893)	mem 460MB
[2022-10-02 18:52:30 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:30 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0204 (0.0204)	loss 0.0074 (0.0074)	grad_norm 0.0366 (0.0366)	mem 460MB
[2022-10-02 18:52:30 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:30 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0203 (0.0203)	loss 0.0127 (0.0127)	grad_norm 0.0545 (0.0545)	mem 460MB
[2022-10-02 18:52:30 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:30 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0205 (0.0205)	loss 0.0099 (0.0099)	grad_norm 0.0590 (0.0590)	mem 460MB
[2022-10-02 18:52:30 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:30 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0185 (0.0185)	loss 0.0086 (0.0086)	grad_norm 0.0585 (0.0585)	mem 460MB
[2022-10-02 18:52:30 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:31 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0200 (0.0200)	loss 0.0086 (0.0086)	grad_norm 0.0363 (0.0363)	mem 460MB
[2022-10-02 18:52:31 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:31 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0137 (0.0137)	loss 0.0104 (0.0104)	grad_norm 0.0553 (0.0553)	mem 460MB
[2022-10-02 18:52:31 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:31 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0210 (0.0210)	loss 0.0087 (0.0087)	grad_norm 0.0518 (0.0518)	mem 460MB
[2022-10-02 18:52:31 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:31 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0140 (0.0140)	loss 0.0102 (0.0102)	grad_norm 0.0382 (0.0382)	mem 460MB
[2022-10-02 18:52:31 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:31 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0208 (0.0208)	loss 0.0086 (0.0086)	grad_norm 0.0428 (0.0428)	mem 460MB
[2022-10-02 18:52:31 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:31 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0083 (0.0083)	loss 0.0086 (0.0086)	grad_norm 0.0662 (0.0662)	mem 460MB
[2022-10-02 18:52:31 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:32 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0100 (0.0100)	loss 0.0079 (0.0079)	grad_norm 0.0532 (0.0532)	mem 460MB
[2022-10-02 18:52:32 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:32 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0082 (0.0082)	loss 0.0075 (0.0075)	grad_norm 0.0363 (0.0363)	mem 460MB
[2022-10-02 18:52:32 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:32 demo] (houston_program2.py 243): INFO Train: [57/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0084 (0.0084)	loss 0.0132 (0.0132)	grad_norm 0.0603 (0.0603)	mem 460MB
[2022-10-02 18:52:32 demo] (houston_program2.py 252): INFO EPOCH 57 training takes 0:00:00
[2022-10-02 18:52:32 demo] (houston_program2.py 333): INFO Train Ep: 57 	Loss1: 0.036972	Loss2: 0.036593	 Dis: 3.793261 Entropy: 4.774819 
[2022-10-02 18:52:32 demo] (houston_program2.py 335): INFO time_57_epoch:7.120923042297363
[2022-10-02 18:52:32 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0113 (0.0113)	loss 0.0103 (0.0103)	grad_norm 0.0444 (0.0444)	mem 460MB
[2022-10-02 18:52:32 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:32 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0224 (0.0224)	loss 0.0080 (0.0080)	grad_norm 0.0620 (0.0620)	mem 460MB
[2022-10-02 18:52:32 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:33 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0226 (0.0226)	loss 0.0099 (0.0099)	grad_norm 0.0351 (0.0351)	mem 460MB
[2022-10-02 18:52:33 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:33 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0208 (0.0208)	loss 0.0091 (0.0091)	grad_norm 0.0939 (0.0939)	mem 460MB
[2022-10-02 18:52:33 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:33 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0219 (0.0219)	loss 0.0095 (0.0095)	grad_norm 0.0686 (0.0686)	mem 460MB
[2022-10-02 18:52:33 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:33 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0156 (0.0156)	loss 0.0074 (0.0074)	grad_norm 0.0324 (0.0324)	mem 460MB
[2022-10-02 18:52:33 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:33 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0222 (0.0222)	loss 0.0116 (0.0116)	grad_norm 0.0487 (0.0487)	mem 460MB
[2022-10-02 18:52:33 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:33 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0203 (0.0203)	loss 0.0130 (0.0130)	grad_norm 0.0809 (0.0809)	mem 460MB
[2022-10-02 18:52:34 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:34 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0214 (0.0214)	loss 0.0156 (0.0156)	grad_norm 0.0712 (0.0712)	mem 460MB
[2022-10-02 18:52:34 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:34 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0215 (0.0215)	loss 0.0078 (0.0078)	grad_norm 0.0360 (0.0360)	mem 460MB
[2022-10-02 18:52:34 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:34 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0181 (0.0181)	loss 0.0094 (0.0094)	grad_norm 0.0387 (0.0387)	mem 460MB
[2022-10-02 18:52:34 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:34 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0208 (0.0208)	loss 0.0100 (0.0100)	grad_norm 0.0653 (0.0653)	mem 460MB
[2022-10-02 18:52:34 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:34 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0257 (0.0257)	loss 0.0080 (0.0080)	grad_norm 0.0318 (0.0318)	mem 460MB
[2022-10-02 18:52:34 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:35 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0247 (0.0247)	loss 0.0077 (0.0077)	grad_norm 0.0363 (0.0363)	mem 460MB
[2022-10-02 18:52:35 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:35 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0152 (0.0152)	loss 0.0076 (0.0076)	grad_norm 0.0292 (0.0292)	mem 460MB
[2022-10-02 18:52:35 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:35 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000019	time 0.0241 (0.0241)	loss 0.0103 (0.0103)	grad_norm 0.0612 (0.0612)	mem 460MB
[2022-10-02 18:52:35 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:35 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0211 (0.0211)	loss 0.0087 (0.0087)	grad_norm 0.0491 (0.0491)	mem 460MB
[2022-10-02 18:52:35 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:35 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0238 (0.0238)	loss 0.0112 (0.0112)	grad_norm 0.0488 (0.0488)	mem 460MB
[2022-10-02 18:52:35 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:36 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0238 (0.0238)	loss 0.0093 (0.0093)	grad_norm 0.0578 (0.0578)	mem 460MB
[2022-10-02 18:52:36 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:36 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0133 (0.0133)	loss 0.0085 (0.0085)	grad_norm 0.0577 (0.0577)	mem 460MB
[2022-10-02 18:52:36 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:36 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0252 (0.0252)	loss 0.0075 (0.0075)	grad_norm 0.0386 (0.0386)	mem 460MB
[2022-10-02 18:52:36 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:36 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0149 (0.0149)	loss 0.0118 (0.0118)	grad_norm 0.0545 (0.0545)	mem 460MB
[2022-10-02 18:52:36 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:36 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0220 (0.0220)	loss 0.0112 (0.0112)	grad_norm 0.0629 (0.0629)	mem 460MB
[2022-10-02 18:52:36 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:37 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0212 (0.0212)	loss 0.0098 (0.0098)	grad_norm 0.0384 (0.0384)	mem 460MB
[2022-10-02 18:52:37 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:37 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0206 (0.0206)	loss 0.0106 (0.0106)	grad_norm 0.0387 (0.0387)	mem 460MB
[2022-10-02 18:52:37 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:37 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0214 (0.0214)	loss 0.0082 (0.0082)	grad_norm 0.0415 (0.0415)	mem 460MB
[2022-10-02 18:52:37 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:37 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0213 (0.0213)	loss 0.0121 (0.0121)	grad_norm 0.0508 (0.0508)	mem 460MB
[2022-10-02 18:52:37 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:37 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0225 (0.0225)	loss 0.0117 (0.0117)	grad_norm 0.0385 (0.0385)	mem 460MB
[2022-10-02 18:52:37 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:37 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0264 (0.0264)	loss 0.0083 (0.0083)	grad_norm 0.0492 (0.0492)	mem 460MB
[2022-10-02 18:52:37 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:38 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0194 (0.0194)	loss 0.0088 (0.0088)	grad_norm 0.0610 (0.0610)	mem 460MB
[2022-10-02 18:52:38 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:38 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0147 (0.0147)	loss 0.0092 (0.0092)	grad_norm 0.0682 (0.0682)	mem 460MB
[2022-10-02 18:52:38 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:38 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0215 (0.0215)	loss 0.0090 (0.0090)	grad_norm 0.0441 (0.0441)	mem 460MB
[2022-10-02 18:52:38 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:38 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0123 (0.0123)	loss 0.0080 (0.0080)	grad_norm 0.0439 (0.0439)	mem 460MB
[2022-10-02 18:52:38 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:38 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0207 (0.0207)	loss 0.0111 (0.0111)	grad_norm 0.0352 (0.0352)	mem 460MB
[2022-10-02 18:52:38 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:39 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0145 (0.0145)	loss 0.0097 (0.0097)	grad_norm 0.0357 (0.0357)	mem 460MB
[2022-10-02 18:52:39 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:39 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0219 (0.0219)	loss 0.0111 (0.0111)	grad_norm 0.0565 (0.0565)	mem 460MB
[2022-10-02 18:52:39 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:39 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0179 (0.0179)	loss 0.0075 (0.0075)	grad_norm 0.0374 (0.0374)	mem 460MB
[2022-10-02 18:52:39 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:39 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0222 (0.0222)	loss 0.0085 (0.0085)	grad_norm 0.0327 (0.0327)	mem 460MB
[2022-10-02 18:52:39 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:39 demo] (houston_program2.py 243): INFO Train: [58/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0200 (0.0200)	loss 0.0072 (0.0072)	grad_norm 0.0392 (0.0392)	mem 460MB
[2022-10-02 18:52:39 demo] (houston_program2.py 252): INFO EPOCH 58 training takes 0:00:00
[2022-10-02 18:52:40 demo] (houston_program2.py 333): INFO Train Ep: 58 	Loss1: 0.040370	Loss2: 0.019783	 Dis: 4.076065 Entropy: 5.592467 
[2022-10-02 18:52:40 demo] (houston_program2.py 335): INFO time_58_epoch:7.642518758773804
[2022-10-02 18:52:40 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0145 (0.0145)	loss 0.0077 (0.0077)	grad_norm 0.0473 (0.0473)	mem 460MB
[2022-10-02 18:52:40 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:40 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0214 (0.0214)	loss 0.0097 (0.0097)	grad_norm 0.0409 (0.0409)	mem 460MB
[2022-10-02 18:52:40 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:40 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0243 (0.0243)	loss 0.0083 (0.0083)	grad_norm 0.0425 (0.0425)	mem 460MB
[2022-10-02 18:52:40 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:40 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0210 (0.0210)	loss 0.0079 (0.0079)	grad_norm 0.0372 (0.0372)	mem 460MB
[2022-10-02 18:52:40 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:41 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0242 (0.0242)	loss 0.0112 (0.0112)	grad_norm 0.0528 (0.0528)	mem 460MB
[2022-10-02 18:52:41 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:41 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0236 (0.0236)	loss 0.0072 (0.0072)	grad_norm 0.0356 (0.0356)	mem 460MB
[2022-10-02 18:52:41 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:41 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0264 (0.0264)	loss 0.0119 (0.0119)	grad_norm 0.0421 (0.0421)	mem 460MB
[2022-10-02 18:52:41 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:41 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0146 (0.0146)	loss 0.0099 (0.0099)	grad_norm 0.0583 (0.0583)	mem 460MB
[2022-10-02 18:52:41 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:41 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0263 (0.0263)	loss 0.0106 (0.0106)	grad_norm 0.0458 (0.0458)	mem 460MB
[2022-10-02 18:52:41 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:42 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0190 (0.0190)	loss 0.0107 (0.0107)	grad_norm 0.0472 (0.0472)	mem 460MB
[2022-10-02 18:52:42 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:42 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0223 (0.0223)	loss 0.0110 (0.0110)	grad_norm 0.0529 (0.0529)	mem 460MB
[2022-10-02 18:52:42 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:42 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0234 (0.0234)	loss 0.0088 (0.0088)	grad_norm 0.0449 (0.0449)	mem 460MB
[2022-10-02 18:52:42 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:42 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0268 (0.0268)	loss 0.0081 (0.0081)	grad_norm 0.0426 (0.0426)	mem 460MB
[2022-10-02 18:52:42 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:42 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0271 (0.0271)	loss 0.0117 (0.0117)	grad_norm 0.0520 (0.0520)	mem 460MB
[2022-10-02 18:52:42 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:43 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0143 (0.0143)	loss 0.0119 (0.0119)	grad_norm 0.0388 (0.0388)	mem 460MB
[2022-10-02 18:52:43 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:43 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0265 (0.0265)	loss 0.0104 (0.0104)	grad_norm 0.0513 (0.0513)	mem 460MB
[2022-10-02 18:52:43 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:43 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0215 (0.0215)	loss 0.0068 (0.0068)	grad_norm 0.0491 (0.0491)	mem 460MB
[2022-10-02 18:52:43 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:43 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0222 (0.0222)	loss 0.0081 (0.0081)	grad_norm 0.0402 (0.0402)	mem 460MB
[2022-10-02 18:52:43 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:43 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0198 (0.0198)	loss 0.0106 (0.0106)	grad_norm 0.0982 (0.0982)	mem 460MB
[2022-10-02 18:52:43 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:43 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0218 (0.0218)	loss 0.0098 (0.0098)	grad_norm 0.0743 (0.0743)	mem 460MB
[2022-10-02 18:52:44 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:44 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0209 (0.0209)	loss 0.0092 (0.0092)	grad_norm 0.0576 (0.0576)	mem 460MB
[2022-10-02 18:52:44 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:44 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0218 (0.0218)	loss 0.0139 (0.0139)	grad_norm 0.0446 (0.0446)	mem 460MB
[2022-10-02 18:52:44 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:44 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0167 (0.0167)	loss 0.0068 (0.0068)	grad_norm 0.0640 (0.0640)	mem 460MB
[2022-10-02 18:52:44 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:44 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0228 (0.0228)	loss 0.0079 (0.0079)	grad_norm 0.0495 (0.0495)	mem 460MB
[2022-10-02 18:52:44 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:44 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0219 (0.0219)	loss 0.0082 (0.0082)	grad_norm 0.0375 (0.0375)	mem 460MB
[2022-10-02 18:52:44 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:45 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0113 (0.0113)	loss 0.0080 (0.0080)	grad_norm 0.0398 (0.0398)	mem 460MB
[2022-10-02 18:52:45 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:45 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0208 (0.0208)	loss 0.0075 (0.0075)	grad_norm 0.0430 (0.0430)	mem 460MB
[2022-10-02 18:52:45 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:45 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0122 (0.0122)	loss 0.0082 (0.0082)	grad_norm 0.0467 (0.0467)	mem 460MB
[2022-10-02 18:52:45 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:45 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000018	time 0.0203 (0.0203)	loss 0.0126 (0.0126)	grad_norm 0.0437 (0.0437)	mem 460MB
[2022-10-02 18:52:45 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:45 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0094 (0.0094)	loss 0.0096 (0.0096)	grad_norm 0.0450 (0.0450)	mem 460MB
[2022-10-02 18:52:45 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:45 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0219 (0.0219)	loss 0.0086 (0.0086)	grad_norm 0.0318 (0.0318)	mem 460MB
[2022-10-02 18:52:45 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:46 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0104 (0.0104)	loss 0.0079 (0.0079)	grad_norm 0.0458 (0.0458)	mem 460MB
[2022-10-02 18:52:46 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:46 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0212 (0.0212)	loss 0.0083 (0.0083)	grad_norm 0.0552 (0.0552)	mem 460MB
[2022-10-02 18:52:46 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:46 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0156 (0.0156)	loss 0.0109 (0.0109)	grad_norm 0.0424 (0.0424)	mem 460MB
[2022-10-02 18:52:46 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:46 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0206 (0.0206)	loss 0.0117 (0.0117)	grad_norm 0.0394 (0.0394)	mem 460MB
[2022-10-02 18:52:46 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:46 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0213 (0.0213)	loss 0.0101 (0.0101)	grad_norm 0.0504 (0.0504)	mem 460MB
[2022-10-02 18:52:46 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:47 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0221 (0.0221)	loss 0.0117 (0.0117)	grad_norm 0.0923 (0.0923)	mem 460MB
[2022-10-02 18:52:47 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:47 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0209 (0.0209)	loss 0.0078 (0.0078)	grad_norm 0.0364 (0.0364)	mem 460MB
[2022-10-02 18:52:47 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:47 demo] (houston_program2.py 243): INFO Train: [59/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0219 (0.0219)	loss 0.0092 (0.0092)	grad_norm 0.0719 (0.0719)	mem 460MB
[2022-10-02 18:52:47 demo] (houston_program2.py 252): INFO EPOCH 59 training takes 0:00:00
[2022-10-02 18:52:47 demo] (houston_program2.py 333): INFO Train Ep: 59 	Loss1: 0.162775	Loss2: 0.154026	 Dis: 2.664520 Entropy: 5.414789 
[2022-10-02 18:52:47 demo] (houston_program2.py 335): INFO time_59_epoch:7.572575330734253
[2022-10-02 18:52:47 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0221 (0.0221)	loss 0.0084 (0.0084)	grad_norm 0.0706 (0.0706)	mem 460MB
[2022-10-02 18:52:48 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:48 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0190 (0.0190)	loss 0.0100 (0.0100)	grad_norm 0.0369 (0.0369)	mem 460MB
[2022-10-02 18:52:48 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:48 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0221 (0.0221)	loss 0.0084 (0.0084)	grad_norm 0.0458 (0.0458)	mem 460MB
[2022-10-02 18:52:48 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:48 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0197 (0.0197)	loss 0.0084 (0.0084)	grad_norm 0.0315 (0.0315)	mem 460MB
[2022-10-02 18:52:48 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:48 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0151 (0.0151)	loss 0.0152 (0.0152)	grad_norm 0.0699 (0.0699)	mem 460MB
[2022-10-02 18:52:48 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:48 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0216 (0.0216)	loss 0.0092 (0.0092)	grad_norm 0.0460 (0.0460)	mem 460MB
[2022-10-02 18:52:48 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:49 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0091 (0.0091)	loss 0.0076 (0.0076)	grad_norm 0.0308 (0.0308)	mem 460MB
[2022-10-02 18:52:49 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:49 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0243 (0.0243)	loss 0.0089 (0.0089)	grad_norm 0.0291 (0.0291)	mem 460MB
[2022-10-02 18:52:49 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:49 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0221 (0.0221)	loss 0.0073 (0.0073)	grad_norm 0.0647 (0.0647)	mem 460MB
[2022-10-02 18:52:49 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:49 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0212 (0.0212)	loss 0.0114 (0.0114)	grad_norm 0.0495 (0.0495)	mem 460MB
[2022-10-02 18:52:49 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:49 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0230 (0.0230)	loss 0.0120 (0.0120)	grad_norm 0.0327 (0.0327)	mem 460MB
[2022-10-02 18:52:49 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:50 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0224 (0.0224)	loss 0.0097 (0.0097)	grad_norm 0.0472 (0.0472)	mem 460MB
[2022-10-02 18:52:50 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:50 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0218 (0.0218)	loss 0.0094 (0.0094)	grad_norm 0.0440 (0.0440)	mem 460MB
[2022-10-02 18:52:50 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:50 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0139 (0.0139)	loss 0.0160 (0.0160)	grad_norm 0.0368 (0.0368)	mem 460MB
[2022-10-02 18:52:50 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:50 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0200 (0.0200)	loss 0.0121 (0.0121)	grad_norm 0.0518 (0.0518)	mem 460MB
[2022-10-02 18:52:50 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:50 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0140 (0.0140)	loss 0.0077 (0.0077)	grad_norm 0.0501 (0.0501)	mem 460MB
[2022-10-02 18:52:50 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:50 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0201 (0.0201)	loss 0.0088 (0.0088)	grad_norm 0.0603 (0.0603)	mem 460MB
[2022-10-02 18:52:51 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:51 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0157 (0.0157)	loss 0.0109 (0.0109)	grad_norm 0.0620 (0.0620)	mem 460MB
[2022-10-02 18:52:51 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:51 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0198 (0.0198)	loss 0.0103 (0.0103)	grad_norm 0.0631 (0.0631)	mem 460MB
[2022-10-02 18:52:51 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:51 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0180 (0.0180)	loss 0.0098 (0.0098)	grad_norm 0.0410 (0.0410)	mem 460MB
[2022-10-02 18:52:51 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:51 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0198 (0.0198)	loss 0.0108 (0.0108)	grad_norm 0.0569 (0.0569)	mem 460MB
[2022-10-02 18:52:51 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:51 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0205 (0.0205)	loss 0.0099 (0.0099)	grad_norm 0.0815 (0.0815)	mem 460MB
[2022-10-02 18:52:51 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:52 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0207 (0.0207)	loss 0.0111 (0.0111)	grad_norm 0.0635 (0.0635)	mem 460MB
[2022-10-02 18:52:52 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:52 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0204 (0.0204)	loss 0.0085 (0.0085)	grad_norm 0.0542 (0.0542)	mem 460MB
[2022-10-02 18:52:52 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:52 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0199 (0.0199)	loss 0.0083 (0.0083)	grad_norm 0.0386 (0.0386)	mem 460MB
[2022-10-02 18:52:52 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:52 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0142 (0.0142)	loss 0.0098 (0.0098)	grad_norm 0.0390 (0.0390)	mem 460MB
[2022-10-02 18:52:52 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:52 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0214 (0.0214)	loss 0.0097 (0.0097)	grad_norm 0.0460 (0.0460)	mem 460MB
[2022-10-02 18:52:52 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:52 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0203 (0.0203)	loss 0.0127 (0.0127)	grad_norm 0.0718 (0.0718)	mem 460MB
[2022-10-02 18:52:52 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:53 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0205 (0.0205)	loss 0.0083 (0.0083)	grad_norm 0.0311 (0.0311)	mem 460MB
[2022-10-02 18:52:53 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:53 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0212 (0.0212)	loss 0.0084 (0.0084)	grad_norm 0.0452 (0.0452)	mem 460MB
[2022-10-02 18:52:53 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:53 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0205 (0.0205)	loss 0.0080 (0.0080)	grad_norm 0.0445 (0.0445)	mem 460MB
[2022-10-02 18:52:53 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:53 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0230 (0.0230)	loss 0.0090 (0.0090)	grad_norm 0.0444 (0.0444)	mem 460MB
[2022-10-02 18:52:53 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:53 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0129 (0.0129)	loss 0.0078 (0.0078)	grad_norm 0.0390 (0.0390)	mem 460MB
[2022-10-02 18:52:53 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:54 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0206 (0.0206)	loss 0.0085 (0.0085)	grad_norm 0.0523 (0.0523)	mem 460MB
[2022-10-02 18:52:54 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:54 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0160 (0.0160)	loss 0.0083 (0.0083)	grad_norm 0.0361 (0.0361)	mem 460MB
[2022-10-02 18:52:54 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:54 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0269 (0.0269)	loss 0.0077 (0.0077)	grad_norm 0.0565 (0.0565)	mem 460MB
[2022-10-02 18:52:54 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:54 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0232 (0.0232)	loss 0.0116 (0.0116)	grad_norm 0.0578 (0.0578)	mem 460MB
[2022-10-02 18:52:54 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:54 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0272 (0.0272)	loss 0.0082 (0.0082)	grad_norm 0.0558 (0.0558)	mem 460MB
[2022-10-02 18:52:54 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:55 demo] (houston_program2.py 243): INFO Train: [60/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0261 (0.0261)	loss 0.0112 (0.0112)	grad_norm 0.0514 (0.0514)	mem 460MB
[2022-10-02 18:52:55 demo] (houston_program2.py 252): INFO EPOCH 60 training takes 0:00:00
[2022-10-02 18:52:55 demo] (houston_program2.py 333): INFO Train Ep: 60 	Loss1: 0.137670	Loss2: 0.117191	 Dis: 3.775379 Entropy: 4.447982 
[2022-10-02 18:52:55 demo] (houston_program2.py 335): INFO time_60_epoch:7.738866567611694
[2022-10-02 18:52:55 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 18:52:55 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 18:52:55 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 18:52:55 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 18:52:55 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 18:52:55 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 18:52:55 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 18:52:55 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 18:52:55 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 18:52:55 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 18:53:01 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.102833	Loss2: 0.092278	 Dis: 4.050423 Entropy: 4.222451 
[2022-10-02 18:53:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.000487	 E_lr_2:0.000487
[2022-10-02 18:53:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 18:53:07 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.063285	Loss2: 0.062592	 Dis: 3.386665 Entropy: 4.399026 
[2022-10-02 18:53:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.000987	 E_lr_2:0.000987
[2022-10-02 18:53:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 18:53:14 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.026812	Loss2: 0.027331	 Dis: 4.071560 Entropy: 5.326269 
[2022-10-02 18:53:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.001487	 E_lr_2:0.001487
[2022-10-02 18:53:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 18:53:20 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.035229	Loss2: 0.045037	 Dis: 4.384045 Entropy: 4.590982 
[2022-10-02 18:53:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.001987	 E_lr_2:0.001987
[2022-10-02 18:53:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 18:53:26 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.112770	Loss2: 0.107116	 Dis: 2.490988 Entropy: 5.785822 
[2022-10-02 18:53:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.002487	 E_lr_2:0.002487
[2022-10-02 18:53:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 18:53:32 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.145867	Loss2: 0.142549	 Dis: 4.581167 Entropy: 4.819264 
[2022-10-02 18:53:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.002987	 E_lr_2:0.002987
[2022-10-02 18:53:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 18:53:38 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.254161	Loss2: 0.296623	 Dis: 5.404890 Entropy: 5.330930 
[2022-10-02 18:53:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.003487	 E_lr_2:0.003487
[2022-10-02 18:53:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 18:53:44 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.277895	Loss2: 0.308032	 Dis: 4.974964 Entropy: 5.832571 
[2022-10-02 18:53:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.003987	 E_lr_2:0.003987
[2022-10-02 18:53:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 18:53:50 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.178377	Loss2: 0.185867	 Dis: 5.478783 Entropy: 5.749417 
[2022-10-02 18:53:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.004487	 E_lr_2:0.004487
[2022-10-02 18:53:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 18:53:56 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.169036	Loss2: 0.149684	 Dis: 4.988737 Entropy: 5.331619 
[2022-10-02 18:53:56 demo] (houston_program2.py 512): INFO E_lr_1: 0.004987	 E_lr_2:0.004987
[2022-10-02 18:53:56 demo] (houston_program2.py 513): INFO C_lr_1: 0.000050	 C_lr_2:0.000050	 C_lr_3:0.000050	 C_lr_4:0.000050
[2022-10-02 18:54:01 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.034665	Loss2: 0.075341	 Dis: 5.253511 Entropy: 6.124475 
[2022-10-02 18:54:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.004853	 E_lr_2:0.004853
[2022-10-02 18:54:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000049	 C_lr_2:0.000049	 C_lr_3:0.000049	 C_lr_4:0.000049
[2022-10-02 18:54:07 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.077843	Loss2: 0.082542	 Dis: 5.529221 Entropy: 4.769674 
[2022-10-02 18:54:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.004825	 E_lr_2:0.004825
[2022-10-02 18:54:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 18:54:13 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.076409	Loss2: 0.079637	 Dis: 6.111494 Entropy: 4.860869 
[2022-10-02 18:54:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.004795	 E_lr_2:0.004795
[2022-10-02 18:54:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 18:54:19 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.201976	Loss2: 0.188041	 Dis: 5.880634 Entropy: 4.733832 
[2022-10-02 18:54:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.004763	 E_lr_2:0.004763
[2022-10-02 18:54:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 18:54:25 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.155959	Loss2: 0.166602	 Dis: 3.974487 Entropy: 4.783555 
[2022-10-02 18:54:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.004728	 E_lr_2:0.004728
[2022-10-02 18:54:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 18:54:31 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.074554	Loss2: 0.075828	 Dis: 5.083862 Entropy: 5.570803 
[2022-10-02 18:54:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.004692	 E_lr_2:0.004692
[2022-10-02 18:54:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 18:54:37 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.137333	Loss2: 0.147825	 Dis: 4.398388 Entropy: 4.786309 
[2022-10-02 18:54:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.004653	 E_lr_2:0.004653
[2022-10-02 18:54:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 18:54:42 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.065808	Loss2: 0.061072	 Dis: 3.552507 Entropy: 4.752007 
[2022-10-02 18:54:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.004612	 E_lr_2:0.004612
[2022-10-02 18:54:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 18:54:48 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.089849	Loss2: 0.075771	 Dis: 5.324312 Entropy: 5.345949 
[2022-10-02 18:54:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.004569	 E_lr_2:0.004569
[2022-10-02 18:54:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 18:54:54 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.109458	Loss2: 0.091294	 Dis: 4.530317 Entropy: 4.519938 
[2022-10-02 18:54:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.004524	 E_lr_2:0.004524
[2022-10-02 18:54:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 18:55:00 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.052312	Loss2: 0.086922	 Dis: 5.866295 Entropy: 5.531281 
[2022-10-02 18:55:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.004477	 E_lr_2:0.004477
[2022-10-02 18:55:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 18:55:06 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.099857	Loss2: 0.101073	 Dis: 6.220659 Entropy: 5.171317 
[2022-10-02 18:55:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.004428	 E_lr_2:0.004428
[2022-10-02 18:55:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 18:55:12 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.044359	Loss2: 0.041563	 Dis: 2.991886 Entropy: 5.416292 
[2022-10-02 18:55:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.004377	 E_lr_2:0.004377
[2022-10-02 18:55:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 18:55:18 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.162533	Loss2: 0.185702	 Dis: 3.754940 Entropy: 5.629004 
[2022-10-02 18:55:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.004324	 E_lr_2:0.004324
[2022-10-02 18:55:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 18:55:24 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.090293	Loss2: 0.080340	 Dis: 5.805416 Entropy: 5.112707 
[2022-10-02 18:55:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.004269	 E_lr_2:0.004269
[2022-10-02 18:55:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 18:55:30 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.070966	Loss2: 0.069086	 Dis: 3.358454 Entropy: 5.299597 
[2022-10-02 18:55:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.004213	 E_lr_2:0.004213
[2022-10-02 18:55:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 18:55:36 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.053108	Loss2: 0.052429	 Dis: 4.734360 Entropy: 4.448221 
[2022-10-02 18:55:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.004155	 E_lr_2:0.004155
[2022-10-02 18:55:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 18:55:42 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.132171	Loss2: 0.165912	 Dis: 4.686199 Entropy: 4.732984 
[2022-10-02 18:55:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.004095	 E_lr_2:0.004095
[2022-10-02 18:55:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000041	 C_lr_2:0.000041	 C_lr_3:0.000041	 C_lr_4:0.000041
[2022-10-02 18:55:48 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.069804	Loss2: 0.052714	 Dis: 2.269108 Entropy: 5.820517 
[2022-10-02 18:55:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.004034	 E_lr_2:0.004034
[2022-10-02 18:55:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 18:55:54 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.103455	Loss2: 0.099474	 Dis: 3.245701 Entropy: 4.549968 
[2022-10-02 18:55:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.003971	 E_lr_2:0.003971
[2022-10-02 18:55:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 18:56:00 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.026448	Loss2: 0.042045	 Dis: 4.077803 Entropy: 5.684945 
[2022-10-02 18:56:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.003907	 E_lr_2:0.003907
[2022-10-02 18:56:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000039	 C_lr_2:0.000039	 C_lr_3:0.000039	 C_lr_4:0.000039
[2022-10-02 18:56:07 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.018052	Loss2: 0.022279	 Dis: 3.735886 Entropy: 6.565974 
[2022-10-02 18:56:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.003841	 E_lr_2:0.003841
[2022-10-02 18:56:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 18:56:13 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.061953	Loss2: 0.040208	 Dis: 3.531649 Entropy: 4.385240 
[2022-10-02 18:56:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.003774	 E_lr_2:0.003774
[2022-10-02 18:56:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 18:56:17 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.037580	Loss2: 0.041958	 Dis: 3.440174 Entropy: 5.519913 
[2022-10-02 18:56:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.003706	 E_lr_2:0.003706
[2022-10-02 18:56:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000037	 C_lr_2:0.000037	 C_lr_3:0.000037	 C_lr_4:0.000037
[2022-10-02 18:56:23 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.093326	Loss2: 0.089434	 Dis: 4.141960 Entropy: 7.200191 
[2022-10-02 18:56:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.003637	 E_lr_2:0.003637
[2022-10-02 18:56:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 18:56:30 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.035051	Loss2: 0.039460	 Dis: 5.569359 Entropy: 5.339219 
[2022-10-02 18:56:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.003566	 E_lr_2:0.003566
[2022-10-02 18:56:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 18:56:36 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.077487	Loss2: 0.055241	 Dis: 2.879673 Entropy: 4.622635 
[2022-10-02 18:56:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.003495	 E_lr_2:0.003495
[2022-10-02 18:56:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 18:56:42 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.018292	Loss2: 0.035327	 Dis: 5.261650 Entropy: 4.825958 
[2022-10-02 18:56:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.003422	 E_lr_2:0.003422
[2022-10-02 18:56:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000034	 C_lr_2:0.000034	 C_lr_3:0.000034	 C_lr_4:0.000034
[2022-10-02 18:56:48 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.047607	Loss2: 0.048299	 Dis: 2.073517 Entropy: 5.168027 
[2022-10-02 18:56:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.003349	 E_lr_2:0.003349
[2022-10-02 18:56:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 18:56:54 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.132623	Loss2: 0.119821	 Dis: 5.965584 Entropy: 4.843778 
[2022-10-02 18:56:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.003274	 E_lr_2:0.003274
[2022-10-02 18:56:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 18:57:00 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.056855	Loss2: 0.027249	 Dis: 3.204348 Entropy: 5.689238 
[2022-10-02 18:57:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.003199	 E_lr_2:0.003199
[2022-10-02 18:57:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000032	 C_lr_2:0.000032	 C_lr_3:0.000032	 C_lr_4:0.000032
[2022-10-02 18:57:06 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.327294	Loss2: 0.284211	 Dis: 4.724995 Entropy: 5.703880 
[2022-10-02 18:57:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.003124	 E_lr_2:0.003124
[2022-10-02 18:57:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000031	 C_lr_2:0.000031	 C_lr_3:0.000031	 C_lr_4:0.000031
[2022-10-02 18:57:13 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.068614	Loss2: 0.061530	 Dis: 2.337370 Entropy: 4.376616 
[2022-10-02 18:57:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.003047	 E_lr_2:0.003047
[2022-10-02 18:57:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 18:57:19 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.009681	Loss2: 0.011968	 Dis: 3.882057 Entropy: 6.215651 
[2022-10-02 18:57:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.002970	 E_lr_2:0.002970
[2022-10-02 18:57:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 18:57:25 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.055230	Loss2: 0.086536	 Dis: 3.338266 Entropy: 4.590204 
[2022-10-02 18:57:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.002893	 E_lr_2:0.002893
[2022-10-02 18:57:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000029	 C_lr_2:0.000029	 C_lr_3:0.000029	 C_lr_4:0.000029
[2022-10-02 18:57:31 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.072534	Loss2: 0.063965	 Dis: 3.031591 Entropy: 5.203804 
[2022-10-02 18:57:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.002815	 E_lr_2:0.002815
[2022-10-02 18:57:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000028	 C_lr_2:0.000028	 C_lr_3:0.000028	 C_lr_4:0.000028
[2022-10-02 18:57:37 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.018231	Loss2: 0.032251	 Dis: 3.453781 Entropy: 6.312906 
[2022-10-02 18:57:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.002737	 E_lr_2:0.002737
[2022-10-02 18:57:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 18:57:43 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.115423	Loss2: 0.094913	 Dis: 5.881443 Entropy: 4.763996 
[2022-10-02 18:57:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.002659	 E_lr_2:0.002659
[2022-10-02 18:57:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 18:57:49 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.032473	Loss2: 0.033163	 Dis: 4.191248 Entropy: 5.974250 
[2022-10-02 18:57:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.002581	 E_lr_2:0.002581
[2022-10-02 18:57:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000026	 C_lr_2:0.000026	 C_lr_3:0.000026	 C_lr_4:0.000026
[2022-10-02 18:57:55 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.055448	Loss2: 0.037422	 Dis: 4.487537 Entropy: 4.626546 
[2022-10-02 18:57:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.002502	 E_lr_2:0.002502
[2022-10-02 18:57:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 18:58:01 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.032872	Loss2: 0.033792	 Dis: 3.130901 Entropy: 4.755335 
[2022-10-02 18:58:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.002423	 E_lr_2:0.002423
[2022-10-02 18:58:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000024	 C_lr_2:0.000024	 C_lr_3:0.000024	 C_lr_4:0.000024
[2022-10-02 18:58:07 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.029764	Loss2: 0.036044	 Dis: 3.196804 Entropy: 4.546860 
[2022-10-02 18:58:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.002345	 E_lr_2:0.002345
[2022-10-02 18:58:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 18:58:14 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.050870	Loss2: 0.059213	 Dis: 4.155849 Entropy: 5.205578 
[2022-10-02 18:58:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.002267	 E_lr_2:0.002267
[2022-10-02 18:58:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 18:58:20 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.078677	Loss2: 0.088700	 Dis: 2.138508 Entropy: 5.477777 
[2022-10-02 18:58:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.002189	 E_lr_2:0.002189
[2022-10-02 18:58:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000022	 C_lr_2:0.000022	 C_lr_3:0.000022	 C_lr_4:0.000022
[2022-10-02 18:58:26 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.007567	Loss2: 0.007783	 Dis: 2.624290 Entropy: 4.469466 
[2022-10-02 18:58:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.002111	 E_lr_2:0.002111
[2022-10-02 18:58:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000021	 C_lr_2:0.000021	 C_lr_3:0.000021	 C_lr_4:0.000021
[2022-10-02 18:58:31 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.015576	Loss2: 0.020633	 Dis: 2.960907 Entropy: 4.170129 
[2022-10-02 18:58:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.002034	 E_lr_2:0.002034
[2022-10-02 18:58:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 18:58:37 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.061292	Loss2: 0.060209	 Dis: 2.509722 Entropy: 4.601462 
[2022-10-02 18:58:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.001957	 E_lr_2:0.001957
[2022-10-02 18:58:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 18:58:43 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.020477	Loss2: 0.015063	 Dis: 2.951284 Entropy: 4.543098 
[2022-10-02 18:58:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.001880	 E_lr_2:0.001880
[2022-10-02 18:58:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000019	 C_lr_2:0.000019	 C_lr_3:0.000019	 C_lr_4:0.000019
[2022-10-02 18:58:49 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.009869	Loss2: 0.013563	 Dis: 2.637896 Entropy: 4.515873 
[2022-10-02 18:58:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.001804	 E_lr_2:0.001804
[2022-10-02 18:58:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000018	 C_lr_2:0.000018	 C_lr_3:0.000018	 C_lr_4:0.000018
[2022-10-02 18:58:55 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.030163	Loss2: 0.040302	 Dis: 3.773464 Entropy: 5.258136 
[2022-10-02 18:58:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.001729	 E_lr_2:0.001729
[2022-10-02 18:58:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 18:59:01 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.133894	Loss2: 0.107890	 Dis: 2.810631 Entropy: 4.539930 
[2022-10-02 18:59:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.001655	 E_lr_2:0.001655
[2022-10-02 18:59:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 18:59:07 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.047085	Loss2: 0.038181	 Dis: 3.106050 Entropy: 4.475097 
[2022-10-02 18:59:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.001582	 E_lr_2:0.001582
[2022-10-02 18:59:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000016	 C_lr_2:0.000016	 C_lr_3:0.000016	 C_lr_4:0.000016
[2022-10-02 18:59:13 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.008052	Loss2: 0.012434	 Dis: 2.751116 Entropy: 4.452119 
[2022-10-02 18:59:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.001509	 E_lr_2:0.001509
[2022-10-02 18:59:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 18:59:19 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.036599	Loss2: 0.041774	 Dis: 2.558905 Entropy: 4.863423 
[2022-10-02 18:59:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.001437	 E_lr_2:0.001437
[2022-10-02 18:59:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 18:59:25 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.032424	Loss2: 0.051585	 Dis: 2.457327 Entropy: 5.957876 
[2022-10-02 18:59:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.001367	 E_lr_2:0.001367
[2022-10-02 18:59:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 18:59:31 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.137659	Loss2: 0.193007	 Dis: 5.057232 Entropy: 5.320993 
[2022-10-02 18:59:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.001297	 E_lr_2:0.001297
[2022-10-02 18:59:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000013	 C_lr_2:0.000013	 C_lr_3:0.000013	 C_lr_4:0.000013
[2022-10-02 18:59:37 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.028020	Loss2: 0.029401	 Dis: 2.578215 Entropy: 4.911230 
[2022-10-02 18:59:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.001229	 E_lr_2:0.001229
[2022-10-02 18:59:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 18:59:43 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.004732	Loss2: 0.005231	 Dis: 2.506876 Entropy: 5.432879 
[2022-10-02 18:59:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.001162	 E_lr_2:0.001162
[2022-10-02 18:59:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 18:59:50 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.016923	Loss2: 0.024718	 Dis: 2.257454 Entropy: 4.339478 
[2022-10-02 18:59:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.001096	 E_lr_2:0.001096
[2022-10-02 18:59:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000011	 C_lr_2:0.000011	 C_lr_3:0.000011	 C_lr_4:0.000011
[2022-10-02 18:59:56 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.023311	Loss2: 0.012632	 Dis: 1.886766 Entropy: 4.541690 
[2022-10-02 18:59:56 demo] (houston_program2.py 512): INFO E_lr_1: 0.001032	 E_lr_2:0.001032
[2022-10-02 18:59:56 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 19:00:02 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.044282	Loss2: 0.049331	 Dis: 1.780264 Entropy: 4.710760 
[2022-10-02 19:00:02 demo] (houston_program2.py 512): INFO E_lr_1: 0.000969	 E_lr_2:0.000969
[2022-10-02 19:00:02 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 19:00:07 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.009474	Loss2: 0.007154	 Dis: 1.889421 Entropy: 4.496693 
[2022-10-02 19:00:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.000908	 E_lr_2:0.000908
[2022-10-02 19:00:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000009	 C_lr_2:0.000009	 C_lr_3:0.000009	 C_lr_4:0.000009
[2022-10-02 19:00:14 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.018492	Loss2: 0.026691	 Dis: 3.682964 Entropy: 5.210285 
[2022-10-02 19:00:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.000848	 E_lr_2:0.000848
[2022-10-02 19:00:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 19:00:19 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.037427	Loss2: 0.044718	 Dis: 2.377638 Entropy: 4.583207 
[2022-10-02 19:00:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.000790	 E_lr_2:0.000790
[2022-10-02 19:00:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 19:00:26 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.076399	Loss2: 0.066523	 Dis: 1.441809 Entropy: 5.036389 
[2022-10-02 19:00:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.000734	 E_lr_2:0.000734
[2022-10-02 19:00:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 19:00:32 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.002988	Loss2: 0.004120	 Dis: 3.335037 Entropy: 4.542806 
[2022-10-02 19:00:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.000679	 E_lr_2:0.000679
[2022-10-02 19:00:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 19:00:38 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.004650	Loss2: 0.003645	 Dis: 2.216209 Entropy: 4.290283 
[2022-10-02 19:00:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.000626	 E_lr_2:0.000626
[2022-10-02 19:00:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 19:00:44 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.036048	Loss2: 0.015422	 Dis: 3.840006 Entropy: 4.876275 
[2022-10-02 19:00:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.000575	 E_lr_2:0.000575
[2022-10-02 19:00:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 19:00:50 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.009621	Loss2: 0.008078	 Dis: 2.575735 Entropy: 4.343229 
[2022-10-02 19:00:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.000526	 E_lr_2:0.000526
[2022-10-02 19:00:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 19:00:56 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.003455	Loss2: 0.002603	 Dis: 1.754341 Entropy: 4.929656 
[2022-10-02 19:00:56 demo] (houston_program2.py 512): INFO E_lr_1: 0.000479	 E_lr_2:0.000479
[2022-10-02 19:00:56 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 19:01:03 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.012492	Loss2: 0.008871	 Dis: 2.231962 Entropy: 4.788184 
[2022-10-02 19:01:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.000433	 E_lr_2:0.000433
[2022-10-02 19:01:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 19:01:09 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.005824	Loss2: 0.006897	 Dis: 2.776047 Entropy: 5.145844 
[2022-10-02 19:01:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.000390	 E_lr_2:0.000390
[2022-10-02 19:01:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 19:01:15 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.090128	Loss2: 0.089453	 Dis: 3.391094 Entropy: 4.266899 
[2022-10-02 19:01:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.000349	 E_lr_2:0.000349
[2022-10-02 19:01:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 19:01:21 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.010648	Loss2: 0.004805	 Dis: 1.573748 Entropy: 5.731464 
[2022-10-02 19:01:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.000310	 E_lr_2:0.000310
[2022-10-02 19:01:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 19:01:28 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.004191	Loss2: 0.005326	 Dis: 3.514145 Entropy: 4.152809 
[2022-10-02 19:01:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.000273	 E_lr_2:0.000273
[2022-10-02 19:01:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 19:01:33 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.012932	Loss2: 0.013130	 Dis: 3.447052 Entropy: 5.183165 
[2022-10-02 19:01:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.000239	 E_lr_2:0.000239
[2022-10-02 19:01:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 19:01:40 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.057239	Loss2: 0.075322	 Dis: 2.125500 Entropy: 4.445942 
[2022-10-02 19:01:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.000206	 E_lr_2:0.000206
[2022-10-02 19:01:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 19:01:46 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.004061	Loss2: 0.005067	 Dis: 2.231409 Entropy: 5.518852 
[2022-10-02 19:01:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.000176	 E_lr_2:0.000176
[2022-10-02 19:01:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 19:01:52 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.003479	Loss2: 0.003940	 Dis: 2.534765 Entropy: 4.550671 
[2022-10-02 19:01:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.000148	 E_lr_2:0.000148
[2022-10-02 19:01:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:01:58 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.002425	Loss2: 0.002352	 Dis: 2.385910 Entropy: 4.679993 
[2022-10-02 19:01:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.000123	 E_lr_2:0.000123
[2022-10-02 19:01:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:02:04 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.005000	Loss2: 0.004311	 Dis: 2.686674 Entropy: 4.207577 
[2022-10-02 19:02:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.000100	 E_lr_2:0.000100
[2022-10-02 19:02:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:02:11 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.011208	Loss2: 0.011492	 Dis: 3.108854 Entropy: 4.437009 
[2022-10-02 19:02:11 demo] (houston_program2.py 512): INFO E_lr_1: 0.000079	 E_lr_2:0.000079
[2022-10-02 19:02:11 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:02:17 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.002039	Loss2: 0.002477	 Dis: 1.568707 Entropy: 4.499201 
[2022-10-02 19:02:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.000061	 E_lr_2:0.000061
[2022-10-02 19:02:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:02:23 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.003938	Loss2: 0.004795	 Dis: 1.899958 Entropy: 5.368349 
[2022-10-02 19:02:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.000045	 E_lr_2:0.000045
[2022-10-02 19:02:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:02:29 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.007850	Loss2: 0.007997	 Dis: 4.022041 Entropy: 5.164026 
[2022-10-02 19:02:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.000031	 E_lr_2:0.000031
[2022-10-02 19:02:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:02:35 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.007658	Loss2: 0.005438	 Dis: 0.725288 Entropy: 4.474464 
[2022-10-02 19:02:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.000020	 E_lr_2:0.000020
[2022-10-02 19:02:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:02:41 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.003076	Loss2: 0.002965	 Dis: 2.501438 Entropy: 5.255088 
[2022-10-02 19:02:41 demo] (houston_program2.py 512): INFO E_lr_1: 0.000011	 E_lr_2:0.000011
[2022-10-02 19:02:41 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:02:48 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.006324	Loss2: 0.004374	 Dis: 0.719862 Entropy: 4.336604 
[2022-10-02 19:02:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.000005	 E_lr_2:0.000005
[2022-10-02 19:02:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:02:53 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.005433	Loss2: 0.004761	 Dis: 4.033415 Entropy: 4.734151 
[2022-10-02 19:02:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.000001	 E_lr_2:0.000001
[2022-10-02 19:02:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:03:00 demo] (houston_program2.py 504): INFO Train Ep: 60 	Loss1: 0.002535	Loss2: 0.003447	 Dis: 3.703333 Entropy: 4.584620 
[2022-10-02 19:03:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.000000	 E_lr_2:0.000000
[2022-10-02 19:03:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:03:00 demo] (houston_program2.py 515): INFO time_60_epoch:604.6794037818909
[2022-10-02 19:03:07 demo] (houston_program2.py 673): INFO 	val_Accuracy: 29933/53200 (56.27%)	
[2022-10-02 19:03:07 demo] (utils.py 47): INFO outputs/demo/finetune_100/ckpt_epoch_60.pth saving......
[2022-10-02 19:03:08 demo] (utils.py 49): INFO outputs/demo/finetune_100/ckpt_epoch_60.pth saved !!!
[2022-10-02 19:03:08 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0147 (0.0147)	loss 0.0080 (0.0080)	grad_norm 0.0460 (0.0460)	mem 460MB
[2022-10-02 19:03:08 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:08 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0194 (0.0194)	loss 0.0108 (0.0108)	grad_norm 0.0762 (0.0762)	mem 460MB
[2022-10-02 19:03:08 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:08 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000017	time 0.0211 (0.0211)	loss 0.0093 (0.0093)	grad_norm 0.0567 (0.0567)	mem 460MB
[2022-10-02 19:03:08 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:08 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0203 (0.0203)	loss 0.0076 (0.0076)	grad_norm 0.0525 (0.0525)	mem 460MB
[2022-10-02 19:03:08 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:09 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0218 (0.0218)	loss 0.0086 (0.0086)	grad_norm 0.0390 (0.0390)	mem 460MB
[2022-10-02 19:03:09 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:09 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0197 (0.0197)	loss 0.0102 (0.0102)	grad_norm 0.0525 (0.0525)	mem 460MB
[2022-10-02 19:03:09 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:09 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0219 (0.0219)	loss 0.0074 (0.0074)	grad_norm 0.0716 (0.0716)	mem 460MB
[2022-10-02 19:03:09 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:09 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0215 (0.0215)	loss 0.0105 (0.0105)	grad_norm 0.0432 (0.0432)	mem 460MB
[2022-10-02 19:03:09 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:09 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0198 (0.0198)	loss 0.0087 (0.0087)	grad_norm 0.0499 (0.0499)	mem 460MB
[2022-10-02 19:03:09 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:10 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0212 (0.0212)	loss 0.0080 (0.0080)	grad_norm 0.0391 (0.0391)	mem 460MB
[2022-10-02 19:03:10 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:10 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0203 (0.0203)	loss 0.0087 (0.0087)	grad_norm 0.0426 (0.0426)	mem 460MB
[2022-10-02 19:03:10 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:10 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0196 (0.0196)	loss 0.0093 (0.0093)	grad_norm 0.0369 (0.0369)	mem 460MB
[2022-10-02 19:03:10 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:10 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0222 (0.0222)	loss 0.0092 (0.0092)	grad_norm 0.0427 (0.0427)	mem 460MB
[2022-10-02 19:03:10 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:10 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0223 (0.0223)	loss 0.0086 (0.0086)	grad_norm 0.0401 (0.0401)	mem 460MB
[2022-10-02 19:03:10 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:10 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0150 (0.0150)	loss 0.0081 (0.0081)	grad_norm 0.0349 (0.0349)	mem 460MB
[2022-10-02 19:03:10 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:11 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0250 (0.0250)	loss 0.0087 (0.0087)	grad_norm 0.0641 (0.0641)	mem 460MB
[2022-10-02 19:03:11 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:11 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0206 (0.0206)	loss 0.0076 (0.0076)	grad_norm 0.0350 (0.0350)	mem 460MB
[2022-10-02 19:03:11 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:11 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0204 (0.0204)	loss 0.0068 (0.0068)	grad_norm 0.0342 (0.0342)	mem 460MB
[2022-10-02 19:03:11 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:11 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0200 (0.0200)	loss 0.0091 (0.0091)	grad_norm 0.0593 (0.0593)	mem 460MB
[2022-10-02 19:03:11 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:11 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0209 (0.0209)	loss 0.0065 (0.0065)	grad_norm 0.0368 (0.0368)	mem 460MB
[2022-10-02 19:03:11 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:12 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0195 (0.0195)	loss 0.0087 (0.0087)	grad_norm 0.0330 (0.0330)	mem 460MB
[2022-10-02 19:03:12 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:12 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0134 (0.0134)	loss 0.0091 (0.0091)	grad_norm 0.0404 (0.0404)	mem 460MB
[2022-10-02 19:03:12 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:12 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0159 (0.0159)	loss 0.0074 (0.0074)	grad_norm 0.0256 (0.0256)	mem 460MB
[2022-10-02 19:03:12 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:12 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0114 (0.0114)	loss 0.0102 (0.0102)	grad_norm 0.0361 (0.0361)	mem 460MB
[2022-10-02 19:03:12 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:12 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0220 (0.0220)	loss 0.0113 (0.0113)	grad_norm 0.0311 (0.0311)	mem 460MB
[2022-10-02 19:03:12 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:13 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0146 (0.0146)	loss 0.0088 (0.0088)	grad_norm 0.0356 (0.0356)	mem 460MB
[2022-10-02 19:03:13 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:13 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0220 (0.0220)	loss 0.0085 (0.0085)	grad_norm 0.0305 (0.0305)	mem 460MB
[2022-10-02 19:03:13 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:13 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0147 (0.0147)	loss 0.0099 (0.0099)	grad_norm 0.0344 (0.0344)	mem 460MB
[2022-10-02 19:03:13 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:13 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0200 (0.0200)	loss 0.0113 (0.0113)	grad_norm 0.0302 (0.0302)	mem 460MB
[2022-10-02 19:03:13 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:13 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0187 (0.0187)	loss 0.0085 (0.0085)	grad_norm 0.0546 (0.0546)	mem 460MB
[2022-10-02 19:03:13 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:13 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0224 (0.0224)	loss 0.0081 (0.0081)	grad_norm 0.0334 (0.0334)	mem 460MB
[2022-10-02 19:03:13 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:14 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0151 (0.0151)	loss 0.0077 (0.0077)	grad_norm 0.0362 (0.0362)	mem 460MB
[2022-10-02 19:03:14 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:14 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0237 (0.0237)	loss 0.0086 (0.0086)	grad_norm 0.0594 (0.0594)	mem 460MB
[2022-10-02 19:03:14 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:14 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0248 (0.0248)	loss 0.0105 (0.0105)	grad_norm 0.0404 (0.0404)	mem 460MB
[2022-10-02 19:03:14 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:14 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0121 (0.0121)	loss 0.0107 (0.0107)	grad_norm 0.0394 (0.0394)	mem 460MB
[2022-10-02 19:03:14 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:14 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0128 (0.0128)	loss 0.0073 (0.0073)	grad_norm 0.0441 (0.0441)	mem 460MB
[2022-10-02 19:03:14 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:14 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0128 (0.0128)	loss 0.0099 (0.0099)	grad_norm 0.0521 (0.0521)	mem 460MB
[2022-10-02 19:03:14 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:15 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0145 (0.0145)	loss 0.0086 (0.0086)	grad_norm 0.0344 (0.0344)	mem 460MB
[2022-10-02 19:03:15 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:15 demo] (houston_program2.py 243): INFO Train: [61/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0236 (0.0236)	loss 0.0117 (0.0117)	grad_norm 0.0488 (0.0488)	mem 460MB
[2022-10-02 19:03:15 demo] (houston_program2.py 252): INFO EPOCH 61 training takes 0:00:00
[2022-10-02 19:03:15 demo] (houston_program2.py 333): INFO Train Ep: 61 	Loss1: 0.184057	Loss2: 0.174979	 Dis: 2.877941 Entropy: 6.084027 
[2022-10-02 19:03:15 demo] (houston_program2.py 335): INFO time_61_epoch:7.4314329624176025
[2022-10-02 19:03:15 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0110 (0.0110)	loss 0.0084 (0.0084)	grad_norm 0.0531 (0.0531)	mem 460MB
[2022-10-02 19:03:15 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:15 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0224 (0.0224)	loss 0.0076 (0.0076)	grad_norm 0.0383 (0.0383)	mem 460MB
[2022-10-02 19:03:16 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:16 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0225 (0.0225)	loss 0.0089 (0.0089)	grad_norm 0.0358 (0.0358)	mem 460MB
[2022-10-02 19:03:16 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:16 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0199 (0.0199)	loss 0.0088 (0.0088)	grad_norm 0.0413 (0.0413)	mem 460MB
[2022-10-02 19:03:16 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:16 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0226 (0.0226)	loss 0.0111 (0.0111)	grad_norm 0.0360 (0.0360)	mem 460MB
[2022-10-02 19:03:16 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:16 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0283 (0.0283)	loss 0.0092 (0.0092)	grad_norm 0.0478 (0.0478)	mem 460MB
[2022-10-02 19:03:16 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:17 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0160 (0.0160)	loss 0.0085 (0.0085)	grad_norm 0.0356 (0.0356)	mem 460MB
[2022-10-02 19:03:17 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:17 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0230 (0.0230)	loss 0.0079 (0.0079)	grad_norm 0.0466 (0.0466)	mem 460MB
[2022-10-02 19:03:17 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:17 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0219 (0.0219)	loss 0.0130 (0.0130)	grad_norm 0.0573 (0.0573)	mem 460MB
[2022-10-02 19:03:17 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:17 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0208 (0.0208)	loss 0.0073 (0.0073)	grad_norm 0.0364 (0.0364)	mem 460MB
[2022-10-02 19:03:17 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:17 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0174 (0.0174)	loss 0.0089 (0.0089)	grad_norm 0.0345 (0.0345)	mem 460MB
[2022-10-02 19:03:17 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:17 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0219 (0.0219)	loss 0.0067 (0.0067)	grad_norm 0.0298 (0.0298)	mem 460MB
[2022-10-02 19:03:17 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:18 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0221 (0.0221)	loss 0.0087 (0.0087)	grad_norm 0.0564 (0.0564)	mem 460MB
[2022-10-02 19:03:18 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:18 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0188 (0.0188)	loss 0.0078 (0.0078)	grad_norm 0.0341 (0.0341)	mem 460MB
[2022-10-02 19:03:18 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:18 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0203 (0.0203)	loss 0.0101 (0.0101)	grad_norm 0.0518 (0.0518)	mem 460MB
[2022-10-02 19:03:18 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:18 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0148 (0.0148)	loss 0.0092 (0.0092)	grad_norm 0.0751 (0.0751)	mem 460MB
[2022-10-02 19:03:18 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:18 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000016	time 0.0223 (0.0223)	loss 0.0074 (0.0074)	grad_norm 0.0445 (0.0445)	mem 460MB
[2022-10-02 19:03:18 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:19 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0201 (0.0201)	loss 0.0122 (0.0122)	grad_norm 0.0663 (0.0663)	mem 460MB
[2022-10-02 19:03:19 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:19 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0209 (0.0209)	loss 0.0074 (0.0074)	grad_norm 0.0297 (0.0297)	mem 460MB
[2022-10-02 19:03:19 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:19 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0203 (0.0203)	loss 0.0093 (0.0093)	grad_norm 0.0420 (0.0420)	mem 460MB
[2022-10-02 19:03:19 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:19 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0204 (0.0204)	loss 0.0086 (0.0086)	grad_norm 0.0637 (0.0637)	mem 460MB
[2022-10-02 19:03:19 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:19 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0189 (0.0189)	loss 0.0089 (0.0089)	grad_norm 0.0372 (0.0372)	mem 460MB
[2022-10-02 19:03:19 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:19 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0229 (0.0229)	loss 0.0121 (0.0121)	grad_norm 0.1195 (0.1195)	mem 460MB
[2022-10-02 19:03:19 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:20 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0219 (0.0219)	loss 0.0089 (0.0089)	grad_norm 0.0482 (0.0482)	mem 460MB
[2022-10-02 19:03:20 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:20 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0250 (0.0250)	loss 0.0090 (0.0090)	grad_norm 0.0349 (0.0349)	mem 460MB
[2022-10-02 19:03:20 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:20 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0209 (0.0209)	loss 0.0087 (0.0087)	grad_norm 0.0345 (0.0345)	mem 460MB
[2022-10-02 19:03:20 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:20 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0234 (0.0234)	loss 0.0082 (0.0082)	grad_norm 0.0386 (0.0386)	mem 460MB
[2022-10-02 19:03:20 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:20 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0222 (0.0222)	loss 0.0110 (0.0110)	grad_norm 0.0439 (0.0439)	mem 460MB
[2022-10-02 19:03:20 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:21 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0108 (0.0108)	loss 0.0141 (0.0141)	grad_norm 0.0616 (0.0616)	mem 460MB
[2022-10-02 19:03:21 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:21 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0214 (0.0214)	loss 0.0094 (0.0094)	grad_norm 0.0444 (0.0444)	mem 460MB
[2022-10-02 19:03:21 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:21 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0103 (0.0103)	loss 0.0086 (0.0086)	grad_norm 0.0605 (0.0605)	mem 460MB
[2022-10-02 19:03:21 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:21 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0252 (0.0252)	loss 0.0094 (0.0094)	grad_norm 0.0450 (0.0450)	mem 460MB
[2022-10-02 19:03:21 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:21 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0104 (0.0104)	loss 0.0073 (0.0073)	grad_norm 0.0642 (0.0642)	mem 460MB
[2022-10-02 19:03:21 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:21 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0206 (0.0206)	loss 0.0066 (0.0066)	grad_norm 0.0301 (0.0301)	mem 460MB
[2022-10-02 19:03:21 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:22 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0110 (0.0110)	loss 0.0097 (0.0097)	grad_norm 0.0457 (0.0457)	mem 460MB
[2022-10-02 19:03:22 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:22 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0244 (0.0244)	loss 0.0096 (0.0096)	grad_norm 0.0570 (0.0570)	mem 460MB
[2022-10-02 19:03:22 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:22 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0189 (0.0189)	loss 0.0120 (0.0120)	grad_norm 0.0482 (0.0482)	mem 460MB
[2022-10-02 19:03:22 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:22 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0205 (0.0205)	loss 0.0104 (0.0104)	grad_norm 0.0668 (0.0668)	mem 460MB
[2022-10-02 19:03:22 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:22 demo] (houston_program2.py 243): INFO Train: [62/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0201 (0.0201)	loss 0.0098 (0.0098)	grad_norm 0.0444 (0.0444)	mem 460MB
[2022-10-02 19:03:22 demo] (houston_program2.py 252): INFO EPOCH 62 training takes 0:00:00
[2022-10-02 19:03:23 demo] (houston_program2.py 333): INFO Train Ep: 62 	Loss1: 0.077386	Loss2: 0.087803	 Dis: 3.156084 Entropy: 5.611171 
[2022-10-02 19:03:23 demo] (houston_program2.py 335): INFO time_62_epoch:7.568562984466553
[2022-10-02 19:03:23 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0118 (0.0118)	loss 0.0086 (0.0086)	grad_norm 0.0507 (0.0507)	mem 460MB
[2022-10-02 19:03:23 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:23 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0194 (0.0194)	loss 0.0128 (0.0128)	grad_norm 0.0384 (0.0384)	mem 460MB
[2022-10-02 19:03:23 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:23 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0216 (0.0216)	loss 0.0071 (0.0071)	grad_norm 0.0424 (0.0424)	mem 460MB
[2022-10-02 19:03:23 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:23 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0224 (0.0224)	loss 0.0106 (0.0106)	grad_norm 0.0377 (0.0377)	mem 460MB
[2022-10-02 19:03:24 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:24 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0229 (0.0229)	loss 0.0074 (0.0074)	grad_norm 0.0542 (0.0542)	mem 460MB
[2022-10-02 19:03:24 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:24 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0232 (0.0232)	loss 0.0102 (0.0102)	grad_norm 0.0455 (0.0455)	mem 460MB
[2022-10-02 19:03:24 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:24 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0208 (0.0208)	loss 0.0127 (0.0127)	grad_norm 0.0463 (0.0463)	mem 460MB
[2022-10-02 19:03:24 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:24 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0221 (0.0221)	loss 0.0103 (0.0103)	grad_norm 0.0667 (0.0667)	mem 460MB
[2022-10-02 19:03:24 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:24 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0223 (0.0223)	loss 0.0065 (0.0065)	grad_norm 0.0470 (0.0470)	mem 460MB
[2022-10-02 19:03:24 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:25 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0132 (0.0132)	loss 0.0105 (0.0105)	grad_norm 0.0719 (0.0719)	mem 460MB
[2022-10-02 19:03:25 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:25 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0223 (0.0223)	loss 0.0083 (0.0083)	grad_norm 0.0447 (0.0447)	mem 460MB
[2022-10-02 19:03:25 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:25 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0136 (0.0136)	loss 0.0087 (0.0087)	grad_norm 0.0705 (0.0705)	mem 460MB
[2022-10-02 19:03:25 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:25 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0214 (0.0214)	loss 0.0095 (0.0095)	grad_norm 0.0357 (0.0357)	mem 460MB
[2022-10-02 19:03:25 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:25 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0163 (0.0163)	loss 0.0105 (0.0105)	grad_norm 0.0468 (0.0468)	mem 460MB
[2022-10-02 19:03:25 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:25 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0217 (0.0217)	loss 0.0089 (0.0089)	grad_norm 0.0367 (0.0367)	mem 460MB
[2022-10-02 19:03:26 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:26 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0218 (0.0218)	loss 0.0100 (0.0100)	grad_norm 0.0591 (0.0591)	mem 460MB
[2022-10-02 19:03:26 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:26 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0199 (0.0199)	loss 0.0086 (0.0086)	grad_norm 0.0437 (0.0437)	mem 460MB
[2022-10-02 19:03:26 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:26 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0100 (0.0100)	loss 0.0134 (0.0134)	grad_norm 0.0436 (0.0436)	mem 460MB
[2022-10-02 19:03:26 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:26 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0104 (0.0104)	loss 0.0092 (0.0092)	grad_norm 0.0410 (0.0410)	mem 460MB
[2022-10-02 19:03:26 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:26 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0094 (0.0094)	loss 0.0070 (0.0070)	grad_norm 0.0432 (0.0432)	mem 460MB
[2022-10-02 19:03:26 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:26 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0164 (0.0164)	loss 0.0100 (0.0100)	grad_norm 0.0387 (0.0387)	mem 460MB
[2022-10-02 19:03:26 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:26 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0232 (0.0232)	loss 0.0083 (0.0083)	grad_norm 0.0372 (0.0372)	mem 460MB
[2022-10-02 19:03:26 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:27 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0136 (0.0136)	loss 0.0085 (0.0085)	grad_norm 0.0336 (0.0336)	mem 460MB
[2022-10-02 19:03:27 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:27 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0204 (0.0204)	loss 0.0073 (0.0073)	grad_norm 0.0408 (0.0408)	mem 460MB
[2022-10-02 19:03:27 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:27 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0237 (0.0237)	loss 0.0083 (0.0083)	grad_norm 0.0329 (0.0329)	mem 460MB
[2022-10-02 19:03:27 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:27 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0240 (0.0240)	loss 0.0075 (0.0075)	grad_norm 0.0529 (0.0529)	mem 460MB
[2022-10-02 19:03:27 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:27 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0218 (0.0218)	loss 0.0101 (0.0101)	grad_norm 0.0324 (0.0324)	mem 460MB
[2022-10-02 19:03:27 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:28 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0119 (0.0119)	loss 0.0083 (0.0083)	grad_norm 0.0407 (0.0407)	mem 460MB
[2022-10-02 19:03:28 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:28 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0237 (0.0237)	loss 0.0088 (0.0088)	grad_norm 0.0843 (0.0843)	mem 460MB
[2022-10-02 19:03:28 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:28 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0155 (0.0155)	loss 0.0110 (0.0110)	grad_norm 0.0444 (0.0444)	mem 460MB
[2022-10-02 19:03:28 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:28 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0230 (0.0230)	loss 0.0081 (0.0081)	grad_norm 0.0318 (0.0318)	mem 460MB
[2022-10-02 19:03:28 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:28 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0229 (0.0229)	loss 0.0101 (0.0101)	grad_norm 0.0428 (0.0428)	mem 460MB
[2022-10-02 19:03:28 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:29 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000015	time 0.0212 (0.0212)	loss 0.0106 (0.0106)	grad_norm 0.0911 (0.0911)	mem 460MB
[2022-10-02 19:03:29 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:29 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0210 (0.0210)	loss 0.0099 (0.0099)	grad_norm 0.0416 (0.0416)	mem 460MB
[2022-10-02 19:03:29 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:29 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0123 (0.0123)	loss 0.0112 (0.0112)	grad_norm 0.0456 (0.0456)	mem 460MB
[2022-10-02 19:03:29 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:29 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0234 (0.0234)	loss 0.0093 (0.0093)	grad_norm 0.0864 (0.0864)	mem 460MB
[2022-10-02 19:03:29 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:29 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0147 (0.0147)	loss 0.0102 (0.0102)	grad_norm 0.0466 (0.0466)	mem 460MB
[2022-10-02 19:03:29 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:29 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0239 (0.0239)	loss 0.0069 (0.0069)	grad_norm 0.0387 (0.0387)	mem 460MB
[2022-10-02 19:03:30 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:30 demo] (houston_program2.py 243): INFO Train: [63/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0131 (0.0131)	loss 0.0077 (0.0077)	grad_norm 0.0406 (0.0406)	mem 460MB
[2022-10-02 19:03:30 demo] (houston_program2.py 252): INFO EPOCH 63 training takes 0:00:00
[2022-10-02 19:03:30 demo] (houston_program2.py 333): INFO Train Ep: 63 	Loss1: 0.169840	Loss2: 0.141372	 Dis: 2.179392 Entropy: 6.345734 
[2022-10-02 19:03:30 demo] (houston_program2.py 335): INFO time_63_epoch:7.39415979385376
[2022-10-02 19:03:30 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0135 (0.0135)	loss 0.0092 (0.0092)	grad_norm 0.0411 (0.0411)	mem 460MB
[2022-10-02 19:03:30 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:31 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0232 (0.0232)	loss 0.0081 (0.0081)	grad_norm 0.0373 (0.0373)	mem 460MB
[2022-10-02 19:03:31 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:31 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0244 (0.0244)	loss 0.0080 (0.0080)	grad_norm 0.0327 (0.0327)	mem 460MB
[2022-10-02 19:03:31 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:31 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0206 (0.0206)	loss 0.0100 (0.0100)	grad_norm 0.0555 (0.0555)	mem 460MB
[2022-10-02 19:03:31 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:31 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0275 (0.0275)	loss 0.0076 (0.0076)	grad_norm 0.0362 (0.0362)	mem 460MB
[2022-10-02 19:03:31 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:31 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0240 (0.0240)	loss 0.0109 (0.0109)	grad_norm 0.0348 (0.0348)	mem 460MB
[2022-10-02 19:03:31 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:31 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0120 (0.0120)	loss 0.0093 (0.0093)	grad_norm 0.0557 (0.0557)	mem 460MB
[2022-10-02 19:03:32 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:32 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0208 (0.0208)	loss 0.0098 (0.0098)	grad_norm 0.0345 (0.0345)	mem 460MB
[2022-10-02 19:03:32 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:32 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0148 (0.0148)	loss 0.0103 (0.0103)	grad_norm 0.0455 (0.0455)	mem 460MB
[2022-10-02 19:03:32 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:32 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0202 (0.0202)	loss 0.0102 (0.0102)	grad_norm 0.0606 (0.0606)	mem 460MB
[2022-10-02 19:03:32 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:32 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0181 (0.0181)	loss 0.0087 (0.0087)	grad_norm 0.0361 (0.0361)	mem 460MB
[2022-10-02 19:03:32 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:32 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0220 (0.0220)	loss 0.0105 (0.0105)	grad_norm 0.0723 (0.0723)	mem 460MB
[2022-10-02 19:03:32 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:33 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0203 (0.0203)	loss 0.0097 (0.0097)	grad_norm 0.0456 (0.0456)	mem 460MB
[2022-10-02 19:03:33 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:33 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0202 (0.0202)	loss 0.0073 (0.0073)	grad_norm 0.0344 (0.0344)	mem 460MB
[2022-10-02 19:03:33 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:33 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0194 (0.0194)	loss 0.0119 (0.0119)	grad_norm 0.0672 (0.0672)	mem 460MB
[2022-10-02 19:03:33 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:33 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0218 (0.0218)	loss 0.0072 (0.0072)	grad_norm 0.0371 (0.0371)	mem 460MB
[2022-10-02 19:03:33 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:33 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0223 (0.0223)	loss 0.0118 (0.0118)	grad_norm 0.0425 (0.0425)	mem 460MB
[2022-10-02 19:03:33 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:33 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0221 (0.0221)	loss 0.0090 (0.0090)	grad_norm 0.0648 (0.0648)	mem 460MB
[2022-10-02 19:03:33 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:34 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0183 (0.0183)	loss 0.0090 (0.0090)	grad_norm 0.0722 (0.0722)	mem 460MB
[2022-10-02 19:03:34 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:34 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0245 (0.0245)	loss 0.0133 (0.0133)	grad_norm 0.0677 (0.0677)	mem 460MB
[2022-10-02 19:03:34 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:34 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0198 (0.0198)	loss 0.0097 (0.0097)	grad_norm 0.0662 (0.0662)	mem 460MB
[2022-10-02 19:03:34 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:34 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0233 (0.0233)	loss 0.0147 (0.0147)	grad_norm 0.0527 (0.0527)	mem 460MB
[2022-10-02 19:03:34 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:34 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0211 (0.0211)	loss 0.0102 (0.0102)	grad_norm 0.0408 (0.0408)	mem 460MB
[2022-10-02 19:03:34 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:35 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0102 (0.0102)	loss 0.0078 (0.0078)	grad_norm 0.0480 (0.0480)	mem 460MB
[2022-10-02 19:03:35 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:35 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0228 (0.0228)	loss 0.0090 (0.0090)	grad_norm 0.0517 (0.0517)	mem 460MB
[2022-10-02 19:03:35 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:35 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0125 (0.0125)	loss 0.0078 (0.0078)	grad_norm 0.0461 (0.0461)	mem 460MB
[2022-10-02 19:03:35 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:35 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0200 (0.0200)	loss 0.0120 (0.0120)	grad_norm 0.0411 (0.0411)	mem 460MB
[2022-10-02 19:03:35 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:35 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0114 (0.0114)	loss 0.0095 (0.0095)	grad_norm 0.0686 (0.0686)	mem 460MB
[2022-10-02 19:03:35 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:35 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0223 (0.0223)	loss 0.0108 (0.0108)	grad_norm 0.0627 (0.0627)	mem 460MB
[2022-10-02 19:03:35 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:36 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0157 (0.0157)	loss 0.0082 (0.0082)	grad_norm 0.0642 (0.0642)	mem 460MB
[2022-10-02 19:03:36 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:36 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0217 (0.0217)	loss 0.0094 (0.0094)	grad_norm 0.0664 (0.0664)	mem 460MB
[2022-10-02 19:03:36 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:36 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0151 (0.0151)	loss 0.0118 (0.0118)	grad_norm 0.0767 (0.0767)	mem 460MB
[2022-10-02 19:03:36 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:36 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0201 (0.0201)	loss 0.0088 (0.0088)	grad_norm 0.0521 (0.0521)	mem 460MB
[2022-10-02 19:03:36 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:36 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0160 (0.0160)	loss 0.0107 (0.0107)	grad_norm 0.0555 (0.0555)	mem 460MB
[2022-10-02 19:03:36 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:36 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0201 (0.0201)	loss 0.0082 (0.0082)	grad_norm 0.0673 (0.0673)	mem 460MB
[2022-10-02 19:03:36 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:37 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0173 (0.0173)	loss 0.0093 (0.0093)	grad_norm 0.0538 (0.0538)	mem 460MB
[2022-10-02 19:03:37 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:37 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0220 (0.0220)	loss 0.0167 (0.0167)	grad_norm 0.0875 (0.0875)	mem 460MB
[2022-10-02 19:03:37 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:37 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0228 (0.0228)	loss 0.0126 (0.0126)	grad_norm 0.0555 (0.0555)	mem 460MB
[2022-10-02 19:03:37 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:37 demo] (houston_program2.py 243): INFO Train: [64/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0249 (0.0249)	loss 0.0073 (0.0073)	grad_norm 0.0644 (0.0644)	mem 460MB
[2022-10-02 19:03:37 demo] (houston_program2.py 252): INFO EPOCH 64 training takes 0:00:00
[2022-10-02 19:03:37 demo] (houston_program2.py 333): INFO Train Ep: 64 	Loss1: 0.015899	Loss2: 0.019534	 Dis: 3.685619 Entropy: 5.472041 
[2022-10-02 19:03:37 demo] (houston_program2.py 335): INFO time_64_epoch:7.487464189529419
[2022-10-02 19:03:38 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0198 (0.0198)	loss 0.0089 (0.0089)	grad_norm 0.1040 (0.1040)	mem 460MB
[2022-10-02 19:03:38 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:38 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0229 (0.0229)	loss 0.0087 (0.0087)	grad_norm 0.0590 (0.0590)	mem 460MB
[2022-10-02 19:03:38 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:38 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0209 (0.0209)	loss 0.0079 (0.0079)	grad_norm 0.0689 (0.0689)	mem 460MB
[2022-10-02 19:03:38 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:38 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0200 (0.0200)	loss 0.0122 (0.0122)	grad_norm 0.0440 (0.0440)	mem 460MB
[2022-10-02 19:03:38 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:39 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0209 (0.0209)	loss 0.0071 (0.0071)	grad_norm 0.0531 (0.0531)	mem 460MB
[2022-10-02 19:03:39 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:39 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0216 (0.0216)	loss 0.0116 (0.0116)	grad_norm 0.0870 (0.0870)	mem 460MB
[2022-10-02 19:03:39 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:39 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0224 (0.0224)	loss 0.0096 (0.0096)	grad_norm 0.0552 (0.0552)	mem 460MB
[2022-10-02 19:03:39 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:39 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0205 (0.0205)	loss 0.0083 (0.0083)	grad_norm 0.0588 (0.0588)	mem 460MB
[2022-10-02 19:03:39 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:39 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0214 (0.0214)	loss 0.0093 (0.0093)	grad_norm 0.0357 (0.0357)	mem 460MB
[2022-10-02 19:03:39 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:40 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000014	time 0.0129 (0.0129)	loss 0.0143 (0.0143)	grad_norm 0.0338 (0.0338)	mem 460MB
[2022-10-02 19:03:40 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:40 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0206 (0.0206)	loss 0.0086 (0.0086)	grad_norm 0.0946 (0.0946)	mem 460MB
[2022-10-02 19:03:40 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:40 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0119 (0.0119)	loss 0.0135 (0.0135)	grad_norm 0.1233 (0.1233)	mem 460MB
[2022-10-02 19:03:40 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:40 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0215 (0.0215)	loss 0.0120 (0.0120)	grad_norm 0.0644 (0.0644)	mem 460MB
[2022-10-02 19:03:40 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:40 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0163 (0.0163)	loss 0.0103 (0.0103)	grad_norm 0.0565 (0.0565)	mem 460MB
[2022-10-02 19:03:40 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:40 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0217 (0.0217)	loss 0.0122 (0.0122)	grad_norm 0.0543 (0.0543)	mem 460MB
[2022-10-02 19:03:40 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:41 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0201 (0.0201)	loss 0.0124 (0.0124)	grad_norm 0.0556 (0.0556)	mem 460MB
[2022-10-02 19:03:41 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:41 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0200 (0.0200)	loss 0.0094 (0.0094)	grad_norm 0.0433 (0.0433)	mem 460MB
[2022-10-02 19:03:41 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:41 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0212 (0.0212)	loss 0.0083 (0.0083)	grad_norm 0.0611 (0.0611)	mem 460MB
[2022-10-02 19:03:41 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:41 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0116 (0.0116)	loss 0.0098 (0.0098)	grad_norm 0.0488 (0.0488)	mem 460MB
[2022-10-02 19:03:41 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:41 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0216 (0.0216)	loss 0.0083 (0.0083)	grad_norm 0.0515 (0.0515)	mem 460MB
[2022-10-02 19:03:41 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:42 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0139 (0.0139)	loss 0.0091 (0.0091)	grad_norm 0.0444 (0.0444)	mem 460MB
[2022-10-02 19:03:42 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:42 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0175 (0.0175)	loss 0.0070 (0.0070)	grad_norm 0.0442 (0.0442)	mem 460MB
[2022-10-02 19:03:42 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:42 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0192 (0.0192)	loss 0.0118 (0.0118)	grad_norm 0.0382 (0.0382)	mem 460MB
[2022-10-02 19:03:42 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:42 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0206 (0.0206)	loss 0.0096 (0.0096)	grad_norm 0.0593 (0.0593)	mem 460MB
[2022-10-02 19:03:42 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:42 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0205 (0.0205)	loss 0.0076 (0.0076)	grad_norm 0.0580 (0.0580)	mem 460MB
[2022-10-02 19:03:42 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:43 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0213 (0.0213)	loss 0.0074 (0.0074)	grad_norm 0.0415 (0.0415)	mem 460MB
[2022-10-02 19:03:43 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:43 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0211 (0.0211)	loss 0.0073 (0.0073)	grad_norm 0.0475 (0.0475)	mem 460MB
[2022-10-02 19:03:43 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:43 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0210 (0.0210)	loss 0.0093 (0.0093)	grad_norm 0.0509 (0.0509)	mem 460MB
[2022-10-02 19:03:43 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:43 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0212 (0.0212)	loss 0.0085 (0.0085)	grad_norm 0.0275 (0.0275)	mem 460MB
[2022-10-02 19:03:43 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:43 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0146 (0.0146)	loss 0.0099 (0.0099)	grad_norm 0.0343 (0.0343)	mem 460MB
[2022-10-02 19:03:43 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:43 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0245 (0.0245)	loss 0.0095 (0.0095)	grad_norm 0.0371 (0.0371)	mem 460MB
[2022-10-02 19:03:43 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:44 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0199 (0.0199)	loss 0.0126 (0.0126)	grad_norm 0.0588 (0.0588)	mem 460MB
[2022-10-02 19:03:44 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:44 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0215 (0.0215)	loss 0.0113 (0.0113)	grad_norm 0.0511 (0.0511)	mem 460MB
[2022-10-02 19:03:44 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:44 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0202 (0.0202)	loss 0.0089 (0.0089)	grad_norm 0.0344 (0.0344)	mem 460MB
[2022-10-02 19:03:44 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:44 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0204 (0.0204)	loss 0.0089 (0.0089)	grad_norm 0.0392 (0.0392)	mem 460MB
[2022-10-02 19:03:44 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:44 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0219 (0.0219)	loss 0.0086 (0.0086)	grad_norm 0.0449 (0.0449)	mem 460MB
[2022-10-02 19:03:44 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:45 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0205 (0.0205)	loss 0.0084 (0.0084)	grad_norm 0.0475 (0.0475)	mem 460MB
[2022-10-02 19:03:45 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:45 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0211 (0.0211)	loss 0.0091 (0.0091)	grad_norm 0.0473 (0.0473)	mem 460MB
[2022-10-02 19:03:45 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:45 demo] (houston_program2.py 243): INFO Train: [65/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0257 (0.0257)	loss 0.0129 (0.0129)	grad_norm 0.0515 (0.0515)	mem 460MB
[2022-10-02 19:03:45 demo] (houston_program2.py 252): INFO EPOCH 65 training takes 0:00:00
[2022-10-02 19:03:45 demo] (houston_program2.py 333): INFO Train Ep: 65 	Loss1: 0.417179	Loss2: 0.409605	 Dis: 10.064041 Entropy: 5.257795 
[2022-10-02 19:03:45 demo] (houston_program2.py 335): INFO time_65_epoch:7.796582460403442
[2022-10-02 19:03:45 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 19:03:45 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 19:03:45 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 19:03:45 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 19:03:45 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 19:03:45 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 19:03:45 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 19:03:45 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 19:03:45 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 19:03:45 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 19:03:51 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.180775	Loss2: 0.176889	 Dis: 12.309252 Entropy: 4.756059 
[2022-10-02 19:03:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.000487	 E_lr_2:0.000487
[2022-10-02 19:03:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 19:03:57 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.261871	Loss2: 0.277371	 Dis: 9.802723 Entropy: 4.494270 
[2022-10-02 19:03:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.000987	 E_lr_2:0.000987
[2022-10-02 19:03:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 19:04:04 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.328295	Loss2: 0.274444	 Dis: 7.735878 Entropy: 4.676208 
[2022-10-02 19:04:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.001487	 E_lr_2:0.001487
[2022-10-02 19:04:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 19:04:10 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.139075	Loss2: 0.106601	 Dis: 5.194538 Entropy: 5.034868 
[2022-10-02 19:04:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.001987	 E_lr_2:0.001987
[2022-10-02 19:04:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 19:04:16 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.461005	Loss2: 0.380642	 Dis: 6.464680 Entropy: 5.390498 
[2022-10-02 19:04:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.002487	 E_lr_2:0.002487
[2022-10-02 19:04:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 19:04:22 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.177078	Loss2: 0.177907	 Dis: 4.418949 Entropy: 5.022529 
[2022-10-02 19:04:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.002987	 E_lr_2:0.002987
[2022-10-02 19:04:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 19:04:28 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.156760	Loss2: 0.146808	 Dis: 4.072460 Entropy: 5.107496 
[2022-10-02 19:04:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.003487	 E_lr_2:0.003487
[2022-10-02 19:04:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 19:04:34 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.199022	Loss2: 0.237574	 Dis: 6.843452 Entropy: 5.656848 
[2022-10-02 19:04:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.003987	 E_lr_2:0.003987
[2022-10-02 19:04:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 19:04:40 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.202394	Loss2: 0.150415	 Dis: 6.960157 Entropy: 5.125401 
[2022-10-02 19:04:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.004487	 E_lr_2:0.004487
[2022-10-02 19:04:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 19:04:46 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.102266	Loss2: 0.087150	 Dis: 5.992483 Entropy: 4.468692 
[2022-10-02 19:04:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.004987	 E_lr_2:0.004987
[2022-10-02 19:04:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000050	 C_lr_2:0.000050	 C_lr_3:0.000050	 C_lr_4:0.000050
[2022-10-02 19:04:52 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.104720	Loss2: 0.105244	 Dis: 4.075630 Entropy: 4.393429 
[2022-10-02 19:04:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.004853	 E_lr_2:0.004853
[2022-10-02 19:04:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000049	 C_lr_2:0.000049	 C_lr_3:0.000049	 C_lr_4:0.000049
[2022-10-02 19:04:58 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.097999	Loss2: 0.092976	 Dis: 4.458786 Entropy: 4.999478 
[2022-10-02 19:04:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.004825	 E_lr_2:0.004825
[2022-10-02 19:04:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 19:05:04 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.171769	Loss2: 0.167955	 Dis: 4.976273 Entropy: 4.985662 
[2022-10-02 19:05:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.004795	 E_lr_2:0.004795
[2022-10-02 19:05:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 19:05:10 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.210456	Loss2: 0.201675	 Dis: 4.764507 Entropy: 4.614978 
[2022-10-02 19:05:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.004763	 E_lr_2:0.004763
[2022-10-02 19:05:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 19:05:15 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 1.117153	Loss2: 1.179804	 Dis: 14.122524 Entropy: 4.466442 
[2022-10-02 19:05:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.004728	 E_lr_2:0.004728
[2022-10-02 19:05:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 19:05:21 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.274785	Loss2: 0.290482	 Dis: 7.827944 Entropy: 4.567262 
[2022-10-02 19:05:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.004692	 E_lr_2:0.004692
[2022-10-02 19:05:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 19:05:27 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.336053	Loss2: 0.347325	 Dis: 8.791225 Entropy: 4.936390 
[2022-10-02 19:05:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.004653	 E_lr_2:0.004653
[2022-10-02 19:05:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 19:05:33 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.164540	Loss2: 0.131232	 Dis: 6.581112 Entropy: 5.071025 
[2022-10-02 19:05:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.004612	 E_lr_2:0.004612
[2022-10-02 19:05:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 19:05:39 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.254814	Loss2: 0.261012	 Dis: 8.297485 Entropy: 5.024695 
[2022-10-02 19:05:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.004569	 E_lr_2:0.004569
[2022-10-02 19:05:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 19:05:45 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.072416	Loss2: 0.069619	 Dis: 7.562431 Entropy: 5.375897 
[2022-10-02 19:05:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.004524	 E_lr_2:0.004524
[2022-10-02 19:05:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 19:05:51 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.393671	Loss2: 0.429997	 Dis: 9.553022 Entropy: 4.511504 
[2022-10-02 19:05:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.004477	 E_lr_2:0.004477
[2022-10-02 19:05:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 19:05:57 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.259147	Loss2: 0.246027	 Dis: 5.976971 Entropy: 4.569059 
[2022-10-02 19:05:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.004428	 E_lr_2:0.004428
[2022-10-02 19:05:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 19:06:03 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.245828	Loss2: 0.236710	 Dis: 7.312515 Entropy: 5.619590 
[2022-10-02 19:06:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.004377	 E_lr_2:0.004377
[2022-10-02 19:06:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 19:06:09 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.225217	Loss2: 0.202436	 Dis: 9.498135 Entropy: 5.018691 
[2022-10-02 19:06:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.004324	 E_lr_2:0.004324
[2022-10-02 19:06:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 19:06:15 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.175744	Loss2: 0.198463	 Dis: 6.102091 Entropy: 4.708613 
[2022-10-02 19:06:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.004269	 E_lr_2:0.004269
[2022-10-02 19:06:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 19:06:21 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.077654	Loss2: 0.095346	 Dis: 7.196396 Entropy: 5.561057 
[2022-10-02 19:06:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.004213	 E_lr_2:0.004213
[2022-10-02 19:06:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 19:06:27 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.207312	Loss2: 0.210360	 Dis: 5.994829 Entropy: 5.535801 
[2022-10-02 19:06:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.004155	 E_lr_2:0.004155
[2022-10-02 19:06:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 19:06:33 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.149813	Loss2: 0.151038	 Dis: 5.128778 Entropy: 5.568073 
[2022-10-02 19:06:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.004095	 E_lr_2:0.004095
[2022-10-02 19:06:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000041	 C_lr_2:0.000041	 C_lr_3:0.000041	 C_lr_4:0.000041
[2022-10-02 19:06:39 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.089834	Loss2: 0.101573	 Dis: 6.549944 Entropy: 4.687973 
[2022-10-02 19:06:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.004034	 E_lr_2:0.004034
[2022-10-02 19:06:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 19:06:45 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.233138	Loss2: 0.240581	 Dis: 7.035110 Entropy: 5.360714 
[2022-10-02 19:06:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.003971	 E_lr_2:0.003971
[2022-10-02 19:06:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 19:06:51 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.201294	Loss2: 0.190430	 Dis: 6.413681 Entropy: 5.905502 
[2022-10-02 19:06:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.003907	 E_lr_2:0.003907
[2022-10-02 19:06:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000039	 C_lr_2:0.000039	 C_lr_3:0.000039	 C_lr_4:0.000039
[2022-10-02 19:06:57 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.071616	Loss2: 0.082442	 Dis: 4.631832 Entropy: 5.186949 
[2022-10-02 19:06:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.003841	 E_lr_2:0.003841
[2022-10-02 19:06:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 19:07:04 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.193350	Loss2: 0.164522	 Dis: 3.400017 Entropy: 4.747144 
[2022-10-02 19:07:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.003774	 E_lr_2:0.003774
[2022-10-02 19:07:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 19:07:10 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.090976	Loss2: 0.109754	 Dis: 4.236250 Entropy: 6.314106 
[2022-10-02 19:07:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.003706	 E_lr_2:0.003706
[2022-10-02 19:07:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000037	 C_lr_2:0.000037	 C_lr_3:0.000037	 C_lr_4:0.000037
[2022-10-02 19:07:16 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.094429	Loss2: 0.093106	 Dis: 6.481298 Entropy: 4.679847 
[2022-10-02 19:07:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.003637	 E_lr_2:0.003637
[2022-10-02 19:07:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 19:07:20 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.038637	Loss2: 0.046166	 Dis: 4.683880 Entropy: 5.123814 
[2022-10-02 19:07:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.003566	 E_lr_2:0.003566
[2022-10-02 19:07:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 19:07:27 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.026634	Loss2: 0.025002	 Dis: 3.026415 Entropy: 5.231993 
[2022-10-02 19:07:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.003495	 E_lr_2:0.003495
[2022-10-02 19:07:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 19:07:33 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.094660	Loss2: 0.086722	 Dis: 2.583019 Entropy: 4.365803 
[2022-10-02 19:07:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.003422	 E_lr_2:0.003422
[2022-10-02 19:07:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000034	 C_lr_2:0.000034	 C_lr_3:0.000034	 C_lr_4:0.000034
[2022-10-02 19:07:39 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.225075	Loss2: 0.267164	 Dis: 4.332520 Entropy: 4.676278 
[2022-10-02 19:07:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.003349	 E_lr_2:0.003349
[2022-10-02 19:07:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 19:07:45 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.105230	Loss2: 0.114351	 Dis: 4.971460 Entropy: 5.760079 
[2022-10-02 19:07:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.003274	 E_lr_2:0.003274
[2022-10-02 19:07:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 19:07:51 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.042879	Loss2: 0.042422	 Dis: 4.736908 Entropy: 5.702326 
[2022-10-02 19:07:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.003199	 E_lr_2:0.003199
[2022-10-02 19:07:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000032	 C_lr_2:0.000032	 C_lr_3:0.000032	 C_lr_4:0.000032
[2022-10-02 19:07:57 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.054779	Loss2: 0.051401	 Dis: 3.139114 Entropy: 6.021007 
[2022-10-02 19:07:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.003124	 E_lr_2:0.003124
[2022-10-02 19:07:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000031	 C_lr_2:0.000031	 C_lr_3:0.000031	 C_lr_4:0.000031
[2022-10-02 19:08:03 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.094468	Loss2: 0.094885	 Dis: 4.021309 Entropy: 4.231072 
[2022-10-02 19:08:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.003047	 E_lr_2:0.003047
[2022-10-02 19:08:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 19:08:10 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.161478	Loss2: 0.148862	 Dis: 2.305204 Entropy: 5.353610 
[2022-10-02 19:08:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.002970	 E_lr_2:0.002970
[2022-10-02 19:08:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 19:08:16 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.039258	Loss2: 0.036804	 Dis: 3.390873 Entropy: 4.750647 
[2022-10-02 19:08:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.002893	 E_lr_2:0.002893
[2022-10-02 19:08:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000029	 C_lr_2:0.000029	 C_lr_3:0.000029	 C_lr_4:0.000029
[2022-10-02 19:08:21 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.049111	Loss2: 0.057685	 Dis: 3.131767 Entropy: 5.521561 
[2022-10-02 19:08:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.002815	 E_lr_2:0.002815
[2022-10-02 19:08:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000028	 C_lr_2:0.000028	 C_lr_3:0.000028	 C_lr_4:0.000028
[2022-10-02 19:08:26 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.233574	Loss2: 0.234889	 Dis: 2.709660 Entropy: 6.131106 
[2022-10-02 19:08:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.002737	 E_lr_2:0.002737
[2022-10-02 19:08:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 19:08:33 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.030874	Loss2: 0.027151	 Dis: 3.620543 Entropy: 5.753588 
[2022-10-02 19:08:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.002659	 E_lr_2:0.002659
[2022-10-02 19:08:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 19:08:39 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.050214	Loss2: 0.076716	 Dis: 3.069569 Entropy: 4.504871 
[2022-10-02 19:08:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.002581	 E_lr_2:0.002581
[2022-10-02 19:08:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000026	 C_lr_2:0.000026	 C_lr_3:0.000026	 C_lr_4:0.000026
[2022-10-02 19:08:45 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.071604	Loss2: 0.076750	 Dis: 3.247585 Entropy: 4.814404 
[2022-10-02 19:08:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.002502	 E_lr_2:0.002502
[2022-10-02 19:08:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 19:08:51 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.033182	Loss2: 0.034959	 Dis: 3.517002 Entropy: 4.705042 
[2022-10-02 19:08:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.002423	 E_lr_2:0.002423
[2022-10-02 19:08:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000024	 C_lr_2:0.000024	 C_lr_3:0.000024	 C_lr_4:0.000024
[2022-10-02 19:08:57 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.029678	Loss2: 0.031462	 Dis: 2.990974 Entropy: 4.923573 
[2022-10-02 19:08:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.002345	 E_lr_2:0.002345
[2022-10-02 19:08:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 19:09:03 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.152949	Loss2: 0.158236	 Dis: 4.015488 Entropy: 5.214738 
[2022-10-02 19:09:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.002267	 E_lr_2:0.002267
[2022-10-02 19:09:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 19:09:09 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.025463	Loss2: 0.050792	 Dis: 3.486313 Entropy: 4.904543 
[2022-10-02 19:09:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.002189	 E_lr_2:0.002189
[2022-10-02 19:09:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000022	 C_lr_2:0.000022	 C_lr_3:0.000022	 C_lr_4:0.000022
[2022-10-02 19:09:16 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.036259	Loss2: 0.034423	 Dis: 1.627968 Entropy: 4.778219 
[2022-10-02 19:09:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.002111	 E_lr_2:0.002111
[2022-10-02 19:09:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000021	 C_lr_2:0.000021	 C_lr_3:0.000021	 C_lr_4:0.000021
[2022-10-02 19:09:22 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.020457	Loss2: 0.023875	 Dis: 2.613394 Entropy: 4.741266 
[2022-10-02 19:09:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.002034	 E_lr_2:0.002034
[2022-10-02 19:09:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 19:09:28 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.006888	Loss2: 0.006367	 Dis: 1.950249 Entropy: 4.990629 
[2022-10-02 19:09:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.001957	 E_lr_2:0.001957
[2022-10-02 19:09:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 19:09:34 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.019120	Loss2: 0.020308	 Dis: 0.953489 Entropy: 5.209656 
[2022-10-02 19:09:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.001880	 E_lr_2:0.001880
[2022-10-02 19:09:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000019	 C_lr_2:0.000019	 C_lr_3:0.000019	 C_lr_4:0.000019
[2022-10-02 19:09:40 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.057458	Loss2: 0.082546	 Dis: 1.867819 Entropy: 4.714917 
[2022-10-02 19:09:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.001804	 E_lr_2:0.001804
[2022-10-02 19:09:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000018	 C_lr_2:0.000018	 C_lr_3:0.000018	 C_lr_4:0.000018
[2022-10-02 19:09:46 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.038126	Loss2: 0.045847	 Dis: 3.080050 Entropy: 5.104240 
[2022-10-02 19:09:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.001729	 E_lr_2:0.001729
[2022-10-02 19:09:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 19:09:52 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.042822	Loss2: 0.040846	 Dis: 4.225468 Entropy: 5.111907 
[2022-10-02 19:09:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.001655	 E_lr_2:0.001655
[2022-10-02 19:09:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 19:09:59 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.286651	Loss2: 0.283505	 Dis: 3.672501 Entropy: 4.308549 
[2022-10-02 19:09:59 demo] (houston_program2.py 512): INFO E_lr_1: 0.001582	 E_lr_2:0.001582
[2022-10-02 19:09:59 demo] (houston_program2.py 513): INFO C_lr_1: 0.000016	 C_lr_2:0.000016	 C_lr_3:0.000016	 C_lr_4:0.000016
[2022-10-02 19:10:05 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.276784	Loss2: 0.256740	 Dis: 4.315876 Entropy: 4.360997 
[2022-10-02 19:10:05 demo] (houston_program2.py 512): INFO E_lr_1: 0.001509	 E_lr_2:0.001509
[2022-10-02 19:10:05 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 19:10:11 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.225171	Loss2: 0.245078	 Dis: 2.943159 Entropy: 5.169729 
[2022-10-02 19:10:11 demo] (houston_program2.py 512): INFO E_lr_1: 0.001437	 E_lr_2:0.001437
[2022-10-02 19:10:11 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 19:10:17 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.063109	Loss2: 0.064675	 Dis: 4.779461 Entropy: 4.694605 
[2022-10-02 19:10:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.001367	 E_lr_2:0.001367
[2022-10-02 19:10:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 19:10:23 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.081900	Loss2: 0.083129	 Dis: 3.460981 Entropy: 5.483216 
[2022-10-02 19:10:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.001297	 E_lr_2:0.001297
[2022-10-02 19:10:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000013	 C_lr_2:0.000013	 C_lr_3:0.000013	 C_lr_4:0.000013
[2022-10-02 19:10:29 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.034621	Loss2: 0.028298	 Dis: 3.258369 Entropy: 5.075676 
[2022-10-02 19:10:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.001229	 E_lr_2:0.001229
[2022-10-02 19:10:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 19:10:35 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.033936	Loss2: 0.039775	 Dis: 2.805357 Entropy: 6.247548 
[2022-10-02 19:10:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.001162	 E_lr_2:0.001162
[2022-10-02 19:10:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 19:10:41 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.056278	Loss2: 0.059702	 Dis: 4.027479 Entropy: 4.762222 
[2022-10-02 19:10:41 demo] (houston_program2.py 512): INFO E_lr_1: 0.001096	 E_lr_2:0.001096
[2022-10-02 19:10:41 demo] (houston_program2.py 513): INFO C_lr_1: 0.000011	 C_lr_2:0.000011	 C_lr_3:0.000011	 C_lr_4:0.000011
[2022-10-02 19:10:47 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.043426	Loss2: 0.040221	 Dis: 3.120193 Entropy: 5.704986 
[2022-10-02 19:10:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.001032	 E_lr_2:0.001032
[2022-10-02 19:10:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 19:10:53 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.094791	Loss2: 0.089226	 Dis: 2.702423 Entropy: 5.765636 
[2022-10-02 19:10:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.000969	 E_lr_2:0.000969
[2022-10-02 19:10:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 19:10:59 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.119606	Loss2: 0.120349	 Dis: 2.052361 Entropy: 4.999359 
[2022-10-02 19:10:59 demo] (houston_program2.py 512): INFO E_lr_1: 0.000908	 E_lr_2:0.000908
[2022-10-02 19:10:59 demo] (houston_program2.py 513): INFO C_lr_1: 0.000009	 C_lr_2:0.000009	 C_lr_3:0.000009	 C_lr_4:0.000009
[2022-10-02 19:11:05 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.013001	Loss2: 0.015855	 Dis: 2.934544 Entropy: 5.983160 
[2022-10-02 19:11:05 demo] (houston_program2.py 512): INFO E_lr_1: 0.000848	 E_lr_2:0.000848
[2022-10-02 19:11:05 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 19:11:10 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.080079	Loss2: 0.059914	 Dis: 1.481516 Entropy: 5.284365 
[2022-10-02 19:11:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.000790	 E_lr_2:0.000790
[2022-10-02 19:11:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 19:11:15 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.028802	Loss2: 0.029863	 Dis: 3.526300 Entropy: 5.541352 
[2022-10-02 19:11:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.000734	 E_lr_2:0.000734
[2022-10-02 19:11:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 19:11:21 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.007188	Loss2: 0.007679	 Dis: 1.371391 Entropy: 4.519779 
[2022-10-02 19:11:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.000679	 E_lr_2:0.000679
[2022-10-02 19:11:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 19:11:27 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.012083	Loss2: 0.007642	 Dis: 1.882406 Entropy: 7.560253 
[2022-10-02 19:11:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.000626	 E_lr_2:0.000626
[2022-10-02 19:11:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 19:11:33 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.017681	Loss2: 0.017365	 Dis: 2.720428 Entropy: 5.793503 
[2022-10-02 19:11:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.000575	 E_lr_2:0.000575
[2022-10-02 19:11:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 19:11:39 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.005056	Loss2: 0.005534	 Dis: 3.082180 Entropy: 4.847729 
[2022-10-02 19:11:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.000526	 E_lr_2:0.000526
[2022-10-02 19:11:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 19:11:45 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.016041	Loss2: 0.015293	 Dis: 1.073248 Entropy: 4.659833 
[2022-10-02 19:11:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.000479	 E_lr_2:0.000479
[2022-10-02 19:11:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 19:11:51 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.031558	Loss2: 0.028880	 Dis: 1.871630 Entropy: 5.503269 
[2022-10-02 19:11:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.000433	 E_lr_2:0.000433
[2022-10-02 19:11:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 19:11:57 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.014575	Loss2: 0.023279	 Dis: 2.783543 Entropy: 6.063828 
[2022-10-02 19:11:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.000390	 E_lr_2:0.000390
[2022-10-02 19:11:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 19:12:03 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.010756	Loss2: 0.007387	 Dis: 2.207685 Entropy: 6.383762 
[2022-10-02 19:12:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.000349	 E_lr_2:0.000349
[2022-10-02 19:12:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 19:12:09 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.006099	Loss2: 0.005871	 Dis: 4.299290 Entropy: 6.849448 
[2022-10-02 19:12:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.000310	 E_lr_2:0.000310
[2022-10-02 19:12:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 19:12:15 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.016691	Loss2: 0.017812	 Dis: 1.491152 Entropy: 5.770921 
[2022-10-02 19:12:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.000273	 E_lr_2:0.000273
[2022-10-02 19:12:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 19:12:21 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.011466	Loss2: 0.007252	 Dis: 1.143593 Entropy: 4.610182 
[2022-10-02 19:12:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.000239	 E_lr_2:0.000239
[2022-10-02 19:12:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 19:12:26 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.013202	Loss2: 0.015235	 Dis: 1.943665 Entropy: 6.126058 
[2022-10-02 19:12:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.000206	 E_lr_2:0.000206
[2022-10-02 19:12:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 19:12:33 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.022522	Loss2: 0.023681	 Dis: 2.538498 Entropy: 4.539407 
[2022-10-02 19:12:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.000176	 E_lr_2:0.000176
[2022-10-02 19:12:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 19:12:39 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.013295	Loss2: 0.007848	 Dis: 3.086716 Entropy: 4.395777 
[2022-10-02 19:12:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.000148	 E_lr_2:0.000148
[2022-10-02 19:12:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:12:45 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.069653	Loss2: 0.074456	 Dis: 1.725349 Entropy: 4.899563 
[2022-10-02 19:12:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.000123	 E_lr_2:0.000123
[2022-10-02 19:12:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:12:51 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.067273	Loss2: 0.054891	 Dis: 2.093487 Entropy: 4.970398 
[2022-10-02 19:12:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.000100	 E_lr_2:0.000100
[2022-10-02 19:12:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:12:57 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.004680	Loss2: 0.005120	 Dis: 2.949406 Entropy: 5.338855 
[2022-10-02 19:12:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.000079	 E_lr_2:0.000079
[2022-10-02 19:12:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:13:03 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.005580	Loss2: 0.006247	 Dis: 2.126484 Entropy: 5.417965 
[2022-10-02 19:13:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.000061	 E_lr_2:0.000061
[2022-10-02 19:13:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:13:09 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.006083	Loss2: 0.005543	 Dis: 2.697361 Entropy: 4.687418 
[2022-10-02 19:13:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.000045	 E_lr_2:0.000045
[2022-10-02 19:13:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:13:15 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.013904	Loss2: 0.015995	 Dis: 2.121830 Entropy: 5.627579 
[2022-10-02 19:13:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.000031	 E_lr_2:0.000031
[2022-10-02 19:13:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:13:21 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.004534	Loss2: 0.005122	 Dis: 3.052137 Entropy: 5.449140 
[2022-10-02 19:13:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.000020	 E_lr_2:0.000020
[2022-10-02 19:13:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:13:28 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.006510	Loss2: 0.005721	 Dis: 2.082100 Entropy: 5.429437 
[2022-10-02 19:13:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.000011	 E_lr_2:0.000011
[2022-10-02 19:13:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:13:34 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.005385	Loss2: 0.006369	 Dis: 1.742771 Entropy: 6.081575 
[2022-10-02 19:13:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.000005	 E_lr_2:0.000005
[2022-10-02 19:13:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:13:40 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.004237	Loss2: 0.003320	 Dis: 1.597336 Entropy: 5.862278 
[2022-10-02 19:13:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.000001	 E_lr_2:0.000001
[2022-10-02 19:13:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:13:45 demo] (houston_program2.py 504): INFO Train Ep: 65 	Loss1: 0.062957	Loss2: 0.076331	 Dis: 2.160378 Entropy: 4.219990 
[2022-10-02 19:13:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.000000	 E_lr_2:0.000000
[2022-10-02 19:13:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:13:45 demo] (houston_program2.py 515): INFO time_65_epoch:600.2162356376648
[2022-10-02 19:13:53 demo] (houston_program2.py 673): INFO 	val_Accuracy: 33282/53200 (62.56%)	
[2022-10-02 19:13:53 demo] (utils.py 47): INFO outputs/demo/finetune_100/ckpt_epoch_65.pth saving......
[2022-10-02 19:13:53 demo] (utils.py 49): INFO outputs/demo/finetune_100/ckpt_epoch_65.pth saved !!!
[2022-10-02 19:13:54 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0121 (0.0121)	loss 0.0100 (0.0100)	grad_norm 0.0320 (0.0320)	mem 460MB
[2022-10-02 19:13:54 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:54 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0220 (0.0220)	loss 0.0100 (0.0100)	grad_norm 0.0392 (0.0392)	mem 460MB
[2022-10-02 19:13:54 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:54 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0221 (0.0221)	loss 0.0107 (0.0107)	grad_norm 0.1045 (0.1045)	mem 460MB
[2022-10-02 19:13:54 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:54 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0196 (0.0196)	loss 0.0087 (0.0087)	grad_norm 0.0632 (0.0632)	mem 460MB
[2022-10-02 19:13:54 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:55 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0216 (0.0216)	loss 0.0065 (0.0065)	grad_norm 0.0335 (0.0335)	mem 460MB
[2022-10-02 19:13:55 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:55 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0248 (0.0248)	loss 0.0089 (0.0089)	grad_norm 0.0435 (0.0435)	mem 460MB
[2022-10-02 19:13:55 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:55 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0215 (0.0215)	loss 0.0114 (0.0114)	grad_norm 0.0523 (0.0523)	mem 460MB
[2022-10-02 19:13:55 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:55 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0106 (0.0106)	loss 0.0097 (0.0097)	grad_norm 0.0449 (0.0449)	mem 460MB
[2022-10-02 19:13:55 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:55 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0218 (0.0218)	loss 0.0081 (0.0081)	grad_norm 0.0318 (0.0318)	mem 460MB
[2022-10-02 19:13:55 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:55 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0131 (0.0131)	loss 0.0085 (0.0085)	grad_norm 0.0328 (0.0328)	mem 460MB
[2022-10-02 19:13:55 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:56 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0199 (0.0199)	loss 0.0075 (0.0075)	grad_norm 0.0437 (0.0437)	mem 460MB
[2022-10-02 19:13:56 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:56 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0189 (0.0189)	loss 0.0083 (0.0083)	grad_norm 0.0426 (0.0426)	mem 460MB
[2022-10-02 19:13:56 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:56 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0227 (0.0227)	loss 0.0074 (0.0074)	grad_norm 0.0321 (0.0321)	mem 460MB
[2022-10-02 19:13:56 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:56 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0213 (0.0213)	loss 0.0078 (0.0078)	grad_norm 0.0687 (0.0687)	mem 460MB
[2022-10-02 19:13:56 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:56 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0208 (0.0208)	loss 0.0125 (0.0125)	grad_norm 0.0534 (0.0534)	mem 460MB
[2022-10-02 19:13:56 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:57 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0212 (0.0212)	loss 0.0103 (0.0103)	grad_norm 0.0550 (0.0550)	mem 460MB
[2022-10-02 19:13:57 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:57 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0233 (0.0233)	loss 0.0126 (0.0126)	grad_norm 0.0324 (0.0324)	mem 460MB
[2022-10-02 19:13:57 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:57 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0214 (0.0214)	loss 0.0070 (0.0070)	grad_norm 0.0519 (0.0519)	mem 460MB
[2022-10-02 19:13:57 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:57 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0099 (0.0099)	loss 0.0086 (0.0086)	grad_norm 0.0441 (0.0441)	mem 460MB
[2022-10-02 19:13:57 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:57 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0215 (0.0215)	loss 0.0099 (0.0099)	grad_norm 0.0419 (0.0419)	mem 460MB
[2022-10-02 19:13:57 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:57 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0131 (0.0131)	loss 0.0082 (0.0082)	grad_norm 0.0484 (0.0484)	mem 460MB
[2022-10-02 19:13:57 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:58 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0217 (0.0217)	loss 0.0088 (0.0088)	grad_norm 0.0490 (0.0490)	mem 460MB
[2022-10-02 19:13:58 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:58 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0132 (0.0132)	loss 0.0077 (0.0077)	grad_norm 0.0374 (0.0374)	mem 460MB
[2022-10-02 19:13:58 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:58 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0204 (0.0204)	loss 0.0082 (0.0082)	grad_norm 0.0330 (0.0330)	mem 460MB
[2022-10-02 19:13:58 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:58 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0162 (0.0162)	loss 0.0085 (0.0085)	grad_norm 0.0599 (0.0599)	mem 460MB
[2022-10-02 19:13:58 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:58 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0207 (0.0207)	loss 0.0088 (0.0088)	grad_norm 0.0499 (0.0499)	mem 460MB
[2022-10-02 19:13:58 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:59 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0207 (0.0207)	loss 0.0074 (0.0074)	grad_norm 0.0601 (0.0601)	mem 460MB
[2022-10-02 19:13:59 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:59 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000013	time 0.0218 (0.0218)	loss 0.0142 (0.0142)	grad_norm 0.0448 (0.0448)	mem 460MB
[2022-10-02 19:13:59 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:59 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0223 (0.0223)	loss 0.0086 (0.0086)	grad_norm 0.0464 (0.0464)	mem 460MB
[2022-10-02 19:13:59 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:59 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0240 (0.0240)	loss 0.0132 (0.0132)	grad_norm 0.0314 (0.0314)	mem 460MB
[2022-10-02 19:13:59 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:59 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0219 (0.0219)	loss 0.0181 (0.0181)	grad_norm 0.0627 (0.0627)	mem 460MB
[2022-10-02 19:13:59 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:13:59 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0184 (0.0184)	loss 0.0117 (0.0117)	grad_norm 0.0526 (0.0526)	mem 460MB
[2022-10-02 19:13:59 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:14:00 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0215 (0.0215)	loss 0.0085 (0.0085)	grad_norm 0.0402 (0.0402)	mem 460MB
[2022-10-02 19:14:00 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:14:00 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0147 (0.0147)	loss 0.0092 (0.0092)	grad_norm 0.0389 (0.0389)	mem 460MB
[2022-10-02 19:14:00 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:14:00 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0203 (0.0203)	loss 0.0099 (0.0099)	grad_norm 0.0361 (0.0361)	mem 460MB
[2022-10-02 19:14:00 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:14:00 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0135 (0.0135)	loss 0.0118 (0.0118)	grad_norm 0.0486 (0.0486)	mem 460MB
[2022-10-02 19:14:00 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:14:00 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0207 (0.0207)	loss 0.0116 (0.0116)	grad_norm 0.0437 (0.0437)	mem 460MB
[2022-10-02 19:14:00 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:14:01 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0160 (0.0160)	loss 0.0080 (0.0080)	grad_norm 0.0477 (0.0477)	mem 460MB
[2022-10-02 19:14:01 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:14:01 demo] (houston_program2.py 243): INFO Train: [66/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0203 (0.0203)	loss 0.0131 (0.0131)	grad_norm 0.0383 (0.0383)	mem 460MB
[2022-10-02 19:14:01 demo] (houston_program2.py 252): INFO EPOCH 66 training takes 0:00:00
[2022-10-02 19:14:01 demo] (houston_program2.py 333): INFO Train Ep: 66 	Loss1: 0.562586	Loss2: 0.545515	 Dis: 7.296181 Entropy: 5.239196 
[2022-10-02 19:14:01 demo] (houston_program2.py 335): INFO time_66_epoch:7.494229793548584
[2022-10-02 19:14:01 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0116 (0.0116)	loss 0.0129 (0.0129)	grad_norm 0.0405 (0.0405)	mem 460MB
[2022-10-02 19:14:01 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:01 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0259 (0.0259)	loss 0.0097 (0.0097)	grad_norm 0.0529 (0.0529)	mem 460MB
[2022-10-02 19:14:01 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:02 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0218 (0.0218)	loss 0.0124 (0.0124)	grad_norm 0.0422 (0.0422)	mem 460MB
[2022-10-02 19:14:02 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:02 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0149 (0.0149)	loss 0.0109 (0.0109)	grad_norm 0.0359 (0.0359)	mem 460MB
[2022-10-02 19:14:02 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:02 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0217 (0.0217)	loss 0.0101 (0.0101)	grad_norm 0.0536 (0.0536)	mem 460MB
[2022-10-02 19:14:02 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:02 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0122 (0.0122)	loss 0.0090 (0.0090)	grad_norm 0.0818 (0.0818)	mem 460MB
[2022-10-02 19:14:02 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:02 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0220 (0.0220)	loss 0.0163 (0.0163)	grad_norm 0.0852 (0.0852)	mem 460MB
[2022-10-02 19:14:02 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:03 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0138 (0.0138)	loss 0.0084 (0.0084)	grad_norm 0.0455 (0.0455)	mem 460MB
[2022-10-02 19:14:03 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:03 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0216 (0.0216)	loss 0.0109 (0.0109)	grad_norm 0.0452 (0.0452)	mem 460MB
[2022-10-02 19:14:03 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:03 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0210 (0.0210)	loss 0.0088 (0.0088)	grad_norm 0.0387 (0.0387)	mem 460MB
[2022-10-02 19:14:03 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:03 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0189 (0.0189)	loss 0.0087 (0.0087)	grad_norm 0.0506 (0.0506)	mem 460MB
[2022-10-02 19:14:03 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:03 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0217 (0.0217)	loss 0.0064 (0.0064)	grad_norm 0.0478 (0.0478)	mem 460MB
[2022-10-02 19:14:03 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:03 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0221 (0.0221)	loss 0.0086 (0.0086)	grad_norm 0.0591 (0.0591)	mem 460MB
[2022-10-02 19:14:03 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:04 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0215 (0.0215)	loss 0.0077 (0.0077)	grad_norm 0.0428 (0.0428)	mem 460MB
[2022-10-02 19:14:04 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:04 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0236 (0.0236)	loss 0.0091 (0.0091)	grad_norm 0.0409 (0.0409)	mem 460MB
[2022-10-02 19:14:04 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:04 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0212 (0.0212)	loss 0.0073 (0.0073)	grad_norm 0.0476 (0.0476)	mem 460MB
[2022-10-02 19:14:04 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:04 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0107 (0.0107)	loss 0.0096 (0.0096)	grad_norm 0.0390 (0.0390)	mem 460MB
[2022-10-02 19:14:04 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:04 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0225 (0.0225)	loss 0.0105 (0.0105)	grad_norm 0.0368 (0.0368)	mem 460MB
[2022-10-02 19:14:04 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:05 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0128 (0.0128)	loss 0.0095 (0.0095)	grad_norm 0.0448 (0.0448)	mem 460MB
[2022-10-02 19:14:05 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:05 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0216 (0.0216)	loss 0.0085 (0.0085)	grad_norm 0.0659 (0.0659)	mem 460MB
[2022-10-02 19:14:05 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:05 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0156 (0.0156)	loss 0.0091 (0.0091)	grad_norm 0.0539 (0.0539)	mem 460MB
[2022-10-02 19:14:05 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:05 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0205 (0.0205)	loss 0.0112 (0.0112)	grad_norm 0.0830 (0.0830)	mem 460MB
[2022-10-02 19:14:05 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:05 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0176 (0.0176)	loss 0.0108 (0.0108)	grad_norm 0.0453 (0.0453)	mem 460MB
[2022-10-02 19:14:05 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:05 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0222 (0.0222)	loss 0.0094 (0.0094)	grad_norm 0.0533 (0.0533)	mem 460MB
[2022-10-02 19:14:05 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:06 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0206 (0.0206)	loss 0.0099 (0.0099)	grad_norm 0.0499 (0.0499)	mem 460MB
[2022-10-02 19:14:06 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:06 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0219 (0.0219)	loss 0.0084 (0.0084)	grad_norm 0.0567 (0.0567)	mem 460MB
[2022-10-02 19:14:06 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:06 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0214 (0.0214)	loss 0.0076 (0.0076)	grad_norm 0.0415 (0.0415)	mem 460MB
[2022-10-02 19:14:06 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:06 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0228 (0.0228)	loss 0.0086 (0.0086)	grad_norm 0.0288 (0.0288)	mem 460MB
[2022-10-02 19:14:06 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:06 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0202 (0.0202)	loss 0.0102 (0.0102)	grad_norm 0.0383 (0.0383)	mem 460MB
[2022-10-02 19:14:06 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:07 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0196 (0.0196)	loss 0.0077 (0.0077)	grad_norm 0.0345 (0.0345)	mem 460MB
[2022-10-02 19:14:07 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:07 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0216 (0.0216)	loss 0.0103 (0.0103)	grad_norm 0.0478 (0.0478)	mem 460MB
[2022-10-02 19:14:07 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:07 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0104 (0.0104)	loss 0.0072 (0.0072)	grad_norm 0.0315 (0.0315)	mem 460MB
[2022-10-02 19:14:07 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:07 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0220 (0.0220)	loss 0.0090 (0.0090)	grad_norm 0.0433 (0.0433)	mem 460MB
[2022-10-02 19:14:07 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:07 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0142 (0.0142)	loss 0.0097 (0.0097)	grad_norm 0.0559 (0.0559)	mem 460MB
[2022-10-02 19:14:07 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:07 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0200 (0.0200)	loss 0.0097 (0.0097)	grad_norm 0.0367 (0.0367)	mem 460MB
[2022-10-02 19:14:07 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:08 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0141 (0.0141)	loss 0.0072 (0.0072)	grad_norm 0.0459 (0.0459)	mem 460MB
[2022-10-02 19:14:08 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:08 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0199 (0.0199)	loss 0.0100 (0.0100)	grad_norm 0.0703 (0.0703)	mem 460MB
[2022-10-02 19:14:08 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:08 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0094 (0.0094)	loss 0.0088 (0.0088)	grad_norm 0.0420 (0.0420)	mem 460MB
[2022-10-02 19:14:08 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:08 demo] (houston_program2.py 243): INFO Train: [67/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0095 (0.0095)	loss 0.0092 (0.0092)	grad_norm 0.0409 (0.0409)	mem 460MB
[2022-10-02 19:14:08 demo] (houston_program2.py 252): INFO EPOCH 67 training takes 0:00:00
[2022-10-02 19:14:08 demo] (houston_program2.py 333): INFO Train Ep: 67 	Loss1: 0.168591	Loss2: 0.134848	 Dis: 5.312763 Entropy: 5.152520 
[2022-10-02 19:14:08 demo] (houston_program2.py 335): INFO time_67_epoch:7.206993579864502
[2022-10-02 19:14:09 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0116 (0.0116)	loss 0.0085 (0.0085)	grad_norm 0.0798 (0.0798)	mem 460MB
[2022-10-02 19:14:09 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:09 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0194 (0.0194)	loss 0.0078 (0.0078)	grad_norm 0.0677 (0.0677)	mem 460MB
[2022-10-02 19:14:09 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:09 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0200 (0.0200)	loss 0.0088 (0.0088)	grad_norm 0.0432 (0.0432)	mem 460MB
[2022-10-02 19:14:09 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:09 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0209 (0.0209)	loss 0.0125 (0.0125)	grad_norm 0.0512 (0.0512)	mem 460MB
[2022-10-02 19:14:09 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:09 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0220 (0.0220)	loss 0.0084 (0.0084)	grad_norm 0.0608 (0.0608)	mem 460MB
[2022-10-02 19:14:09 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:09 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0203 (0.0203)	loss 0.0099 (0.0099)	grad_norm 0.0680 (0.0680)	mem 460MB
[2022-10-02 19:14:09 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:10 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0210 (0.0210)	loss 0.0097 (0.0097)	grad_norm 0.0400 (0.0400)	mem 460MB
[2022-10-02 19:14:10 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:10 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000012	time 0.0211 (0.0211)	loss 0.0079 (0.0079)	grad_norm 0.0613 (0.0613)	mem 460MB
[2022-10-02 19:14:10 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:10 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0219 (0.0219)	loss 0.0102 (0.0102)	grad_norm 0.0592 (0.0592)	mem 460MB
[2022-10-02 19:14:10 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:10 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0220 (0.0220)	loss 0.0086 (0.0086)	grad_norm 0.0313 (0.0313)	mem 460MB
[2022-10-02 19:14:10 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:10 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0296 (0.0296)	loss 0.0099 (0.0099)	grad_norm 0.0427 (0.0427)	mem 460MB
[2022-10-02 19:14:10 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:11 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0266 (0.0266)	loss 0.0080 (0.0080)	grad_norm 0.0590 (0.0590)	mem 460MB
[2022-10-02 19:14:11 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:11 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0128 (0.0128)	loss 0.0070 (0.0070)	grad_norm 0.0426 (0.0426)	mem 460MB
[2022-10-02 19:14:11 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:11 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0265 (0.0265)	loss 0.0092 (0.0092)	grad_norm 0.0339 (0.0339)	mem 460MB
[2022-10-02 19:14:11 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:11 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0144 (0.0144)	loss 0.0089 (0.0089)	grad_norm 0.0477 (0.0477)	mem 460MB
[2022-10-02 19:14:11 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:11 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0230 (0.0230)	loss 0.0084 (0.0084)	grad_norm 0.0402 (0.0402)	mem 460MB
[2022-10-02 19:14:11 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:11 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0197 (0.0197)	loss 0.0079 (0.0079)	grad_norm 0.0514 (0.0514)	mem 460MB
[2022-10-02 19:14:11 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:12 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0213 (0.0213)	loss 0.0154 (0.0154)	grad_norm 0.0535 (0.0535)	mem 460MB
[2022-10-02 19:14:12 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:12 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0214 (0.0214)	loss 0.0091 (0.0091)	grad_norm 0.0385 (0.0385)	mem 460MB
[2022-10-02 19:14:12 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:12 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0200 (0.0200)	loss 0.0090 (0.0090)	grad_norm 0.0543 (0.0543)	mem 460MB
[2022-10-02 19:14:12 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:12 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0202 (0.0202)	loss 0.0084 (0.0084)	grad_norm 0.0627 (0.0627)	mem 460MB
[2022-10-02 19:14:12 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:12 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0206 (0.0206)	loss 0.0081 (0.0081)	grad_norm 0.0515 (0.0515)	mem 460MB
[2022-10-02 19:14:12 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:13 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0224 (0.0224)	loss 0.0078 (0.0078)	grad_norm 0.0505 (0.0505)	mem 460MB
[2022-10-02 19:14:13 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:13 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0224 (0.0224)	loss 0.0098 (0.0098)	grad_norm 0.0490 (0.0490)	mem 460MB
[2022-10-02 19:14:13 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:13 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0218 (0.0218)	loss 0.0079 (0.0079)	grad_norm 0.0498 (0.0498)	mem 460MB
[2022-10-02 19:14:13 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:13 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0252 (0.0252)	loss 0.0084 (0.0084)	grad_norm 0.0393 (0.0393)	mem 460MB
[2022-10-02 19:14:13 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:13 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0220 (0.0220)	loss 0.0092 (0.0092)	grad_norm 0.0621 (0.0621)	mem 460MB
[2022-10-02 19:14:13 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:13 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0204 (0.0204)	loss 0.0108 (0.0108)	grad_norm 0.0360 (0.0360)	mem 460MB
[2022-10-02 19:14:13 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:14 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0219 (0.0219)	loss 0.0096 (0.0096)	grad_norm 0.0372 (0.0372)	mem 460MB
[2022-10-02 19:14:14 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:14 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0180 (0.0180)	loss 0.0097 (0.0097)	grad_norm 0.0368 (0.0368)	mem 460MB
[2022-10-02 19:14:14 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:14 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0215 (0.0215)	loss 0.0087 (0.0087)	grad_norm 0.0443 (0.0443)	mem 460MB
[2022-10-02 19:14:14 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:14 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0214 (0.0214)	loss 0.0067 (0.0067)	grad_norm 0.0746 (0.0746)	mem 460MB
[2022-10-02 19:14:14 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:14 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0222 (0.0222)	loss 0.0087 (0.0087)	grad_norm 0.0964 (0.0964)	mem 460MB
[2022-10-02 19:14:14 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:15 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0221 (0.0221)	loss 0.0074 (0.0074)	grad_norm 0.0350 (0.0350)	mem 460MB
[2022-10-02 19:14:15 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:15 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0190 (0.0190)	loss 0.0104 (0.0104)	grad_norm 0.0560 (0.0560)	mem 460MB
[2022-10-02 19:14:15 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:15 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0220 (0.0220)	loss 0.0095 (0.0095)	grad_norm 0.0735 (0.0735)	mem 460MB
[2022-10-02 19:14:15 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:15 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0138 (0.0138)	loss 0.0091 (0.0091)	grad_norm 0.0810 (0.0810)	mem 460MB
[2022-10-02 19:14:15 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:15 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0215 (0.0215)	loss 0.0098 (0.0098)	grad_norm 0.0794 (0.0794)	mem 460MB
[2022-10-02 19:14:15 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:15 demo] (houston_program2.py 243): INFO Train: [68/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0140 (0.0140)	loss 0.0094 (0.0094)	grad_norm 0.0687 (0.0687)	mem 460MB
[2022-10-02 19:14:15 demo] (houston_program2.py 252): INFO EPOCH 68 training takes 0:00:00
[2022-10-02 19:14:16 demo] (houston_program2.py 333): INFO Train Ep: 68 	Loss1: 0.331565	Loss2: 0.413529	 Dis: 6.078297 Entropy: 4.707369 
[2022-10-02 19:14:16 demo] (houston_program2.py 335): INFO time_68_epoch:7.590667724609375
[2022-10-02 19:14:16 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0130 (0.0130)	loss 0.0101 (0.0101)	grad_norm 0.0821 (0.0821)	mem 460MB
[2022-10-02 19:14:16 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:16 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0223 (0.0223)	loss 0.0120 (0.0120)	grad_norm 0.0773 (0.0773)	mem 460MB
[2022-10-02 19:14:16 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:16 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0221 (0.0221)	loss 0.0087 (0.0087)	grad_norm 0.0358 (0.0358)	mem 460MB
[2022-10-02 19:14:16 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:17 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0227 (0.0227)	loss 0.0098 (0.0098)	grad_norm 0.0803 (0.0803)	mem 460MB
[2022-10-02 19:14:17 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:17 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0233 (0.0233)	loss 0.0108 (0.0108)	grad_norm 0.0386 (0.0386)	mem 460MB
[2022-10-02 19:14:17 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:17 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0175 (0.0175)	loss 0.0115 (0.0115)	grad_norm 0.0425 (0.0425)	mem 460MB
[2022-10-02 19:14:17 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:17 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0219 (0.0219)	loss 0.0106 (0.0106)	grad_norm 0.0425 (0.0425)	mem 460MB
[2022-10-02 19:14:17 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:17 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0124 (0.0124)	loss 0.0071 (0.0071)	grad_norm 0.0621 (0.0621)	mem 460MB
[2022-10-02 19:14:17 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:18 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0222 (0.0222)	loss 0.0088 (0.0088)	grad_norm 0.0402 (0.0402)	mem 460MB
[2022-10-02 19:14:18 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:18 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0109 (0.0109)	loss 0.0098 (0.0098)	grad_norm 0.0426 (0.0426)	mem 460MB
[2022-10-02 19:14:18 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:18 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0220 (0.0220)	loss 0.0081 (0.0081)	grad_norm 0.0490 (0.0490)	mem 460MB
[2022-10-02 19:14:18 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:18 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0194 (0.0194)	loss 0.0081 (0.0081)	grad_norm 0.0513 (0.0513)	mem 460MB
[2022-10-02 19:14:18 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:18 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0221 (0.0221)	loss 0.0103 (0.0103)	grad_norm 0.0629 (0.0629)	mem 460MB
[2022-10-02 19:14:18 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:18 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0163 (0.0163)	loss 0.0114 (0.0114)	grad_norm 0.0380 (0.0380)	mem 460MB
[2022-10-02 19:14:18 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:19 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0201 (0.0201)	loss 0.0109 (0.0109)	grad_norm 0.0487 (0.0487)	mem 460MB
[2022-10-02 19:14:19 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:19 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0128 (0.0128)	loss 0.0095 (0.0095)	grad_norm 0.0495 (0.0495)	mem 460MB
[2022-10-02 19:14:19 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:19 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0206 (0.0206)	loss 0.0069 (0.0069)	grad_norm 0.0405 (0.0405)	mem 460MB
[2022-10-02 19:14:19 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:19 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0189 (0.0189)	loss 0.0062 (0.0062)	grad_norm 0.0351 (0.0351)	mem 460MB
[2022-10-02 19:14:19 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:19 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0219 (0.0219)	loss 0.0080 (0.0080)	grad_norm 0.0357 (0.0357)	mem 460MB
[2022-10-02 19:14:19 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:20 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0216 (0.0216)	loss 0.0099 (0.0099)	grad_norm 0.0355 (0.0355)	mem 460MB
[2022-10-02 19:14:20 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:20 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0205 (0.0205)	loss 0.0080 (0.0080)	grad_norm 0.0348 (0.0348)	mem 460MB
[2022-10-02 19:14:20 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:20 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0234 (0.0234)	loss 0.0071 (0.0071)	grad_norm 0.0422 (0.0422)	mem 460MB
[2022-10-02 19:14:20 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:20 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0200 (0.0200)	loss 0.0080 (0.0080)	grad_norm 0.0317 (0.0317)	mem 460MB
[2022-10-02 19:14:20 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:20 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0201 (0.0201)	loss 0.0092 (0.0092)	grad_norm 0.0356 (0.0356)	mem 460MB
[2022-10-02 19:14:20 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:20 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0206 (0.0206)	loss 0.0079 (0.0079)	grad_norm 0.0358 (0.0358)	mem 460MB
[2022-10-02 19:14:20 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:21 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0196 (0.0196)	loss 0.0102 (0.0102)	grad_norm 0.0792 (0.0792)	mem 460MB
[2022-10-02 19:14:21 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:21 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0154 (0.0154)	loss 0.0088 (0.0088)	grad_norm 0.0380 (0.0380)	mem 460MB
[2022-10-02 19:14:21 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:21 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0215 (0.0215)	loss 0.0134 (0.0134)	grad_norm 0.0547 (0.0547)	mem 460MB
[2022-10-02 19:14:21 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:21 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000011	time 0.0212 (0.0212)	loss 0.0119 (0.0119)	grad_norm 0.0375 (0.0375)	mem 460MB
[2022-10-02 19:14:21 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:21 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0189 (0.0189)	loss 0.0119 (0.0119)	grad_norm 0.0536 (0.0536)	mem 460MB
[2022-10-02 19:14:21 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:22 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0144 (0.0144)	loss 0.0086 (0.0086)	grad_norm 0.0263 (0.0263)	mem 460MB
[2022-10-02 19:14:22 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:22 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0213 (0.0213)	loss 0.0110 (0.0110)	grad_norm 0.0317 (0.0317)	mem 460MB
[2022-10-02 19:14:22 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:22 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0200 (0.0200)	loss 0.0104 (0.0104)	grad_norm 0.0493 (0.0493)	mem 460MB
[2022-10-02 19:14:22 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:22 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0207 (0.0207)	loss 0.0068 (0.0068)	grad_norm 0.0343 (0.0343)	mem 460MB
[2022-10-02 19:14:22 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:22 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0196 (0.0196)	loss 0.0088 (0.0088)	grad_norm 0.0439 (0.0439)	mem 460MB
[2022-10-02 19:14:22 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:22 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0216 (0.0216)	loss 0.0106 (0.0106)	grad_norm 0.0643 (0.0643)	mem 460MB
[2022-10-02 19:14:22 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:23 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0206 (0.0206)	loss 0.0070 (0.0070)	grad_norm 0.0406 (0.0406)	mem 460MB
[2022-10-02 19:14:23 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:23 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0204 (0.0204)	loss 0.0113 (0.0113)	grad_norm 0.0423 (0.0423)	mem 460MB
[2022-10-02 19:14:23 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:23 demo] (houston_program2.py 243): INFO Train: [69/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0201 (0.0201)	loss 0.0097 (0.0097)	grad_norm 0.0383 (0.0383)	mem 460MB
[2022-10-02 19:14:23 demo] (houston_program2.py 252): INFO EPOCH 69 training takes 0:00:00
[2022-10-02 19:14:23 demo] (houston_program2.py 333): INFO Train Ep: 69 	Loss1: 0.305293	Loss2: 0.346482	 Dis: 5.529022 Entropy: 4.696060 
[2022-10-02 19:14:23 demo] (houston_program2.py 335): INFO time_69_epoch:7.475437641143799
[2022-10-02 19:14:24 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0115 (0.0115)	loss 0.0089 (0.0089)	grad_norm 0.0367 (0.0367)	mem 460MB
[2022-10-02 19:14:24 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:24 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0211 (0.0211)	loss 0.0092 (0.0092)	grad_norm 0.0689 (0.0689)	mem 460MB
[2022-10-02 19:14:24 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:24 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0220 (0.0220)	loss 0.0107 (0.0107)	grad_norm 0.0456 (0.0456)	mem 460MB
[2022-10-02 19:14:24 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:24 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0248 (0.0248)	loss 0.0087 (0.0087)	grad_norm 0.0388 (0.0388)	mem 460MB
[2022-10-02 19:14:24 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:24 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0221 (0.0221)	loss 0.0087 (0.0087)	grad_norm 0.0700 (0.0700)	mem 460MB
[2022-10-02 19:14:24 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:24 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0250 (0.0250)	loss 0.0137 (0.0137)	grad_norm 0.0690 (0.0690)	mem 460MB
[2022-10-02 19:14:25 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:25 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0206 (0.0206)	loss 0.0113 (0.0113)	grad_norm 0.0458 (0.0458)	mem 460MB
[2022-10-02 19:14:25 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:25 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0126 (0.0126)	loss 0.0099 (0.0099)	grad_norm 0.0351 (0.0351)	mem 460MB
[2022-10-02 19:14:25 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:25 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0211 (0.0211)	loss 0.0119 (0.0119)	grad_norm 0.0356 (0.0356)	mem 460MB
[2022-10-02 19:14:25 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:25 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0123 (0.0123)	loss 0.0097 (0.0097)	grad_norm 0.0350 (0.0350)	mem 460MB
[2022-10-02 19:14:25 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:25 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0188 (0.0188)	loss 0.0094 (0.0094)	grad_norm 0.0375 (0.0375)	mem 460MB
[2022-10-02 19:14:25 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:26 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0140 (0.0140)	loss 0.0091 (0.0091)	grad_norm 0.0389 (0.0389)	mem 460MB
[2022-10-02 19:14:26 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:26 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0204 (0.0204)	loss 0.0084 (0.0084)	grad_norm 0.0498 (0.0498)	mem 460MB
[2022-10-02 19:14:26 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:26 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0136 (0.0136)	loss 0.0121 (0.0121)	grad_norm 0.0471 (0.0471)	mem 460MB
[2022-10-02 19:14:26 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:26 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0210 (0.0210)	loss 0.0095 (0.0095)	grad_norm 0.0368 (0.0368)	mem 460MB
[2022-10-02 19:14:26 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:26 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0127 (0.0127)	loss 0.0102 (0.0102)	grad_norm 0.0391 (0.0391)	mem 460MB
[2022-10-02 19:14:26 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:26 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0200 (0.0200)	loss 0.0087 (0.0087)	grad_norm 0.0432 (0.0432)	mem 460MB
[2022-10-02 19:14:26 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:27 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0192 (0.0192)	loss 0.0090 (0.0090)	grad_norm 0.0324 (0.0324)	mem 460MB
[2022-10-02 19:14:27 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:27 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0203 (0.0203)	loss 0.0082 (0.0082)	grad_norm 0.0508 (0.0508)	mem 460MB
[2022-10-02 19:14:27 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:27 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0158 (0.0158)	loss 0.0098 (0.0098)	grad_norm 0.0425 (0.0425)	mem 460MB
[2022-10-02 19:14:27 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:27 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0200 (0.0200)	loss 0.0106 (0.0106)	grad_norm 0.0690 (0.0690)	mem 460MB
[2022-10-02 19:14:27 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:27 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0217 (0.0217)	loss 0.0107 (0.0107)	grad_norm 0.0406 (0.0406)	mem 460MB
[2022-10-02 19:14:27 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:27 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0204 (0.0204)	loss 0.0089 (0.0089)	grad_norm 0.0384 (0.0384)	mem 460MB
[2022-10-02 19:14:28 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:28 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0223 (0.0223)	loss 0.0072 (0.0072)	grad_norm 0.0428 (0.0428)	mem 460MB
[2022-10-02 19:14:28 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:28 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0229 (0.0229)	loss 0.0073 (0.0073)	grad_norm 0.0607 (0.0607)	mem 460MB
[2022-10-02 19:14:28 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:28 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0218 (0.0218)	loss 0.0097 (0.0097)	grad_norm 0.0444 (0.0444)	mem 460MB
[2022-10-02 19:14:28 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:28 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0234 (0.0234)	loss 0.0088 (0.0088)	grad_norm 0.0331 (0.0331)	mem 460MB
[2022-10-02 19:14:28 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:28 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0210 (0.0210)	loss 0.0081 (0.0081)	grad_norm 0.0407 (0.0407)	mem 460MB
[2022-10-02 19:14:28 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:29 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0140 (0.0140)	loss 0.0095 (0.0095)	grad_norm 0.0498 (0.0498)	mem 460MB
[2022-10-02 19:14:29 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:29 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0217 (0.0217)	loss 0.0088 (0.0088)	grad_norm 0.0562 (0.0562)	mem 460MB
[2022-10-02 19:14:29 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:29 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0117 (0.0117)	loss 0.0077 (0.0077)	grad_norm 0.0689 (0.0689)	mem 460MB
[2022-10-02 19:14:29 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:29 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0211 (0.0211)	loss 0.0080 (0.0080)	grad_norm 0.0453 (0.0453)	mem 460MB
[2022-10-02 19:14:29 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:29 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0124 (0.0124)	loss 0.0096 (0.0096)	grad_norm 0.0439 (0.0439)	mem 460MB
[2022-10-02 19:14:29 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:29 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0208 (0.0208)	loss 0.0076 (0.0076)	grad_norm 0.0566 (0.0566)	mem 460MB
[2022-10-02 19:14:29 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:30 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0143 (0.0143)	loss 0.0098 (0.0098)	grad_norm 0.0403 (0.0403)	mem 460MB
[2022-10-02 19:14:30 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:30 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0213 (0.0213)	loss 0.0119 (0.0119)	grad_norm 0.0733 (0.0733)	mem 460MB
[2022-10-02 19:14:30 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:30 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0197 (0.0197)	loss 0.0091 (0.0091)	grad_norm 0.0411 (0.0411)	mem 460MB
[2022-10-02 19:14:30 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:30 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0211 (0.0211)	loss 0.0082 (0.0082)	grad_norm 0.0739 (0.0739)	mem 460MB
[2022-10-02 19:14:30 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:30 demo] (houston_program2.py 243): INFO Train: [70/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0204 (0.0204)	loss 0.0078 (0.0078)	grad_norm 0.0385 (0.0385)	mem 460MB
[2022-10-02 19:14:30 demo] (houston_program2.py 252): INFO EPOCH 70 training takes 0:00:00
[2022-10-02 19:14:31 demo] (houston_program2.py 333): INFO Train Ep: 70 	Loss1: 0.118244	Loss2: 0.127938	 Dis: 4.551769 Entropy: 5.054210 
[2022-10-02 19:14:31 demo] (houston_program2.py 335): INFO time_70_epoch:7.383592128753662
[2022-10-02 19:14:31 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 19:14:31 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 19:14:31 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 19:14:31 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 19:14:31 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 19:14:31 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 19:14:31 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 19:14:31 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 19:14:31 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 19:14:31 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 19:14:36 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.030460	Loss2: 0.036725	 Dis: 6.454296 Entropy: 5.214254 
[2022-10-02 19:14:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.000487	 E_lr_2:0.000487
[2022-10-02 19:14:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 19:14:42 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.057167	Loss2: 0.057110	 Dis: 6.661819 Entropy: 5.216207 
[2022-10-02 19:14:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.000987	 E_lr_2:0.000987
[2022-10-02 19:14:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 19:14:48 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.224394	Loss2: 0.240673	 Dis: 3.550381 Entropy: 4.650880 
[2022-10-02 19:14:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.001487	 E_lr_2:0.001487
[2022-10-02 19:14:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 19:14:55 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.258309	Loss2: 0.249391	 Dis: 5.773121 Entropy: 5.128613 
[2022-10-02 19:14:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.001987	 E_lr_2:0.001987
[2022-10-02 19:14:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 19:15:01 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.042612	Loss2: 0.030160	 Dis: 4.634102 Entropy: 5.300568 
[2022-10-02 19:15:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.002487	 E_lr_2:0.002487
[2022-10-02 19:15:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 19:15:07 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.167186	Loss2: 0.164685	 Dis: 5.818619 Entropy: 5.040724 
[2022-10-02 19:15:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.002987	 E_lr_2:0.002987
[2022-10-02 19:15:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 19:15:13 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.042778	Loss2: 0.038723	 Dis: 4.845270 Entropy: 5.521427 
[2022-10-02 19:15:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.003487	 E_lr_2:0.003487
[2022-10-02 19:15:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 19:15:19 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.054598	Loss2: 0.050721	 Dis: 6.587191 Entropy: 5.546740 
[2022-10-02 19:15:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.003987	 E_lr_2:0.003987
[2022-10-02 19:15:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 19:15:25 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.053201	Loss2: 0.059980	 Dis: 5.231117 Entropy: 5.228781 
[2022-10-02 19:15:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.004487	 E_lr_2:0.004487
[2022-10-02 19:15:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 19:15:31 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.065994	Loss2: 0.070928	 Dis: 4.833185 Entropy: 4.949563 
[2022-10-02 19:15:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.004987	 E_lr_2:0.004987
[2022-10-02 19:15:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000050	 C_lr_2:0.000050	 C_lr_3:0.000050	 C_lr_4:0.000050
[2022-10-02 19:15:37 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.042426	Loss2: 0.037886	 Dis: 4.847466 Entropy: 6.480206 
[2022-10-02 19:15:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.004853	 E_lr_2:0.004853
[2022-10-02 19:15:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000049	 C_lr_2:0.000049	 C_lr_3:0.000049	 C_lr_4:0.000049
[2022-10-02 19:15:43 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.399685	Loss2: 0.481111	 Dis: 5.215969 Entropy: 5.249630 
[2022-10-02 19:15:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.004825	 E_lr_2:0.004825
[2022-10-02 19:15:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 19:15:49 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.035294	Loss2: 0.050597	 Dis: 4.606373 Entropy: 4.812720 
[2022-10-02 19:15:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.004795	 E_lr_2:0.004795
[2022-10-02 19:15:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 19:15:55 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.021431	Loss2: 0.015247	 Dis: 3.357336 Entropy: 5.934705 
[2022-10-02 19:15:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.004763	 E_lr_2:0.004763
[2022-10-02 19:15:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 19:16:01 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.202745	Loss2: 0.242673	 Dis: 2.654154 Entropy: 6.218908 
[2022-10-02 19:16:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.004728	 E_lr_2:0.004728
[2022-10-02 19:16:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 19:16:07 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.207911	Loss2: 0.209242	 Dis: 3.887182 Entropy: 5.649464 
[2022-10-02 19:16:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.004692	 E_lr_2:0.004692
[2022-10-02 19:16:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 19:16:13 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.215714	Loss2: 0.186221	 Dis: 6.076843 Entropy: 4.801476 
[2022-10-02 19:16:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.004653	 E_lr_2:0.004653
[2022-10-02 19:16:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 19:16:19 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.173021	Loss2: 0.146787	 Dis: 3.976324 Entropy: 4.991309 
[2022-10-02 19:16:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.004612	 E_lr_2:0.004612
[2022-10-02 19:16:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 19:16:25 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.059492	Loss2: 0.065967	 Dis: 6.961311 Entropy: 5.208160 
[2022-10-02 19:16:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.004569	 E_lr_2:0.004569
[2022-10-02 19:16:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 19:16:31 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.102633	Loss2: 0.080636	 Dis: 3.661837 Entropy: 5.147743 
[2022-10-02 19:16:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.004524	 E_lr_2:0.004524
[2022-10-02 19:16:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 19:16:38 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.028468	Loss2: 0.023261	 Dis: 3.438999 Entropy: 4.462750 
[2022-10-02 19:16:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.004477	 E_lr_2:0.004477
[2022-10-02 19:16:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 19:16:44 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.086556	Loss2: 0.088902	 Dis: 4.770336 Entropy: 4.933100 
[2022-10-02 19:16:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.004428	 E_lr_2:0.004428
[2022-10-02 19:16:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 19:16:49 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.115203	Loss2: 0.139715	 Dis: 4.277401 Entropy: 5.064929 
[2022-10-02 19:16:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.004377	 E_lr_2:0.004377
[2022-10-02 19:16:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 19:16:55 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.098059	Loss2: 0.094111	 Dis: 2.411844 Entropy: 4.448452 
[2022-10-02 19:16:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.004324	 E_lr_2:0.004324
[2022-10-02 19:16:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 19:17:01 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.072637	Loss2: 0.075865	 Dis: 5.432692 Entropy: 4.517488 
[2022-10-02 19:17:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.004269	 E_lr_2:0.004269
[2022-10-02 19:17:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 19:17:08 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.083429	Loss2: 0.086339	 Dis: 2.667675 Entropy: 4.373387 
[2022-10-02 19:17:08 demo] (houston_program2.py 512): INFO E_lr_1: 0.004213	 E_lr_2:0.004213
[2022-10-02 19:17:08 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 19:17:13 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.733296	Loss2: 0.706979	 Dis: 10.744379 Entropy: 4.297947 
[2022-10-02 19:17:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.004155	 E_lr_2:0.004155
[2022-10-02 19:17:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 19:17:19 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.180650	Loss2: 0.251652	 Dis: 6.460419 Entropy: 4.577035 
[2022-10-02 19:17:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.004095	 E_lr_2:0.004095
[2022-10-02 19:17:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000041	 C_lr_2:0.000041	 C_lr_3:0.000041	 C_lr_4:0.000041
[2022-10-02 19:17:25 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.157920	Loss2: 0.153645	 Dis: 6.309275 Entropy: 4.584663 
[2022-10-02 19:17:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.004034	 E_lr_2:0.004034
[2022-10-02 19:17:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 19:17:31 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.064001	Loss2: 0.064786	 Dis: 2.057695 Entropy: 4.695173 
[2022-10-02 19:17:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.003971	 E_lr_2:0.003971
[2022-10-02 19:17:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 19:17:37 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.043343	Loss2: 0.043854	 Dis: 3.135756 Entropy: 5.683470 
[2022-10-02 19:17:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.003907	 E_lr_2:0.003907
[2022-10-02 19:17:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000039	 C_lr_2:0.000039	 C_lr_3:0.000039	 C_lr_4:0.000039
[2022-10-02 19:17:43 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.211043	Loss2: 0.217273	 Dis: 3.686396 Entropy: 4.692565 
[2022-10-02 19:17:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.003841	 E_lr_2:0.003841
[2022-10-02 19:17:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 19:17:49 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.057982	Loss2: 0.053990	 Dis: 2.727598 Entropy: 5.421804 
[2022-10-02 19:17:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.003774	 E_lr_2:0.003774
[2022-10-02 19:17:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 19:17:55 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.127501	Loss2: 0.121690	 Dis: 3.429087 Entropy: 4.453619 
[2022-10-02 19:17:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.003706	 E_lr_2:0.003706
[2022-10-02 19:17:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000037	 C_lr_2:0.000037	 C_lr_3:0.000037	 C_lr_4:0.000037
[2022-10-02 19:18:01 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.039146	Loss2: 0.036341	 Dis: 4.532021 Entropy: 4.542645 
[2022-10-02 19:18:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.003637	 E_lr_2:0.003637
[2022-10-02 19:18:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 19:18:08 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.016888	Loss2: 0.018794	 Dis: 3.371050 Entropy: 5.284620 
[2022-10-02 19:18:08 demo] (houston_program2.py 512): INFO E_lr_1: 0.003566	 E_lr_2:0.003566
[2022-10-02 19:18:08 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 19:18:14 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.093413	Loss2: 0.092577	 Dis: 3.958456 Entropy: 5.983932 
[2022-10-02 19:18:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.003495	 E_lr_2:0.003495
[2022-10-02 19:18:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 19:18:20 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.038186	Loss2: 0.034046	 Dis: 4.460918 Entropy: 5.291724 
[2022-10-02 19:18:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.003422	 E_lr_2:0.003422
[2022-10-02 19:18:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000034	 C_lr_2:0.000034	 C_lr_3:0.000034	 C_lr_4:0.000034
[2022-10-02 19:18:26 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.092467	Loss2: 0.086738	 Dis: 4.080616 Entropy: 5.160806 
[2022-10-02 19:18:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.003349	 E_lr_2:0.003349
[2022-10-02 19:18:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 19:18:32 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.133356	Loss2: 0.134474	 Dis: 4.153202 Entropy: 4.494967 
[2022-10-02 19:18:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.003274	 E_lr_2:0.003274
[2022-10-02 19:18:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 19:18:38 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.112375	Loss2: 0.128387	 Dis: 7.821402 Entropy: 5.198287 
[2022-10-02 19:18:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.003199	 E_lr_2:0.003199
[2022-10-02 19:18:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000032	 C_lr_2:0.000032	 C_lr_3:0.000032	 C_lr_4:0.000032
[2022-10-02 19:18:45 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.176319	Loss2: 0.152444	 Dis: 8.170084 Entropy: 4.710349 
[2022-10-02 19:18:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.003124	 E_lr_2:0.003124
[2022-10-02 19:18:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000031	 C_lr_2:0.000031	 C_lr_3:0.000031	 C_lr_4:0.000031
[2022-10-02 19:18:51 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.065039	Loss2: 0.071501	 Dis: 5.986454 Entropy: 5.986394 
[2022-10-02 19:18:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.003047	 E_lr_2:0.003047
[2022-10-02 19:18:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 19:18:57 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.105983	Loss2: 0.089077	 Dis: 5.674530 Entropy: 5.427818 
[2022-10-02 19:18:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.002970	 E_lr_2:0.002970
[2022-10-02 19:18:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 19:19:03 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.213095	Loss2: 0.241384	 Dis: 5.549873 Entropy: 4.600417 
[2022-10-02 19:19:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.002893	 E_lr_2:0.002893
[2022-10-02 19:19:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000029	 C_lr_2:0.000029	 C_lr_3:0.000029	 C_lr_4:0.000029
[2022-10-02 19:19:09 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.043764	Loss2: 0.050721	 Dis: 4.458523 Entropy: 4.345763 
[2022-10-02 19:19:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.002815	 E_lr_2:0.002815
[2022-10-02 19:19:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000028	 C_lr_2:0.000028	 C_lr_3:0.000028	 C_lr_4:0.000028
[2022-10-02 19:19:16 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.155373	Loss2: 0.169664	 Dis: 6.298018 Entropy: 6.423213 
[2022-10-02 19:19:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.002737	 E_lr_2:0.002737
[2022-10-02 19:19:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 19:19:22 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.084450	Loss2: 0.090496	 Dis: 4.036053 Entropy: 5.564540 
[2022-10-02 19:19:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.002659	 E_lr_2:0.002659
[2022-10-02 19:19:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 19:19:28 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.034089	Loss2: 0.040146	 Dis: 4.193808 Entropy: 4.987311 
[2022-10-02 19:19:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.002581	 E_lr_2:0.002581
[2022-10-02 19:19:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000026	 C_lr_2:0.000026	 C_lr_3:0.000026	 C_lr_4:0.000026
[2022-10-02 19:19:34 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.064827	Loss2: 0.043147	 Dis: 4.281776 Entropy: 4.409046 
[2022-10-02 19:19:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.002502	 E_lr_2:0.002502
[2022-10-02 19:19:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 19:19:40 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.054603	Loss2: 0.041012	 Dis: 4.700808 Entropy: 4.539823 
[2022-10-02 19:19:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.002423	 E_lr_2:0.002423
[2022-10-02 19:19:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000024	 C_lr_2:0.000024	 C_lr_3:0.000024	 C_lr_4:0.000024
[2022-10-02 19:19:46 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.012362	Loss2: 0.014145	 Dis: 4.338087 Entropy: 4.562984 
[2022-10-02 19:19:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.002345	 E_lr_2:0.002345
[2022-10-02 19:19:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 19:19:52 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.026858	Loss2: 0.037045	 Dis: 2.734219 Entropy: 4.489704 
[2022-10-02 19:19:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.002267	 E_lr_2:0.002267
[2022-10-02 19:19:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 19:19:58 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.017711	Loss2: 0.028015	 Dis: 6.313978 Entropy: 5.154615 
[2022-10-02 19:19:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.002189	 E_lr_2:0.002189
[2022-10-02 19:19:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000022	 C_lr_2:0.000022	 C_lr_3:0.000022	 C_lr_4:0.000022
[2022-10-02 19:20:04 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.016809	Loss2: 0.011914	 Dis: 3.830233 Entropy: 4.450359 
[2022-10-02 19:20:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.002111	 E_lr_2:0.002111
[2022-10-02 19:20:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000021	 C_lr_2:0.000021	 C_lr_3:0.000021	 C_lr_4:0.000021
[2022-10-02 19:20:10 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.076672	Loss2: 0.083235	 Dis: 3.421841 Entropy: 5.747686 
[2022-10-02 19:20:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.002034	 E_lr_2:0.002034
[2022-10-02 19:20:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 19:20:15 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.015363	Loss2: 0.013915	 Dis: 2.492920 Entropy: 4.334172 
[2022-10-02 19:20:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.001957	 E_lr_2:0.001957
[2022-10-02 19:20:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 19:20:21 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.009834	Loss2: 0.008864	 Dis: 3.073557 Entropy: 5.505779 
[2022-10-02 19:20:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.001880	 E_lr_2:0.001880
[2022-10-02 19:20:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000019	 C_lr_2:0.000019	 C_lr_3:0.000019	 C_lr_4:0.000019
[2022-10-02 19:20:27 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.104237	Loss2: 0.090510	 Dis: 3.103733 Entropy: 5.402572 
[2022-10-02 19:20:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.001804	 E_lr_2:0.001804
[2022-10-02 19:20:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000018	 C_lr_2:0.000018	 C_lr_3:0.000018	 C_lr_4:0.000018
[2022-10-02 19:20:32 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.019476	Loss2: 0.020051	 Dis: 3.510048 Entropy: 5.502592 
[2022-10-02 19:20:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.001729	 E_lr_2:0.001729
[2022-10-02 19:20:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 19:20:38 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.015570	Loss2: 0.012810	 Dis: 3.114040 Entropy: 4.620806 
[2022-10-02 19:20:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.001655	 E_lr_2:0.001655
[2022-10-02 19:20:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 19:20:43 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.029575	Loss2: 0.031567	 Dis: 3.526779 Entropy: 4.649077 
[2022-10-02 19:20:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.001582	 E_lr_2:0.001582
[2022-10-02 19:20:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000016	 C_lr_2:0.000016	 C_lr_3:0.000016	 C_lr_4:0.000016
[2022-10-02 19:20:48 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.095760	Loss2: 0.121628	 Dis: 3.999107 Entropy: 4.349078 
[2022-10-02 19:20:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.001509	 E_lr_2:0.001509
[2022-10-02 19:20:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 19:20:53 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.011053	Loss2: 0.009504	 Dis: 3.009150 Entropy: 5.710083 
[2022-10-02 19:20:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.001437	 E_lr_2:0.001437
[2022-10-02 19:20:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 19:20:59 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.006296	Loss2: 0.004773	 Dis: 2.366720 Entropy: 6.240004 
[2022-10-02 19:20:59 demo] (houston_program2.py 512): INFO E_lr_1: 0.001367	 E_lr_2:0.001367
[2022-10-02 19:20:59 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 19:21:05 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.010994	Loss2: 0.014420	 Dis: 1.810703 Entropy: 5.140742 
[2022-10-02 19:21:05 demo] (houston_program2.py 512): INFO E_lr_1: 0.001297	 E_lr_2:0.001297
[2022-10-02 19:21:05 demo] (houston_program2.py 513): INFO C_lr_1: 0.000013	 C_lr_2:0.000013	 C_lr_3:0.000013	 C_lr_4:0.000013
[2022-10-02 19:21:11 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.005575	Loss2: 0.005417	 Dis: 3.099697 Entropy: 4.465856 
[2022-10-02 19:21:11 demo] (houston_program2.py 512): INFO E_lr_1: 0.001229	 E_lr_2:0.001229
[2022-10-02 19:21:11 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 19:21:17 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.026886	Loss2: 0.036076	 Dis: 1.644930 Entropy: 4.276101 
[2022-10-02 19:21:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.001162	 E_lr_2:0.001162
[2022-10-02 19:21:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 19:21:22 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.025760	Loss2: 0.033073	 Dis: 2.534960 Entropy: 5.022918 
[2022-10-02 19:21:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.001096	 E_lr_2:0.001096
[2022-10-02 19:21:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000011	 C_lr_2:0.000011	 C_lr_3:0.000011	 C_lr_4:0.000011
[2022-10-02 19:21:29 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.012513	Loss2: 0.010147	 Dis: 2.392101 Entropy: 4.777236 
[2022-10-02 19:21:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.001032	 E_lr_2:0.001032
[2022-10-02 19:21:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 19:21:34 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.018508	Loss2: 0.014559	 Dis: 1.647062 Entropy: 4.971189 
[2022-10-02 19:21:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.000969	 E_lr_2:0.000969
[2022-10-02 19:21:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 19:21:41 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.013411	Loss2: 0.012965	 Dis: 3.655804 Entropy: 4.999878 
[2022-10-02 19:21:41 demo] (houston_program2.py 512): INFO E_lr_1: 0.000908	 E_lr_2:0.000908
[2022-10-02 19:21:41 demo] (houston_program2.py 513): INFO C_lr_1: 0.000009	 C_lr_2:0.000009	 C_lr_3:0.000009	 C_lr_4:0.000009
[2022-10-02 19:21:47 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.005669	Loss2: 0.005105	 Dis: 2.707840 Entropy: 4.551862 
[2022-10-02 19:21:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.000848	 E_lr_2:0.000848
[2022-10-02 19:21:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 19:21:53 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.006529	Loss2: 0.006704	 Dis: 2.040174 Entropy: 4.323276 
[2022-10-02 19:21:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.000790	 E_lr_2:0.000790
[2022-10-02 19:21:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 19:21:59 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.020538	Loss2: 0.024146	 Dis: 1.249510 Entropy: 6.011414 
[2022-10-02 19:21:59 demo] (houston_program2.py 512): INFO E_lr_1: 0.000734	 E_lr_2:0.000734
[2022-10-02 19:21:59 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 19:22:05 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.006833	Loss2: 0.007895	 Dis: 1.785728 Entropy: 5.000713 
[2022-10-02 19:22:05 demo] (houston_program2.py 512): INFO E_lr_1: 0.000679	 E_lr_2:0.000679
[2022-10-02 19:22:05 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 19:22:11 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.026753	Loss2: 0.025226	 Dis: 5.057486 Entropy: 4.831281 
[2022-10-02 19:22:11 demo] (houston_program2.py 512): INFO E_lr_1: 0.000626	 E_lr_2:0.000626
[2022-10-02 19:22:11 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 19:22:17 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.014771	Loss2: 0.014664	 Dis: 2.453331 Entropy: 5.341913 
[2022-10-02 19:22:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.000575	 E_lr_2:0.000575
[2022-10-02 19:22:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 19:22:23 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.004897	Loss2: 0.003666	 Dis: 1.816753 Entropy: 4.864729 
[2022-10-02 19:22:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.000526	 E_lr_2:0.000526
[2022-10-02 19:22:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 19:22:30 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.007851	Loss2: 0.007432	 Dis: 2.788017 Entropy: 4.625539 
[2022-10-02 19:22:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.000479	 E_lr_2:0.000479
[2022-10-02 19:22:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 19:22:36 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.002687	Loss2: 0.002955	 Dis: 1.325764 Entropy: 5.590569 
[2022-10-02 19:22:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.000433	 E_lr_2:0.000433
[2022-10-02 19:22:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 19:22:42 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.008457	Loss2: 0.008232	 Dis: 0.846092 Entropy: 4.426566 
[2022-10-02 19:22:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.000390	 E_lr_2:0.000390
[2022-10-02 19:22:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 19:22:48 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.007968	Loss2: 0.011918	 Dis: 1.902939 Entropy: 5.490144 
[2022-10-02 19:22:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.000349	 E_lr_2:0.000349
[2022-10-02 19:22:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 19:22:55 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.013936	Loss2: 0.012209	 Dis: 1.572838 Entropy: 4.263838 
[2022-10-02 19:22:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.000310	 E_lr_2:0.000310
[2022-10-02 19:22:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 19:23:01 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.002368	Loss2: 0.002475	 Dis: 1.452633 Entropy: 5.348815 
[2022-10-02 19:23:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.000273	 E_lr_2:0.000273
[2022-10-02 19:23:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 19:23:07 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.002941	Loss2: 0.002556	 Dis: 0.906143 Entropy: 4.320596 
[2022-10-02 19:23:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.000239	 E_lr_2:0.000239
[2022-10-02 19:23:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 19:23:13 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.002153	Loss2: 0.002810	 Dis: 0.935619 Entropy: 4.580039 
[2022-10-02 19:23:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.000206	 E_lr_2:0.000206
[2022-10-02 19:23:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 19:23:19 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.004651	Loss2: 0.004481	 Dis: 2.928654 Entropy: 5.390846 
[2022-10-02 19:23:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.000176	 E_lr_2:0.000176
[2022-10-02 19:23:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 19:23:26 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.003806	Loss2: 0.006891	 Dis: 3.060778 Entropy: 4.362949 
[2022-10-02 19:23:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.000148	 E_lr_2:0.000148
[2022-10-02 19:23:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:23:31 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.001744	Loss2: 0.001423	 Dis: 1.017210 Entropy: 4.450866 
[2022-10-02 19:23:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.000123	 E_lr_2:0.000123
[2022-10-02 19:23:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:23:38 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.014128	Loss2: 0.007893	 Dis: 1.175241 Entropy: 5.587823 
[2022-10-02 19:23:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.000100	 E_lr_2:0.000100
[2022-10-02 19:23:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:23:44 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.002955	Loss2: 0.002303	 Dis: 1.397350 Entropy: 5.892348 
[2022-10-02 19:23:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.000079	 E_lr_2:0.000079
[2022-10-02 19:23:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:23:50 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.002867	Loss2: 0.004062	 Dis: 2.542171 Entropy: 4.403895 
[2022-10-02 19:23:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.000061	 E_lr_2:0.000061
[2022-10-02 19:23:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:23:56 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.004205	Loss2: 0.003979	 Dis: 1.790920 Entropy: 5.003484 
[2022-10-02 19:23:56 demo] (houston_program2.py 512): INFO E_lr_1: 0.000045	 E_lr_2:0.000045
[2022-10-02 19:23:56 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:24:02 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.002159	Loss2: 0.001657	 Dis: 2.170183 Entropy: 4.642304 
[2022-10-02 19:24:02 demo] (houston_program2.py 512): INFO E_lr_1: 0.000031	 E_lr_2:0.000031
[2022-10-02 19:24:02 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:24:09 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.009417	Loss2: 0.008268	 Dis: 3.880880 Entropy: 5.316446 
[2022-10-02 19:24:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.000020	 E_lr_2:0.000020
[2022-10-02 19:24:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:24:15 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.002585	Loss2: 0.002684	 Dis: 0.896763 Entropy: 4.298512 
[2022-10-02 19:24:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.000011	 E_lr_2:0.000011
[2022-10-02 19:24:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:24:21 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.001478	Loss2: 0.001407	 Dis: 1.167343 Entropy: 4.662005 
[2022-10-02 19:24:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.000005	 E_lr_2:0.000005
[2022-10-02 19:24:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:24:25 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.006766	Loss2: 0.008665	 Dis: 0.688639 Entropy: 6.754086 
[2022-10-02 19:24:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.000001	 E_lr_2:0.000001
[2022-10-02 19:24:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:24:31 demo] (houston_program2.py 504): INFO Train Ep: 70 	Loss1: 0.009429	Loss2: 0.005430	 Dis: 1.958681 Entropy: 5.284992 
[2022-10-02 19:24:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.000000	 E_lr_2:0.000000
[2022-10-02 19:24:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:24:31 demo] (houston_program2.py 515): INFO time_70_epoch:600.0051121711731
[2022-10-02 19:24:38 demo] (houston_program2.py 673): INFO 	val_Accuracy: 32281/53200 (60.68%)	
[2022-10-02 19:24:38 demo] (utils.py 47): INFO outputs/demo/finetune_100/ckpt_epoch_70.pth saving......
[2022-10-02 19:24:38 demo] (utils.py 49): INFO outputs/demo/finetune_100/ckpt_epoch_70.pth saved !!!
[2022-10-02 19:24:39 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0122 (0.0122)	loss 0.0092 (0.0092)	grad_norm 0.0523 (0.0523)	mem 460MB
[2022-10-02 19:24:39 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:39 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0216 (0.0216)	loss 0.0086 (0.0086)	grad_norm 0.0362 (0.0362)	mem 460MB
[2022-10-02 19:24:39 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:39 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0193 (0.0193)	loss 0.0088 (0.0088)	grad_norm 0.0348 (0.0348)	mem 460MB
[2022-10-02 19:24:39 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:39 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0241 (0.0241)	loss 0.0100 (0.0100)	grad_norm 0.0339 (0.0339)	mem 460MB
[2022-10-02 19:24:39 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:40 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0212 (0.0212)	loss 0.0076 (0.0076)	grad_norm 0.0479 (0.0479)	mem 460MB
[2022-10-02 19:24:40 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:40 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0210 (0.0210)	loss 0.0097 (0.0097)	grad_norm 0.0471 (0.0471)	mem 460MB
[2022-10-02 19:24:40 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:40 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0215 (0.0215)	loss 0.0090 (0.0090)	grad_norm 0.0329 (0.0329)	mem 460MB
[2022-10-02 19:24:40 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:40 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0201 (0.0201)	loss 0.0113 (0.0113)	grad_norm 0.0495 (0.0495)	mem 460MB
[2022-10-02 19:24:40 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:40 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0208 (0.0208)	loss 0.0089 (0.0089)	grad_norm 0.0487 (0.0487)	mem 460MB
[2022-10-02 19:24:40 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:40 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0154 (0.0154)	loss 0.0077 (0.0077)	grad_norm 0.0284 (0.0284)	mem 460MB
[2022-10-02 19:24:40 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:41 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0216 (0.0216)	loss 0.0107 (0.0107)	grad_norm 0.0548 (0.0548)	mem 460MB
[2022-10-02 19:24:41 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:41 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0159 (0.0159)	loss 0.0080 (0.0080)	grad_norm 0.0306 (0.0306)	mem 460MB
[2022-10-02 19:24:41 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:41 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000010	time 0.0204 (0.0204)	loss 0.0090 (0.0090)	grad_norm 0.0313 (0.0313)	mem 460MB
[2022-10-02 19:24:41 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:41 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0176 (0.0176)	loss 0.0077 (0.0077)	grad_norm 0.0435 (0.0435)	mem 460MB
[2022-10-02 19:24:41 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:41 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0214 (0.0214)	loss 0.0143 (0.0143)	grad_norm 0.0456 (0.0456)	mem 460MB
[2022-10-02 19:24:41 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:42 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0206 (0.0206)	loss 0.0080 (0.0080)	grad_norm 0.0392 (0.0392)	mem 460MB
[2022-10-02 19:24:42 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:42 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0220 (0.0220)	loss 0.0066 (0.0066)	grad_norm 0.0345 (0.0345)	mem 460MB
[2022-10-02 19:24:42 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:42 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0225 (0.0225)	loss 0.0118 (0.0118)	grad_norm 0.0486 (0.0486)	mem 460MB
[2022-10-02 19:24:42 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:42 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0222 (0.0222)	loss 0.0064 (0.0064)	grad_norm 0.0341 (0.0341)	mem 460MB
[2022-10-02 19:24:42 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:42 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0212 (0.0212)	loss 0.0090 (0.0090)	grad_norm 0.0409 (0.0409)	mem 460MB
[2022-10-02 19:24:42 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:42 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0115 (0.0115)	loss 0.0104 (0.0104)	grad_norm 0.0654 (0.0654)	mem 460MB
[2022-10-02 19:24:42 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:43 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0099 (0.0099)	loss 0.0070 (0.0070)	grad_norm 0.0370 (0.0370)	mem 460MB
[2022-10-02 19:24:43 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:43 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0084 (0.0084)	loss 0.0108 (0.0108)	grad_norm 0.0441 (0.0441)	mem 460MB
[2022-10-02 19:24:43 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:43 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0086 (0.0086)	loss 0.0095 (0.0095)	grad_norm 0.0732 (0.0732)	mem 460MB
[2022-10-02 19:24:43 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:43 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0112 (0.0112)	loss 0.0068 (0.0068)	grad_norm 0.0658 (0.0658)	mem 460MB
[2022-10-02 19:24:43 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:43 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0088 (0.0088)	loss 0.0128 (0.0128)	grad_norm 0.0563 (0.0563)	mem 460MB
[2022-10-02 19:24:43 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:43 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0084 (0.0084)	loss 0.0089 (0.0089)	grad_norm 0.0503 (0.0503)	mem 460MB
[2022-10-02 19:24:43 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:43 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0088 (0.0088)	loss 0.0093 (0.0093)	grad_norm 0.0544 (0.0544)	mem 460MB
[2022-10-02 19:24:43 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:43 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0109 (0.0109)	loss 0.0075 (0.0075)	grad_norm 0.0535 (0.0535)	mem 460MB
[2022-10-02 19:24:43 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:43 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0208 (0.0208)	loss 0.0084 (0.0084)	grad_norm 0.0305 (0.0305)	mem 460MB
[2022-10-02 19:24:43 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:43 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0237 (0.0237)	loss 0.0067 (0.0067)	grad_norm 0.0617 (0.0617)	mem 460MB
[2022-10-02 19:24:43 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:44 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0212 (0.0212)	loss 0.0091 (0.0091)	grad_norm 0.0595 (0.0595)	mem 460MB
[2022-10-02 19:24:44 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:44 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0207 (0.0207)	loss 0.0093 (0.0093)	grad_norm 0.0535 (0.0535)	mem 460MB
[2022-10-02 19:24:44 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:44 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0204 (0.0204)	loss 0.0085 (0.0085)	grad_norm 0.0423 (0.0423)	mem 460MB
[2022-10-02 19:24:44 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:44 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0102 (0.0102)	loss 0.0093 (0.0093)	grad_norm 0.0552 (0.0552)	mem 460MB
[2022-10-02 19:24:44 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:44 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0220 (0.0220)	loss 0.0075 (0.0075)	grad_norm 0.0381 (0.0381)	mem 460MB
[2022-10-02 19:24:44 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:45 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0135 (0.0135)	loss 0.0095 (0.0095)	grad_norm 0.0434 (0.0434)	mem 460MB
[2022-10-02 19:24:45 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:45 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0173 (0.0173)	loss 0.0078 (0.0078)	grad_norm 0.0460 (0.0460)	mem 460MB
[2022-10-02 19:24:45 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:45 demo] (houston_program2.py 243): INFO Train: [71/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0135 (0.0135)	loss 0.0090 (0.0090)	grad_norm 0.0401 (0.0401)	mem 460MB
[2022-10-02 19:24:45 demo] (houston_program2.py 252): INFO EPOCH 71 training takes 0:00:00
[2022-10-02 19:24:45 demo] (houston_program2.py 333): INFO Train Ep: 71 	Loss1: 0.031258	Loss2: 0.035056	 Dis: 6.100292 Entropy: 5.710596 
[2022-10-02 19:24:45 demo] (houston_program2.py 335): INFO time_71_epoch:6.62711763381958
[2022-10-02 19:24:45 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0214 (0.0214)	loss 0.0098 (0.0098)	grad_norm 0.0464 (0.0464)	mem 460MB
[2022-10-02 19:24:45 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:46 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0287 (0.0287)	loss 0.0109 (0.0109)	grad_norm 0.0368 (0.0368)	mem 460MB
[2022-10-02 19:24:46 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:46 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0172 (0.0172)	loss 0.0100 (0.0100)	grad_norm 0.0395 (0.0395)	mem 460MB
[2022-10-02 19:24:46 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:46 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0206 (0.0206)	loss 0.0090 (0.0090)	grad_norm 0.0512 (0.0512)	mem 460MB
[2022-10-02 19:24:46 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:46 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0169 (0.0169)	loss 0.0110 (0.0110)	grad_norm 0.0303 (0.0303)	mem 460MB
[2022-10-02 19:24:46 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:46 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0225 (0.0225)	loss 0.0081 (0.0081)	grad_norm 0.0331 (0.0331)	mem 460MB
[2022-10-02 19:24:46 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:47 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0195 (0.0195)	loss 0.0097 (0.0097)	grad_norm 0.0701 (0.0701)	mem 460MB
[2022-10-02 19:24:47 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:47 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0202 (0.0202)	loss 0.0118 (0.0118)	grad_norm 0.0486 (0.0486)	mem 460MB
[2022-10-02 19:24:47 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:47 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0189 (0.0189)	loss 0.0096 (0.0096)	grad_norm 0.0406 (0.0406)	mem 460MB
[2022-10-02 19:24:47 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:47 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0205 (0.0205)	loss 0.0085 (0.0085)	grad_norm 0.0328 (0.0328)	mem 460MB
[2022-10-02 19:24:47 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:47 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0196 (0.0196)	loss 0.0090 (0.0090)	grad_norm 0.0454 (0.0454)	mem 460MB
[2022-10-02 19:24:47 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:47 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0202 (0.0202)	loss 0.0126 (0.0126)	grad_norm 0.0444 (0.0444)	mem 460MB
[2022-10-02 19:24:47 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:48 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0207 (0.0207)	loss 0.0113 (0.0113)	grad_norm 0.0394 (0.0394)	mem 460MB
[2022-10-02 19:24:48 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:48 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0221 (0.0221)	loss 0.0103 (0.0103)	grad_norm 0.0405 (0.0405)	mem 460MB
[2022-10-02 19:24:48 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:48 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0196 (0.0196)	loss 0.0102 (0.0102)	grad_norm 0.0415 (0.0415)	mem 460MB
[2022-10-02 19:24:48 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:48 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0223 (0.0223)	loss 0.0087 (0.0087)	grad_norm 0.0581 (0.0581)	mem 460MB
[2022-10-02 19:24:48 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:48 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0229 (0.0229)	loss 0.0080 (0.0080)	grad_norm 0.0409 (0.0409)	mem 460MB
[2022-10-02 19:24:48 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:49 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0101 (0.0101)	loss 0.0081 (0.0081)	grad_norm 0.0278 (0.0278)	mem 460MB
[2022-10-02 19:24:49 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:49 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0222 (0.0222)	loss 0.0074 (0.0074)	grad_norm 0.0636 (0.0636)	mem 460MB
[2022-10-02 19:24:49 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:49 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0158 (0.0158)	loss 0.0099 (0.0099)	grad_norm 0.0454 (0.0454)	mem 460MB
[2022-10-02 19:24:49 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:49 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0247 (0.0247)	loss 0.0091 (0.0091)	grad_norm 0.0485 (0.0485)	mem 460MB
[2022-10-02 19:24:49 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:49 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0214 (0.0214)	loss 0.0084 (0.0084)	grad_norm 0.0404 (0.0404)	mem 460MB
[2022-10-02 19:24:49 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:50 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0287 (0.0287)	loss 0.0101 (0.0101)	grad_norm 0.0427 (0.0427)	mem 460MB
[2022-10-02 19:24:50 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:50 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0245 (0.0245)	loss 0.0082 (0.0082)	grad_norm 0.0428 (0.0428)	mem 460MB
[2022-10-02 19:24:50 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:50 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0145 (0.0145)	loss 0.0118 (0.0118)	grad_norm 0.0382 (0.0382)	mem 460MB
[2022-10-02 19:24:50 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:50 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0250 (0.0250)	loss 0.0073 (0.0073)	grad_norm 0.0523 (0.0523)	mem 460MB
[2022-10-02 19:24:50 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:50 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0196 (0.0196)	loss 0.0067 (0.0067)	grad_norm 0.0320 (0.0320)	mem 460MB
[2022-10-02 19:24:50 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:51 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0286 (0.0286)	loss 0.0085 (0.0085)	grad_norm 0.0462 (0.0462)	mem 460MB
[2022-10-02 19:24:51 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:51 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0226 (0.0226)	loss 0.0113 (0.0113)	grad_norm 0.0442 (0.0442)	mem 460MB
[2022-10-02 19:24:51 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:51 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0192 (0.0192)	loss 0.0089 (0.0089)	grad_norm 0.0356 (0.0356)	mem 460MB
[2022-10-02 19:24:51 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:51 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0234 (0.0234)	loss 0.0101 (0.0101)	grad_norm 0.0406 (0.0406)	mem 460MB
[2022-10-02 19:24:51 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:51 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0250 (0.0250)	loss 0.0071 (0.0071)	grad_norm 0.0449 (0.0449)	mem 460MB
[2022-10-02 19:24:51 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:52 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0256 (0.0256)	loss 0.0102 (0.0102)	grad_norm 0.0860 (0.0860)	mem 460MB
[2022-10-02 19:24:52 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:52 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0210 (0.0210)	loss 0.0080 (0.0080)	grad_norm 0.0326 (0.0326)	mem 460MB
[2022-10-02 19:24:52 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:52 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0119 (0.0119)	loss 0.0072 (0.0072)	grad_norm 0.0292 (0.0292)	mem 460MB
[2022-10-02 19:24:52 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:52 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0242 (0.0242)	loss 0.0101 (0.0101)	grad_norm 0.0553 (0.0553)	mem 460MB
[2022-10-02 19:24:52 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:52 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0219 (0.0219)	loss 0.0083 (0.0083)	grad_norm 0.0318 (0.0318)	mem 460MB
[2022-10-02 19:24:52 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:53 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0220 (0.0220)	loss 0.0082 (0.0082)	grad_norm 0.0521 (0.0521)	mem 460MB
[2022-10-02 19:24:53 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:53 demo] (houston_program2.py 243): INFO Train: [72/100][0/2]	eta 0:00:00 lr 0.000009	time 0.0221 (0.0221)	loss 0.0102 (0.0102)	grad_norm 0.0550 (0.0550)	mem 460MB
[2022-10-02 19:24:53 demo] (houston_program2.py 252): INFO EPOCH 72 training takes 0:00:00
[2022-10-02 19:24:53 demo] (houston_program2.py 333): INFO Train Ep: 72 	Loss1: 0.050720	Loss2: 0.045274	 Dis: 4.294846 Entropy: 6.196102 
[2022-10-02 19:24:53 demo] (houston_program2.py 335): INFO time_72_epoch:7.945356845855713
[2022-10-02 19:24:53 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0118 (0.0118)	loss 0.0084 (0.0084)	grad_norm 0.0386 (0.0386)	mem 460MB
[2022-10-02 19:24:53 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:54 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0187 (0.0187)	loss 0.0078 (0.0078)	grad_norm 0.0459 (0.0459)	mem 460MB
[2022-10-02 19:24:54 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:54 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0219 (0.0219)	loss 0.0097 (0.0097)	grad_norm 0.0396 (0.0396)	mem 460MB
[2022-10-02 19:24:54 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:54 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0206 (0.0206)	loss 0.0095 (0.0095)	grad_norm 0.0434 (0.0434)	mem 460MB
[2022-10-02 19:24:54 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:54 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0221 (0.0221)	loss 0.0089 (0.0089)	grad_norm 0.0471 (0.0471)	mem 460MB
[2022-10-02 19:24:54 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:54 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0218 (0.0218)	loss 0.0071 (0.0071)	grad_norm 0.0512 (0.0512)	mem 460MB
[2022-10-02 19:24:54 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:55 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0224 (0.0224)	loss 0.0087 (0.0087)	grad_norm 0.0339 (0.0339)	mem 460MB
[2022-10-02 19:24:55 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:55 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0223 (0.0223)	loss 0.0080 (0.0080)	grad_norm 0.0387 (0.0387)	mem 460MB
[2022-10-02 19:24:55 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:55 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0192 (0.0192)	loss 0.0094 (0.0094)	grad_norm 0.0469 (0.0469)	mem 460MB
[2022-10-02 19:24:55 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:55 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0222 (0.0222)	loss 0.0089 (0.0089)	grad_norm 0.0274 (0.0274)	mem 460MB
[2022-10-02 19:24:55 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:55 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0185 (0.0185)	loss 0.0074 (0.0074)	grad_norm 0.0542 (0.0542)	mem 460MB
[2022-10-02 19:24:55 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:55 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0201 (0.0201)	loss 0.0080 (0.0080)	grad_norm 0.0332 (0.0332)	mem 460MB
[2022-10-02 19:24:56 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:56 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0215 (0.0215)	loss 0.0090 (0.0090)	grad_norm 0.0766 (0.0766)	mem 460MB
[2022-10-02 19:24:56 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:56 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0207 (0.0207)	loss 0.0109 (0.0109)	grad_norm 0.0503 (0.0503)	mem 460MB
[2022-10-02 19:24:56 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:56 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0213 (0.0213)	loss 0.0092 (0.0092)	grad_norm 0.0307 (0.0307)	mem 460MB
[2022-10-02 19:24:56 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:56 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0223 (0.0223)	loss 0.0092 (0.0092)	grad_norm 0.0340 (0.0340)	mem 460MB
[2022-10-02 19:24:56 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:56 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0222 (0.0222)	loss 0.0064 (0.0064)	grad_norm 0.0422 (0.0422)	mem 460MB
[2022-10-02 19:24:56 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:57 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0126 (0.0126)	loss 0.0079 (0.0079)	grad_norm 0.0361 (0.0361)	mem 460MB
[2022-10-02 19:24:57 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:57 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0213 (0.0213)	loss 0.0079 (0.0079)	grad_norm 0.0378 (0.0378)	mem 460MB
[2022-10-02 19:24:57 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:57 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0111 (0.0111)	loss 0.0073 (0.0073)	grad_norm 0.0319 (0.0319)	mem 460MB
[2022-10-02 19:24:57 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:57 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0210 (0.0210)	loss 0.0114 (0.0114)	grad_norm 0.0350 (0.0350)	mem 460MB
[2022-10-02 19:24:57 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:57 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0152 (0.0152)	loss 0.0110 (0.0110)	grad_norm 0.0550 (0.0550)	mem 460MB
[2022-10-02 19:24:57 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:58 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0206 (0.0206)	loss 0.0080 (0.0080)	grad_norm 0.0435 (0.0435)	mem 460MB
[2022-10-02 19:24:58 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:58 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0216 (0.0216)	loss 0.0058 (0.0058)	grad_norm 0.0333 (0.0333)	mem 460MB
[2022-10-02 19:24:58 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:58 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0198 (0.0198)	loss 0.0091 (0.0091)	grad_norm 0.0317 (0.0317)	mem 460MB
[2022-10-02 19:24:58 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:58 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0206 (0.0206)	loss 0.0093 (0.0093)	grad_norm 0.0601 (0.0601)	mem 460MB
[2022-10-02 19:24:58 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:58 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0219 (0.0219)	loss 0.0075 (0.0075)	grad_norm 0.0355 (0.0355)	mem 460MB
[2022-10-02 19:24:58 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:58 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0219 (0.0219)	loss 0.0129 (0.0129)	grad_norm 0.0291 (0.0291)	mem 460MB
[2022-10-02 19:24:58 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:59 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0203 (0.0203)	loss 0.0081 (0.0081)	grad_norm 0.0349 (0.0349)	mem 460MB
[2022-10-02 19:24:59 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:59 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0187 (0.0187)	loss 0.0094 (0.0094)	grad_norm 0.0512 (0.0512)	mem 460MB
[2022-10-02 19:24:59 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:59 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0196 (0.0196)	loss 0.0103 (0.0103)	grad_norm 0.0445 (0.0445)	mem 460MB
[2022-10-02 19:24:59 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:59 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0214 (0.0214)	loss 0.0096 (0.0096)	grad_norm 0.0520 (0.0520)	mem 460MB
[2022-10-02 19:24:59 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:24:59 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0147 (0.0147)	loss 0.0091 (0.0091)	grad_norm 0.0412 (0.0412)	mem 460MB
[2022-10-02 19:24:59 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:25:00 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0217 (0.0217)	loss 0.0091 (0.0091)	grad_norm 0.0482 (0.0482)	mem 460MB
[2022-10-02 19:25:00 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:25:00 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0146 (0.0146)	loss 0.0082 (0.0082)	grad_norm 0.0364 (0.0364)	mem 460MB
[2022-10-02 19:25:00 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:25:00 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0218 (0.0218)	loss 0.0081 (0.0081)	grad_norm 0.0441 (0.0441)	mem 460MB
[2022-10-02 19:25:00 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:25:00 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0213 (0.0213)	loss 0.0102 (0.0102)	grad_norm 0.0799 (0.0799)	mem 460MB
[2022-10-02 19:25:00 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:25:00 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0246 (0.0246)	loss 0.0097 (0.0097)	grad_norm 0.0350 (0.0350)	mem 460MB
[2022-10-02 19:25:00 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:25:00 demo] (houston_program2.py 243): INFO Train: [73/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0216 (0.0216)	loss 0.0085 (0.0085)	grad_norm 0.0341 (0.0341)	mem 460MB
[2022-10-02 19:25:00 demo] (houston_program2.py 252): INFO EPOCH 73 training takes 0:00:00
[2022-10-02 19:25:01 demo] (houston_program2.py 333): INFO Train Ep: 73 	Loss1: 0.063158	Loss2: 0.079091	 Dis: 4.694212 Entropy: 5.192283 
[2022-10-02 19:25:01 demo] (houston_program2.py 335): INFO time_73_epoch:7.67881178855896
[2022-10-02 19:25:01 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0112 (0.0112)	loss 0.0117 (0.0117)	grad_norm 0.0590 (0.0590)	mem 460MB
[2022-10-02 19:25:01 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:01 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0217 (0.0217)	loss 0.0084 (0.0084)	grad_norm 0.0367 (0.0367)	mem 460MB
[2022-10-02 19:25:01 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:01 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0218 (0.0218)	loss 0.0072 (0.0072)	grad_norm 0.0347 (0.0347)	mem 460MB
[2022-10-02 19:25:01 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:02 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0194 (0.0194)	loss 0.0091 (0.0091)	grad_norm 0.0396 (0.0396)	mem 460MB
[2022-10-02 19:25:02 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:02 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0181 (0.0181)	loss 0.0121 (0.0121)	grad_norm 0.0389 (0.0389)	mem 460MB
[2022-10-02 19:25:02 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:02 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0216 (0.0216)	loss 0.0130 (0.0130)	grad_norm 0.0392 (0.0392)	mem 460MB
[2022-10-02 19:25:02 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:02 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0218 (0.0218)	loss 0.0125 (0.0125)	grad_norm 0.0413 (0.0413)	mem 460MB
[2022-10-02 19:25:02 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:02 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0220 (0.0220)	loss 0.0125 (0.0125)	grad_norm 0.0498 (0.0498)	mem 460MB
[2022-10-02 19:25:02 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:02 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0217 (0.0217)	loss 0.0086 (0.0086)	grad_norm 0.0532 (0.0532)	mem 460MB
[2022-10-02 19:25:02 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:03 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0269 (0.0269)	loss 0.0090 (0.0090)	grad_norm 0.0467 (0.0467)	mem 460MB
[2022-10-02 19:25:03 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:03 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0204 (0.0204)	loss 0.0080 (0.0080)	grad_norm 0.0431 (0.0431)	mem 460MB
[2022-10-02 19:25:03 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:03 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0111 (0.0111)	loss 0.0089 (0.0089)	grad_norm 0.0370 (0.0370)	mem 460MB
[2022-10-02 19:25:03 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:03 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0202 (0.0202)	loss 0.0098 (0.0098)	grad_norm 0.0469 (0.0469)	mem 460MB
[2022-10-02 19:25:03 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:03 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0153 (0.0153)	loss 0.0105 (0.0105)	grad_norm 0.0512 (0.0512)	mem 460MB
[2022-10-02 19:25:03 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:04 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0203 (0.0203)	loss 0.0117 (0.0117)	grad_norm 0.0391 (0.0391)	mem 460MB
[2022-10-02 19:25:04 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:04 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0156 (0.0156)	loss 0.0098 (0.0098)	grad_norm 0.0564 (0.0564)	mem 460MB
[2022-10-02 19:25:04 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:04 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0204 (0.0204)	loss 0.0091 (0.0091)	grad_norm 0.1215 (0.1215)	mem 460MB
[2022-10-02 19:25:04 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:04 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0184 (0.0184)	loss 0.0071 (0.0071)	grad_norm 0.0487 (0.0487)	mem 460MB
[2022-10-02 19:25:04 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:04 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0205 (0.0205)	loss 0.0083 (0.0083)	grad_norm 0.0477 (0.0477)	mem 460MB
[2022-10-02 19:25:04 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:04 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0194 (0.0194)	loss 0.0070 (0.0070)	grad_norm 0.0467 (0.0467)	mem 460MB
[2022-10-02 19:25:04 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:05 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0220 (0.0220)	loss 0.0068 (0.0068)	grad_norm 0.0404 (0.0404)	mem 460MB
[2022-10-02 19:25:05 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:05 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0181 (0.0181)	loss 0.0080 (0.0080)	grad_norm 0.0363 (0.0363)	mem 460MB
[2022-10-02 19:25:05 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:05 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0238 (0.0238)	loss 0.0085 (0.0085)	grad_norm 0.0468 (0.0468)	mem 460MB
[2022-10-02 19:25:05 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:05 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0203 (0.0203)	loss 0.0080 (0.0080)	grad_norm 0.0412 (0.0412)	mem 460MB
[2022-10-02 19:25:05 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:05 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0200 (0.0200)	loss 0.0077 (0.0077)	grad_norm 0.0307 (0.0307)	mem 460MB
[2022-10-02 19:25:05 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:06 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0211 (0.0211)	loss 0.0077 (0.0077)	grad_norm 0.0469 (0.0469)	mem 460MB
[2022-10-02 19:25:06 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:06 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0117 (0.0117)	loss 0.0070 (0.0070)	grad_norm 0.0798 (0.0798)	mem 460MB
[2022-10-02 19:25:06 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:06 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0209 (0.0209)	loss 0.0091 (0.0091)	grad_norm 0.0283 (0.0283)	mem 460MB
[2022-10-02 19:25:06 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:06 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000008	time 0.0138 (0.0138)	loss 0.0072 (0.0072)	grad_norm 0.0405 (0.0405)	mem 460MB
[2022-10-02 19:25:06 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:06 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0222 (0.0222)	loss 0.0122 (0.0122)	grad_norm 0.0562 (0.0562)	mem 460MB
[2022-10-02 19:25:06 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:06 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0164 (0.0164)	loss 0.0067 (0.0067)	grad_norm 0.0325 (0.0325)	mem 460MB
[2022-10-02 19:25:06 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:07 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0217 (0.0217)	loss 0.0081 (0.0081)	grad_norm 0.0333 (0.0333)	mem 460MB
[2022-10-02 19:25:07 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:07 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0215 (0.0215)	loss 0.0093 (0.0093)	grad_norm 0.0427 (0.0427)	mem 460MB
[2022-10-02 19:25:07 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:07 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0208 (0.0208)	loss 0.0098 (0.0098)	grad_norm 0.0612 (0.0612)	mem 460MB
[2022-10-02 19:25:07 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:07 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0218 (0.0218)	loss 0.0100 (0.0100)	grad_norm 0.0559 (0.0559)	mem 460MB
[2022-10-02 19:25:07 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:07 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0229 (0.0229)	loss 0.0106 (0.0106)	grad_norm 0.0291 (0.0291)	mem 460MB
[2022-10-02 19:25:07 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:08 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0207 (0.0207)	loss 0.0117 (0.0117)	grad_norm 0.0950 (0.0950)	mem 460MB
[2022-10-02 19:25:08 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:08 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0215 (0.0215)	loss 0.0069 (0.0069)	grad_norm 0.0354 (0.0354)	mem 460MB
[2022-10-02 19:25:08 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:08 demo] (houston_program2.py 243): INFO Train: [74/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0203 (0.0203)	loss 0.0110 (0.0110)	grad_norm 0.0438 (0.0438)	mem 460MB
[2022-10-02 19:25:08 demo] (houston_program2.py 252): INFO EPOCH 74 training takes 0:00:00
[2022-10-02 19:25:08 demo] (houston_program2.py 333): INFO Train Ep: 74 	Loss1: 0.013139	Loss2: 0.010744	 Dis: 5.088776 Entropy: 4.882530 
[2022-10-02 19:25:08 demo] (houston_program2.py 335): INFO time_74_epoch:7.426501035690308
[2022-10-02 19:25:08 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0112 (0.0112)	loss 0.0098 (0.0098)	grad_norm 0.0420 (0.0420)	mem 460MB
[2022-10-02 19:25:09 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:09 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0184 (0.0184)	loss 0.0105 (0.0105)	grad_norm 0.0419 (0.0419)	mem 460MB
[2022-10-02 19:25:09 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:09 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0206 (0.0206)	loss 0.0078 (0.0078)	grad_norm 0.0365 (0.0365)	mem 460MB
[2022-10-02 19:25:09 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:09 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0206 (0.0206)	loss 0.0088 (0.0088)	grad_norm 0.0472 (0.0472)	mem 460MB
[2022-10-02 19:25:09 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:09 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0147 (0.0147)	loss 0.0087 (0.0087)	grad_norm 0.0660 (0.0660)	mem 460MB
[2022-10-02 19:25:09 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:09 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0268 (0.0268)	loss 0.0077 (0.0077)	grad_norm 0.0378 (0.0378)	mem 460MB
[2022-10-02 19:25:09 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:10 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0147 (0.0147)	loss 0.0087 (0.0087)	grad_norm 0.0541 (0.0541)	mem 460MB
[2022-10-02 19:25:10 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:10 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0203 (0.0203)	loss 0.0085 (0.0085)	grad_norm 0.0363 (0.0363)	mem 460MB
[2022-10-02 19:25:10 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:10 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0137 (0.0137)	loss 0.0092 (0.0092)	grad_norm 0.0395 (0.0395)	mem 460MB
[2022-10-02 19:25:10 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:10 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0223 (0.0223)	loss 0.0096 (0.0096)	grad_norm 0.0374 (0.0374)	mem 460MB
[2022-10-02 19:25:10 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:10 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0222 (0.0222)	loss 0.0107 (0.0107)	grad_norm 0.0674 (0.0674)	mem 460MB
[2022-10-02 19:25:10 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:10 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0200 (0.0200)	loss 0.0079 (0.0079)	grad_norm 0.0514 (0.0514)	mem 460MB
[2022-10-02 19:25:10 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:11 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0206 (0.0206)	loss 0.0103 (0.0103)	grad_norm 0.0450 (0.0450)	mem 460MB
[2022-10-02 19:25:11 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:11 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0205 (0.0205)	loss 0.0096 (0.0096)	grad_norm 0.0447 (0.0447)	mem 460MB
[2022-10-02 19:25:11 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:11 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0185 (0.0185)	loss 0.0095 (0.0095)	grad_norm 0.0422 (0.0422)	mem 460MB
[2022-10-02 19:25:11 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:11 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0216 (0.0216)	loss 0.0071 (0.0071)	grad_norm 0.0544 (0.0544)	mem 460MB
[2022-10-02 19:25:11 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:11 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0211 (0.0211)	loss 0.0098 (0.0098)	grad_norm 0.0325 (0.0325)	mem 460MB
[2022-10-02 19:25:11 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:12 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0160 (0.0160)	loss 0.0103 (0.0103)	grad_norm 0.0324 (0.0324)	mem 460MB
[2022-10-02 19:25:12 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:12 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0199 (0.0199)	loss 0.0134 (0.0134)	grad_norm 0.0642 (0.0642)	mem 460MB
[2022-10-02 19:25:12 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:12 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0100 (0.0100)	loss 0.0084 (0.0084)	grad_norm 0.0429 (0.0429)	mem 460MB
[2022-10-02 19:25:12 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:12 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0205 (0.0205)	loss 0.0106 (0.0106)	grad_norm 0.0467 (0.0467)	mem 460MB
[2022-10-02 19:25:12 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:12 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0118 (0.0118)	loss 0.0104 (0.0104)	grad_norm 0.0402 (0.0402)	mem 460MB
[2022-10-02 19:25:12 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:12 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0261 (0.0261)	loss 0.0100 (0.0100)	grad_norm 0.0387 (0.0387)	mem 460MB
[2022-10-02 19:25:12 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:13 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0247 (0.0247)	loss 0.0085 (0.0085)	grad_norm 0.1230 (0.1230)	mem 460MB
[2022-10-02 19:25:13 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:13 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0192 (0.0192)	loss 0.0081 (0.0081)	grad_norm 0.0425 (0.0425)	mem 460MB
[2022-10-02 19:25:13 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:13 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0218 (0.0218)	loss 0.0094 (0.0094)	grad_norm 0.0517 (0.0517)	mem 460MB
[2022-10-02 19:25:13 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:13 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0221 (0.0221)	loss 0.0069 (0.0069)	grad_norm 0.0302 (0.0302)	mem 460MB
[2022-10-02 19:25:13 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:13 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0218 (0.0218)	loss 0.0079 (0.0079)	grad_norm 0.0372 (0.0372)	mem 460MB
[2022-10-02 19:25:13 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:14 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0262 (0.0262)	loss 0.0078 (0.0078)	grad_norm 0.0556 (0.0556)	mem 460MB
[2022-10-02 19:25:14 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:14 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0211 (0.0211)	loss 0.0093 (0.0093)	grad_norm 0.0332 (0.0332)	mem 460MB
[2022-10-02 19:25:14 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:14 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0127 (0.0127)	loss 0.0102 (0.0102)	grad_norm 0.0457 (0.0457)	mem 460MB
[2022-10-02 19:25:14 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:14 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0206 (0.0206)	loss 0.0129 (0.0129)	grad_norm 0.0300 (0.0300)	mem 460MB
[2022-10-02 19:25:14 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:14 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0101 (0.0101)	loss 0.0086 (0.0086)	grad_norm 0.0466 (0.0466)	mem 460MB
[2022-10-02 19:25:14 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:14 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0210 (0.0210)	loss 0.0086 (0.0086)	grad_norm 0.0459 (0.0459)	mem 460MB
[2022-10-02 19:25:14 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:15 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0138 (0.0138)	loss 0.0073 (0.0073)	grad_norm 0.0430 (0.0430)	mem 460MB
[2022-10-02 19:25:15 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:15 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0222 (0.0222)	loss 0.0083 (0.0083)	grad_norm 0.0674 (0.0674)	mem 460MB
[2022-10-02 19:25:15 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:15 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0143 (0.0143)	loss 0.0098 (0.0098)	grad_norm 0.0386 (0.0386)	mem 460MB
[2022-10-02 19:25:15 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:15 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0203 (0.0203)	loss 0.0092 (0.0092)	grad_norm 0.0483 (0.0483)	mem 460MB
[2022-10-02 19:25:15 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:15 demo] (houston_program2.py 243): INFO Train: [75/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0200 (0.0200)	loss 0.0099 (0.0099)	grad_norm 0.0545 (0.0545)	mem 460MB
[2022-10-02 19:25:15 demo] (houston_program2.py 252): INFO EPOCH 75 training takes 0:00:00
[2022-10-02 19:25:16 demo] (houston_program2.py 333): INFO Train Ep: 75 	Loss1: 0.146661	Loss2: 0.185249	 Dis: 4.794289 Entropy: 4.568720 
[2022-10-02 19:25:16 demo] (houston_program2.py 335): INFO time_75_epoch:7.456000328063965
[2022-10-02 19:25:16 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 19:25:16 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 19:25:16 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 19:25:16 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 19:25:16 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 19:25:16 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 19:25:16 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 19:25:16 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 19:25:16 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 19:25:16 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 19:25:22 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.066126	Loss2: 0.059144	 Dis: 4.938080 Entropy: 4.563899 
[2022-10-02 19:25:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.000487	 E_lr_2:0.000487
[2022-10-02 19:25:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 19:25:28 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.022863	Loss2: 0.024993	 Dis: 3.182367 Entropy: 5.027819 
[2022-10-02 19:25:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.000987	 E_lr_2:0.000987
[2022-10-02 19:25:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 19:25:33 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.189133	Loss2: 0.172408	 Dis: 5.700321 Entropy: 4.550981 
[2022-10-02 19:25:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.001487	 E_lr_2:0.001487
[2022-10-02 19:25:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 19:25:39 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.061827	Loss2: 0.072505	 Dis: 4.771618 Entropy: 5.144513 
[2022-10-02 19:25:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.001987	 E_lr_2:0.001987
[2022-10-02 19:25:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 19:25:45 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.194080	Loss2: 0.205305	 Dis: 5.177935 Entropy: 6.494874 
[2022-10-02 19:25:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.002487	 E_lr_2:0.002487
[2022-10-02 19:25:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 19:25:51 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.039532	Loss2: 0.033302	 Dis: 5.198977 Entropy: 5.193782 
[2022-10-02 19:25:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.002987	 E_lr_2:0.002987
[2022-10-02 19:25:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 19:25:58 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.364794	Loss2: 0.401224	 Dis: 4.965796 Entropy: 4.951445 
[2022-10-02 19:25:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.003487	 E_lr_2:0.003487
[2022-10-02 19:25:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 19:26:04 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.094768	Loss2: 0.089561	 Dis: 3.940794 Entropy: 4.621479 
[2022-10-02 19:26:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.003987	 E_lr_2:0.003987
[2022-10-02 19:26:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 19:26:10 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.074895	Loss2: 0.103178	 Dis: 2.444855 Entropy: 5.532177 
[2022-10-02 19:26:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.004487	 E_lr_2:0.004487
[2022-10-02 19:26:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 19:26:15 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.135312	Loss2: 0.104419	 Dis: 4.558838 Entropy: 4.320966 
[2022-10-02 19:26:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.004987	 E_lr_2:0.004987
[2022-10-02 19:26:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000050	 C_lr_2:0.000050	 C_lr_3:0.000050	 C_lr_4:0.000050
[2022-10-02 19:26:21 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.143337	Loss2: 0.134518	 Dis: 4.397612 Entropy: 5.578308 
[2022-10-02 19:26:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.004853	 E_lr_2:0.004853
[2022-10-02 19:26:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000049	 C_lr_2:0.000049	 C_lr_3:0.000049	 C_lr_4:0.000049
[2022-10-02 19:26:27 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.018783	Loss2: 0.017658	 Dis: 4.137754 Entropy: 5.621013 
[2022-10-02 19:26:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.004825	 E_lr_2:0.004825
[2022-10-02 19:26:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 19:26:33 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.201735	Loss2: 0.210654	 Dis: 6.410242 Entropy: 6.028881 
[2022-10-02 19:26:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.004795	 E_lr_2:0.004795
[2022-10-02 19:26:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 19:26:39 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.249772	Loss2: 0.245312	 Dis: 5.408266 Entropy: 4.992223 
[2022-10-02 19:26:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.004763	 E_lr_2:0.004763
[2022-10-02 19:26:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 19:26:44 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.223432	Loss2: 0.210097	 Dis: 5.171944 Entropy: 5.679882 
[2022-10-02 19:26:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.004728	 E_lr_2:0.004728
[2022-10-02 19:26:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 19:26:51 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.280937	Loss2: 0.232631	 Dis: 4.379162 Entropy: 4.908902 
[2022-10-02 19:26:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.004692	 E_lr_2:0.004692
[2022-10-02 19:26:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 19:26:57 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.087225	Loss2: 0.089459	 Dis: 3.226824 Entropy: 7.257775 
[2022-10-02 19:26:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.004653	 E_lr_2:0.004653
[2022-10-02 19:26:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 19:27:02 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.153141	Loss2: 0.150851	 Dis: 5.138847 Entropy: 4.691315 
[2022-10-02 19:27:02 demo] (houston_program2.py 512): INFO E_lr_1: 0.004612	 E_lr_2:0.004612
[2022-10-02 19:27:02 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 19:27:09 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.244125	Loss2: 0.269711	 Dis: 4.716553 Entropy: 5.743239 
[2022-10-02 19:27:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.004569	 E_lr_2:0.004569
[2022-10-02 19:27:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 19:27:15 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.203097	Loss2: 0.198970	 Dis: 4.692890 Entropy: 6.251323 
[2022-10-02 19:27:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.004524	 E_lr_2:0.004524
[2022-10-02 19:27:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 19:27:21 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.047299	Loss2: 0.044415	 Dis: 4.236589 Entropy: 4.840117 
[2022-10-02 19:27:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.004477	 E_lr_2:0.004477
[2022-10-02 19:27:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 19:27:27 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.062287	Loss2: 0.063860	 Dis: 4.515362 Entropy: 5.717106 
[2022-10-02 19:27:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.004428	 E_lr_2:0.004428
[2022-10-02 19:27:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 19:27:32 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.150021	Loss2: 0.155503	 Dis: 4.047176 Entropy: 5.193164 
[2022-10-02 19:27:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.004377	 E_lr_2:0.004377
[2022-10-02 19:27:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 19:27:38 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.032790	Loss2: 0.021914	 Dis: 3.645660 Entropy: 4.915469 
[2022-10-02 19:27:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.004324	 E_lr_2:0.004324
[2022-10-02 19:27:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 19:27:44 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.082136	Loss2: 0.080601	 Dis: 5.203812 Entropy: 5.112554 
[2022-10-02 19:27:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.004269	 E_lr_2:0.004269
[2022-10-02 19:27:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 19:27:51 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.065960	Loss2: 0.047365	 Dis: 4.451965 Entropy: 5.123643 
[2022-10-02 19:27:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.004213	 E_lr_2:0.004213
[2022-10-02 19:27:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 19:27:57 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.214444	Loss2: 0.210596	 Dis: 8.443840 Entropy: 4.259949 
[2022-10-02 19:27:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.004155	 E_lr_2:0.004155
[2022-10-02 19:27:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 19:28:03 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.103519	Loss2: 0.113390	 Dis: 3.073572 Entropy: 4.489210 
[2022-10-02 19:28:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.004095	 E_lr_2:0.004095
[2022-10-02 19:28:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000041	 C_lr_2:0.000041	 C_lr_3:0.000041	 C_lr_4:0.000041
[2022-10-02 19:28:09 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.091178	Loss2: 0.101099	 Dis: 3.557745 Entropy: 4.853750 
[2022-10-02 19:28:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.004034	 E_lr_2:0.004034
[2022-10-02 19:28:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 19:28:14 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.052983	Loss2: 0.068282	 Dis: 2.774622 Entropy: 4.231889 
[2022-10-02 19:28:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.003971	 E_lr_2:0.003971
[2022-10-02 19:28:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 19:28:20 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.025073	Loss2: 0.030046	 Dis: 3.307711 Entropy: 4.911004 
[2022-10-02 19:28:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.003907	 E_lr_2:0.003907
[2022-10-02 19:28:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000039	 C_lr_2:0.000039	 C_lr_3:0.000039	 C_lr_4:0.000039
[2022-10-02 19:28:26 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.030723	Loss2: 0.024462	 Dis: 2.806013 Entropy: 4.474984 
[2022-10-02 19:28:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.003841	 E_lr_2:0.003841
[2022-10-02 19:28:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 19:28:32 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.046472	Loss2: 0.050327	 Dis: 4.071194 Entropy: 5.468067 
[2022-10-02 19:28:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.003774	 E_lr_2:0.003774
[2022-10-02 19:28:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 19:28:39 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.040521	Loss2: 0.044418	 Dis: 3.995300 Entropy: 4.522733 
[2022-10-02 19:28:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.003706	 E_lr_2:0.003706
[2022-10-02 19:28:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000037	 C_lr_2:0.000037	 C_lr_3:0.000037	 C_lr_4:0.000037
[2022-10-02 19:28:44 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.036621	Loss2: 0.035317	 Dis: 3.394779 Entropy: 5.868315 
[2022-10-02 19:28:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.003637	 E_lr_2:0.003637
[2022-10-02 19:28:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 19:28:51 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.100087	Loss2: 0.100562	 Dis: 4.343779 Entropy: 5.575616 
[2022-10-02 19:28:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.003566	 E_lr_2:0.003566
[2022-10-02 19:28:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 19:28:56 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.017990	Loss2: 0.027292	 Dis: 4.678944 Entropy: 5.017519 
[2022-10-02 19:28:56 demo] (houston_program2.py 512): INFO E_lr_1: 0.003495	 E_lr_2:0.003495
[2022-10-02 19:28:56 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 19:29:02 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.022733	Loss2: 0.022242	 Dis: 3.828928 Entropy: 4.756270 
[2022-10-02 19:29:02 demo] (houston_program2.py 512): INFO E_lr_1: 0.003422	 E_lr_2:0.003422
[2022-10-02 19:29:02 demo] (houston_program2.py 513): INFO C_lr_1: 0.000034	 C_lr_2:0.000034	 C_lr_3:0.000034	 C_lr_4:0.000034
[2022-10-02 19:29:08 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.064637	Loss2: 0.056781	 Dis: 3.440056 Entropy: 5.285485 
[2022-10-02 19:29:08 demo] (houston_program2.py 512): INFO E_lr_1: 0.003349	 E_lr_2:0.003349
[2022-10-02 19:29:08 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 19:29:14 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.024900	Loss2: 0.029723	 Dis: 2.686941 Entropy: 5.269834 
[2022-10-02 19:29:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.003274	 E_lr_2:0.003274
[2022-10-02 19:29:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 19:29:21 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.312978	Loss2: 0.349332	 Dis: 2.452438 Entropy: 6.590410 
[2022-10-02 19:29:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.003199	 E_lr_2:0.003199
[2022-10-02 19:29:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000032	 C_lr_2:0.000032	 C_lr_3:0.000032	 C_lr_4:0.000032
[2022-10-02 19:29:27 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.059376	Loss2: 0.066662	 Dis: 4.031538 Entropy: 5.067868 
[2022-10-02 19:29:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.003124	 E_lr_2:0.003124
[2022-10-02 19:29:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000031	 C_lr_2:0.000031	 C_lr_3:0.000031	 C_lr_4:0.000031
[2022-10-02 19:29:33 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.020850	Loss2: 0.019467	 Dis: 3.516548 Entropy: 4.226880 
[2022-10-02 19:29:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.003047	 E_lr_2:0.003047
[2022-10-02 19:29:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 19:29:39 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.026740	Loss2: 0.036151	 Dis: 3.550886 Entropy: 6.318946 
[2022-10-02 19:29:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.002970	 E_lr_2:0.002970
[2022-10-02 19:29:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 19:29:45 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.138837	Loss2: 0.163277	 Dis: 3.612925 Entropy: 4.515948 
[2022-10-02 19:29:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.002893	 E_lr_2:0.002893
[2022-10-02 19:29:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000029	 C_lr_2:0.000029	 C_lr_3:0.000029	 C_lr_4:0.000029
[2022-10-02 19:29:49 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.037221	Loss2: 0.031219	 Dis: 1.901560 Entropy: 4.570185 
[2022-10-02 19:29:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.002815	 E_lr_2:0.002815
[2022-10-02 19:29:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000028	 C_lr_2:0.000028	 C_lr_3:0.000028	 C_lr_4:0.000028
[2022-10-02 19:29:55 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.088670	Loss2: 0.078822	 Dis: 2.390856 Entropy: 5.473266 
[2022-10-02 19:29:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.002737	 E_lr_2:0.002737
[2022-10-02 19:29:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 19:30:01 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.029361	Loss2: 0.027626	 Dis: 4.973219 Entropy: 4.432662 
[2022-10-02 19:30:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.002659	 E_lr_2:0.002659
[2022-10-02 19:30:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 19:30:07 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.026555	Loss2: 0.024562	 Dis: 2.386883 Entropy: 4.858487 
[2022-10-02 19:30:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.002581	 E_lr_2:0.002581
[2022-10-02 19:30:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000026	 C_lr_2:0.000026	 C_lr_3:0.000026	 C_lr_4:0.000026
[2022-10-02 19:30:13 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.010074	Loss2: 0.008378	 Dis: 2.402391 Entropy: 5.919592 
[2022-10-02 19:30:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.002502	 E_lr_2:0.002502
[2022-10-02 19:30:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 19:30:19 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.018904	Loss2: 0.015174	 Dis: 2.711279 Entropy: 5.513280 
[2022-10-02 19:30:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.002423	 E_lr_2:0.002423
[2022-10-02 19:30:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000024	 C_lr_2:0.000024	 C_lr_3:0.000024	 C_lr_4:0.000024
[2022-10-02 19:30:26 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.028729	Loss2: 0.019565	 Dis: 1.900772 Entropy: 4.619819 
[2022-10-02 19:30:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.002345	 E_lr_2:0.002345
[2022-10-02 19:30:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 19:30:32 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.224327	Loss2: 0.148764	 Dis: 5.012039 Entropy: 5.989903 
[2022-10-02 19:30:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.002267	 E_lr_2:0.002267
[2022-10-02 19:30:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 19:30:38 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.087591	Loss2: 0.134502	 Dis: 3.025759 Entropy: 6.333866 
[2022-10-02 19:30:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.002189	 E_lr_2:0.002189
[2022-10-02 19:30:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000022	 C_lr_2:0.000022	 C_lr_3:0.000022	 C_lr_4:0.000022
[2022-10-02 19:30:44 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.075754	Loss2: 0.085562	 Dis: 3.294277 Entropy: 5.715394 
[2022-10-02 19:30:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.002111	 E_lr_2:0.002111
[2022-10-02 19:30:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000021	 C_lr_2:0.000021	 C_lr_3:0.000021	 C_lr_4:0.000021
[2022-10-02 19:30:50 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.024908	Loss2: 0.034100	 Dis: 2.764334 Entropy: 5.397130 
[2022-10-02 19:30:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.002034	 E_lr_2:0.002034
[2022-10-02 19:30:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 19:30:56 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.021544	Loss2: 0.029534	 Dis: 2.318380 Entropy: 5.136508 
[2022-10-02 19:30:56 demo] (houston_program2.py 512): INFO E_lr_1: 0.001957	 E_lr_2:0.001957
[2022-10-02 19:30:56 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 19:31:02 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.006273	Loss2: 0.006791	 Dis: 2.503494 Entropy: 5.127649 
[2022-10-02 19:31:02 demo] (houston_program2.py 512): INFO E_lr_1: 0.001880	 E_lr_2:0.001880
[2022-10-02 19:31:02 demo] (houston_program2.py 513): INFO C_lr_1: 0.000019	 C_lr_2:0.000019	 C_lr_3:0.000019	 C_lr_4:0.000019
[2022-10-02 19:31:08 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.008857	Loss2: 0.007342	 Dis: 3.194292 Entropy: 5.268166 
[2022-10-02 19:31:08 demo] (houston_program2.py 512): INFO E_lr_1: 0.001804	 E_lr_2:0.001804
[2022-10-02 19:31:08 demo] (houston_program2.py 513): INFO C_lr_1: 0.000018	 C_lr_2:0.000018	 C_lr_3:0.000018	 C_lr_4:0.000018
[2022-10-02 19:31:14 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.082171	Loss2: 0.083128	 Dis: 1.773010 Entropy: 5.076561 
[2022-10-02 19:31:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.001729	 E_lr_2:0.001729
[2022-10-02 19:31:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 19:31:20 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.041254	Loss2: 0.033662	 Dis: 3.505896 Entropy: 4.822142 
[2022-10-02 19:31:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.001655	 E_lr_2:0.001655
[2022-10-02 19:31:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 19:31:26 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.015294	Loss2: 0.022405	 Dis: 2.100079 Entropy: 6.397142 
[2022-10-02 19:31:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.001582	 E_lr_2:0.001582
[2022-10-02 19:31:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000016	 C_lr_2:0.000016	 C_lr_3:0.000016	 C_lr_4:0.000016
[2022-10-02 19:31:32 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.009456	Loss2: 0.010517	 Dis: 2.843433 Entropy: 4.689456 
[2022-10-02 19:31:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.001509	 E_lr_2:0.001509
[2022-10-02 19:31:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 19:31:37 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.011785	Loss2: 0.011964	 Dis: 1.599979 Entropy: 5.120608 
[2022-10-02 19:31:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.001437	 E_lr_2:0.001437
[2022-10-02 19:31:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 19:31:43 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.013898	Loss2: 0.009108	 Dis: 2.332602 Entropy: 5.317509 
[2022-10-02 19:31:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.001367	 E_lr_2:0.001367
[2022-10-02 19:31:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 19:31:49 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.027346	Loss2: 0.029622	 Dis: 2.985073 Entropy: 5.327820 
[2022-10-02 19:31:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.001297	 E_lr_2:0.001297
[2022-10-02 19:31:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000013	 C_lr_2:0.000013	 C_lr_3:0.000013	 C_lr_4:0.000013
[2022-10-02 19:31:55 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.007744	Loss2: 0.006522	 Dis: 2.125458 Entropy: 6.588585 
[2022-10-02 19:31:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.001229	 E_lr_2:0.001229
[2022-10-02 19:31:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 19:32:01 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.006817	Loss2: 0.006635	 Dis: 2.415457 Entropy: 4.725301 
[2022-10-02 19:32:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.001162	 E_lr_2:0.001162
[2022-10-02 19:32:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 19:32:07 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.005054	Loss2: 0.005324	 Dis: 2.283960 Entropy: 4.636029 
[2022-10-02 19:32:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.001096	 E_lr_2:0.001096
[2022-10-02 19:32:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000011	 C_lr_2:0.000011	 C_lr_3:0.000011	 C_lr_4:0.000011
[2022-10-02 19:32:14 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.014002	Loss2: 0.010593	 Dis: 2.901323 Entropy: 4.510784 
[2022-10-02 19:32:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.001032	 E_lr_2:0.001032
[2022-10-02 19:32:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 19:32:20 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.001718	Loss2: 0.001813	 Dis: 1.130838 Entropy: 5.203038 
[2022-10-02 19:32:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.000969	 E_lr_2:0.000969
[2022-10-02 19:32:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 19:32:26 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.020944	Loss2: 0.013973	 Dis: 2.848909 Entropy: 5.549569 
[2022-10-02 19:32:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.000908	 E_lr_2:0.000908
[2022-10-02 19:32:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000009	 C_lr_2:0.000009	 C_lr_3:0.000009	 C_lr_4:0.000009
[2022-10-02 19:32:32 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.006669	Loss2: 0.006382	 Dis: 2.097128 Entropy: 4.635175 
[2022-10-02 19:32:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.000848	 E_lr_2:0.000848
[2022-10-02 19:32:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 19:32:38 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.005895	Loss2: 0.004062	 Dis: 1.937981 Entropy: 5.868232 
[2022-10-02 19:32:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.000790	 E_lr_2:0.000790
[2022-10-02 19:32:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 19:32:44 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.009748	Loss2: 0.008852	 Dis: 2.515120 Entropy: 5.730480 
[2022-10-02 19:32:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.000734	 E_lr_2:0.000734
[2022-10-02 19:32:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 19:32:51 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.002807	Loss2: 0.002328	 Dis: 2.384550 Entropy: 5.984095 
[2022-10-02 19:32:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.000679	 E_lr_2:0.000679
[2022-10-02 19:32:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 19:32:57 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.046807	Loss2: 0.030374	 Dis: 2.934200 Entropy: 4.620187 
[2022-10-02 19:32:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.000626	 E_lr_2:0.000626
[2022-10-02 19:32:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 19:33:03 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.008004	Loss2: 0.006433	 Dis: 1.797548 Entropy: 4.380877 
[2022-10-02 19:33:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.000575	 E_lr_2:0.000575
[2022-10-02 19:33:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 19:33:09 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.004982	Loss2: 0.004446	 Dis: 2.170746 Entropy: 6.239534 
[2022-10-02 19:33:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.000526	 E_lr_2:0.000526
[2022-10-02 19:33:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 19:33:15 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.002432	Loss2: 0.002078	 Dis: 1.085144 Entropy: 5.333142 
[2022-10-02 19:33:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.000479	 E_lr_2:0.000479
[2022-10-02 19:33:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 19:33:21 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.008679	Loss2: 0.009577	 Dis: 1.539509 Entropy: 4.450069 
[2022-10-02 19:33:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.000433	 E_lr_2:0.000433
[2022-10-02 19:33:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 19:33:27 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.011340	Loss2: 0.009704	 Dis: 3.174131 Entropy: 4.696691 
[2022-10-02 19:33:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.000390	 E_lr_2:0.000390
[2022-10-02 19:33:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 19:33:34 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.004695	Loss2: 0.004652	 Dis: 3.413355 Entropy: 4.438754 
[2022-10-02 19:33:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.000349	 E_lr_2:0.000349
[2022-10-02 19:33:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 19:33:39 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.003460	Loss2: 0.002337	 Dis: 2.180439 Entropy: 4.626832 
[2022-10-02 19:33:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.000310	 E_lr_2:0.000310
[2022-10-02 19:33:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 19:33:45 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.003923	Loss2: 0.004954	 Dis: 1.604984 Entropy: 4.611755 
[2022-10-02 19:33:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.000273	 E_lr_2:0.000273
[2022-10-02 19:33:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 19:33:51 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.003179	Loss2: 0.003693	 Dis: 3.038776 Entropy: 4.248231 
[2022-10-02 19:33:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.000239	 E_lr_2:0.000239
[2022-10-02 19:33:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 19:33:57 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.006733	Loss2: 0.013541	 Dis: 1.888084 Entropy: 4.518035 
[2022-10-02 19:33:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.000206	 E_lr_2:0.000206
[2022-10-02 19:33:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 19:34:04 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.002274	Loss2: 0.002188	 Dis: 1.581511 Entropy: 5.051702 
[2022-10-02 19:34:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.000176	 E_lr_2:0.000176
[2022-10-02 19:34:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 19:34:10 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.011494	Loss2: 0.007762	 Dis: 1.107601 Entropy: 4.362428 
[2022-10-02 19:34:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.000148	 E_lr_2:0.000148
[2022-10-02 19:34:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:34:16 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.002982	Loss2: 0.002697	 Dis: 1.396458 Entropy: 4.430422 
[2022-10-02 19:34:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.000123	 E_lr_2:0.000123
[2022-10-02 19:34:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:34:22 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.016274	Loss2: 0.024200	 Dis: 2.205341 Entropy: 4.502227 
[2022-10-02 19:34:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.000100	 E_lr_2:0.000100
[2022-10-02 19:34:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:34:28 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.022280	Loss2: 0.018601	 Dis: 2.223763 Entropy: 4.295463 
[2022-10-02 19:34:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.000079	 E_lr_2:0.000079
[2022-10-02 19:34:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:34:34 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.006383	Loss2: 0.014031	 Dis: 2.056204 Entropy: 4.610120 
[2022-10-02 19:34:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.000061	 E_lr_2:0.000061
[2022-10-02 19:34:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:34:40 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.001980	Loss2: 0.001696	 Dis: 3.076103 Entropy: 5.756124 
[2022-10-02 19:34:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.000045	 E_lr_2:0.000045
[2022-10-02 19:34:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:34:46 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.001845	Loss2: 0.002016	 Dis: 2.274059 Entropy: 5.592793 
[2022-10-02 19:34:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.000031	 E_lr_2:0.000031
[2022-10-02 19:34:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:34:52 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.006451	Loss2: 0.012242	 Dis: 1.390144 Entropy: 4.549461 
[2022-10-02 19:34:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.000020	 E_lr_2:0.000020
[2022-10-02 19:34:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:34:58 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.005368	Loss2: 0.008667	 Dis: 1.305531 Entropy: 4.567207 
[2022-10-02 19:34:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.000011	 E_lr_2:0.000011
[2022-10-02 19:34:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:35:04 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.001494	Loss2: 0.001725	 Dis: 1.424559 Entropy: 4.930490 
[2022-10-02 19:35:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.000005	 E_lr_2:0.000005
[2022-10-02 19:35:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:35:10 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.003001	Loss2: 0.005988	 Dis: 1.328030 Entropy: 5.392116 
[2022-10-02 19:35:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.000001	 E_lr_2:0.000001
[2022-10-02 19:35:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:35:16 demo] (houston_program2.py 504): INFO Train Ep: 75 	Loss1: 0.015176	Loss2: 0.013495	 Dis: 2.115316 Entropy: 4.333666 
[2022-10-02 19:35:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.000000	 E_lr_2:0.000000
[2022-10-02 19:35:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:35:16 demo] (houston_program2.py 515): INFO time_75_epoch:600.5304186344147
[2022-10-02 19:35:24 demo] (houston_program2.py 673): INFO 	val_Accuracy: 32951/53200 (61.94%)	
[2022-10-02 19:35:24 demo] (utils.py 47): INFO outputs/demo/finetune_100/ckpt_epoch_75.pth saving......
[2022-10-02 19:35:25 demo] (utils.py 49): INFO outputs/demo/finetune_100/ckpt_epoch_75.pth saved !!!
[2022-10-02 19:35:25 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0118 (0.0118)	loss 0.0078 (0.0078)	grad_norm 0.0414 (0.0414)	mem 460MB
[2022-10-02 19:35:25 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:25 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0213 (0.0213)	loss 0.0113 (0.0113)	grad_norm 0.0461 (0.0461)	mem 460MB
[2022-10-02 19:35:25 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:25 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0204 (0.0204)	loss 0.0104 (0.0104)	grad_norm 0.0599 (0.0599)	mem 460MB
[2022-10-02 19:35:25 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:26 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0193 (0.0193)	loss 0.0087 (0.0087)	grad_norm 0.0328 (0.0328)	mem 460MB
[2022-10-02 19:35:26 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:26 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0235 (0.0235)	loss 0.0115 (0.0115)	grad_norm 0.0428 (0.0428)	mem 460MB
[2022-10-02 19:35:26 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:26 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0218 (0.0218)	loss 0.0098 (0.0098)	grad_norm 0.0478 (0.0478)	mem 460MB
[2022-10-02 19:35:26 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:26 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0282 (0.0282)	loss 0.0104 (0.0104)	grad_norm 0.0411 (0.0411)	mem 460MB
[2022-10-02 19:35:26 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:26 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0206 (0.0206)	loss 0.0110 (0.0110)	grad_norm 0.0504 (0.0504)	mem 460MB
[2022-10-02 19:35:26 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:26 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0209 (0.0209)	loss 0.0067 (0.0067)	grad_norm 0.0354 (0.0354)	mem 460MB
[2022-10-02 19:35:26 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:27 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0185 (0.0185)	loss 0.0096 (0.0096)	grad_norm 0.0468 (0.0468)	mem 460MB
[2022-10-02 19:35:27 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:27 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0136 (0.0136)	loss 0.0086 (0.0086)	grad_norm 0.0445 (0.0445)	mem 460MB
[2022-10-02 19:35:27 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:27 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0212 (0.0212)	loss 0.0073 (0.0073)	grad_norm 0.0355 (0.0355)	mem 460MB
[2022-10-02 19:35:27 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:27 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0203 (0.0203)	loss 0.0073 (0.0073)	grad_norm 0.0313 (0.0313)	mem 460MB
[2022-10-02 19:35:27 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:27 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0200 (0.0200)	loss 0.0117 (0.0117)	grad_norm 0.0507 (0.0507)	mem 460MB
[2022-10-02 19:35:27 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:28 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0199 (0.0199)	loss 0.0075 (0.0075)	grad_norm 0.0335 (0.0335)	mem 460MB
[2022-10-02 19:35:28 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:28 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0212 (0.0212)	loss 0.0095 (0.0095)	grad_norm 0.0378 (0.0378)	mem 460MB
[2022-10-02 19:35:28 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:28 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0139 (0.0139)	loss 0.0096 (0.0096)	grad_norm 0.0332 (0.0332)	mem 460MB
[2022-10-02 19:35:28 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:28 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0221 (0.0221)	loss 0.0106 (0.0106)	grad_norm 0.0571 (0.0571)	mem 460MB
[2022-10-02 19:35:28 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:28 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0134 (0.0134)	loss 0.0090 (0.0090)	grad_norm 0.0565 (0.0565)	mem 460MB
[2022-10-02 19:35:28 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:28 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0209 (0.0209)	loss 0.0079 (0.0079)	grad_norm 0.0381 (0.0381)	mem 460MB
[2022-10-02 19:35:28 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:29 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0213 (0.0213)	loss 0.0074 (0.0074)	grad_norm 0.0324 (0.0324)	mem 460MB
[2022-10-02 19:35:29 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:29 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000007	time 0.0205 (0.0205)	loss 0.0098 (0.0098)	grad_norm 0.0458 (0.0458)	mem 460MB
[2022-10-02 19:35:29 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:29 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0088 (0.0088)	loss 0.0076 (0.0076)	grad_norm 0.0620 (0.0620)	mem 460MB
[2022-10-02 19:35:29 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:29 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0088 (0.0088)	loss 0.0098 (0.0098)	grad_norm 0.0534 (0.0534)	mem 460MB
[2022-10-02 19:35:29 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:29 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0081 (0.0081)	loss 0.0076 (0.0076)	grad_norm 0.0347 (0.0347)	mem 460MB
[2022-10-02 19:35:29 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:29 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0082 (0.0082)	loss 0.0105 (0.0105)	grad_norm 0.0556 (0.0556)	mem 460MB
[2022-10-02 19:35:29 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:29 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0189 (0.0189)	loss 0.0100 (0.0100)	grad_norm 0.0322 (0.0322)	mem 460MB
[2022-10-02 19:35:29 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:29 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0203 (0.0203)	loss 0.0088 (0.0088)	grad_norm 0.0304 (0.0304)	mem 460MB
[2022-10-02 19:35:30 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:30 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0175 (0.0175)	loss 0.0093 (0.0093)	grad_norm 0.0421 (0.0421)	mem 460MB
[2022-10-02 19:35:30 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:30 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0212 (0.0212)	loss 0.0091 (0.0091)	grad_norm 0.0556 (0.0556)	mem 460MB
[2022-10-02 19:35:30 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:30 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0251 (0.0251)	loss 0.0109 (0.0109)	grad_norm 0.0396 (0.0396)	mem 460MB
[2022-10-02 19:35:30 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:30 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0171 (0.0171)	loss 0.0077 (0.0077)	grad_norm 0.0300 (0.0300)	mem 460MB
[2022-10-02 19:35:30 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:31 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0223 (0.0223)	loss 0.0086 (0.0086)	grad_norm 0.0464 (0.0464)	mem 460MB
[2022-10-02 19:35:31 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:31 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0237 (0.0237)	loss 0.0062 (0.0062)	grad_norm 0.0274 (0.0274)	mem 460MB
[2022-10-02 19:35:31 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:31 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0277 (0.0277)	loss 0.0107 (0.0107)	grad_norm 0.0338 (0.0338)	mem 460MB
[2022-10-02 19:35:31 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:31 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0239 (0.0239)	loss 0.0111 (0.0111)	grad_norm 0.0414 (0.0414)	mem 460MB
[2022-10-02 19:35:31 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:31 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0137 (0.0137)	loss 0.0102 (0.0102)	grad_norm 0.0508 (0.0508)	mem 460MB
[2022-10-02 19:35:31 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:32 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0245 (0.0245)	loss 0.0065 (0.0065)	grad_norm 0.0511 (0.0511)	mem 460MB
[2022-10-02 19:35:32 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:32 demo] (houston_program2.py 243): INFO Train: [76/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0244 (0.0244)	loss 0.0078 (0.0078)	grad_norm 0.0475 (0.0475)	mem 460MB
[2022-10-02 19:35:32 demo] (houston_program2.py 252): INFO EPOCH 76 training takes 0:00:00
[2022-10-02 19:35:32 demo] (houston_program2.py 333): INFO Train Ep: 76 	Loss1: 0.136500	Loss2: 0.134564	 Dis: 4.028576 Entropy: 5.280429 
[2022-10-02 19:35:32 demo] (houston_program2.py 335): INFO time_76_epoch:7.395182371139526
[2022-10-02 19:35:32 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0128 (0.0128)	loss 0.0089 (0.0089)	grad_norm 0.0361 (0.0361)	mem 460MB
[2022-10-02 19:35:32 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:33 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0241 (0.0241)	loss 0.0103 (0.0103)	grad_norm 0.0431 (0.0431)	mem 460MB
[2022-10-02 19:35:33 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:33 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0225 (0.0225)	loss 0.0095 (0.0095)	grad_norm 0.0532 (0.0532)	mem 460MB
[2022-10-02 19:35:33 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:33 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0202 (0.0202)	loss 0.0078 (0.0078)	grad_norm 0.0385 (0.0385)	mem 460MB
[2022-10-02 19:35:33 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:33 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0220 (0.0220)	loss 0.0108 (0.0108)	grad_norm 0.0689 (0.0689)	mem 460MB
[2022-10-02 19:35:33 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:33 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0205 (0.0205)	loss 0.0093 (0.0093)	grad_norm 0.0308 (0.0308)	mem 460MB
[2022-10-02 19:35:33 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:34 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0210 (0.0210)	loss 0.0074 (0.0074)	grad_norm 0.0492 (0.0492)	mem 460MB
[2022-10-02 19:35:34 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:34 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0216 (0.0216)	loss 0.0109 (0.0109)	grad_norm 0.0335 (0.0335)	mem 460MB
[2022-10-02 19:35:34 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:34 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0219 (0.0219)	loss 0.0077 (0.0077)	grad_norm 0.0379 (0.0379)	mem 460MB
[2022-10-02 19:35:34 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:34 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0214 (0.0214)	loss 0.0106 (0.0106)	grad_norm 0.0309 (0.0309)	mem 460MB
[2022-10-02 19:35:34 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:34 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0245 (0.0245)	loss 0.0084 (0.0084)	grad_norm 0.0611 (0.0611)	mem 460MB
[2022-10-02 19:35:34 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:34 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0140 (0.0140)	loss 0.0080 (0.0080)	grad_norm 0.0426 (0.0426)	mem 460MB
[2022-10-02 19:35:34 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:35 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0140 (0.0140)	loss 0.0085 (0.0085)	grad_norm 0.0450 (0.0450)	mem 460MB
[2022-10-02 19:35:35 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:35 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0134 (0.0134)	loss 0.0083 (0.0083)	grad_norm 0.0503 (0.0503)	mem 460MB
[2022-10-02 19:35:35 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:35 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0211 (0.0211)	loss 0.0096 (0.0096)	grad_norm 0.0383 (0.0383)	mem 460MB
[2022-10-02 19:35:35 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:35 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0215 (0.0215)	loss 0.0077 (0.0077)	grad_norm 0.0530 (0.0530)	mem 460MB
[2022-10-02 19:35:35 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:35 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0213 (0.0213)	loss 0.0084 (0.0084)	grad_norm 0.0392 (0.0392)	mem 460MB
[2022-10-02 19:35:35 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:35 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0203 (0.0203)	loss 0.0058 (0.0058)	grad_norm 0.0235 (0.0235)	mem 460MB
[2022-10-02 19:35:35 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:36 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0230 (0.0230)	loss 0.0075 (0.0075)	grad_norm 0.0328 (0.0328)	mem 460MB
[2022-10-02 19:35:36 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:36 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0185 (0.0185)	loss 0.0085 (0.0085)	grad_norm 0.0381 (0.0381)	mem 460MB
[2022-10-02 19:35:36 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:36 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0173 (0.0173)	loss 0.0082 (0.0082)	grad_norm 0.0519 (0.0519)	mem 460MB
[2022-10-02 19:35:36 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:36 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0219 (0.0219)	loss 0.0077 (0.0077)	grad_norm 0.0440 (0.0440)	mem 460MB
[2022-10-02 19:35:36 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:36 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0137 (0.0137)	loss 0.0082 (0.0082)	grad_norm 0.0310 (0.0310)	mem 460MB
[2022-10-02 19:35:36 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:37 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0209 (0.0209)	loss 0.0096 (0.0096)	grad_norm 0.0572 (0.0572)	mem 460MB
[2022-10-02 19:35:37 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:37 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0110 (0.0110)	loss 0.0081 (0.0081)	grad_norm 0.0492 (0.0492)	mem 460MB
[2022-10-02 19:35:37 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:37 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0213 (0.0213)	loss 0.0069 (0.0069)	grad_norm 0.0358 (0.0358)	mem 460MB
[2022-10-02 19:35:37 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:37 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0117 (0.0117)	loss 0.0092 (0.0092)	grad_norm 0.0450 (0.0450)	mem 460MB
[2022-10-02 19:35:37 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:37 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0223 (0.0223)	loss 0.0114 (0.0114)	grad_norm 0.0303 (0.0303)	mem 460MB
[2022-10-02 19:35:37 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:37 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0210 (0.0210)	loss 0.0093 (0.0093)	grad_norm 0.0454 (0.0454)	mem 460MB
[2022-10-02 19:35:37 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:38 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0188 (0.0188)	loss 0.0068 (0.0068)	grad_norm 0.0308 (0.0308)	mem 460MB
[2022-10-02 19:35:38 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:38 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0208 (0.0208)	loss 0.0076 (0.0076)	grad_norm 0.0373 (0.0373)	mem 460MB
[2022-10-02 19:35:38 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:38 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0203 (0.0203)	loss 0.0073 (0.0073)	grad_norm 0.0648 (0.0648)	mem 460MB
[2022-10-02 19:35:38 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:38 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0212 (0.0212)	loss 0.0081 (0.0081)	grad_norm 0.0393 (0.0393)	mem 460MB
[2022-10-02 19:35:38 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:38 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0232 (0.0232)	loss 0.0089 (0.0089)	grad_norm 0.0290 (0.0290)	mem 460MB
[2022-10-02 19:35:38 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:39 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0220 (0.0220)	loss 0.0108 (0.0108)	grad_norm 0.0512 (0.0512)	mem 460MB
[2022-10-02 19:35:39 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:39 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0190 (0.0190)	loss 0.0083 (0.0083)	grad_norm 0.0468 (0.0468)	mem 460MB
[2022-10-02 19:35:39 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:39 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0210 (0.0210)	loss 0.0101 (0.0101)	grad_norm 0.0371 (0.0371)	mem 460MB
[2022-10-02 19:35:39 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:39 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0107 (0.0107)	loss 0.0094 (0.0094)	grad_norm 0.0478 (0.0478)	mem 460MB
[2022-10-02 19:35:39 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:39 demo] (houston_program2.py 243): INFO Train: [77/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0206 (0.0206)	loss 0.0086 (0.0086)	grad_norm 0.1100 (0.1100)	mem 460MB
[2022-10-02 19:35:39 demo] (houston_program2.py 252): INFO EPOCH 77 training takes 0:00:00
[2022-10-02 19:35:40 demo] (houston_program2.py 333): INFO Train Ep: 77 	Loss1: 0.104904	Loss2: 0.099666	 Dis: 5.131413 Entropy: 4.658245 
[2022-10-02 19:35:40 demo] (houston_program2.py 335): INFO time_77_epoch:7.467798471450806
[2022-10-02 19:35:40 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0134 (0.0134)	loss 0.0108 (0.0108)	grad_norm 0.0453 (0.0453)	mem 460MB
[2022-10-02 19:35:40 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:40 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0264 (0.0264)	loss 0.0086 (0.0086)	grad_norm 0.0309 (0.0309)	mem 460MB
[2022-10-02 19:35:40 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:40 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0217 (0.0217)	loss 0.0067 (0.0067)	grad_norm 0.0280 (0.0280)	mem 460MB
[2022-10-02 19:35:40 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:40 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0102 (0.0102)	loss 0.0125 (0.0125)	grad_norm 0.0401 (0.0401)	mem 460MB
[2022-10-02 19:35:40 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:41 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0138 (0.0138)	loss 0.0093 (0.0093)	grad_norm 0.0312 (0.0312)	mem 460MB
[2022-10-02 19:35:41 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:41 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0121 (0.0121)	loss 0.0081 (0.0081)	grad_norm 0.0307 (0.0307)	mem 460MB
[2022-10-02 19:35:41 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:41 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0216 (0.0216)	loss 0.0081 (0.0081)	grad_norm 0.0348 (0.0348)	mem 460MB
[2022-10-02 19:35:41 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:41 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0111 (0.0111)	loss 0.0091 (0.0091)	grad_norm 0.0450 (0.0450)	mem 460MB
[2022-10-02 19:35:41 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:41 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0221 (0.0221)	loss 0.0104 (0.0104)	grad_norm 0.0351 (0.0351)	mem 460MB
[2022-10-02 19:35:41 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:41 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0144 (0.0144)	loss 0.0074 (0.0074)	grad_norm 0.0336 (0.0336)	mem 460MB
[2022-10-02 19:35:41 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:42 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0219 (0.0219)	loss 0.0071 (0.0071)	grad_norm 0.0334 (0.0334)	mem 460MB
[2022-10-02 19:35:42 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:42 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0200 (0.0200)	loss 0.0088 (0.0088)	grad_norm 0.0357 (0.0357)	mem 460MB
[2022-10-02 19:35:42 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:42 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0206 (0.0206)	loss 0.0079 (0.0079)	grad_norm 0.0303 (0.0303)	mem 460MB
[2022-10-02 19:35:42 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:42 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0221 (0.0221)	loss 0.0087 (0.0087)	grad_norm 0.0467 (0.0467)	mem 460MB
[2022-10-02 19:35:42 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:42 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0214 (0.0214)	loss 0.0108 (0.0108)	grad_norm 0.0798 (0.0798)	mem 460MB
[2022-10-02 19:35:42 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:43 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0219 (0.0219)	loss 0.0071 (0.0071)	grad_norm 0.0414 (0.0414)	mem 460MB
[2022-10-02 19:35:43 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:43 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0207 (0.0207)	loss 0.0076 (0.0076)	grad_norm 0.0398 (0.0398)	mem 460MB
[2022-10-02 19:35:43 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:43 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0221 (0.0221)	loss 0.0068 (0.0068)	grad_norm 0.0342 (0.0342)	mem 460MB
[2022-10-02 19:35:43 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:43 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0112 (0.0112)	loss 0.0087 (0.0087)	grad_norm 0.0487 (0.0487)	mem 460MB
[2022-10-02 19:35:43 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:43 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0222 (0.0222)	loss 0.0077 (0.0077)	grad_norm 0.0302 (0.0302)	mem 460MB
[2022-10-02 19:35:43 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:43 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000006	time 0.0138 (0.0138)	loss 0.0088 (0.0088)	grad_norm 0.0815 (0.0815)	mem 460MB
[2022-10-02 19:35:43 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:44 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0204 (0.0204)	loss 0.0110 (0.0110)	grad_norm 0.0543 (0.0543)	mem 460MB
[2022-10-02 19:35:44 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:44 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0151 (0.0151)	loss 0.0072 (0.0072)	grad_norm 0.0430 (0.0430)	mem 460MB
[2022-10-02 19:35:44 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:44 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0203 (0.0203)	loss 0.0078 (0.0078)	grad_norm 0.0734 (0.0734)	mem 460MB
[2022-10-02 19:35:44 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:44 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0184 (0.0184)	loss 0.0115 (0.0115)	grad_norm 0.0326 (0.0326)	mem 460MB
[2022-10-02 19:35:44 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:44 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0221 (0.0221)	loss 0.0081 (0.0081)	grad_norm 0.0363 (0.0363)	mem 460MB
[2022-10-02 19:35:44 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:45 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0221 (0.0221)	loss 0.0096 (0.0096)	grad_norm 0.0372 (0.0372)	mem 460MB
[2022-10-02 19:35:45 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:45 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0202 (0.0202)	loss 0.0144 (0.0144)	grad_norm 0.0732 (0.0732)	mem 460MB
[2022-10-02 19:35:45 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:45 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0207 (0.0207)	loss 0.0124 (0.0124)	grad_norm 0.0556 (0.0556)	mem 460MB
[2022-10-02 19:35:45 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:45 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0193 (0.0193)	loss 0.0099 (0.0099)	grad_norm 0.0384 (0.0384)	mem 460MB
[2022-10-02 19:35:45 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:45 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0221 (0.0221)	loss 0.0083 (0.0083)	grad_norm 0.0386 (0.0386)	mem 460MB
[2022-10-02 19:35:45 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:45 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0134 (0.0134)	loss 0.0066 (0.0066)	grad_norm 0.0333 (0.0333)	mem 460MB
[2022-10-02 19:35:45 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:46 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0208 (0.0208)	loss 0.0074 (0.0074)	grad_norm 0.0324 (0.0324)	mem 460MB
[2022-10-02 19:35:46 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:46 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0107 (0.0107)	loss 0.0127 (0.0127)	grad_norm 0.0552 (0.0552)	mem 460MB
[2022-10-02 19:35:46 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:46 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0211 (0.0211)	loss 0.0074 (0.0074)	grad_norm 0.0668 (0.0668)	mem 460MB
[2022-10-02 19:35:46 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:46 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0118 (0.0118)	loss 0.0113 (0.0113)	grad_norm 0.0483 (0.0483)	mem 460MB
[2022-10-02 19:35:46 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:46 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0207 (0.0207)	loss 0.0084 (0.0084)	grad_norm 0.0290 (0.0290)	mem 460MB
[2022-10-02 19:35:46 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:46 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0136 (0.0136)	loss 0.0098 (0.0098)	grad_norm 0.0407 (0.0407)	mem 460MB
[2022-10-02 19:35:46 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:47 demo] (houston_program2.py 243): INFO Train: [78/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0180 (0.0180)	loss 0.0078 (0.0078)	grad_norm 0.0389 (0.0389)	mem 460MB
[2022-10-02 19:35:47 demo] (houston_program2.py 252): INFO EPOCH 78 training takes 0:00:00
[2022-10-02 19:35:47 demo] (houston_program2.py 333): INFO Train Ep: 78 	Loss1: 0.074937	Loss2: 0.080948	 Dis: 2.498747 Entropy: 5.122236 
[2022-10-02 19:35:47 demo] (houston_program2.py 335): INFO time_78_epoch:7.451066255569458
[2022-10-02 19:35:47 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0113 (0.0113)	loss 0.0070 (0.0070)	grad_norm 0.0406 (0.0406)	mem 460MB
[2022-10-02 19:35:47 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:47 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0122 (0.0122)	loss 0.0083 (0.0083)	grad_norm 0.0303 (0.0303)	mem 460MB
[2022-10-02 19:35:48 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:48 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0223 (0.0223)	loss 0.0080 (0.0080)	grad_norm 0.0369 (0.0369)	mem 460MB
[2022-10-02 19:35:48 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:48 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0175 (0.0175)	loss 0.0086 (0.0086)	grad_norm 0.0401 (0.0401)	mem 460MB
[2022-10-02 19:35:48 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:48 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0218 (0.0218)	loss 0.0093 (0.0093)	grad_norm 0.1017 (0.1017)	mem 460MB
[2022-10-02 19:35:48 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:48 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0220 (0.0220)	loss 0.0097 (0.0097)	grad_norm 0.0381 (0.0381)	mem 460MB
[2022-10-02 19:35:48 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:48 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0218 (0.0218)	loss 0.0091 (0.0091)	grad_norm 0.0413 (0.0413)	mem 460MB
[2022-10-02 19:35:48 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:49 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0210 (0.0210)	loss 0.0136 (0.0136)	grad_norm 0.0523 (0.0523)	mem 460MB
[2022-10-02 19:35:49 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:49 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0202 (0.0202)	loss 0.0132 (0.0132)	grad_norm 0.0768 (0.0768)	mem 460MB
[2022-10-02 19:35:49 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:49 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0212 (0.0212)	loss 0.0100 (0.0100)	grad_norm 0.0422 (0.0422)	mem 460MB
[2022-10-02 19:35:49 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:49 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0220 (0.0220)	loss 0.0090 (0.0090)	grad_norm 0.0497 (0.0497)	mem 460MB
[2022-10-02 19:35:49 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:49 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0220 (0.0220)	loss 0.0104 (0.0104)	grad_norm 0.0390 (0.0390)	mem 460MB
[2022-10-02 19:35:49 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:50 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0248 (0.0248)	loss 0.0091 (0.0091)	grad_norm 0.0478 (0.0478)	mem 460MB
[2022-10-02 19:35:50 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:50 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0129 (0.0129)	loss 0.0075 (0.0075)	grad_norm 0.0372 (0.0372)	mem 460MB
[2022-10-02 19:35:50 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:50 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0213 (0.0213)	loss 0.0090 (0.0090)	grad_norm 0.0324 (0.0324)	mem 460MB
[2022-10-02 19:35:50 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:50 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0240 (0.0240)	loss 0.0081 (0.0081)	grad_norm 0.0772 (0.0772)	mem 460MB
[2022-10-02 19:35:50 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:50 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0129 (0.0129)	loss 0.0077 (0.0077)	grad_norm 0.0367 (0.0367)	mem 460MB
[2022-10-02 19:35:50 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:50 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0245 (0.0245)	loss 0.0088 (0.0088)	grad_norm 0.0350 (0.0350)	mem 460MB
[2022-10-02 19:35:50 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:51 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0130 (0.0130)	loss 0.0124 (0.0124)	grad_norm 0.0463 (0.0463)	mem 460MB
[2022-10-02 19:35:51 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:51 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0133 (0.0133)	loss 0.0089 (0.0089)	grad_norm 0.0649 (0.0649)	mem 460MB
[2022-10-02 19:35:51 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:51 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0130 (0.0130)	loss 0.0086 (0.0086)	grad_norm 0.0382 (0.0382)	mem 460MB
[2022-10-02 19:35:51 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:51 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0220 (0.0220)	loss 0.0097 (0.0097)	grad_norm 0.0420 (0.0420)	mem 460MB
[2022-10-02 19:35:51 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:51 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0176 (0.0176)	loss 0.0076 (0.0076)	grad_norm 0.0445 (0.0445)	mem 460MB
[2022-10-02 19:35:51 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:52 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0204 (0.0204)	loss 0.0079 (0.0079)	grad_norm 0.0307 (0.0307)	mem 460MB
[2022-10-02 19:35:52 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:52 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0168 (0.0168)	loss 0.0083 (0.0083)	grad_norm 0.0400 (0.0400)	mem 460MB
[2022-10-02 19:35:52 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:52 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0224 (0.0224)	loss 0.0084 (0.0084)	grad_norm 0.0286 (0.0286)	mem 460MB
[2022-10-02 19:35:52 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:52 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0203 (0.0203)	loss 0.0081 (0.0081)	grad_norm 0.0475 (0.0475)	mem 460MB
[2022-10-02 19:35:52 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:52 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0206 (0.0206)	loss 0.0088 (0.0088)	grad_norm 0.0353 (0.0353)	mem 460MB
[2022-10-02 19:35:52 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:52 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0221 (0.0221)	loss 0.0098 (0.0098)	grad_norm 0.0436 (0.0436)	mem 460MB
[2022-10-02 19:35:52 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:53 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0172 (0.0172)	loss 0.0068 (0.0068)	grad_norm 0.0325 (0.0325)	mem 460MB
[2022-10-02 19:35:53 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:53 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0206 (0.0206)	loss 0.0082 (0.0082)	grad_norm 0.0304 (0.0304)	mem 460MB
[2022-10-02 19:35:53 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:53 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0136 (0.0136)	loss 0.0112 (0.0112)	grad_norm 0.0552 (0.0552)	mem 460MB
[2022-10-02 19:35:53 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:53 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0208 (0.0208)	loss 0.0114 (0.0114)	grad_norm 0.1051 (0.1051)	mem 460MB
[2022-10-02 19:35:53 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:53 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0143 (0.0143)	loss 0.0085 (0.0085)	grad_norm 0.0366 (0.0366)	mem 460MB
[2022-10-02 19:35:53 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:54 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0204 (0.0204)	loss 0.0101 (0.0101)	grad_norm 0.0484 (0.0484)	mem 460MB
[2022-10-02 19:35:54 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:54 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0150 (0.0150)	loss 0.0076 (0.0076)	grad_norm 0.0344 (0.0344)	mem 460MB
[2022-10-02 19:35:54 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:54 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0200 (0.0200)	loss 0.0097 (0.0097)	grad_norm 0.0387 (0.0387)	mem 460MB
[2022-10-02 19:35:54 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:54 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0139 (0.0139)	loss 0.0079 (0.0079)	grad_norm 0.0477 (0.0477)	mem 460MB
[2022-10-02 19:35:54 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:54 demo] (houston_program2.py 243): INFO Train: [79/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0217 (0.0217)	loss 0.0095 (0.0095)	grad_norm 0.0376 (0.0376)	mem 460MB
[2022-10-02 19:35:54 demo] (houston_program2.py 252): INFO EPOCH 79 training takes 0:00:00
[2022-10-02 19:35:54 demo] (houston_program2.py 333): INFO Train Ep: 79 	Loss1: 0.093932	Loss2: 0.097569	 Dis: 4.236942 Entropy: 5.596841 
[2022-10-02 19:35:54 demo] (houston_program2.py 335): INFO time_79_epoch:7.52357029914856
[2022-10-02 19:35:55 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0118 (0.0118)	loss 0.0078 (0.0078)	grad_norm 0.0324 (0.0324)	mem 460MB
[2022-10-02 19:35:55 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:35:55 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0227 (0.0227)	loss 0.0076 (0.0076)	grad_norm 0.0411 (0.0411)	mem 460MB
[2022-10-02 19:35:55 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:35:55 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0216 (0.0216)	loss 0.0088 (0.0088)	grad_norm 0.0507 (0.0507)	mem 460MB
[2022-10-02 19:35:55 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:35:55 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0131 (0.0131)	loss 0.0101 (0.0101)	grad_norm 0.0348 (0.0348)	mem 460MB
[2022-10-02 19:35:55 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:35:56 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0219 (0.0219)	loss 0.0098 (0.0098)	grad_norm 0.0357 (0.0357)	mem 460MB
[2022-10-02 19:35:56 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:35:56 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0153 (0.0153)	loss 0.0091 (0.0091)	grad_norm 0.0288 (0.0288)	mem 460MB
[2022-10-02 19:35:56 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:35:56 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0214 (0.0214)	loss 0.0072 (0.0072)	grad_norm 0.0333 (0.0333)	mem 460MB
[2022-10-02 19:35:56 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:35:56 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0179 (0.0179)	loss 0.0087 (0.0087)	grad_norm 0.0348 (0.0348)	mem 460MB
[2022-10-02 19:35:56 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:35:56 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0217 (0.0217)	loss 0.0072 (0.0072)	grad_norm 0.0766 (0.0766)	mem 460MB
[2022-10-02 19:35:56 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:35:56 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0217 (0.0217)	loss 0.0075 (0.0075)	grad_norm 0.0302 (0.0302)	mem 460MB
[2022-10-02 19:35:56 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:35:57 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0195 (0.0195)	loss 0.0074 (0.0074)	grad_norm 0.0368 (0.0368)	mem 460MB
[2022-10-02 19:35:57 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:35:57 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0175 (0.0175)	loss 0.0088 (0.0088)	grad_norm 0.0348 (0.0348)	mem 460MB
[2022-10-02 19:35:57 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:35:57 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0216 (0.0216)	loss 0.0096 (0.0096)	grad_norm 0.0334 (0.0334)	mem 460MB
[2022-10-02 19:35:57 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:35:57 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0204 (0.0204)	loss 0.0086 (0.0086)	grad_norm 0.0411 (0.0411)	mem 460MB
[2022-10-02 19:35:57 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:35:57 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0203 (0.0203)	loss 0.0071 (0.0071)	grad_norm 0.0299 (0.0299)	mem 460MB
[2022-10-02 19:35:57 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:35:57 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0199 (0.0199)	loss 0.0078 (0.0078)	grad_norm 0.0354 (0.0354)	mem 460MB
[2022-10-02 19:35:58 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:35:58 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0238 (0.0238)	loss 0.0079 (0.0079)	grad_norm 0.0654 (0.0654)	mem 460MB
[2022-10-02 19:35:58 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:35:58 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0223 (0.0223)	loss 0.0139 (0.0139)	grad_norm 0.0313 (0.0313)	mem 460MB
[2022-10-02 19:35:58 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:35:58 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0135 (0.0135)	loss 0.0083 (0.0083)	grad_norm 0.0308 (0.0308)	mem 460MB
[2022-10-02 19:35:58 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:35:58 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0217 (0.0217)	loss 0.0077 (0.0077)	grad_norm 0.0359 (0.0359)	mem 460MB
[2022-10-02 19:35:58 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:35:58 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0166 (0.0166)	loss 0.0073 (0.0073)	grad_norm 0.0374 (0.0374)	mem 460MB
[2022-10-02 19:35:58 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:35:59 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0221 (0.0221)	loss 0.0099 (0.0099)	grad_norm 0.0368 (0.0368)	mem 460MB
[2022-10-02 19:35:59 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:35:59 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0216 (0.0216)	loss 0.0083 (0.0083)	grad_norm 0.0325 (0.0325)	mem 460MB
[2022-10-02 19:35:59 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:35:59 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0218 (0.0218)	loss 0.0074 (0.0074)	grad_norm 0.0375 (0.0375)	mem 460MB
[2022-10-02 19:35:59 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:35:59 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0194 (0.0194)	loss 0.0085 (0.0085)	grad_norm 0.0302 (0.0302)	mem 460MB
[2022-10-02 19:35:59 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:35:59 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000005	time 0.0220 (0.0220)	loss 0.0091 (0.0091)	grad_norm 0.0528 (0.0528)	mem 460MB
[2022-10-02 19:35:59 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:36:00 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0207 (0.0207)	loss 0.0076 (0.0076)	grad_norm 0.0365 (0.0365)	mem 460MB
[2022-10-02 19:36:00 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:36:00 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0248 (0.0248)	loss 0.0086 (0.0086)	grad_norm 0.0341 (0.0341)	mem 460MB
[2022-10-02 19:36:00 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:36:00 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0218 (0.0218)	loss 0.0077 (0.0077)	grad_norm 0.0521 (0.0521)	mem 460MB
[2022-10-02 19:36:00 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:36:00 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0143 (0.0143)	loss 0.0078 (0.0078)	grad_norm 0.0375 (0.0375)	mem 460MB
[2022-10-02 19:36:00 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:36:00 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0201 (0.0201)	loss 0.0104 (0.0104)	grad_norm 0.0368 (0.0368)	mem 460MB
[2022-10-02 19:36:00 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:36:00 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0109 (0.0109)	loss 0.0104 (0.0104)	grad_norm 0.0409 (0.0409)	mem 460MB
[2022-10-02 19:36:00 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:36:01 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0207 (0.0207)	loss 0.0088 (0.0088)	grad_norm 0.0459 (0.0459)	mem 460MB
[2022-10-02 19:36:01 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:36:01 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0133 (0.0133)	loss 0.0102 (0.0102)	grad_norm 0.0299 (0.0299)	mem 460MB
[2022-10-02 19:36:01 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:36:01 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0208 (0.0208)	loss 0.0081 (0.0081)	grad_norm 0.0359 (0.0359)	mem 460MB
[2022-10-02 19:36:01 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:36:01 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0173 (0.0173)	loss 0.0091 (0.0091)	grad_norm 0.0519 (0.0519)	mem 460MB
[2022-10-02 19:36:01 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:36:01 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0215 (0.0215)	loss 0.0091 (0.0091)	grad_norm 0.0391 (0.0391)	mem 460MB
[2022-10-02 19:36:01 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:36:01 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0142 (0.0142)	loss 0.0062 (0.0062)	grad_norm 0.0292 (0.0292)	mem 460MB
[2022-10-02 19:36:01 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:36:02 demo] (houston_program2.py 243): INFO Train: [80/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0239 (0.0239)	loss 0.0080 (0.0080)	grad_norm 0.0674 (0.0674)	mem 460MB
[2022-10-02 19:36:02 demo] (houston_program2.py 252): INFO EPOCH 80 training takes 0:00:00
[2022-10-02 19:36:02 demo] (houston_program2.py 333): INFO Train Ep: 80 	Loss1: 0.046306	Loss2: 0.038232	 Dis: 3.174034 Entropy: 4.434206 
[2022-10-02 19:36:02 demo] (houston_program2.py 335): INFO time_80_epoch:7.443741798400879
[2022-10-02 19:36:02 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 19:36:02 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 19:36:02 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 19:36:02 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 19:36:02 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 19:36:02 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 19:36:02 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 19:36:02 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 19:36:02 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 19:36:02 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 19:36:08 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.082872	Loss2: 0.076344	 Dis: 4.080490 Entropy: 5.644933 
[2022-10-02 19:36:08 demo] (houston_program2.py 512): INFO E_lr_1: 0.000487	 E_lr_2:0.000487
[2022-10-02 19:36:08 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 19:36:14 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.032000	Loss2: 0.027476	 Dis: 3.162224 Entropy: 4.928864 
[2022-10-02 19:36:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.000987	 E_lr_2:0.000987
[2022-10-02 19:36:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 19:36:19 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.091072	Loss2: 0.071020	 Dis: 5.022936 Entropy: 4.627022 
[2022-10-02 19:36:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.001487	 E_lr_2:0.001487
[2022-10-02 19:36:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 19:36:26 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.186135	Loss2: 0.254393	 Dis: 5.683760 Entropy: 4.486951 
[2022-10-02 19:36:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.001987	 E_lr_2:0.001987
[2022-10-02 19:36:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 19:36:31 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.116300	Loss2: 0.153161	 Dis: 3.409763 Entropy: 4.693115 
[2022-10-02 19:36:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.002487	 E_lr_2:0.002487
[2022-10-02 19:36:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 19:36:38 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.074449	Loss2: 0.092087	 Dis: 6.684175 Entropy: 4.653085 
[2022-10-02 19:36:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.002987	 E_lr_2:0.002987
[2022-10-02 19:36:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 19:36:44 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.059832	Loss2: 0.059513	 Dis: 3.776632 Entropy: 5.264105 
[2022-10-02 19:36:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.003487	 E_lr_2:0.003487
[2022-10-02 19:36:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 19:36:49 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.112342	Loss2: 0.115580	 Dis: 6.411758 Entropy: 7.419706 
[2022-10-02 19:36:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.003987	 E_lr_2:0.003987
[2022-10-02 19:36:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 19:36:55 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.167636	Loss2: 0.150349	 Dis: 4.225410 Entropy: 5.473918 
[2022-10-02 19:36:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.004487	 E_lr_2:0.004487
[2022-10-02 19:36:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 19:37:01 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.131528	Loss2: 0.119687	 Dis: 1.925968 Entropy: 4.147193 
[2022-10-02 19:37:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.004987	 E_lr_2:0.004987
[2022-10-02 19:37:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000050	 C_lr_2:0.000050	 C_lr_3:0.000050	 C_lr_4:0.000050
[2022-10-02 19:37:06 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.043070	Loss2: 0.037126	 Dis: 7.401402 Entropy: 5.094557 
[2022-10-02 19:37:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.004853	 E_lr_2:0.004853
[2022-10-02 19:37:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000049	 C_lr_2:0.000049	 C_lr_3:0.000049	 C_lr_4:0.000049
[2022-10-02 19:37:11 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.082695	Loss2: 0.094611	 Dis: 5.349625 Entropy: 4.875752 
[2022-10-02 19:37:11 demo] (houston_program2.py 512): INFO E_lr_1: 0.004825	 E_lr_2:0.004825
[2022-10-02 19:37:11 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 19:37:17 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 1.439554	Loss2: 1.370120	 Dis: 17.849480 Entropy: 4.190886 
[2022-10-02 19:37:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.004795	 E_lr_2:0.004795
[2022-10-02 19:37:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 19:37:24 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.329318	Loss2: 0.280907	 Dis: 8.958809 Entropy: 4.473563 
[2022-10-02 19:37:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.004763	 E_lr_2:0.004763
[2022-10-02 19:37:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 19:37:30 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.332374	Loss2: 0.305419	 Dis: 6.232111 Entropy: 4.414350 
[2022-10-02 19:37:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.004728	 E_lr_2:0.004728
[2022-10-02 19:37:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 19:37:35 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.151691	Loss2: 0.169401	 Dis: 7.945423 Entropy: 4.840250 
[2022-10-02 19:37:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.004692	 E_lr_2:0.004692
[2022-10-02 19:37:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 19:37:42 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.315750	Loss2: 0.316960	 Dis: 6.361906 Entropy: 4.765396 
[2022-10-02 19:37:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.004653	 E_lr_2:0.004653
[2022-10-02 19:37:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 19:37:48 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.233812	Loss2: 0.213026	 Dis: 6.596996 Entropy: 4.656198 
[2022-10-02 19:37:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.004612	 E_lr_2:0.004612
[2022-10-02 19:37:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 19:37:53 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.191442	Loss2: 0.168845	 Dis: 7.751669 Entropy: 4.988912 
[2022-10-02 19:37:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.004569	 E_lr_2:0.004569
[2022-10-02 19:37:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 19:37:59 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.175847	Loss2: 0.162433	 Dis: 3.683481 Entropy: 4.428051 
[2022-10-02 19:37:59 demo] (houston_program2.py 512): INFO E_lr_1: 0.004524	 E_lr_2:0.004524
[2022-10-02 19:37:59 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 19:38:05 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.128152	Loss2: 0.138436	 Dis: 5.726454 Entropy: 5.009684 
[2022-10-02 19:38:05 demo] (houston_program2.py 512): INFO E_lr_1: 0.004477	 E_lr_2:0.004477
[2022-10-02 19:38:05 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 19:38:11 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.236546	Loss2: 0.215980	 Dis: 6.387518 Entropy: 4.610737 
[2022-10-02 19:38:11 demo] (houston_program2.py 512): INFO E_lr_1: 0.004428	 E_lr_2:0.004428
[2022-10-02 19:38:11 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 19:38:17 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.160489	Loss2: 0.158883	 Dis: 5.926283 Entropy: 4.896311 
[2022-10-02 19:38:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.004377	 E_lr_2:0.004377
[2022-10-02 19:38:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 19:38:24 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.054628	Loss2: 0.049801	 Dis: 3.034203 Entropy: 4.826401 
[2022-10-02 19:38:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.004324	 E_lr_2:0.004324
[2022-10-02 19:38:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 19:38:30 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.092075	Loss2: 0.081537	 Dis: 4.778355 Entropy: 5.079798 
[2022-10-02 19:38:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.004269	 E_lr_2:0.004269
[2022-10-02 19:38:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 19:38:36 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.164482	Loss2: 0.177966	 Dis: 5.700031 Entropy: 5.366069 
[2022-10-02 19:38:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.004213	 E_lr_2:0.004213
[2022-10-02 19:38:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 19:38:43 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.113954	Loss2: 0.102295	 Dis: 3.982315 Entropy: 4.717485 
[2022-10-02 19:38:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.004155	 E_lr_2:0.004155
[2022-10-02 19:38:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 19:38:48 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.188011	Loss2: 0.183141	 Dis: 4.691029 Entropy: 5.476452 
[2022-10-02 19:38:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.004095	 E_lr_2:0.004095
[2022-10-02 19:38:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000041	 C_lr_2:0.000041	 C_lr_3:0.000041	 C_lr_4:0.000041
[2022-10-02 19:38:54 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.122995	Loss2: 0.116399	 Dis: 3.408562 Entropy: 5.530161 
[2022-10-02 19:38:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.004034	 E_lr_2:0.004034
[2022-10-02 19:38:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 19:39:01 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.139706	Loss2: 0.120230	 Dis: 5.537056 Entropy: 5.521279 
[2022-10-02 19:39:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.003971	 E_lr_2:0.003971
[2022-10-02 19:39:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 19:39:07 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.079380	Loss2: 0.076512	 Dis: 4.369402 Entropy: 4.622142 
[2022-10-02 19:39:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.003907	 E_lr_2:0.003907
[2022-10-02 19:39:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000039	 C_lr_2:0.000039	 C_lr_3:0.000039	 C_lr_4:0.000039
[2022-10-02 19:39:13 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.164875	Loss2: 0.110211	 Dis: 4.170509 Entropy: 5.035289 
[2022-10-02 19:39:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.003841	 E_lr_2:0.003841
[2022-10-02 19:39:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 19:39:19 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.245069	Loss2: 0.249604	 Dis: 4.489212 Entropy: 4.306496 
[2022-10-02 19:39:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.003774	 E_lr_2:0.003774
[2022-10-02 19:39:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 19:39:26 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.061016	Loss2: 0.077210	 Dis: 4.036821 Entropy: 4.524933 
[2022-10-02 19:39:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.003706	 E_lr_2:0.003706
[2022-10-02 19:39:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000037	 C_lr_2:0.000037	 C_lr_3:0.000037	 C_lr_4:0.000037
[2022-10-02 19:39:31 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.093782	Loss2: 0.060570	 Dis: 4.392452 Entropy: 6.257642 
[2022-10-02 19:39:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.003637	 E_lr_2:0.003637
[2022-10-02 19:39:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 19:39:37 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.017522	Loss2: 0.018421	 Dis: 4.276455 Entropy: 5.657387 
[2022-10-02 19:39:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.003566	 E_lr_2:0.003566
[2022-10-02 19:39:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 19:39:44 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.115505	Loss2: 0.135350	 Dis: 3.342831 Entropy: 5.251989 
[2022-10-02 19:39:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.003495	 E_lr_2:0.003495
[2022-10-02 19:39:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 19:39:50 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.139404	Loss2: 0.183700	 Dis: 5.109852 Entropy: 6.037061 
[2022-10-02 19:39:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.003422	 E_lr_2:0.003422
[2022-10-02 19:39:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000034	 C_lr_2:0.000034	 C_lr_3:0.000034	 C_lr_4:0.000034
[2022-10-02 19:39:55 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.045384	Loss2: 0.041561	 Dis: 3.045879 Entropy: 4.920089 
[2022-10-02 19:39:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.003349	 E_lr_2:0.003349
[2022-10-02 19:39:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 19:40:01 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.078645	Loss2: 0.065297	 Dis: 5.196056 Entropy: 5.594537 
[2022-10-02 19:40:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.003274	 E_lr_2:0.003274
[2022-10-02 19:40:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 19:40:07 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.094313	Loss2: 0.108141	 Dis: 5.934185 Entropy: 4.476371 
[2022-10-02 19:40:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.003199	 E_lr_2:0.003199
[2022-10-02 19:40:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000032	 C_lr_2:0.000032	 C_lr_3:0.000032	 C_lr_4:0.000032
[2022-10-02 19:40:13 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.046030	Loss2: 0.056070	 Dis: 6.968027 Entropy: 4.614694 
[2022-10-02 19:40:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.003124	 E_lr_2:0.003124
[2022-10-02 19:40:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000031	 C_lr_2:0.000031	 C_lr_3:0.000031	 C_lr_4:0.000031
[2022-10-02 19:40:19 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.045081	Loss2: 0.035789	 Dis: 4.585585 Entropy: 5.229507 
[2022-10-02 19:40:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.003047	 E_lr_2:0.003047
[2022-10-02 19:40:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 19:40:25 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.051277	Loss2: 0.053812	 Dis: 5.433830 Entropy: 4.664942 
[2022-10-02 19:40:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.002970	 E_lr_2:0.002970
[2022-10-02 19:40:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 19:40:30 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.059875	Loss2: 0.068935	 Dis: 4.150059 Entropy: 5.734051 
[2022-10-02 19:40:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.002893	 E_lr_2:0.002893
[2022-10-02 19:40:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000029	 C_lr_2:0.000029	 C_lr_3:0.000029	 C_lr_4:0.000029
[2022-10-02 19:40:36 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.062666	Loss2: 0.072398	 Dis: 2.902161 Entropy: 4.685986 
[2022-10-02 19:40:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.002815	 E_lr_2:0.002815
[2022-10-02 19:40:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000028	 C_lr_2:0.000028	 C_lr_3:0.000028	 C_lr_4:0.000028
[2022-10-02 19:40:43 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.044303	Loss2: 0.046840	 Dis: 4.969227 Entropy: 5.524336 
[2022-10-02 19:40:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.002737	 E_lr_2:0.002737
[2022-10-02 19:40:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 19:40:49 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.091261	Loss2: 0.108684	 Dis: 2.828953 Entropy: 4.504163 
[2022-10-02 19:40:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.002659	 E_lr_2:0.002659
[2022-10-02 19:40:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 19:40:55 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.059405	Loss2: 0.088496	 Dis: 5.075979 Entropy: 4.548379 
[2022-10-02 19:40:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.002581	 E_lr_2:0.002581
[2022-10-02 19:40:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000026	 C_lr_2:0.000026	 C_lr_3:0.000026	 C_lr_4:0.000026
[2022-10-02 19:41:01 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.053511	Loss2: 0.048658	 Dis: 5.079510 Entropy: 4.811597 
[2022-10-02 19:41:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.002502	 E_lr_2:0.002502
[2022-10-02 19:41:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 19:41:07 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.007187	Loss2: 0.008708	 Dis: 3.738468 Entropy: 4.993671 
[2022-10-02 19:41:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.002423	 E_lr_2:0.002423
[2022-10-02 19:41:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000024	 C_lr_2:0.000024	 C_lr_3:0.000024	 C_lr_4:0.000024
[2022-10-02 19:41:13 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.019068	Loss2: 0.024415	 Dis: 2.338749 Entropy: 5.407494 
[2022-10-02 19:41:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.002345	 E_lr_2:0.002345
[2022-10-02 19:41:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 19:41:19 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.125335	Loss2: 0.132780	 Dis: 4.634321 Entropy: 4.255854 
[2022-10-02 19:41:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.002267	 E_lr_2:0.002267
[2022-10-02 19:41:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 19:41:25 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.038528	Loss2: 0.045303	 Dis: 3.326603 Entropy: 5.299850 
[2022-10-02 19:41:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.002189	 E_lr_2:0.002189
[2022-10-02 19:41:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000022	 C_lr_2:0.000022	 C_lr_3:0.000022	 C_lr_4:0.000022
[2022-10-02 19:41:31 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.010061	Loss2: 0.014690	 Dis: 2.922047 Entropy: 5.620453 
[2022-10-02 19:41:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.002111	 E_lr_2:0.002111
[2022-10-02 19:41:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000021	 C_lr_2:0.000021	 C_lr_3:0.000021	 C_lr_4:0.000021
[2022-10-02 19:41:37 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.139109	Loss2: 0.172316	 Dis: 3.243069 Entropy: 4.426484 
[2022-10-02 19:41:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.002034	 E_lr_2:0.002034
[2022-10-02 19:41:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 19:41:43 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.133216	Loss2: 0.129263	 Dis: 2.190790 Entropy: 4.287986 
[2022-10-02 19:41:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.001957	 E_lr_2:0.001957
[2022-10-02 19:41:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 19:41:49 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.008213	Loss2: 0.007840	 Dis: 3.145445 Entropy: 4.759331 
[2022-10-02 19:41:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.001880	 E_lr_2:0.001880
[2022-10-02 19:41:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000019	 C_lr_2:0.000019	 C_lr_3:0.000019	 C_lr_4:0.000019
[2022-10-02 19:41:55 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.126178	Loss2: 0.123024	 Dis: 2.371601 Entropy: 4.937337 
[2022-10-02 19:41:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.001804	 E_lr_2:0.001804
[2022-10-02 19:41:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000018	 C_lr_2:0.000018	 C_lr_3:0.000018	 C_lr_4:0.000018
[2022-10-02 19:42:01 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.054193	Loss2: 0.060718	 Dis: 1.904339 Entropy: 4.294089 
[2022-10-02 19:42:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.001729	 E_lr_2:0.001729
[2022-10-02 19:42:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 19:42:08 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.133967	Loss2: 0.151423	 Dis: 2.277166 Entropy: 5.231924 
[2022-10-02 19:42:08 demo] (houston_program2.py 512): INFO E_lr_1: 0.001655	 E_lr_2:0.001655
[2022-10-02 19:42:08 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 19:42:14 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.029449	Loss2: 0.030750	 Dis: 2.022036 Entropy: 4.979540 
[2022-10-02 19:42:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.001582	 E_lr_2:0.001582
[2022-10-02 19:42:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000016	 C_lr_2:0.000016	 C_lr_3:0.000016	 C_lr_4:0.000016
[2022-10-02 19:42:20 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.031094	Loss2: 0.030539	 Dis: 3.180923 Entropy: 4.334931 
[2022-10-02 19:42:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.001509	 E_lr_2:0.001509
[2022-10-02 19:42:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 19:42:25 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.011202	Loss2: 0.009684	 Dis: 2.116066 Entropy: 4.692317 
[2022-10-02 19:42:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.001437	 E_lr_2:0.001437
[2022-10-02 19:42:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 19:42:32 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.022230	Loss2: 0.027552	 Dis: 3.235369 Entropy: 4.177468 
[2022-10-02 19:42:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.001367	 E_lr_2:0.001367
[2022-10-02 19:42:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 19:42:38 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.039970	Loss2: 0.033589	 Dis: 2.461372 Entropy: 4.875897 
[2022-10-02 19:42:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.001297	 E_lr_2:0.001297
[2022-10-02 19:42:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000013	 C_lr_2:0.000013	 C_lr_3:0.000013	 C_lr_4:0.000013
[2022-10-02 19:42:44 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.007246	Loss2: 0.015343	 Dis: 1.854149 Entropy: 5.222471 
[2022-10-02 19:42:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.001229	 E_lr_2:0.001229
[2022-10-02 19:42:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 19:42:50 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.005405	Loss2: 0.006719	 Dis: 3.421064 Entropy: 5.058724 
[2022-10-02 19:42:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.001162	 E_lr_2:0.001162
[2022-10-02 19:42:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 19:42:56 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.021041	Loss2: 0.027114	 Dis: 1.205650 Entropy: 5.790506 
[2022-10-02 19:42:56 demo] (houston_program2.py 512): INFO E_lr_1: 0.001096	 E_lr_2:0.001096
[2022-10-02 19:42:56 demo] (houston_program2.py 513): INFO C_lr_1: 0.000011	 C_lr_2:0.000011	 C_lr_3:0.000011	 C_lr_4:0.000011
[2022-10-02 19:43:02 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.006594	Loss2: 0.005435	 Dis: 1.243820 Entropy: 4.638258 
[2022-10-02 19:43:02 demo] (houston_program2.py 512): INFO E_lr_1: 0.001032	 E_lr_2:0.001032
[2022-10-02 19:43:02 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 19:43:07 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.033245	Loss2: 0.035706	 Dis: 2.363968 Entropy: 5.395925 
[2022-10-02 19:43:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.000969	 E_lr_2:0.000969
[2022-10-02 19:43:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 19:43:13 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.004186	Loss2: 0.003872	 Dis: 1.518715 Entropy: 4.216585 
[2022-10-02 19:43:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.000908	 E_lr_2:0.000908
[2022-10-02 19:43:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000009	 C_lr_2:0.000009	 C_lr_3:0.000009	 C_lr_4:0.000009
[2022-10-02 19:43:19 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.034437	Loss2: 0.046644	 Dis: 3.110548 Entropy: 5.165679 
[2022-10-02 19:43:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.000848	 E_lr_2:0.000848
[2022-10-02 19:43:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 19:43:26 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.025156	Loss2: 0.029684	 Dis: 3.630838 Entropy: 5.384492 
[2022-10-02 19:43:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.000790	 E_lr_2:0.000790
[2022-10-02 19:43:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 19:43:32 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.012386	Loss2: 0.008652	 Dis: 2.814877 Entropy: 4.880097 
[2022-10-02 19:43:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.000734	 E_lr_2:0.000734
[2022-10-02 19:43:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 19:43:39 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.005915	Loss2: 0.010363	 Dis: 1.979568 Entropy: 4.751021 
[2022-10-02 19:43:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.000679	 E_lr_2:0.000679
[2022-10-02 19:43:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 19:43:45 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.002752	Loss2: 0.002891	 Dis: 3.741369 Entropy: 4.863288 
[2022-10-02 19:43:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.000626	 E_lr_2:0.000626
[2022-10-02 19:43:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 19:43:51 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.005535	Loss2: 0.005506	 Dis: 2.534622 Entropy: 4.245894 
[2022-10-02 19:43:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.000575	 E_lr_2:0.000575
[2022-10-02 19:43:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 19:43:57 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.003070	Loss2: 0.003733	 Dis: 2.345951 Entropy: 4.104071 
[2022-10-02 19:43:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.000526	 E_lr_2:0.000526
[2022-10-02 19:43:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 19:44:03 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.005712	Loss2: 0.005373	 Dis: 1.745415 Entropy: 5.956353 
[2022-10-02 19:44:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.000479	 E_lr_2:0.000479
[2022-10-02 19:44:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 19:44:09 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.002734	Loss2: 0.002857	 Dis: 0.901299 Entropy: 4.634603 
[2022-10-02 19:44:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.000433	 E_lr_2:0.000433
[2022-10-02 19:44:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 19:44:16 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.004153	Loss2: 0.005370	 Dis: 0.971268 Entropy: 4.578110 
[2022-10-02 19:44:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.000390	 E_lr_2:0.000390
[2022-10-02 19:44:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 19:44:22 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.003152	Loss2: 0.001871	 Dis: 1.046421 Entropy: 6.730049 
[2022-10-02 19:44:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.000349	 E_lr_2:0.000349
[2022-10-02 19:44:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 19:44:27 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.003318	Loss2: 0.003788	 Dis: 1.691641 Entropy: 5.442321 
[2022-10-02 19:44:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.000310	 E_lr_2:0.000310
[2022-10-02 19:44:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 19:44:33 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.003840	Loss2: 0.003638	 Dis: 0.443230 Entropy: 5.248156 
[2022-10-02 19:44:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.000273	 E_lr_2:0.000273
[2022-10-02 19:44:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 19:44:39 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.007650	Loss2: 0.014939	 Dis: 2.086830 Entropy: 4.673112 
[2022-10-02 19:44:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.000239	 E_lr_2:0.000239
[2022-10-02 19:44:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 19:44:46 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.005874	Loss2: 0.006918	 Dis: 1.131367 Entropy: 4.149885 
[2022-10-02 19:44:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.000206	 E_lr_2:0.000206
[2022-10-02 19:44:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 19:44:52 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.004897	Loss2: 0.004082	 Dis: 0.980383 Entropy: 5.793718 
[2022-10-02 19:44:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.000176	 E_lr_2:0.000176
[2022-10-02 19:44:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 19:44:58 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.006162	Loss2: 0.006334	 Dis: 1.700527 Entropy: 4.664661 
[2022-10-02 19:44:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.000148	 E_lr_2:0.000148
[2022-10-02 19:44:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:45:04 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.002322	Loss2: 0.002230	 Dis: 0.653402 Entropy: 5.657907 
[2022-10-02 19:45:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.000123	 E_lr_2:0.000123
[2022-10-02 19:45:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:45:10 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.000990	Loss2: 0.001059	 Dis: 4.115337 Entropy: 4.918387 
[2022-10-02 19:45:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.000100	 E_lr_2:0.000100
[2022-10-02 19:45:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:45:16 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.004277	Loss2: 0.004200	 Dis: 1.110779 Entropy: 4.906087 
[2022-10-02 19:45:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.000079	 E_lr_2:0.000079
[2022-10-02 19:45:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:45:22 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.008373	Loss2: 0.007492	 Dis: 1.921894 Entropy: 4.551060 
[2022-10-02 19:45:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.000061	 E_lr_2:0.000061
[2022-10-02 19:45:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:45:26 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.006032	Loss2: 0.006075	 Dis: 2.121511 Entropy: 4.829788 
[2022-10-02 19:45:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.000045	 E_lr_2:0.000045
[2022-10-02 19:45:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:45:33 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.003414	Loss2: 0.004244	 Dis: 1.040461 Entropy: 4.376727 
[2022-10-02 19:45:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.000031	 E_lr_2:0.000031
[2022-10-02 19:45:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:45:39 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.004070	Loss2: 0.003189	 Dis: 2.535946 Entropy: 5.941657 
[2022-10-02 19:45:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.000020	 E_lr_2:0.000020
[2022-10-02 19:45:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:45:45 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.002201	Loss2: 0.005198	 Dis: 1.546230 Entropy: 4.903908 
[2022-10-02 19:45:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.000011	 E_lr_2:0.000011
[2022-10-02 19:45:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:45:51 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.005280	Loss2: 0.006492	 Dis: 1.928104 Entropy: 5.512358 
[2022-10-02 19:45:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.000005	 E_lr_2:0.000005
[2022-10-02 19:45:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:45:57 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.011791	Loss2: 0.006380	 Dis: 1.227921 Entropy: 6.529931 
[2022-10-02 19:45:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.000001	 E_lr_2:0.000001
[2022-10-02 19:45:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:46:03 demo] (houston_program2.py 504): INFO Train Ep: 80 	Loss1: 0.003319	Loss2: 0.003235	 Dis: 1.674868 Entropy: 5.280271 
[2022-10-02 19:46:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.000000	 E_lr_2:0.000000
[2022-10-02 19:46:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:46:03 demo] (houston_program2.py 515): INFO time_80_epoch:600.957319021225
[2022-10-02 19:46:11 demo] (houston_program2.py 673): INFO 	val_Accuracy: 31823/53200 (59.82%)	
[2022-10-02 19:46:11 demo] (utils.py 47): INFO outputs/demo/finetune_100/ckpt_epoch_80.pth saving......
[2022-10-02 19:46:11 demo] (utils.py 49): INFO outputs/demo/finetune_100/ckpt_epoch_80.pth saved !!!
[2022-10-02 19:46:11 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0210 (0.0210)	loss 0.0112 (0.0112)	grad_norm 0.0382 (0.0382)	mem 460MB
[2022-10-02 19:46:11 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:11 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0231 (0.0231)	loss 0.0101 (0.0101)	grad_norm 0.0495 (0.0495)	mem 460MB
[2022-10-02 19:46:11 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:12 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0213 (0.0213)	loss 0.0125 (0.0125)	grad_norm 0.0587 (0.0587)	mem 460MB
[2022-10-02 19:46:12 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:12 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0181 (0.0181)	loss 0.0081 (0.0081)	grad_norm 0.0386 (0.0386)	mem 460MB
[2022-10-02 19:46:12 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:12 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0214 (0.0214)	loss 0.0108 (0.0108)	grad_norm 0.0454 (0.0454)	mem 460MB
[2022-10-02 19:46:12 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:12 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0199 (0.0199)	loss 0.0091 (0.0091)	grad_norm 0.0449 (0.0449)	mem 460MB
[2022-10-02 19:46:12 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:12 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0217 (0.0217)	loss 0.0085 (0.0085)	grad_norm 0.0459 (0.0459)	mem 460MB
[2022-10-02 19:46:12 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:13 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0192 (0.0192)	loss 0.0092 (0.0092)	grad_norm 0.0548 (0.0548)	mem 460MB
[2022-10-02 19:46:13 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:13 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0231 (0.0231)	loss 0.0076 (0.0076)	grad_norm 0.0649 (0.0649)	mem 460MB
[2022-10-02 19:46:13 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:13 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0202 (0.0202)	loss 0.0080 (0.0080)	grad_norm 0.0371 (0.0371)	mem 460MB
[2022-10-02 19:46:13 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:13 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0248 (0.0248)	loss 0.0090 (0.0090)	grad_norm 0.0443 (0.0443)	mem 460MB
[2022-10-02 19:46:13 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:13 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0204 (0.0204)	loss 0.0102 (0.0102)	grad_norm 0.0666 (0.0666)	mem 460MB
[2022-10-02 19:46:13 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:13 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0213 (0.0213)	loss 0.0112 (0.0112)	grad_norm 0.0343 (0.0343)	mem 460MB
[2022-10-02 19:46:13 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:14 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0219 (0.0219)	loss 0.0083 (0.0083)	grad_norm 0.0369 (0.0369)	mem 460MB
[2022-10-02 19:46:14 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:14 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0117 (0.0117)	loss 0.0068 (0.0068)	grad_norm 0.0474 (0.0474)	mem 460MB
[2022-10-02 19:46:14 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:14 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0219 (0.0219)	loss 0.0076 (0.0076)	grad_norm 0.0296 (0.0296)	mem 460MB
[2022-10-02 19:46:14 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:14 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0127 (0.0127)	loss 0.0067 (0.0067)	grad_norm 0.0546 (0.0546)	mem 460MB
[2022-10-02 19:46:14 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:14 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0214 (0.0214)	loss 0.0099 (0.0099)	grad_norm 0.0431 (0.0431)	mem 460MB
[2022-10-02 19:46:14 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:14 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0171 (0.0171)	loss 0.0075 (0.0075)	grad_norm 0.0305 (0.0305)	mem 460MB
[2022-10-02 19:46:15 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:15 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0195 (0.0195)	loss 0.0078 (0.0078)	grad_norm 0.0362 (0.0362)	mem 460MB
[2022-10-02 19:46:15 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:15 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0210 (0.0210)	loss 0.0074 (0.0074)	grad_norm 0.0447 (0.0447)	mem 460MB
[2022-10-02 19:46:15 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:15 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0219 (0.0219)	loss 0.0086 (0.0086)	grad_norm 0.0818 (0.0818)	mem 460MB
[2022-10-02 19:46:15 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:15 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0167 (0.0167)	loss 0.0083 (0.0083)	grad_norm 0.0410 (0.0410)	mem 460MB
[2022-10-02 19:46:15 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:15 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0196 (0.0196)	loss 0.0102 (0.0102)	grad_norm 0.0332 (0.0332)	mem 460MB
[2022-10-02 19:46:15 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:16 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0207 (0.0207)	loss 0.0091 (0.0091)	grad_norm 0.0959 (0.0959)	mem 460MB
[2022-10-02 19:46:16 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:16 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0220 (0.0220)	loss 0.0082 (0.0082)	grad_norm 0.0332 (0.0332)	mem 460MB
[2022-10-02 19:46:16 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:16 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0198 (0.0198)	loss 0.0080 (0.0080)	grad_norm 0.0326 (0.0326)	mem 460MB
[2022-10-02 19:46:16 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:16 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0234 (0.0234)	loss 0.0078 (0.0078)	grad_norm 0.0403 (0.0403)	mem 460MB
[2022-10-02 19:46:16 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:16 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0211 (0.0211)	loss 0.0105 (0.0105)	grad_norm 0.0516 (0.0516)	mem 460MB
[2022-10-02 19:46:16 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:16 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0105 (0.0105)	loss 0.0079 (0.0079)	grad_norm 0.0356 (0.0356)	mem 460MB
[2022-10-02 19:46:16 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:17 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0214 (0.0214)	loss 0.0088 (0.0088)	grad_norm 0.0384 (0.0384)	mem 460MB
[2022-10-02 19:46:17 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:17 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0113 (0.0113)	loss 0.0080 (0.0080)	grad_norm 0.0448 (0.0448)	mem 460MB
[2022-10-02 19:46:17 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:17 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0195 (0.0195)	loss 0.0080 (0.0080)	grad_norm 0.0275 (0.0275)	mem 460MB
[2022-10-02 19:46:17 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:17 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0143 (0.0143)	loss 0.0077 (0.0077)	grad_norm 0.0373 (0.0373)	mem 460MB
[2022-10-02 19:46:17 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:17 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0206 (0.0206)	loss 0.0073 (0.0073)	grad_norm 0.0588 (0.0588)	mem 460MB
[2022-10-02 19:46:17 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:18 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0120 (0.0120)	loss 0.0080 (0.0080)	grad_norm 0.0462 (0.0462)	mem 460MB
[2022-10-02 19:46:18 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:18 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0219 (0.0219)	loss 0.0075 (0.0075)	grad_norm 0.0480 (0.0480)	mem 460MB
[2022-10-02 19:46:18 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:18 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0152 (0.0152)	loss 0.0085 (0.0085)	grad_norm 0.0563 (0.0563)	mem 460MB
[2022-10-02 19:46:18 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:18 demo] (houston_program2.py 243): INFO Train: [81/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0201 (0.0201)	loss 0.0108 (0.0108)	grad_norm 0.0515 (0.0515)	mem 460MB
[2022-10-02 19:46:18 demo] (houston_program2.py 252): INFO EPOCH 81 training takes 0:00:00
[2022-10-02 19:46:18 demo] (houston_program2.py 333): INFO Train Ep: 81 	Loss1: 0.088756	Loss2: 0.067070	 Dis: 3.548458 Entropy: 4.784193 
[2022-10-02 19:46:18 demo] (houston_program2.py 335): INFO time_81_epoch:7.421836614608765
[2022-10-02 19:46:19 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0108 (0.0108)	loss 0.0081 (0.0081)	grad_norm 0.0406 (0.0406)	mem 460MB
[2022-10-02 19:46:19 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:19 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0218 (0.0218)	loss 0.0079 (0.0079)	grad_norm 0.0413 (0.0413)	mem 460MB
[2022-10-02 19:46:19 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:19 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0205 (0.0205)	loss 0.0097 (0.0097)	grad_norm 0.0467 (0.0467)	mem 460MB
[2022-10-02 19:46:19 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:19 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0197 (0.0197)	loss 0.0114 (0.0114)	grad_norm 0.0544 (0.0544)	mem 460MB
[2022-10-02 19:46:19 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:19 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0215 (0.0215)	loss 0.0080 (0.0080)	grad_norm 0.0571 (0.0571)	mem 460MB
[2022-10-02 19:46:19 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:20 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0141 (0.0141)	loss 0.0107 (0.0107)	grad_norm 0.0382 (0.0382)	mem 460MB
[2022-10-02 19:46:20 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:20 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0138 (0.0138)	loss 0.0085 (0.0085)	grad_norm 0.0617 (0.0617)	mem 460MB
[2022-10-02 19:46:20 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:20 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0169 (0.0169)	loss 0.0084 (0.0084)	grad_norm 0.0298 (0.0298)	mem 460MB
[2022-10-02 19:46:20 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:20 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0218 (0.0218)	loss 0.0104 (0.0104)	grad_norm 0.0349 (0.0349)	mem 460MB
[2022-10-02 19:46:20 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:20 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0168 (0.0168)	loss 0.0077 (0.0077)	grad_norm 0.0561 (0.0561)	mem 460MB
[2022-10-02 19:46:20 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:20 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0223 (0.0223)	loss 0.0085 (0.0085)	grad_norm 0.0434 (0.0434)	mem 460MB
[2022-10-02 19:46:20 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:21 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0211 (0.0211)	loss 0.0080 (0.0080)	grad_norm 0.0481 (0.0481)	mem 460MB
[2022-10-02 19:46:21 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:21 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0219 (0.0219)	loss 0.0088 (0.0088)	grad_norm 0.0380 (0.0380)	mem 460MB
[2022-10-02 19:46:21 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:21 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0240 (0.0240)	loss 0.0090 (0.0090)	grad_norm 0.0359 (0.0359)	mem 460MB
[2022-10-02 19:46:21 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:21 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0263 (0.0263)	loss 0.0086 (0.0086)	grad_norm 0.0512 (0.0512)	mem 460MB
[2022-10-02 19:46:21 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:21 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0270 (0.0270)	loss 0.0078 (0.0078)	grad_norm 0.0423 (0.0423)	mem 460MB
[2022-10-02 19:46:21 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:22 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0141 (0.0141)	loss 0.0094 (0.0094)	grad_norm 0.0371 (0.0371)	mem 460MB
[2022-10-02 19:46:22 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:22 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0252 (0.0252)	loss 0.0071 (0.0071)	grad_norm 0.0359 (0.0359)	mem 460MB
[2022-10-02 19:46:22 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:22 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0160 (0.0160)	loss 0.0072 (0.0072)	grad_norm 0.0655 (0.0655)	mem 460MB
[2022-10-02 19:46:22 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:22 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0227 (0.0227)	loss 0.0068 (0.0068)	grad_norm 0.0423 (0.0423)	mem 460MB
[2022-10-02 19:46:22 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:22 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0206 (0.0206)	loss 0.0082 (0.0082)	grad_norm 0.0376 (0.0376)	mem 460MB
[2022-10-02 19:46:22 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:22 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0222 (0.0222)	loss 0.0139 (0.0139)	grad_norm 0.0498 (0.0498)	mem 460MB
[2022-10-02 19:46:22 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:23 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0216 (0.0216)	loss 0.0072 (0.0072)	grad_norm 0.0322 (0.0322)	mem 460MB
[2022-10-02 19:46:23 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:23 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0235 (0.0235)	loss 0.0112 (0.0112)	grad_norm 0.0451 (0.0451)	mem 460MB
[2022-10-02 19:46:23 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:23 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0259 (0.0259)	loss 0.0076 (0.0076)	grad_norm 0.0300 (0.0300)	mem 460MB
[2022-10-02 19:46:23 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:23 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0217 (0.0217)	loss 0.0082 (0.0082)	grad_norm 0.0390 (0.0390)	mem 460MB
[2022-10-02 19:46:23 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:23 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0271 (0.0271)	loss 0.0088 (0.0088)	grad_norm 0.0341 (0.0341)	mem 460MB
[2022-10-02 19:46:23 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:24 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0156 (0.0156)	loss 0.0078 (0.0078)	grad_norm 0.0591 (0.0591)	mem 460MB
[2022-10-02 19:46:24 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:24 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0241 (0.0241)	loss 0.0075 (0.0075)	grad_norm 0.0446 (0.0446)	mem 460MB
[2022-10-02 19:46:24 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:24 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0209 (0.0209)	loss 0.0076 (0.0076)	grad_norm 0.0399 (0.0399)	mem 460MB
[2022-10-02 19:46:24 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:24 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0217 (0.0217)	loss 0.0078 (0.0078)	grad_norm 0.0360 (0.0360)	mem 460MB
[2022-10-02 19:46:24 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:24 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0240 (0.0240)	loss 0.0089 (0.0089)	grad_norm 0.0603 (0.0603)	mem 460MB
[2022-10-02 19:46:24 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:25 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0109 (0.0109)	loss 0.0075 (0.0075)	grad_norm 0.0274 (0.0274)	mem 460MB
[2022-10-02 19:46:25 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:25 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0180 (0.0180)	loss 0.0087 (0.0087)	grad_norm 0.0492 (0.0492)	mem 460MB
[2022-10-02 19:46:25 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:25 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0101 (0.0101)	loss 0.0099 (0.0099)	grad_norm 0.0516 (0.0516)	mem 460MB
[2022-10-02 19:46:25 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:25 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0213 (0.0213)	loss 0.0101 (0.0101)	grad_norm 0.0492 (0.0492)	mem 460MB
[2022-10-02 19:46:25 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:25 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0132 (0.0132)	loss 0.0090 (0.0090)	grad_norm 0.1174 (0.1174)	mem 460MB
[2022-10-02 19:46:25 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:25 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0121 (0.0121)	loss 0.0098 (0.0098)	grad_norm 0.0381 (0.0381)	mem 460MB
[2022-10-02 19:46:25 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:25 demo] (houston_program2.py 243): INFO Train: [82/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0135 (0.0135)	loss 0.0074 (0.0074)	grad_norm 0.0357 (0.0357)	mem 460MB
[2022-10-02 19:46:25 demo] (houston_program2.py 252): INFO EPOCH 82 training takes 0:00:00
[2022-10-02 19:46:26 demo] (houston_program2.py 333): INFO Train Ep: 82 	Loss1: 0.138260	Loss2: 0.111787	 Dis: 3.597700 Entropy: 4.809737 
[2022-10-02 19:46:26 demo] (houston_program2.py 335): INFO time_82_epoch:7.360090255737305
[2022-10-02 19:46:26 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000004	time 0.0120 (0.0120)	loss 0.0077 (0.0077)	grad_norm 0.0373 (0.0373)	mem 460MB
[2022-10-02 19:46:26 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:26 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0219 (0.0219)	loss 0.0087 (0.0087)	grad_norm 0.0344 (0.0344)	mem 460MB
[2022-10-02 19:46:26 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:26 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0212 (0.0212)	loss 0.0101 (0.0101)	grad_norm 0.0524 (0.0524)	mem 460MB
[2022-10-02 19:46:26 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:27 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0105 (0.0105)	loss 0.0113 (0.0113)	grad_norm 0.0471 (0.0471)	mem 460MB
[2022-10-02 19:46:27 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:27 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0217 (0.0217)	loss 0.0127 (0.0127)	grad_norm 0.0530 (0.0530)	mem 460MB
[2022-10-02 19:46:27 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:27 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0110 (0.0110)	loss 0.0094 (0.0094)	grad_norm 0.0368 (0.0368)	mem 460MB
[2022-10-02 19:46:27 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:27 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0217 (0.0217)	loss 0.0084 (0.0084)	grad_norm 0.0341 (0.0341)	mem 460MB
[2022-10-02 19:46:27 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:27 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0113 (0.0113)	loss 0.0080 (0.0080)	grad_norm 0.0489 (0.0489)	mem 460MB
[2022-10-02 19:46:27 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:27 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0214 (0.0214)	loss 0.0099 (0.0099)	grad_norm 0.0409 (0.0409)	mem 460MB
[2022-10-02 19:46:27 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:28 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0174 (0.0174)	loss 0.0082 (0.0082)	grad_norm 0.0388 (0.0388)	mem 460MB
[2022-10-02 19:46:28 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:28 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0234 (0.0234)	loss 0.0115 (0.0115)	grad_norm 0.0553 (0.0553)	mem 460MB
[2022-10-02 19:46:28 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:28 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0210 (0.0210)	loss 0.0074 (0.0074)	grad_norm 0.0340 (0.0340)	mem 460MB
[2022-10-02 19:46:28 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:28 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0231 (0.0231)	loss 0.0072 (0.0072)	grad_norm 0.0245 (0.0245)	mem 460MB
[2022-10-02 19:46:28 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:28 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0201 (0.0201)	loss 0.0080 (0.0080)	grad_norm 0.0310 (0.0310)	mem 460MB
[2022-10-02 19:46:28 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:29 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0248 (0.0248)	loss 0.0077 (0.0077)	grad_norm 0.0354 (0.0354)	mem 460MB
[2022-10-02 19:46:29 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:29 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0266 (0.0266)	loss 0.0132 (0.0132)	grad_norm 0.0847 (0.0847)	mem 460MB
[2022-10-02 19:46:29 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:29 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0220 (0.0220)	loss 0.0128 (0.0128)	grad_norm 0.0511 (0.0511)	mem 460MB
[2022-10-02 19:46:29 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:29 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0250 (0.0250)	loss 0.0070 (0.0070)	grad_norm 0.0217 (0.0217)	mem 460MB
[2022-10-02 19:46:29 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:29 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0275 (0.0275)	loss 0.0077 (0.0077)	grad_norm 0.0273 (0.0273)	mem 460MB
[2022-10-02 19:46:29 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:30 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0158 (0.0158)	loss 0.0076 (0.0076)	grad_norm 0.0357 (0.0357)	mem 460MB
[2022-10-02 19:46:30 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:30 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0205 (0.0205)	loss 0.0066 (0.0066)	grad_norm 0.0285 (0.0285)	mem 460MB
[2022-10-02 19:46:30 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:30 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0203 (0.0203)	loss 0.0100 (0.0100)	grad_norm 0.0791 (0.0791)	mem 460MB
[2022-10-02 19:46:30 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:30 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0113 (0.0113)	loss 0.0080 (0.0080)	grad_norm 0.0320 (0.0320)	mem 460MB
[2022-10-02 19:46:30 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:30 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0205 (0.0205)	loss 0.0073 (0.0073)	grad_norm 0.0299 (0.0299)	mem 460MB
[2022-10-02 19:46:30 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:30 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0100 (0.0100)	loss 0.0073 (0.0073)	grad_norm 0.0290 (0.0290)	mem 460MB
[2022-10-02 19:46:30 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:31 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0205 (0.0205)	loss 0.0097 (0.0097)	grad_norm 0.0630 (0.0630)	mem 460MB
[2022-10-02 19:46:31 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:31 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0144 (0.0144)	loss 0.0078 (0.0078)	grad_norm 0.0590 (0.0590)	mem 460MB
[2022-10-02 19:46:31 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:31 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0220 (0.0220)	loss 0.0076 (0.0076)	grad_norm 0.0352 (0.0352)	mem 460MB
[2022-10-02 19:46:31 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:31 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0155 (0.0155)	loss 0.0110 (0.0110)	grad_norm 0.0370 (0.0370)	mem 460MB
[2022-10-02 19:46:31 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:31 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0205 (0.0205)	loss 0.0090 (0.0090)	grad_norm 0.0475 (0.0475)	mem 460MB
[2022-10-02 19:46:31 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:32 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0208 (0.0208)	loss 0.0078 (0.0078)	grad_norm 0.0321 (0.0321)	mem 460MB
[2022-10-02 19:46:32 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:32 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0223 (0.0223)	loss 0.0082 (0.0082)	grad_norm 0.0533 (0.0533)	mem 460MB
[2022-10-02 19:46:32 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:32 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0221 (0.0221)	loss 0.0096 (0.0096)	grad_norm 0.0387 (0.0387)	mem 460MB
[2022-10-02 19:46:32 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:32 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0237 (0.0237)	loss 0.0105 (0.0105)	grad_norm 0.0364 (0.0364)	mem 460MB
[2022-10-02 19:46:32 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:32 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0209 (0.0209)	loss 0.0079 (0.0079)	grad_norm 0.0271 (0.0271)	mem 460MB
[2022-10-02 19:46:32 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:32 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0133 (0.0133)	loss 0.0085 (0.0085)	grad_norm 0.0358 (0.0358)	mem 460MB
[2022-10-02 19:46:32 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:33 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0149 (0.0149)	loss 0.0073 (0.0073)	grad_norm 0.0443 (0.0443)	mem 460MB
[2022-10-02 19:46:33 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:33 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0211 (0.0211)	loss 0.0069 (0.0069)	grad_norm 0.0464 (0.0464)	mem 460MB
[2022-10-02 19:46:33 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:33 demo] (houston_program2.py 243): INFO Train: [83/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0214 (0.0214)	loss 0.0074 (0.0074)	grad_norm 0.0667 (0.0667)	mem 460MB
[2022-10-02 19:46:33 demo] (houston_program2.py 252): INFO EPOCH 83 training takes 0:00:00
[2022-10-02 19:46:33 demo] (houston_program2.py 333): INFO Train Ep: 83 	Loss1: 0.102908	Loss2: 0.110257	 Dis: 2.990658 Entropy: 4.829154 
[2022-10-02 19:46:33 demo] (houston_program2.py 335): INFO time_83_epoch:7.594728708267212
[2022-10-02 19:46:34 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0113 (0.0113)	loss 0.0109 (0.0109)	grad_norm 0.0311 (0.0311)	mem 460MB
[2022-10-02 19:46:34 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:34 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0205 (0.0205)	loss 0.0080 (0.0080)	grad_norm 0.0321 (0.0321)	mem 460MB
[2022-10-02 19:46:34 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:34 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0194 (0.0194)	loss 0.0083 (0.0083)	grad_norm 0.0399 (0.0399)	mem 460MB
[2022-10-02 19:46:34 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:34 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0207 (0.0207)	loss 0.0130 (0.0130)	grad_norm 0.0506 (0.0506)	mem 460MB
[2022-10-02 19:46:34 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:34 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0216 (0.0216)	loss 0.0087 (0.0087)	grad_norm 0.0301 (0.0301)	mem 460MB
[2022-10-02 19:46:34 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:35 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0158 (0.0158)	loss 0.0101 (0.0101)	grad_norm 0.0415 (0.0415)	mem 460MB
[2022-10-02 19:46:35 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:35 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0218 (0.0218)	loss 0.0086 (0.0086)	grad_norm 0.0316 (0.0316)	mem 460MB
[2022-10-02 19:46:35 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:35 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0207 (0.0207)	loss 0.0108 (0.0108)	grad_norm 0.0521 (0.0521)	mem 460MB
[2022-10-02 19:46:35 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:35 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0200 (0.0200)	loss 0.0080 (0.0080)	grad_norm 0.0342 (0.0342)	mem 460MB
[2022-10-02 19:46:35 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:35 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0204 (0.0204)	loss 0.0084 (0.0084)	grad_norm 0.0356 (0.0356)	mem 460MB
[2022-10-02 19:46:35 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:35 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0224 (0.0224)	loss 0.0086 (0.0086)	grad_norm 0.0353 (0.0353)	mem 460MB
[2022-10-02 19:46:35 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:36 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0182 (0.0182)	loss 0.0133 (0.0133)	grad_norm 0.0377 (0.0377)	mem 460MB
[2022-10-02 19:46:36 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:36 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0199 (0.0199)	loss 0.0091 (0.0091)	grad_norm 0.0374 (0.0374)	mem 460MB
[2022-10-02 19:46:36 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:36 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0230 (0.0230)	loss 0.0082 (0.0082)	grad_norm 0.0374 (0.0374)	mem 460MB
[2022-10-02 19:46:36 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:36 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0234 (0.0234)	loss 0.0101 (0.0101)	grad_norm 0.0335 (0.0335)	mem 460MB
[2022-10-02 19:46:36 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:36 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0216 (0.0216)	loss 0.0107 (0.0107)	grad_norm 0.0569 (0.0569)	mem 460MB
[2022-10-02 19:46:36 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:37 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0134 (0.0134)	loss 0.0130 (0.0130)	grad_norm 0.0741 (0.0741)	mem 460MB
[2022-10-02 19:46:37 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:37 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0249 (0.0249)	loss 0.0081 (0.0081)	grad_norm 0.0302 (0.0302)	mem 460MB
[2022-10-02 19:46:37 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:37 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0185 (0.0185)	loss 0.0082 (0.0082)	grad_norm 0.0388 (0.0388)	mem 460MB
[2022-10-02 19:46:37 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:37 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0208 (0.0208)	loss 0.0101 (0.0101)	grad_norm 0.0364 (0.0364)	mem 460MB
[2022-10-02 19:46:37 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:37 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0219 (0.0219)	loss 0.0063 (0.0063)	grad_norm 0.0303 (0.0303)	mem 460MB
[2022-10-02 19:46:37 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:38 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0197 (0.0197)	loss 0.0104 (0.0104)	grad_norm 0.0557 (0.0557)	mem 460MB
[2022-10-02 19:46:38 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:38 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0188 (0.0188)	loss 0.0076 (0.0076)	grad_norm 0.0397 (0.0397)	mem 460MB
[2022-10-02 19:46:38 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:38 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0248 (0.0248)	loss 0.0098 (0.0098)	grad_norm 0.0412 (0.0412)	mem 460MB
[2022-10-02 19:46:38 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:38 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0215 (0.0215)	loss 0.0071 (0.0071)	grad_norm 0.0449 (0.0449)	mem 460MB
[2022-10-02 19:46:38 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:38 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0149 (0.0149)	loss 0.0086 (0.0086)	grad_norm 0.0321 (0.0321)	mem 460MB
[2022-10-02 19:46:38 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:38 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0232 (0.0232)	loss 0.0078 (0.0078)	grad_norm 0.0332 (0.0332)	mem 460MB
[2022-10-02 19:46:38 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:39 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0148 (0.0148)	loss 0.0093 (0.0093)	grad_norm 0.0591 (0.0591)	mem 460MB
[2022-10-02 19:46:39 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:39 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0185 (0.0185)	loss 0.0098 (0.0098)	grad_norm 0.0613 (0.0613)	mem 460MB
[2022-10-02 19:46:39 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:39 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0157 (0.0157)	loss 0.0055 (0.0055)	grad_norm 0.0533 (0.0533)	mem 460MB
[2022-10-02 19:46:39 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:39 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0264 (0.0264)	loss 0.0076 (0.0076)	grad_norm 0.0260 (0.0260)	mem 460MB
[2022-10-02 19:46:39 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:39 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0216 (0.0216)	loss 0.0080 (0.0080)	grad_norm 0.0398 (0.0398)	mem 460MB
[2022-10-02 19:46:39 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:40 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0210 (0.0210)	loss 0.0107 (0.0107)	grad_norm 0.0434 (0.0434)	mem 460MB
[2022-10-02 19:46:40 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:40 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0207 (0.0207)	loss 0.0070 (0.0070)	grad_norm 0.0342 (0.0342)	mem 460MB
[2022-10-02 19:46:40 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:40 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0269 (0.0269)	loss 0.0095 (0.0095)	grad_norm 0.0406 (0.0406)	mem 460MB
[2022-10-02 19:46:40 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:40 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0210 (0.0210)	loss 0.0077 (0.0077)	grad_norm 0.0360 (0.0360)	mem 460MB
[2022-10-02 19:46:40 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:40 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0152 (0.0152)	loss 0.0105 (0.0105)	grad_norm 0.0513 (0.0513)	mem 460MB
[2022-10-02 19:46:40 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:40 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0216 (0.0216)	loss 0.0097 (0.0097)	grad_norm 0.0578 (0.0578)	mem 460MB
[2022-10-02 19:46:40 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:41 demo] (houston_program2.py 243): INFO Train: [84/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0131 (0.0131)	loss 0.0086 (0.0086)	grad_norm 0.0387 (0.0387)	mem 460MB
[2022-10-02 19:46:41 demo] (houston_program2.py 252): INFO EPOCH 84 training takes 0:00:00
[2022-10-02 19:46:41 demo] (houston_program2.py 333): INFO Train Ep: 84 	Loss1: 0.034094	Loss2: 0.034810	 Dis: 2.125074 Entropy: 5.824072 
[2022-10-02 19:46:41 demo] (houston_program2.py 335): INFO time_84_epoch:7.6231794357299805
[2022-10-02 19:46:41 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0119 (0.0119)	loss 0.0119 (0.0119)	grad_norm 0.1601 (0.1601)	mem 460MB
[2022-10-02 19:46:41 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:41 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0208 (0.0208)	loss 0.0072 (0.0072)	grad_norm 0.0351 (0.0351)	mem 460MB
[2022-10-02 19:46:41 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:42 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0210 (0.0210)	loss 0.0087 (0.0087)	grad_norm 0.0345 (0.0345)	mem 460MB
[2022-10-02 19:46:42 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:42 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0217 (0.0217)	loss 0.0093 (0.0093)	grad_norm 0.0368 (0.0368)	mem 460MB
[2022-10-02 19:46:42 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:42 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0206 (0.0206)	loss 0.0116 (0.0116)	grad_norm 0.0442 (0.0442)	mem 460MB
[2022-10-02 19:46:42 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:42 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0212 (0.0212)	loss 0.0107 (0.0107)	grad_norm 0.0657 (0.0657)	mem 460MB
[2022-10-02 19:46:42 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:42 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0211 (0.0211)	loss 0.0071 (0.0071)	grad_norm 0.0524 (0.0524)	mem 460MB
[2022-10-02 19:46:42 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:43 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0221 (0.0221)	loss 0.0073 (0.0073)	grad_norm 0.0448 (0.0448)	mem 460MB
[2022-10-02 19:46:43 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:43 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0206 (0.0206)	loss 0.0104 (0.0104)	grad_norm 0.0533 (0.0533)	mem 460MB
[2022-10-02 19:46:43 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:43 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0217 (0.0217)	loss 0.0088 (0.0088)	grad_norm 0.0670 (0.0670)	mem 460MB
[2022-10-02 19:46:43 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:43 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0225 (0.0225)	loss 0.0101 (0.0101)	grad_norm 0.0459 (0.0459)	mem 460MB
[2022-10-02 19:46:43 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:43 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0179 (0.0179)	loss 0.0076 (0.0076)	grad_norm 0.0415 (0.0415)	mem 460MB
[2022-10-02 19:46:43 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:43 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0214 (0.0214)	loss 0.0067 (0.0067)	grad_norm 0.0343 (0.0343)	mem 460MB
[2022-10-02 19:46:43 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:44 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0173 (0.0173)	loss 0.0097 (0.0097)	grad_norm 0.0559 (0.0559)	mem 460MB
[2022-10-02 19:46:44 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:44 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0209 (0.0209)	loss 0.0120 (0.0120)	grad_norm 0.0478 (0.0478)	mem 460MB
[2022-10-02 19:46:44 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:44 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0213 (0.0213)	loss 0.0070 (0.0070)	grad_norm 0.0280 (0.0280)	mem 460MB
[2022-10-02 19:46:44 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:44 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0199 (0.0199)	loss 0.0119 (0.0119)	grad_norm 0.0476 (0.0476)	mem 460MB
[2022-10-02 19:46:44 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:44 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0206 (0.0206)	loss 0.0089 (0.0089)	grad_norm 0.0340 (0.0340)	mem 460MB
[2022-10-02 19:46:44 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:45 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0203 (0.0203)	loss 0.0091 (0.0091)	grad_norm 0.0800 (0.0800)	mem 460MB
[2022-10-02 19:46:45 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:45 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0206 (0.0206)	loss 0.0082 (0.0082)	grad_norm 0.0326 (0.0326)	mem 460MB
[2022-10-02 19:46:45 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:45 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0319 (0.0319)	loss 0.0112 (0.0112)	grad_norm 0.0722 (0.0722)	mem 460MB
[2022-10-02 19:46:45 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:45 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0189 (0.0189)	loss 0.0081 (0.0081)	grad_norm 0.0305 (0.0305)	mem 460MB
[2022-10-02 19:46:45 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:45 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0151 (0.0151)	loss 0.0103 (0.0103)	grad_norm 0.0620 (0.0620)	mem 460MB
[2022-10-02 19:46:45 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:45 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0138 (0.0138)	loss 0.0113 (0.0113)	grad_norm 0.0387 (0.0387)	mem 460MB
[2022-10-02 19:46:45 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:46 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0196 (0.0196)	loss 0.0110 (0.0110)	grad_norm 0.0542 (0.0542)	mem 460MB
[2022-10-02 19:46:46 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:46 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0203 (0.0203)	loss 0.0100 (0.0100)	grad_norm 0.0512 (0.0512)	mem 460MB
[2022-10-02 19:46:46 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:46 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0236 (0.0236)	loss 0.0109 (0.0109)	grad_norm 0.0382 (0.0382)	mem 460MB
[2022-10-02 19:46:46 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:46 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000003	time 0.0211 (0.0211)	loss 0.0101 (0.0101)	grad_norm 0.0397 (0.0397)	mem 460MB
[2022-10-02 19:46:46 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:46 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0114 (0.0114)	loss 0.0096 (0.0096)	grad_norm 0.0421 (0.0421)	mem 460MB
[2022-10-02 19:46:46 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:46 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0209 (0.0209)	loss 0.0102 (0.0102)	grad_norm 0.0338 (0.0338)	mem 460MB
[2022-10-02 19:46:46 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:47 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0130 (0.0130)	loss 0.0113 (0.0113)	grad_norm 0.0846 (0.0846)	mem 460MB
[2022-10-02 19:46:47 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:47 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0145 (0.0145)	loss 0.0086 (0.0086)	grad_norm 0.0387 (0.0387)	mem 460MB
[2022-10-02 19:46:47 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:47 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0155 (0.0155)	loss 0.0104 (0.0104)	grad_norm 0.0593 (0.0593)	mem 460MB
[2022-10-02 19:46:47 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:47 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0220 (0.0220)	loss 0.0094 (0.0094)	grad_norm 0.0419 (0.0419)	mem 460MB
[2022-10-02 19:46:47 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:47 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0121 (0.0121)	loss 0.0098 (0.0098)	grad_norm 0.0318 (0.0318)	mem 460MB
[2022-10-02 19:46:47 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:47 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0205 (0.0205)	loss 0.0058 (0.0058)	grad_norm 0.0273 (0.0273)	mem 460MB
[2022-10-02 19:46:47 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:48 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0124 (0.0124)	loss 0.0103 (0.0103)	grad_norm 0.0446 (0.0446)	mem 460MB
[2022-10-02 19:46:48 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:48 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0223 (0.0223)	loss 0.0076 (0.0076)	grad_norm 0.0478 (0.0478)	mem 460MB
[2022-10-02 19:46:48 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:48 demo] (houston_program2.py 243): INFO Train: [85/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0141 (0.0141)	loss 0.0071 (0.0071)	grad_norm 0.0321 (0.0321)	mem 460MB
[2022-10-02 19:46:48 demo] (houston_program2.py 252): INFO EPOCH 85 training takes 0:00:00
[2022-10-02 19:46:48 demo] (houston_program2.py 333): INFO Train Ep: 85 	Loss1: 0.040238	Loss2: 0.033021	 Dis: 3.741589 Entropy: 4.329929 
[2022-10-02 19:46:48 demo] (houston_program2.py 335): INFO time_85_epoch:7.356361627578735
[2022-10-02 19:46:48 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 19:46:48 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 19:46:48 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 19:46:48 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 19:46:48 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 19:46:48 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 19:46:48 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 19:46:48 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 19:46:48 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 19:46:48 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 19:46:54 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.100468	Loss2: 0.110758	 Dis: 3.072443 Entropy: 5.778917 
[2022-10-02 19:46:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.000487	 E_lr_2:0.000487
[2022-10-02 19:46:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 19:47:00 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.028554	Loss2: 0.033468	 Dis: 3.200914 Entropy: 5.116498 
[2022-10-02 19:47:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.000987	 E_lr_2:0.000987
[2022-10-02 19:47:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 19:47:07 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.142643	Loss2: 0.163741	 Dis: 3.661734 Entropy: 4.808591 
[2022-10-02 19:47:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.001487	 E_lr_2:0.001487
[2022-10-02 19:47:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 19:47:13 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.043235	Loss2: 0.034884	 Dis: 4.275269 Entropy: 5.215919 
[2022-10-02 19:47:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.001987	 E_lr_2:0.001987
[2022-10-02 19:47:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 19:47:19 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.117600	Loss2: 0.129493	 Dis: 4.436176 Entropy: 4.774013 
[2022-10-02 19:47:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.002487	 E_lr_2:0.002487
[2022-10-02 19:47:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 19:47:26 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.078814	Loss2: 0.082889	 Dis: 1.977535 Entropy: 5.258137 
[2022-10-02 19:47:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.002987	 E_lr_2:0.002987
[2022-10-02 19:47:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 19:47:32 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.100109	Loss2: 0.095261	 Dis: 3.939871 Entropy: 5.387080 
[2022-10-02 19:47:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.003487	 E_lr_2:0.003487
[2022-10-02 19:47:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 19:47:38 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.172001	Loss2: 0.150105	 Dis: 4.156086 Entropy: 5.624422 
[2022-10-02 19:47:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.003987	 E_lr_2:0.003987
[2022-10-02 19:47:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 19:47:44 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.276622	Loss2: 0.289170	 Dis: 5.806864 Entropy: 4.921833 
[2022-10-02 19:47:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.004487	 E_lr_2:0.004487
[2022-10-02 19:47:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 19:47:50 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.227871	Loss2: 0.271685	 Dis: 6.232458 Entropy: 4.532288 
[2022-10-02 19:47:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.004987	 E_lr_2:0.004987
[2022-10-02 19:47:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000050	 C_lr_2:0.000050	 C_lr_3:0.000050	 C_lr_4:0.000050
[2022-10-02 19:47:55 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.044093	Loss2: 0.049975	 Dis: 5.883186 Entropy: 5.094138 
[2022-10-02 19:47:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.004853	 E_lr_2:0.004853
[2022-10-02 19:47:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000049	 C_lr_2:0.000049	 C_lr_3:0.000049	 C_lr_4:0.000049
[2022-10-02 19:48:01 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.088971	Loss2: 0.081416	 Dis: 3.557224 Entropy: 5.381821 
[2022-10-02 19:48:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.004825	 E_lr_2:0.004825
[2022-10-02 19:48:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 19:48:07 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.190253	Loss2: 0.194347	 Dis: 4.238123 Entropy: 4.814035 
[2022-10-02 19:48:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.004795	 E_lr_2:0.004795
[2022-10-02 19:48:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 19:48:13 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.131578	Loss2: 0.100535	 Dis: 7.197210 Entropy: 4.943689 
[2022-10-02 19:48:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.004763	 E_lr_2:0.004763
[2022-10-02 19:48:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 19:48:19 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.048615	Loss2: 0.045288	 Dis: 3.499573 Entropy: 5.551666 
[2022-10-02 19:48:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.004728	 E_lr_2:0.004728
[2022-10-02 19:48:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 19:48:25 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.249263	Loss2: 0.243355	 Dis: 4.004553 Entropy: 4.782794 
[2022-10-02 19:48:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.004692	 E_lr_2:0.004692
[2022-10-02 19:48:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 19:48:31 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.233305	Loss2: 0.221349	 Dis: 3.526716 Entropy: 5.946586 
[2022-10-02 19:48:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.004653	 E_lr_2:0.004653
[2022-10-02 19:48:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 19:48:37 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.075534	Loss2: 0.098554	 Dis: 4.389891 Entropy: 4.671093 
[2022-10-02 19:48:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.004612	 E_lr_2:0.004612
[2022-10-02 19:48:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 19:48:43 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.139201	Loss2: 0.114783	 Dis: 6.108829 Entropy: 4.646511 
[2022-10-02 19:48:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.004569	 E_lr_2:0.004569
[2022-10-02 19:48:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 19:48:48 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.119680	Loss2: 0.115108	 Dis: 4.369263 Entropy: 4.438915 
[2022-10-02 19:48:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.004524	 E_lr_2:0.004524
[2022-10-02 19:48:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 19:48:53 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.109006	Loss2: 0.117448	 Dis: 3.374882 Entropy: 5.567190 
[2022-10-02 19:48:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.004477	 E_lr_2:0.004477
[2022-10-02 19:48:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 19:49:00 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.125020	Loss2: 0.108649	 Dis: 7.364899 Entropy: 4.639638 
[2022-10-02 19:49:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.004428	 E_lr_2:0.004428
[2022-10-02 19:49:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 19:49:05 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.130684	Loss2: 0.132553	 Dis: 5.600225 Entropy: 4.550236 
[2022-10-02 19:49:05 demo] (houston_program2.py 512): INFO E_lr_1: 0.004377	 E_lr_2:0.004377
[2022-10-02 19:49:05 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 19:49:11 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.156551	Loss2: 0.156878	 Dis: 4.031147 Entropy: 5.155063 
[2022-10-02 19:49:11 demo] (houston_program2.py 512): INFO E_lr_1: 0.004324	 E_lr_2:0.004324
[2022-10-02 19:49:11 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 19:49:18 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.024941	Loss2: 0.038530	 Dis: 3.617575 Entropy: 4.729154 
[2022-10-02 19:49:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.004269	 E_lr_2:0.004269
[2022-10-02 19:49:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 19:49:24 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.132831	Loss2: 0.129353	 Dis: 5.123306 Entropy: 4.701869 
[2022-10-02 19:49:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.004213	 E_lr_2:0.004213
[2022-10-02 19:49:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 19:49:30 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.160441	Loss2: 0.111797	 Dis: 2.914717 Entropy: 5.695144 
[2022-10-02 19:49:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.004155	 E_lr_2:0.004155
[2022-10-02 19:49:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 19:49:36 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.562640	Loss2: 0.502886	 Dis: 11.575691 Entropy: 5.104711 
[2022-10-02 19:49:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.004095	 E_lr_2:0.004095
[2022-10-02 19:49:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000041	 C_lr_2:0.000041	 C_lr_3:0.000041	 C_lr_4:0.000041
[2022-10-02 19:49:42 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.283186	Loss2: 0.264450	 Dis: 9.006676 Entropy: 4.579951 
[2022-10-02 19:49:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.004034	 E_lr_2:0.004034
[2022-10-02 19:49:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 19:49:48 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.246412	Loss2: 0.298657	 Dis: 6.125437 Entropy: 5.503005 
[2022-10-02 19:49:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.003971	 E_lr_2:0.003971
[2022-10-02 19:49:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 19:49:54 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.557790	Loss2: 0.538296	 Dis: 4.858288 Entropy: 5.282739 
[2022-10-02 19:49:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.003907	 E_lr_2:0.003907
[2022-10-02 19:49:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000039	 C_lr_2:0.000039	 C_lr_3:0.000039	 C_lr_4:0.000039
[2022-10-02 19:50:00 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.075451	Loss2: 0.068832	 Dis: 4.740604 Entropy: 4.351803 
[2022-10-02 19:50:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.003841	 E_lr_2:0.003841
[2022-10-02 19:50:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 19:50:06 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.348520	Loss2: 0.388164	 Dis: 5.267967 Entropy: 4.800667 
[2022-10-02 19:50:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.003774	 E_lr_2:0.003774
[2022-10-02 19:50:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 19:50:12 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.118233	Loss2: 0.151150	 Dis: 4.040806 Entropy: 6.955883 
[2022-10-02 19:50:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.003706	 E_lr_2:0.003706
[2022-10-02 19:50:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000037	 C_lr_2:0.000037	 C_lr_3:0.000037	 C_lr_4:0.000037
[2022-10-02 19:50:18 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.082996	Loss2: 0.098248	 Dis: 4.775379 Entropy: 4.564246 
[2022-10-02 19:50:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.003637	 E_lr_2:0.003637
[2022-10-02 19:50:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 19:50:24 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.047989	Loss2: 0.043396	 Dis: 2.695990 Entropy: 4.676937 
[2022-10-02 19:50:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.003566	 E_lr_2:0.003566
[2022-10-02 19:50:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 19:50:30 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.145427	Loss2: 0.148818	 Dis: 5.196165 Entropy: 5.813427 
[2022-10-02 19:50:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.003495	 E_lr_2:0.003495
[2022-10-02 19:50:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 19:50:36 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.057515	Loss2: 0.049125	 Dis: 5.050121 Entropy: 5.517361 
[2022-10-02 19:50:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.003422	 E_lr_2:0.003422
[2022-10-02 19:50:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000034	 C_lr_2:0.000034	 C_lr_3:0.000034	 C_lr_4:0.000034
[2022-10-02 19:50:42 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.016023	Loss2: 0.016592	 Dis: 5.424656 Entropy: 4.814875 
[2022-10-02 19:50:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.003349	 E_lr_2:0.003349
[2022-10-02 19:50:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 19:50:48 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.139306	Loss2: 0.153497	 Dis: 3.346344 Entropy: 4.739262 
[2022-10-02 19:50:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.003274	 E_lr_2:0.003274
[2022-10-02 19:50:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 19:50:54 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.367427	Loss2: 0.303907	 Dis: 9.757511 Entropy: 4.441035 
[2022-10-02 19:50:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.003199	 E_lr_2:0.003199
[2022-10-02 19:50:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000032	 C_lr_2:0.000032	 C_lr_3:0.000032	 C_lr_4:0.000032
[2022-10-02 19:51:00 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.200872	Loss2: 0.175126	 Dis: 6.202572 Entropy: 5.728790 
[2022-10-02 19:51:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.003124	 E_lr_2:0.003124
[2022-10-02 19:51:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000031	 C_lr_2:0.000031	 C_lr_3:0.000031	 C_lr_4:0.000031
[2022-10-02 19:51:06 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.077253	Loss2: 0.081149	 Dis: 6.477631 Entropy: 4.737986 
[2022-10-02 19:51:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.003047	 E_lr_2:0.003047
[2022-10-02 19:51:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 19:51:12 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.045146	Loss2: 0.050830	 Dis: 4.294384 Entropy: 5.082330 
[2022-10-02 19:51:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.002970	 E_lr_2:0.002970
[2022-10-02 19:51:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 19:51:18 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.029999	Loss2: 0.025850	 Dis: 3.784683 Entropy: 5.621696 
[2022-10-02 19:51:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.002893	 E_lr_2:0.002893
[2022-10-02 19:51:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000029	 C_lr_2:0.000029	 C_lr_3:0.000029	 C_lr_4:0.000029
[2022-10-02 19:51:24 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.072765	Loss2: 0.079288	 Dis: 2.531693 Entropy: 4.575610 
[2022-10-02 19:51:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.002815	 E_lr_2:0.002815
[2022-10-02 19:51:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000028	 C_lr_2:0.000028	 C_lr_3:0.000028	 C_lr_4:0.000028
[2022-10-02 19:51:30 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.059219	Loss2: 0.052854	 Dis: 5.320057 Entropy: 4.528291 
[2022-10-02 19:51:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.002737	 E_lr_2:0.002737
[2022-10-02 19:51:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 19:51:36 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.064007	Loss2: 0.058288	 Dis: 4.228207 Entropy: 4.504447 
[2022-10-02 19:51:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.002659	 E_lr_2:0.002659
[2022-10-02 19:51:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 19:51:43 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.073320	Loss2: 0.068842	 Dis: 4.034109 Entropy: 5.602982 
[2022-10-02 19:51:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.002581	 E_lr_2:0.002581
[2022-10-02 19:51:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000026	 C_lr_2:0.000026	 C_lr_3:0.000026	 C_lr_4:0.000026
[2022-10-02 19:51:49 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.081535	Loss2: 0.078882	 Dis: 3.299656 Entropy: 4.748988 
[2022-10-02 19:51:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.002502	 E_lr_2:0.002502
[2022-10-02 19:51:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 19:51:55 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.045685	Loss2: 0.040349	 Dis: 3.590145 Entropy: 5.513307 
[2022-10-02 19:51:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.002423	 E_lr_2:0.002423
[2022-10-02 19:51:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000024	 C_lr_2:0.000024	 C_lr_3:0.000024	 C_lr_4:0.000024
[2022-10-02 19:52:01 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.021801	Loss2: 0.021164	 Dis: 2.583229 Entropy: 5.560936 
[2022-10-02 19:52:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.002345	 E_lr_2:0.002345
[2022-10-02 19:52:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 19:52:07 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.030426	Loss2: 0.030118	 Dis: 3.195652 Entropy: 5.173056 
[2022-10-02 19:52:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.002267	 E_lr_2:0.002267
[2022-10-02 19:52:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 19:52:13 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.114785	Loss2: 0.113299	 Dis: 4.164007 Entropy: 5.198277 
[2022-10-02 19:52:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.002189	 E_lr_2:0.002189
[2022-10-02 19:52:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000022	 C_lr_2:0.000022	 C_lr_3:0.000022	 C_lr_4:0.000022
[2022-10-02 19:52:19 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.607610	Loss2: 0.605253	 Dis: 1.697847 Entropy: 5.224733 
[2022-10-02 19:52:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.002111	 E_lr_2:0.002111
[2022-10-02 19:52:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000021	 C_lr_2:0.000021	 C_lr_3:0.000021	 C_lr_4:0.000021
[2022-10-02 19:52:25 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.044468	Loss2: 0.041827	 Dis: 3.752857 Entropy: 5.471656 
[2022-10-02 19:52:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.002034	 E_lr_2:0.002034
[2022-10-02 19:52:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 19:52:32 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.064988	Loss2: 0.052462	 Dis: 4.964882 Entropy: 4.481994 
[2022-10-02 19:52:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.001957	 E_lr_2:0.001957
[2022-10-02 19:52:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 19:52:38 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.031787	Loss2: 0.041766	 Dis: 4.220539 Entropy: 5.143285 
[2022-10-02 19:52:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.001880	 E_lr_2:0.001880
[2022-10-02 19:52:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000019	 C_lr_2:0.000019	 C_lr_3:0.000019	 C_lr_4:0.000019
[2022-10-02 19:52:44 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.029826	Loss2: 0.026879	 Dis: 2.197161 Entropy: 4.568144 
[2022-10-02 19:52:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.001804	 E_lr_2:0.001804
[2022-10-02 19:52:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000018	 C_lr_2:0.000018	 C_lr_3:0.000018	 C_lr_4:0.000018
[2022-10-02 19:52:50 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.027789	Loss2: 0.043532	 Dis: 2.898220 Entropy: 4.579758 
[2022-10-02 19:52:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.001729	 E_lr_2:0.001729
[2022-10-02 19:52:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 19:52:55 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.008371	Loss2: 0.007315	 Dis: 4.813707 Entropy: 5.839872 
[2022-10-02 19:52:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.001655	 E_lr_2:0.001655
[2022-10-02 19:52:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 19:53:01 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.060569	Loss2: 0.075931	 Dis: 1.492273 Entropy: 5.752864 
[2022-10-02 19:53:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.001582	 E_lr_2:0.001582
[2022-10-02 19:53:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000016	 C_lr_2:0.000016	 C_lr_3:0.000016	 C_lr_4:0.000016
[2022-10-02 19:53:07 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.035054	Loss2: 0.042674	 Dis: 2.243526 Entropy: 5.685513 
[2022-10-02 19:53:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.001509	 E_lr_2:0.001509
[2022-10-02 19:53:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 19:53:14 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.076881	Loss2: 0.067075	 Dis: 4.037994 Entropy: 4.520290 
[2022-10-02 19:53:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.001437	 E_lr_2:0.001437
[2022-10-02 19:53:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 19:53:20 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.033206	Loss2: 0.025847	 Dis: 3.488226 Entropy: 4.932652 
[2022-10-02 19:53:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.001367	 E_lr_2:0.001367
[2022-10-02 19:53:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 19:53:26 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.015842	Loss2: 0.014938	 Dis: 4.361204 Entropy: 4.810567 
[2022-10-02 19:53:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.001297	 E_lr_2:0.001297
[2022-10-02 19:53:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000013	 C_lr_2:0.000013	 C_lr_3:0.000013	 C_lr_4:0.000013
[2022-10-02 19:53:32 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.093470	Loss2: 0.063808	 Dis: 3.502598 Entropy: 4.437237 
[2022-10-02 19:53:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.001229	 E_lr_2:0.001229
[2022-10-02 19:53:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 19:53:38 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.013602	Loss2: 0.019036	 Dis: 2.847408 Entropy: 4.722589 
[2022-10-02 19:53:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.001162	 E_lr_2:0.001162
[2022-10-02 19:53:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 19:53:44 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.017053	Loss2: 0.014441	 Dis: 3.989639 Entropy: 4.439603 
[2022-10-02 19:53:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.001096	 E_lr_2:0.001096
[2022-10-02 19:53:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000011	 C_lr_2:0.000011	 C_lr_3:0.000011	 C_lr_4:0.000011
[2022-10-02 19:53:51 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.025448	Loss2: 0.036472	 Dis: 1.274391 Entropy: 4.935225 
[2022-10-02 19:53:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.001032	 E_lr_2:0.001032
[2022-10-02 19:53:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 19:53:57 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.119645	Loss2: 0.116719	 Dis: 2.295162 Entropy: 5.014471 
[2022-10-02 19:53:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.000969	 E_lr_2:0.000969
[2022-10-02 19:53:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 19:54:01 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.040244	Loss2: 0.050787	 Dis: 3.214558 Entropy: 5.312242 
[2022-10-02 19:54:02 demo] (houston_program2.py 512): INFO E_lr_1: 0.000908	 E_lr_2:0.000908
[2022-10-02 19:54:02 demo] (houston_program2.py 513): INFO C_lr_1: 0.000009	 C_lr_2:0.000009	 C_lr_3:0.000009	 C_lr_4:0.000009
[2022-10-02 19:54:07 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.028994	Loss2: 0.030170	 Dis: 2.000328 Entropy: 4.987751 
[2022-10-02 19:54:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.000848	 E_lr_2:0.000848
[2022-10-02 19:54:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 19:54:13 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.011200	Loss2: 0.008619	 Dis: 1.942661 Entropy: 5.494725 
[2022-10-02 19:54:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.000790	 E_lr_2:0.000790
[2022-10-02 19:54:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 19:54:19 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.004301	Loss2: 0.004656	 Dis: 0.823156 Entropy: 4.855043 
[2022-10-02 19:54:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.000734	 E_lr_2:0.000734
[2022-10-02 19:54:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 19:54:26 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.055893	Loss2: 0.049435	 Dis: 2.152369 Entropy: 4.349802 
[2022-10-02 19:54:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.000679	 E_lr_2:0.000679
[2022-10-02 19:54:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 19:54:32 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.084507	Loss2: 0.067788	 Dis: 1.264034 Entropy: 5.426696 
[2022-10-02 19:54:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.000626	 E_lr_2:0.000626
[2022-10-02 19:54:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 19:54:38 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.006058	Loss2: 0.007353	 Dis: 2.185368 Entropy: 5.346259 
[2022-10-02 19:54:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.000575	 E_lr_2:0.000575
[2022-10-02 19:54:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 19:54:44 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.035952	Loss2: 0.039949	 Dis: 1.269964 Entropy: 5.260988 
[2022-10-02 19:54:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.000526	 E_lr_2:0.000526
[2022-10-02 19:54:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 19:54:50 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.002695	Loss2: 0.001964	 Dis: 1.949457 Entropy: 4.963242 
[2022-10-02 19:54:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.000479	 E_lr_2:0.000479
[2022-10-02 19:54:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 19:54:55 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.006691	Loss2: 0.005817	 Dis: 3.398441 Entropy: 4.808229 
[2022-10-02 19:54:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.000433	 E_lr_2:0.000433
[2022-10-02 19:54:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 19:55:01 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.010454	Loss2: 0.011311	 Dis: 3.069992 Entropy: 4.870881 
[2022-10-02 19:55:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.000390	 E_lr_2:0.000390
[2022-10-02 19:55:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 19:55:07 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.008352	Loss2: 0.009177	 Dis: 2.543169 Entropy: 4.658224 
[2022-10-02 19:55:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.000349	 E_lr_2:0.000349
[2022-10-02 19:55:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 19:55:13 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.021912	Loss2: 0.015969	 Dis: 1.477394 Entropy: 4.775153 
[2022-10-02 19:55:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.000310	 E_lr_2:0.000310
[2022-10-02 19:55:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 19:55:20 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.005966	Loss2: 0.005894	 Dis: 1.750599 Entropy: 5.748608 
[2022-10-02 19:55:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.000273	 E_lr_2:0.000273
[2022-10-02 19:55:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 19:55:26 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.004461	Loss2: 0.002349	 Dis: 2.362539 Entropy: 4.881032 
[2022-10-02 19:55:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.000239	 E_lr_2:0.000239
[2022-10-02 19:55:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 19:55:33 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.020893	Loss2: 0.021489	 Dis: 1.076004 Entropy: 4.987471 
[2022-10-02 19:55:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.000206	 E_lr_2:0.000206
[2022-10-02 19:55:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 19:55:40 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.101250	Loss2: 0.087229	 Dis: 3.307930 Entropy: 4.438247 
[2022-10-02 19:55:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.000176	 E_lr_2:0.000176
[2022-10-02 19:55:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 19:55:47 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.004646	Loss2: 0.003472	 Dis: 1.351629 Entropy: 4.515566 
[2022-10-02 19:55:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.000148	 E_lr_2:0.000148
[2022-10-02 19:55:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:55:54 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.003290	Loss2: 0.003834	 Dis: 3.011673 Entropy: 4.845844 
[2022-10-02 19:55:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.000123	 E_lr_2:0.000123
[2022-10-02 19:55:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:56:00 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.004058	Loss2: 0.004265	 Dis: 3.638952 Entropy: 5.216157 
[2022-10-02 19:56:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.000100	 E_lr_2:0.000100
[2022-10-02 19:56:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:56:07 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.013354	Loss2: 0.017738	 Dis: 1.222178 Entropy: 4.762130 
[2022-10-02 19:56:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.000079	 E_lr_2:0.000079
[2022-10-02 19:56:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:56:14 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.009173	Loss2: 0.007615	 Dis: 0.458195 Entropy: 5.982041 
[2022-10-02 19:56:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.000061	 E_lr_2:0.000061
[2022-10-02 19:56:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 19:56:20 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.010128	Loss2: 0.010363	 Dis: 1.922161 Entropy: 4.875354 
[2022-10-02 19:56:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.000045	 E_lr_2:0.000045
[2022-10-02 19:56:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:56:27 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.004668	Loss2: 0.003203	 Dis: 1.012152 Entropy: 4.919048 
[2022-10-02 19:56:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.000031	 E_lr_2:0.000031
[2022-10-02 19:56:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:56:34 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.004311	Loss2: 0.003156	 Dis: 1.392157 Entropy: 5.100727 
[2022-10-02 19:56:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.000020	 E_lr_2:0.000020
[2022-10-02 19:56:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:56:40 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.003859	Loss2: 0.004281	 Dis: 1.523098 Entropy: 4.498363 
[2022-10-02 19:56:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.000011	 E_lr_2:0.000011
[2022-10-02 19:56:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:56:47 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.003803	Loss2: 0.003372	 Dis: 1.109119 Entropy: 4.418513 
[2022-10-02 19:56:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.000005	 E_lr_2:0.000005
[2022-10-02 19:56:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:56:54 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.002158	Loss2: 0.001963	 Dis: 2.275059 Entropy: 4.916925 
[2022-10-02 19:56:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.000001	 E_lr_2:0.000001
[2022-10-02 19:56:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:57:01 demo] (houston_program2.py 504): INFO Train Ep: 85 	Loss1: 0.002401	Loss2: 0.002050	 Dis: 1.621723 Entropy: 4.411956 
[2022-10-02 19:57:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.000000	 E_lr_2:0.000000
[2022-10-02 19:57:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 19:57:01 demo] (houston_program2.py 515): INFO time_85_epoch:612.3636703491211
[2022-10-02 19:57:08 demo] (houston_program2.py 673): INFO 	val_Accuracy: 32221/53200 (60.57%)	
[2022-10-02 19:57:08 demo] (utils.py 47): INFO outputs/demo/finetune_100/ckpt_epoch_85.pth saving......
[2022-10-02 19:57:08 demo] (utils.py 49): INFO outputs/demo/finetune_100/ckpt_epoch_85.pth saved !!!
[2022-10-02 19:57:09 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0118 (0.0118)	loss 0.0099 (0.0099)	grad_norm 0.0381 (0.0381)	mem 460MB
[2022-10-02 19:57:09 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:09 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0214 (0.0214)	loss 0.0103 (0.0103)	grad_norm 0.0367 (0.0367)	mem 460MB
[2022-10-02 19:57:09 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:09 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0255 (0.0255)	loss 0.0097 (0.0097)	grad_norm 0.0420 (0.0420)	mem 460MB
[2022-10-02 19:57:09 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:09 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0237 (0.0237)	loss 0.0084 (0.0084)	grad_norm 0.0336 (0.0336)	mem 460MB
[2022-10-02 19:57:09 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:09 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0235 (0.0235)	loss 0.0081 (0.0081)	grad_norm 0.0494 (0.0494)	mem 460MB
[2022-10-02 19:57:09 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:10 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0238 (0.0238)	loss 0.0097 (0.0097)	grad_norm 0.0359 (0.0359)	mem 460MB
[2022-10-02 19:57:10 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:10 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0258 (0.0258)	loss 0.0079 (0.0079)	grad_norm 0.0424 (0.0424)	mem 460MB
[2022-10-02 19:57:10 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:10 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0267 (0.0267)	loss 0.0083 (0.0083)	grad_norm 0.0531 (0.0531)	mem 460MB
[2022-10-02 19:57:10 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:10 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0244 (0.0244)	loss 0.0078 (0.0078)	grad_norm 0.0422 (0.0422)	mem 460MB
[2022-10-02 19:57:10 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:10 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0252 (0.0252)	loss 0.0089 (0.0089)	grad_norm 0.0442 (0.0442)	mem 460MB
[2022-10-02 19:57:10 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:11 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0274 (0.0274)	loss 0.0094 (0.0094)	grad_norm 0.0571 (0.0571)	mem 460MB
[2022-10-02 19:57:11 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:11 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0256 (0.0256)	loss 0.0099 (0.0099)	grad_norm 0.0399 (0.0399)	mem 460MB
[2022-10-02 19:57:11 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:11 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0259 (0.0259)	loss 0.0086 (0.0086)	grad_norm 0.0324 (0.0324)	mem 460MB
[2022-10-02 19:57:11 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:11 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0264 (0.0264)	loss 0.0070 (0.0070)	grad_norm 0.0463 (0.0463)	mem 460MB
[2022-10-02 19:57:11 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:11 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0241 (0.0241)	loss 0.0095 (0.0095)	grad_norm 0.0426 (0.0426)	mem 460MB
[2022-10-02 19:57:11 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:12 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0239 (0.0239)	loss 0.0074 (0.0074)	grad_norm 0.0498 (0.0498)	mem 460MB
[2022-10-02 19:57:12 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:12 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0235 (0.0235)	loss 0.0071 (0.0071)	grad_norm 0.0458 (0.0458)	mem 460MB
[2022-10-02 19:57:12 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:12 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0254 (0.0254)	loss 0.0094 (0.0094)	grad_norm 0.0288 (0.0288)	mem 460MB
[2022-10-02 19:57:12 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:12 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0267 (0.0267)	loss 0.0089 (0.0089)	grad_norm 0.0357 (0.0357)	mem 460MB
[2022-10-02 19:57:12 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:12 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0283 (0.0283)	loss 0.0063 (0.0063)	grad_norm 0.0333 (0.0333)	mem 460MB
[2022-10-02 19:57:12 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:13 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0265 (0.0265)	loss 0.0087 (0.0087)	grad_norm 0.0309 (0.0309)	mem 460MB
[2022-10-02 19:57:13 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:13 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0267 (0.0267)	loss 0.0077 (0.0077)	grad_norm 0.0258 (0.0258)	mem 460MB
[2022-10-02 19:57:13 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:13 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0248 (0.0248)	loss 0.0063 (0.0063)	grad_norm 0.0357 (0.0357)	mem 460MB
[2022-10-02 19:57:13 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:13 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0222 (0.0222)	loss 0.0085 (0.0085)	grad_norm 0.0623 (0.0623)	mem 460MB
[2022-10-02 19:57:13 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:13 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0226 (0.0226)	loss 0.0077 (0.0077)	grad_norm 0.0322 (0.0322)	mem 460MB
[2022-10-02 19:57:13 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:14 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0211 (0.0211)	loss 0.0093 (0.0093)	grad_norm 0.0363 (0.0363)	mem 460MB
[2022-10-02 19:57:14 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:14 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0202 (0.0202)	loss 0.0106 (0.0106)	grad_norm 0.0377 (0.0377)	mem 460MB
[2022-10-02 19:57:14 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:14 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0208 (0.0208)	loss 0.0112 (0.0112)	grad_norm 0.0471 (0.0471)	mem 460MB
[2022-10-02 19:57:14 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:14 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0205 (0.0205)	loss 0.0077 (0.0077)	grad_norm 0.0323 (0.0323)	mem 460MB
[2022-10-02 19:57:14 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:14 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0219 (0.0219)	loss 0.0091 (0.0091)	grad_norm 0.0538 (0.0538)	mem 460MB
[2022-10-02 19:57:14 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:15 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0220 (0.0220)	loss 0.0080 (0.0080)	grad_norm 0.0318 (0.0318)	mem 460MB
[2022-10-02 19:57:15 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:15 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0212 (0.0212)	loss 0.0117 (0.0117)	grad_norm 0.0439 (0.0439)	mem 460MB
[2022-10-02 19:57:15 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:15 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0205 (0.0205)	loss 0.0076 (0.0076)	grad_norm 0.0317 (0.0317)	mem 460MB
[2022-10-02 19:57:15 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:15 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0212 (0.0212)	loss 0.0093 (0.0093)	grad_norm 0.0401 (0.0401)	mem 460MB
[2022-10-02 19:57:15 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:15 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0222 (0.0222)	loss 0.0076 (0.0076)	grad_norm 0.0492 (0.0492)	mem 460MB
[2022-10-02 19:57:15 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:16 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0223 (0.0223)	loss 0.0093 (0.0093)	grad_norm 0.0677 (0.0677)	mem 460MB
[2022-10-02 19:57:16 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:16 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0204 (0.0204)	loss 0.0126 (0.0126)	grad_norm 0.0346 (0.0346)	mem 460MB
[2022-10-02 19:57:16 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:16 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0208 (0.0208)	loss 0.0080 (0.0080)	grad_norm 0.0345 (0.0345)	mem 460MB
[2022-10-02 19:57:16 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:16 demo] (houston_program2.py 243): INFO Train: [86/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0211 (0.0211)	loss 0.0082 (0.0082)	grad_norm 0.0433 (0.0433)	mem 460MB
[2022-10-02 19:57:16 demo] (houston_program2.py 252): INFO EPOCH 86 training takes 0:00:00
[2022-10-02 19:57:16 demo] (houston_program2.py 333): INFO Train Ep: 86 	Loss1: 0.014367	Loss2: 0.014834	 Dis: 2.608952 Entropy: 4.485716 
[2022-10-02 19:57:16 demo] (houston_program2.py 335): INFO time_86_epoch:8.175349473953247
[2022-10-02 19:57:17 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0113 (0.0113)	loss 0.0069 (0.0069)	grad_norm 0.0324 (0.0324)	mem 460MB
[2022-10-02 19:57:17 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:17 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0215 (0.0215)	loss 0.0073 (0.0073)	grad_norm 0.0330 (0.0330)	mem 460MB
[2022-10-02 19:57:17 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:17 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0214 (0.0214)	loss 0.0069 (0.0069)	grad_norm 0.0265 (0.0265)	mem 460MB
[2022-10-02 19:57:17 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:17 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0191 (0.0191)	loss 0.0069 (0.0069)	grad_norm 0.0315 (0.0315)	mem 460MB
[2022-10-02 19:57:17 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:17 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0221 (0.0221)	loss 0.0098 (0.0098)	grad_norm 0.0419 (0.0419)	mem 460MB
[2022-10-02 19:57:18 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:18 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0222 (0.0222)	loss 0.0070 (0.0070)	grad_norm 0.0374 (0.0374)	mem 460MB
[2022-10-02 19:57:18 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:18 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0215 (0.0215)	loss 0.0072 (0.0072)	grad_norm 0.0300 (0.0300)	mem 460MB
[2022-10-02 19:57:18 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:18 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0201 (0.0201)	loss 0.0061 (0.0061)	grad_norm 0.0371 (0.0371)	mem 460MB
[2022-10-02 19:57:18 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:18 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0217 (0.0217)	loss 0.0092 (0.0092)	grad_norm 0.0715 (0.0715)	mem 460MB
[2022-10-02 19:57:18 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:18 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0209 (0.0209)	loss 0.0113 (0.0113)	grad_norm 0.0409 (0.0409)	mem 460MB
[2022-10-02 19:57:18 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:19 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0206 (0.0206)	loss 0.0088 (0.0088)	grad_norm 0.0665 (0.0665)	mem 460MB
[2022-10-02 19:57:19 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:19 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0218 (0.0218)	loss 0.0092 (0.0092)	grad_norm 0.0374 (0.0374)	mem 460MB
[2022-10-02 19:57:19 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:19 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0203 (0.0203)	loss 0.0084 (0.0084)	grad_norm 0.0442 (0.0442)	mem 460MB
[2022-10-02 19:57:19 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:19 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0227 (0.0227)	loss 0.0092 (0.0092)	grad_norm 0.0355 (0.0355)	mem 460MB
[2022-10-02 19:57:19 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:19 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0220 (0.0220)	loss 0.0115 (0.0115)	grad_norm 0.0378 (0.0378)	mem 460MB
[2022-10-02 19:57:19 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:20 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0218 (0.0218)	loss 0.0080 (0.0080)	grad_norm 0.0323 (0.0323)	mem 460MB
[2022-10-02 19:57:20 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:20 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0219 (0.0219)	loss 0.0082 (0.0082)	grad_norm 0.0288 (0.0288)	mem 460MB
[2022-10-02 19:57:20 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:20 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0205 (0.0205)	loss 0.0091 (0.0091)	grad_norm 0.0487 (0.0487)	mem 460MB
[2022-10-02 19:57:20 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:20 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0219 (0.0219)	loss 0.0080 (0.0080)	grad_norm 0.0331 (0.0331)	mem 460MB
[2022-10-02 19:57:20 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:20 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0216 (0.0216)	loss 0.0080 (0.0080)	grad_norm 0.0368 (0.0368)	mem 460MB
[2022-10-02 19:57:20 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:21 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0218 (0.0218)	loss 0.0060 (0.0060)	grad_norm 0.0308 (0.0308)	mem 460MB
[2022-10-02 19:57:21 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:21 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0218 (0.0218)	loss 0.0077 (0.0077)	grad_norm 0.0274 (0.0274)	mem 460MB
[2022-10-02 19:57:21 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:21 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0217 (0.0217)	loss 0.0070 (0.0070)	grad_norm 0.0321 (0.0321)	mem 460MB
[2022-10-02 19:57:21 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:21 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0220 (0.0220)	loss 0.0111 (0.0111)	grad_norm 0.0363 (0.0363)	mem 460MB
[2022-10-02 19:57:21 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:21 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0210 (0.0210)	loss 0.0065 (0.0065)	grad_norm 0.0313 (0.0313)	mem 460MB
[2022-10-02 19:57:21 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:22 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0198 (0.0198)	loss 0.0083 (0.0083)	grad_norm 0.0396 (0.0396)	mem 460MB
[2022-10-02 19:57:22 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:22 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0215 (0.0215)	loss 0.0076 (0.0076)	grad_norm 0.0362 (0.0362)	mem 460MB
[2022-10-02 19:57:22 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:22 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0221 (0.0221)	loss 0.0078 (0.0078)	grad_norm 0.0298 (0.0298)	mem 460MB
[2022-10-02 19:57:22 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:22 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0219 (0.0219)	loss 0.0072 (0.0072)	grad_norm 0.0505 (0.0505)	mem 460MB
[2022-10-02 19:57:22 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:22 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0183 (0.0183)	loss 0.0092 (0.0092)	grad_norm 0.0374 (0.0374)	mem 460MB
[2022-10-02 19:57:22 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:23 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0216 (0.0216)	loss 0.0080 (0.0080)	grad_norm 0.0318 (0.0318)	mem 460MB
[2022-10-02 19:57:23 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:23 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0217 (0.0217)	loss 0.0106 (0.0106)	grad_norm 0.0459 (0.0459)	mem 460MB
[2022-10-02 19:57:23 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:23 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0202 (0.0202)	loss 0.0079 (0.0079)	grad_norm 0.0626 (0.0626)	mem 460MB
[2022-10-02 19:57:23 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:23 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0207 (0.0207)	loss 0.0083 (0.0083)	grad_norm 0.0483 (0.0483)	mem 460MB
[2022-10-02 19:57:23 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:23 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0195 (0.0195)	loss 0.0091 (0.0091)	grad_norm 0.0477 (0.0477)	mem 460MB
[2022-10-02 19:57:23 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:23 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0207 (0.0207)	loss 0.0072 (0.0072)	grad_norm 0.0363 (0.0363)	mem 460MB
[2022-10-02 19:57:23 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:24 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0206 (0.0206)	loss 0.0064 (0.0064)	grad_norm 0.0307 (0.0307)	mem 460MB
[2022-10-02 19:57:24 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:24 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0203 (0.0203)	loss 0.0071 (0.0071)	grad_norm 0.0326 (0.0326)	mem 460MB
[2022-10-02 19:57:24 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:24 demo] (houston_program2.py 243): INFO Train: [87/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0216 (0.0216)	loss 0.0111 (0.0111)	grad_norm 0.0438 (0.0438)	mem 460MB
[2022-10-02 19:57:24 demo] (houston_program2.py 252): INFO EPOCH 87 training takes 0:00:00
[2022-10-02 19:57:24 demo] (houston_program2.py 333): INFO Train Ep: 87 	Loss1: 0.099178	Loss2: 0.084371	 Dis: 2.835180 Entropy: 6.299773 
[2022-10-02 19:57:24 demo] (houston_program2.py 335): INFO time_87_epoch:7.92043662071228
[2022-10-02 19:57:25 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0114 (0.0114)	loss 0.0083 (0.0083)	grad_norm 0.0345 (0.0345)	mem 460MB
[2022-10-02 19:57:25 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:25 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0212 (0.0212)	loss 0.0110 (0.0110)	grad_norm 0.0305 (0.0305)	mem 460MB
[2022-10-02 19:57:25 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:25 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0220 (0.0220)	loss 0.0079 (0.0079)	grad_norm 0.0399 (0.0399)	mem 460MB
[2022-10-02 19:57:25 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:25 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0192 (0.0192)	loss 0.0087 (0.0087)	grad_norm 0.0358 (0.0358)	mem 460MB
[2022-10-02 19:57:25 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:25 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0213 (0.0213)	loss 0.0075 (0.0075)	grad_norm 0.0389 (0.0389)	mem 460MB
[2022-10-02 19:57:25 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:26 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0216 (0.0216)	loss 0.0057 (0.0057)	grad_norm 0.0272 (0.0272)	mem 460MB
[2022-10-02 19:57:26 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:26 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0217 (0.0217)	loss 0.0097 (0.0097)	grad_norm 0.0492 (0.0492)	mem 460MB
[2022-10-02 19:57:26 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:26 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0212 (0.0212)	loss 0.0105 (0.0105)	grad_norm 0.0261 (0.0261)	mem 460MB
[2022-10-02 19:57:26 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:26 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0206 (0.0206)	loss 0.0136 (0.0136)	grad_norm 0.0424 (0.0424)	mem 460MB
[2022-10-02 19:57:26 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:26 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0208 (0.0208)	loss 0.0103 (0.0103)	grad_norm 0.1186 (0.1186)	mem 460MB
[2022-10-02 19:57:26 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:27 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0181 (0.0181)	loss 0.0085 (0.0085)	grad_norm 0.0392 (0.0392)	mem 460MB
[2022-10-02 19:57:27 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:27 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0219 (0.0219)	loss 0.0084 (0.0084)	grad_norm 0.0279 (0.0279)	mem 460MB
[2022-10-02 19:57:27 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:27 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0205 (0.0205)	loss 0.0112 (0.0112)	grad_norm 0.0346 (0.0346)	mem 460MB
[2022-10-02 19:57:27 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:27 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0204 (0.0204)	loss 0.0102 (0.0102)	grad_norm 0.0536 (0.0536)	mem 460MB
[2022-10-02 19:57:27 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:27 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0221 (0.0221)	loss 0.0094 (0.0094)	grad_norm 0.0683 (0.0683)	mem 460MB
[2022-10-02 19:57:27 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:28 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0217 (0.0217)	loss 0.0068 (0.0068)	grad_norm 0.0285 (0.0285)	mem 460MB
[2022-10-02 19:57:28 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:28 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0209 (0.0209)	loss 0.0087 (0.0087)	grad_norm 0.0338 (0.0338)	mem 460MB
[2022-10-02 19:57:28 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:28 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0218 (0.0218)	loss 0.0131 (0.0131)	grad_norm 0.0327 (0.0327)	mem 460MB
[2022-10-02 19:57:28 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:28 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0217 (0.0217)	loss 0.0077 (0.0077)	grad_norm 0.0328 (0.0328)	mem 460MB
[2022-10-02 19:57:28 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:28 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0217 (0.0217)	loss 0.0064 (0.0064)	grad_norm 0.0332 (0.0332)	mem 460MB
[2022-10-02 19:57:28 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:29 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0198 (0.0198)	loss 0.0093 (0.0093)	grad_norm 0.0510 (0.0510)	mem 460MB
[2022-10-02 19:57:29 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:29 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0211 (0.0211)	loss 0.0082 (0.0082)	grad_norm 0.0361 (0.0361)	mem 460MB
[2022-10-02 19:57:29 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:29 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0201 (0.0201)	loss 0.0091 (0.0091)	grad_norm 0.0486 (0.0486)	mem 460MB
[2022-10-02 19:57:29 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:29 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0206 (0.0206)	loss 0.0087 (0.0087)	grad_norm 0.0273 (0.0273)	mem 460MB
[2022-10-02 19:57:29 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:29 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0206 (0.0206)	loss 0.0078 (0.0078)	grad_norm 0.0275 (0.0275)	mem 460MB
[2022-10-02 19:57:29 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:29 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0200 (0.0200)	loss 0.0081 (0.0081)	grad_norm 0.0480 (0.0480)	mem 460MB
[2022-10-02 19:57:29 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:30 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0197 (0.0197)	loss 0.0087 (0.0087)	grad_norm 0.0317 (0.0317)	mem 460MB
[2022-10-02 19:57:30 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:30 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0207 (0.0207)	loss 0.0086 (0.0086)	grad_norm 0.0557 (0.0557)	mem 460MB
[2022-10-02 19:57:30 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:30 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0200 (0.0200)	loss 0.0087 (0.0087)	grad_norm 0.0312 (0.0312)	mem 460MB
[2022-10-02 19:57:30 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:30 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0179 (0.0179)	loss 0.0070 (0.0070)	grad_norm 0.0366 (0.0366)	mem 460MB
[2022-10-02 19:57:30 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:30 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0204 (0.0204)	loss 0.0115 (0.0115)	grad_norm 0.0406 (0.0406)	mem 460MB
[2022-10-02 19:57:30 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:31 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0205 (0.0205)	loss 0.0097 (0.0097)	grad_norm 0.0568 (0.0568)	mem 460MB
[2022-10-02 19:57:31 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:31 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0203 (0.0203)	loss 0.0073 (0.0073)	grad_norm 0.0314 (0.0314)	mem 460MB
[2022-10-02 19:57:31 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:31 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0220 (0.0220)	loss 0.0135 (0.0135)	grad_norm 0.0450 (0.0450)	mem 460MB
[2022-10-02 19:57:31 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:31 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0218 (0.0218)	loss 0.0090 (0.0090)	grad_norm 0.0326 (0.0326)	mem 460MB
[2022-10-02 19:57:31 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:31 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0219 (0.0219)	loss 0.0084 (0.0084)	grad_norm 0.0357 (0.0357)	mem 460MB
[2022-10-02 19:57:31 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:32 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0214 (0.0214)	loss 0.0075 (0.0075)	grad_norm 0.0343 (0.0343)	mem 460MB
[2022-10-02 19:57:32 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:32 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0205 (0.0205)	loss 0.0075 (0.0075)	grad_norm 0.0263 (0.0263)	mem 460MB
[2022-10-02 19:57:32 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:32 demo] (houston_program2.py 243): INFO Train: [88/100][0/2]	eta 0:00:00 lr 0.000002	time 0.0198 (0.0198)	loss 0.0076 (0.0076)	grad_norm 0.0465 (0.0465)	mem 460MB
[2022-10-02 19:57:32 demo] (houston_program2.py 252): INFO EPOCH 88 training takes 0:00:00
[2022-10-02 19:57:32 demo] (houston_program2.py 333): INFO Train Ep: 88 	Loss1: 0.012716	Loss2: 0.014276	 Dis: 3.117489 Entropy: 6.395006 
[2022-10-02 19:57:32 demo] (houston_program2.py 335): INFO time_88_epoch:7.868247747421265
[2022-10-02 19:57:33 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0113 (0.0113)	loss 0.0068 (0.0068)	grad_norm 0.0525 (0.0525)	mem 460MB
[2022-10-02 19:57:33 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:33 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0217 (0.0217)	loss 0.0085 (0.0085)	grad_norm 0.0307 (0.0307)	mem 460MB
[2022-10-02 19:57:33 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:33 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0190 (0.0190)	loss 0.0108 (0.0108)	grad_norm 0.0330 (0.0330)	mem 460MB
[2022-10-02 19:57:33 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:33 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0192 (0.0192)	loss 0.0102 (0.0102)	grad_norm 0.0461 (0.0461)	mem 460MB
[2022-10-02 19:57:33 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:33 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0219 (0.0219)	loss 0.0118 (0.0118)	grad_norm 0.0379 (0.0379)	mem 460MB
[2022-10-02 19:57:33 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:34 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0209 (0.0209)	loss 0.0103 (0.0103)	grad_norm 0.0725 (0.0725)	mem 460MB
[2022-10-02 19:57:34 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:34 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0212 (0.0212)	loss 0.0100 (0.0100)	grad_norm 0.0377 (0.0377)	mem 460MB
[2022-10-02 19:57:34 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:34 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0218 (0.0218)	loss 0.0077 (0.0077)	grad_norm 0.0375 (0.0375)	mem 460MB
[2022-10-02 19:57:34 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:34 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0202 (0.0202)	loss 0.0081 (0.0081)	grad_norm 0.0339 (0.0339)	mem 460MB
[2022-10-02 19:57:34 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:34 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0216 (0.0216)	loss 0.0077 (0.0077)	grad_norm 0.0294 (0.0294)	mem 460MB
[2022-10-02 19:57:34 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:35 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0183 (0.0183)	loss 0.0066 (0.0066)	grad_norm 0.0330 (0.0330)	mem 460MB
[2022-10-02 19:57:35 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:35 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0220 (0.0220)	loss 0.0130 (0.0130)	grad_norm 0.0405 (0.0405)	mem 460MB
[2022-10-02 19:57:35 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:35 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0199 (0.0199)	loss 0.0089 (0.0089)	grad_norm 0.0427 (0.0427)	mem 460MB
[2022-10-02 19:57:35 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:35 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0219 (0.0219)	loss 0.0090 (0.0090)	grad_norm 0.0320 (0.0320)	mem 460MB
[2022-10-02 19:57:35 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:35 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0202 (0.0202)	loss 0.0129 (0.0129)	grad_norm 0.0406 (0.0406)	mem 460MB
[2022-10-02 19:57:35 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:35 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0219 (0.0219)	loss 0.0110 (0.0110)	grad_norm 0.0350 (0.0350)	mem 460MB
[2022-10-02 19:57:35 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:36 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0218 (0.0218)	loss 0.0071 (0.0071)	grad_norm 0.0531 (0.0531)	mem 460MB
[2022-10-02 19:57:36 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:36 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0213 (0.0213)	loss 0.0071 (0.0071)	grad_norm 0.0302 (0.0302)	mem 460MB
[2022-10-02 19:57:36 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:36 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0205 (0.0205)	loss 0.0080 (0.0080)	grad_norm 0.0315 (0.0315)	mem 460MB
[2022-10-02 19:57:36 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:36 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0217 (0.0217)	loss 0.0065 (0.0065)	grad_norm 0.0308 (0.0308)	mem 460MB
[2022-10-02 19:57:36 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:36 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0206 (0.0206)	loss 0.0066 (0.0066)	grad_norm 0.0341 (0.0341)	mem 460MB
[2022-10-02 19:57:36 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:37 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0215 (0.0215)	loss 0.0072 (0.0072)	grad_norm 0.0285 (0.0285)	mem 460MB
[2022-10-02 19:57:37 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:37 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0205 (0.0205)	loss 0.0085 (0.0085)	grad_norm 0.0281 (0.0281)	mem 460MB
[2022-10-02 19:57:37 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:37 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0210 (0.0210)	loss 0.0108 (0.0108)	grad_norm 0.1667 (0.1667)	mem 460MB
[2022-10-02 19:57:37 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:37 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0213 (0.0213)	loss 0.0095 (0.0095)	grad_norm 0.0480 (0.0480)	mem 460MB
[2022-10-02 19:57:37 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:37 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0208 (0.0208)	loss 0.0104 (0.0104)	grad_norm 0.0900 (0.0900)	mem 460MB
[2022-10-02 19:57:37 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:38 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0217 (0.0217)	loss 0.0097 (0.0097)	grad_norm 0.0301 (0.0301)	mem 460MB
[2022-10-02 19:57:38 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:38 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0208 (0.0208)	loss 0.0083 (0.0083)	grad_norm 0.0374 (0.0374)	mem 460MB
[2022-10-02 19:57:38 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:38 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0220 (0.0220)	loss 0.0090 (0.0090)	grad_norm 0.0398 (0.0398)	mem 460MB
[2022-10-02 19:57:38 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:38 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0213 (0.0213)	loss 0.0092 (0.0092)	grad_norm 0.0409 (0.0409)	mem 460MB
[2022-10-02 19:57:38 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:38 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0217 (0.0217)	loss 0.0093 (0.0093)	grad_norm 0.0385 (0.0385)	mem 460MB
[2022-10-02 19:57:38 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:39 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0207 (0.0207)	loss 0.0093 (0.0093)	grad_norm 0.0358 (0.0358)	mem 460MB
[2022-10-02 19:57:39 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:39 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0217 (0.0217)	loss 0.0075 (0.0075)	grad_norm 0.0350 (0.0350)	mem 460MB
[2022-10-02 19:57:39 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:39 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0208 (0.0208)	loss 0.0106 (0.0106)	grad_norm 0.0436 (0.0436)	mem 460MB
[2022-10-02 19:57:39 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:39 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0204 (0.0204)	loss 0.0080 (0.0080)	grad_norm 0.0362 (0.0362)	mem 460MB
[2022-10-02 19:57:39 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:39 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0214 (0.0214)	loss 0.0081 (0.0081)	grad_norm 0.0330 (0.0330)	mem 460MB
[2022-10-02 19:57:39 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:40 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0203 (0.0203)	loss 0.0072 (0.0072)	grad_norm 0.0380 (0.0380)	mem 460MB
[2022-10-02 19:57:40 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:40 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0212 (0.0212)	loss 0.0090 (0.0090)	grad_norm 0.0376 (0.0376)	mem 460MB
[2022-10-02 19:57:40 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:40 demo] (houston_program2.py 243): INFO Train: [89/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0229 (0.0229)	loss 0.0114 (0.0114)	grad_norm 0.0443 (0.0443)	mem 460MB
[2022-10-02 19:57:40 demo] (houston_program2.py 252): INFO EPOCH 89 training takes 0:00:00
[2022-10-02 19:57:40 demo] (houston_program2.py 333): INFO Train Ep: 89 	Loss1: 0.012174	Loss2: 0.013198	 Dis: 3.472063 Entropy: 5.741257 
[2022-10-02 19:57:40 demo] (houston_program2.py 335): INFO time_89_epoch:7.98199987411499
[2022-10-02 19:57:41 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0129 (0.0129)	loss 0.0106 (0.0106)	grad_norm 0.0757 (0.0757)	mem 460MB
[2022-10-02 19:57:41 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:41 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0202 (0.0202)	loss 0.0080 (0.0080)	grad_norm 0.0270 (0.0270)	mem 460MB
[2022-10-02 19:57:41 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:41 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0186 (0.0186)	loss 0.0069 (0.0069)	grad_norm 0.0304 (0.0304)	mem 460MB
[2022-10-02 19:57:41 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:41 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0184 (0.0184)	loss 0.0103 (0.0103)	grad_norm 0.0411 (0.0411)	mem 460MB
[2022-10-02 19:57:41 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:41 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0215 (0.0215)	loss 0.0102 (0.0102)	grad_norm 0.0292 (0.0292)	mem 460MB
[2022-10-02 19:57:41 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:41 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0219 (0.0219)	loss 0.0098 (0.0098)	grad_norm 0.0420 (0.0420)	mem 460MB
[2022-10-02 19:57:42 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:42 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0216 (0.0216)	loss 0.0085 (0.0085)	grad_norm 0.0380 (0.0380)	mem 460MB
[2022-10-02 19:57:42 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:42 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0218 (0.0218)	loss 0.0090 (0.0090)	grad_norm 0.0386 (0.0386)	mem 460MB
[2022-10-02 19:57:42 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:42 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0208 (0.0208)	loss 0.0088 (0.0088)	grad_norm 0.0362 (0.0362)	mem 460MB
[2022-10-02 19:57:42 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:42 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0206 (0.0206)	loss 0.0090 (0.0090)	grad_norm 0.0488 (0.0488)	mem 460MB
[2022-10-02 19:57:42 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:42 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0186 (0.0186)	loss 0.0123 (0.0123)	grad_norm 0.0353 (0.0353)	mem 460MB
[2022-10-02 19:57:42 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:43 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0221 (0.0221)	loss 0.0141 (0.0141)	grad_norm 0.0536 (0.0536)	mem 460MB
[2022-10-02 19:57:43 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:43 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0212 (0.0212)	loss 0.0081 (0.0081)	grad_norm 0.0358 (0.0358)	mem 460MB
[2022-10-02 19:57:43 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:43 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0205 (0.0205)	loss 0.0094 (0.0094)	grad_norm 0.0379 (0.0379)	mem 460MB
[2022-10-02 19:57:43 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:43 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0221 (0.0221)	loss 0.0096 (0.0096)	grad_norm 0.0395 (0.0395)	mem 460MB
[2022-10-02 19:57:43 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:43 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0220 (0.0220)	loss 0.0082 (0.0082)	grad_norm 0.0409 (0.0409)	mem 460MB
[2022-10-02 19:57:43 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:44 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0219 (0.0219)	loss 0.0085 (0.0085)	grad_norm 0.0322 (0.0322)	mem 460MB
[2022-10-02 19:57:44 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:44 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0209 (0.0209)	loss 0.0076 (0.0076)	grad_norm 0.0391 (0.0391)	mem 460MB
[2022-10-02 19:57:44 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:44 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0210 (0.0210)	loss 0.0070 (0.0070)	grad_norm 0.0318 (0.0318)	mem 460MB
[2022-10-02 19:57:44 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:44 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0219 (0.0219)	loss 0.0075 (0.0075)	grad_norm 0.0367 (0.0367)	mem 460MB
[2022-10-02 19:57:44 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:44 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0222 (0.0222)	loss 0.0078 (0.0078)	grad_norm 0.0390 (0.0390)	mem 460MB
[2022-10-02 19:57:44 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:45 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0216 (0.0216)	loss 0.0077 (0.0077)	grad_norm 0.0310 (0.0310)	mem 460MB
[2022-10-02 19:57:45 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:45 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0202 (0.0202)	loss 0.0099 (0.0099)	grad_norm 0.0491 (0.0491)	mem 460MB
[2022-10-02 19:57:45 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:45 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0217 (0.0217)	loss 0.0083 (0.0083)	grad_norm 0.0361 (0.0361)	mem 460MB
[2022-10-02 19:57:45 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:45 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0218 (0.0218)	loss 0.0085 (0.0085)	grad_norm 0.0413 (0.0413)	mem 460MB
[2022-10-02 19:57:45 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:45 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0203 (0.0203)	loss 0.0076 (0.0076)	grad_norm 0.0332 (0.0332)	mem 460MB
[2022-10-02 19:57:45 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:46 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0204 (0.0204)	loss 0.0086 (0.0086)	grad_norm 0.0274 (0.0274)	mem 460MB
[2022-10-02 19:57:46 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:46 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0204 (0.0204)	loss 0.0099 (0.0099)	grad_norm 0.0309 (0.0309)	mem 460MB
[2022-10-02 19:57:46 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:46 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0199 (0.0199)	loss 0.0095 (0.0095)	grad_norm 0.0486 (0.0486)	mem 460MB
[2022-10-02 19:57:46 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:46 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0147 (0.0147)	loss 0.0080 (0.0080)	grad_norm 0.0639 (0.0639)	mem 460MB
[2022-10-02 19:57:46 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:46 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0205 (0.0205)	loss 0.0103 (0.0103)	grad_norm 0.0645 (0.0645)	mem 460MB
[2022-10-02 19:57:46 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:47 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0202 (0.0202)	loss 0.0089 (0.0089)	grad_norm 0.0472 (0.0472)	mem 460MB
[2022-10-02 19:57:47 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:47 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0201 (0.0201)	loss 0.0094 (0.0094)	grad_norm 0.0286 (0.0286)	mem 460MB
[2022-10-02 19:57:47 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:47 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0209 (0.0209)	loss 0.0080 (0.0080)	grad_norm 0.0402 (0.0402)	mem 460MB
[2022-10-02 19:57:47 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:47 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0204 (0.0204)	loss 0.0085 (0.0085)	grad_norm 0.0342 (0.0342)	mem 460MB
[2022-10-02 19:57:47 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:47 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0216 (0.0216)	loss 0.0094 (0.0094)	grad_norm 0.0311 (0.0311)	mem 460MB
[2022-10-02 19:57:47 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:47 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0223 (0.0223)	loss 0.0084 (0.0084)	grad_norm 0.0546 (0.0546)	mem 460MB
[2022-10-02 19:57:47 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:48 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0222 (0.0222)	loss 0.0127 (0.0127)	grad_norm 0.0452 (0.0452)	mem 460MB
[2022-10-02 19:57:48 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:48 demo] (houston_program2.py 243): INFO Train: [90/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0219 (0.0219)	loss 0.0099 (0.0099)	grad_norm 0.0465 (0.0465)	mem 460MB
[2022-10-02 19:57:48 demo] (houston_program2.py 252): INFO EPOCH 90 training takes 0:00:00
[2022-10-02 19:57:48 demo] (houston_program2.py 333): INFO Train Ep: 90 	Loss1: 0.041003	Loss2: 0.041866	 Dis: 2.238472 Entropy: 4.584542 
[2022-10-02 19:57:48 demo] (houston_program2.py 335): INFO time_90_epoch:7.940624475479126
[2022-10-02 19:57:48 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 19:57:48 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 19:57:48 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 19:57:48 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 19:57:48 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 19:57:48 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 19:57:48 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 19:57:48 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 19:57:48 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 19:57:48 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 19:57:55 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.013859	Loss2: 0.013894	 Dis: 3.617001 Entropy: 4.917103 
[2022-10-02 19:57:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.000487	 E_lr_2:0.000487
[2022-10-02 19:57:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 19:58:01 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.027289	Loss2: 0.027488	 Dis: 2.274204 Entropy: 5.484796 
[2022-10-02 19:58:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.000987	 E_lr_2:0.000987
[2022-10-02 19:58:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 19:58:08 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.289299	Loss2: 0.236328	 Dis: 2.588413 Entropy: 5.781106 
[2022-10-02 19:58:08 demo] (houston_program2.py 512): INFO E_lr_1: 0.001487	 E_lr_2:0.001487
[2022-10-02 19:58:08 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 19:58:15 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.076428	Loss2: 0.055308	 Dis: 4.309662 Entropy: 5.019285 
[2022-10-02 19:58:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.001987	 E_lr_2:0.001987
[2022-10-02 19:58:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 19:58:21 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.063506	Loss2: 0.066391	 Dis: 3.722979 Entropy: 4.619624 
[2022-10-02 19:58:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.002487	 E_lr_2:0.002487
[2022-10-02 19:58:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 19:58:28 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.114451	Loss2: 0.149991	 Dis: 4.599562 Entropy: 4.456443 
[2022-10-02 19:58:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.002987	 E_lr_2:0.002987
[2022-10-02 19:58:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 19:58:34 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.130105	Loss2: 0.167009	 Dis: 3.158485 Entropy: 4.536880 
[2022-10-02 19:58:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.003487	 E_lr_2:0.003487
[2022-10-02 19:58:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 19:58:40 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.181813	Loss2: 0.204669	 Dis: 3.783348 Entropy: 5.311504 
[2022-10-02 19:58:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.003987	 E_lr_2:0.003987
[2022-10-02 19:58:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 19:58:47 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.249071	Loss2: 0.237116	 Dis: 7.064074 Entropy: 4.705606 
[2022-10-02 19:58:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.004487	 E_lr_2:0.004487
[2022-10-02 19:58:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 19:58:53 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.227089	Loss2: 0.273789	 Dis: 5.179039 Entropy: 4.417157 
[2022-10-02 19:58:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.004987	 E_lr_2:0.004987
[2022-10-02 19:58:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000050	 C_lr_2:0.000050	 C_lr_3:0.000050	 C_lr_4:0.000050
[2022-10-02 19:59:00 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.018316	Loss2: 0.016382	 Dis: 2.774693 Entropy: 5.520153 
[2022-10-02 19:59:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.004853	 E_lr_2:0.004853
[2022-10-02 19:59:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000049	 C_lr_2:0.000049	 C_lr_3:0.000049	 C_lr_4:0.000049
[2022-10-02 19:59:06 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.101447	Loss2: 0.123060	 Dis: 4.645905 Entropy: 4.551571 
[2022-10-02 19:59:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.004825	 E_lr_2:0.004825
[2022-10-02 19:59:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 19:59:13 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.127785	Loss2: 0.136260	 Dis: 4.237925 Entropy: 5.111044 
[2022-10-02 19:59:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.004795	 E_lr_2:0.004795
[2022-10-02 19:59:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 19:59:19 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.041636	Loss2: 0.040761	 Dis: 3.760824 Entropy: 4.591490 
[2022-10-02 19:59:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.004763	 E_lr_2:0.004763
[2022-10-02 19:59:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 19:59:26 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 1.286051	Loss2: 1.296666	 Dis: 15.738739 Entropy: 4.443805 
[2022-10-02 19:59:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.004728	 E_lr_2:0.004728
[2022-10-02 19:59:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 19:59:33 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.548053	Loss2: 0.536857	 Dis: 10.115234 Entropy: 4.412046 
[2022-10-02 19:59:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.004692	 E_lr_2:0.004692
[2022-10-02 19:59:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 19:59:39 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.263053	Loss2: 0.226203	 Dis: 8.631289 Entropy: 4.867372 
[2022-10-02 19:59:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.004653	 E_lr_2:0.004653
[2022-10-02 19:59:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 19:59:46 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.147190	Loss2: 0.178836	 Dis: 5.369678 Entropy: 4.930312 
[2022-10-02 19:59:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.004612	 E_lr_2:0.004612
[2022-10-02 19:59:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 19:59:53 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.212564	Loss2: 0.261268	 Dis: 7.283613 Entropy: 5.173284 
[2022-10-02 19:59:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.004569	 E_lr_2:0.004569
[2022-10-02 19:59:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 19:59:59 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.183459	Loss2: 0.175545	 Dis: 6.982004 Entropy: 4.755787 
[2022-10-02 19:59:59 demo] (houston_program2.py 512): INFO E_lr_1: 0.004524	 E_lr_2:0.004524
[2022-10-02 19:59:59 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 20:00:06 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.353529	Loss2: 0.353810	 Dis: 4.210197 Entropy: 5.006568 
[2022-10-02 20:00:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.004477	 E_lr_2:0.004477
[2022-10-02 20:00:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 20:00:12 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.210140	Loss2: 0.206775	 Dis: 5.988405 Entropy: 4.348933 
[2022-10-02 20:00:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.004428	 E_lr_2:0.004428
[2022-10-02 20:00:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 20:00:18 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.264389	Loss2: 0.288605	 Dis: 5.562550 Entropy: 4.553305 
[2022-10-02 20:00:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.004377	 E_lr_2:0.004377
[2022-10-02 20:00:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 20:00:25 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.260808	Loss2: 0.226070	 Dis: 4.484629 Entropy: 4.685723 
[2022-10-02 20:00:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.004324	 E_lr_2:0.004324
[2022-10-02 20:00:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 20:00:32 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.060140	Loss2: 0.074758	 Dis: 3.230370 Entropy: 5.623338 
[2022-10-02 20:00:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.004269	 E_lr_2:0.004269
[2022-10-02 20:00:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 20:00:38 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.206589	Loss2: 0.230009	 Dis: 5.188572 Entropy: 4.829215 
[2022-10-02 20:00:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.004213	 E_lr_2:0.004213
[2022-10-02 20:00:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 20:00:45 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.092432	Loss2: 0.071249	 Dis: 3.975021 Entropy: 5.015641 
[2022-10-02 20:00:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.004155	 E_lr_2:0.004155
[2022-10-02 20:00:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 20:00:51 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.042635	Loss2: 0.051171	 Dis: 4.356224 Entropy: 4.564902 
[2022-10-02 20:00:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.004095	 E_lr_2:0.004095
[2022-10-02 20:00:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000041	 C_lr_2:0.000041	 C_lr_3:0.000041	 C_lr_4:0.000041
[2022-10-02 20:00:57 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.127060	Loss2: 0.127423	 Dis: 3.013079 Entropy: 5.484919 
[2022-10-02 20:00:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.004034	 E_lr_2:0.004034
[2022-10-02 20:00:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 20:01:04 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.342140	Loss2: 0.330944	 Dis: 5.273743 Entropy: 5.158750 
[2022-10-02 20:01:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.003971	 E_lr_2:0.003971
[2022-10-02 20:01:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 20:01:10 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.023809	Loss2: 0.025803	 Dis: 3.669121 Entropy: 5.099113 
[2022-10-02 20:01:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.003907	 E_lr_2:0.003907
[2022-10-02 20:01:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000039	 C_lr_2:0.000039	 C_lr_3:0.000039	 C_lr_4:0.000039
[2022-10-02 20:01:17 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.039885	Loss2: 0.037605	 Dis: 4.906958 Entropy: 4.749809 
[2022-10-02 20:01:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.003841	 E_lr_2:0.003841
[2022-10-02 20:01:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 20:01:23 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.135626	Loss2: 0.095872	 Dis: 6.187582 Entropy: 4.547973 
[2022-10-02 20:01:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.003774	 E_lr_2:0.003774
[2022-10-02 20:01:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 20:01:30 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.068950	Loss2: 0.060728	 Dis: 3.171349 Entropy: 6.530479 
[2022-10-02 20:01:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.003706	 E_lr_2:0.003706
[2022-10-02 20:01:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000037	 C_lr_2:0.000037	 C_lr_3:0.000037	 C_lr_4:0.000037
[2022-10-02 20:01:36 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.033129	Loss2: 0.048154	 Dis: 3.859554 Entropy: 6.111922 
[2022-10-02 20:01:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.003637	 E_lr_2:0.003637
[2022-10-02 20:01:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 20:01:43 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.084084	Loss2: 0.088529	 Dis: 6.170290 Entropy: 4.719626 
[2022-10-02 20:01:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.003566	 E_lr_2:0.003566
[2022-10-02 20:01:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 20:01:45 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.055759	Loss2: 0.040659	 Dis: 2.870434 Entropy: 4.634287 
[2022-10-02 20:01:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.003495	 E_lr_2:0.003495
[2022-10-02 20:01:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 20:01:52 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.066164	Loss2: 0.047172	 Dis: 3.738394 Entropy: 5.615129 
[2022-10-02 20:01:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.003422	 E_lr_2:0.003422
[2022-10-02 20:01:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000034	 C_lr_2:0.000034	 C_lr_3:0.000034	 C_lr_4:0.000034
[2022-10-02 20:01:59 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.061938	Loss2: 0.051929	 Dis: 2.274591 Entropy: 4.633994 
[2022-10-02 20:01:59 demo] (houston_program2.py 512): INFO E_lr_1: 0.003349	 E_lr_2:0.003349
[2022-10-02 20:01:59 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 20:02:05 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.040748	Loss2: 0.037632	 Dis: 5.464079 Entropy: 4.433026 
[2022-10-02 20:02:05 demo] (houston_program2.py 512): INFO E_lr_1: 0.003274	 E_lr_2:0.003274
[2022-10-02 20:02:05 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 20:02:11 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.110277	Loss2: 0.140331	 Dis: 5.236622 Entropy: 4.517407 
[2022-10-02 20:02:11 demo] (houston_program2.py 512): INFO E_lr_1: 0.003199	 E_lr_2:0.003199
[2022-10-02 20:02:11 demo] (houston_program2.py 513): INFO C_lr_1: 0.000032	 C_lr_2:0.000032	 C_lr_3:0.000032	 C_lr_4:0.000032
[2022-10-02 20:02:18 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.102770	Loss2: 0.130500	 Dis: 4.586140 Entropy: 4.363135 
[2022-10-02 20:02:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.003124	 E_lr_2:0.003124
[2022-10-02 20:02:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000031	 C_lr_2:0.000031	 C_lr_3:0.000031	 C_lr_4:0.000031
[2022-10-02 20:02:25 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.041211	Loss2: 0.030690	 Dis: 3.882702 Entropy: 5.166067 
[2022-10-02 20:02:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.003047	 E_lr_2:0.003047
[2022-10-02 20:02:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 20:02:32 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.071436	Loss2: 0.116637	 Dis: 3.766376 Entropy: 4.427067 
[2022-10-02 20:02:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.002970	 E_lr_2:0.002970
[2022-10-02 20:02:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 20:02:38 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.038796	Loss2: 0.037439	 Dis: 4.170296 Entropy: 4.634791 
[2022-10-02 20:02:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.002893	 E_lr_2:0.002893
[2022-10-02 20:02:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000029	 C_lr_2:0.000029	 C_lr_3:0.000029	 C_lr_4:0.000029
[2022-10-02 20:02:45 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.148268	Loss2: 0.105519	 Dis: 2.188169 Entropy: 4.998774 
[2022-10-02 20:02:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.002815	 E_lr_2:0.002815
[2022-10-02 20:02:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000028	 C_lr_2:0.000028	 C_lr_3:0.000028	 C_lr_4:0.000028
[2022-10-02 20:02:52 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.110016	Loss2: 0.069775	 Dis: 4.679239 Entropy: 4.373920 
[2022-10-02 20:02:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.002737	 E_lr_2:0.002737
[2022-10-02 20:02:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 20:02:59 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.016838	Loss2: 0.039961	 Dis: 1.612976 Entropy: 5.535080 
[2022-10-02 20:02:59 demo] (houston_program2.py 512): INFO E_lr_1: 0.002659	 E_lr_2:0.002659
[2022-10-02 20:02:59 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 20:03:05 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.093173	Loss2: 0.056498	 Dis: 3.083151 Entropy: 4.268888 
[2022-10-02 20:03:05 demo] (houston_program2.py 512): INFO E_lr_1: 0.002581	 E_lr_2:0.002581
[2022-10-02 20:03:05 demo] (houston_program2.py 513): INFO C_lr_1: 0.000026	 C_lr_2:0.000026	 C_lr_3:0.000026	 C_lr_4:0.000026
[2022-10-02 20:03:12 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.037220	Loss2: 0.034165	 Dis: 4.119867 Entropy: 4.809276 
[2022-10-02 20:03:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.002502	 E_lr_2:0.002502
[2022-10-02 20:03:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 20:03:19 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.010896	Loss2: 0.011910	 Dis: 2.158085 Entropy: 4.666375 
[2022-10-02 20:03:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.002423	 E_lr_2:0.002423
[2022-10-02 20:03:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000024	 C_lr_2:0.000024	 C_lr_3:0.000024	 C_lr_4:0.000024
[2022-10-02 20:03:26 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.029998	Loss2: 0.023341	 Dis: 1.996136 Entropy: 4.846508 
[2022-10-02 20:03:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.002345	 E_lr_2:0.002345
[2022-10-02 20:03:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 20:03:32 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.155239	Loss2: 0.167585	 Dis: 6.355770 Entropy: 4.767331 
[2022-10-02 20:03:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.002267	 E_lr_2:0.002267
[2022-10-02 20:03:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 20:03:39 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.044734	Loss2: 0.039497	 Dis: 4.515148 Entropy: 5.251634 
[2022-10-02 20:03:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.002189	 E_lr_2:0.002189
[2022-10-02 20:03:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000022	 C_lr_2:0.000022	 C_lr_3:0.000022	 C_lr_4:0.000022
[2022-10-02 20:03:46 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.037450	Loss2: 0.030469	 Dis: 3.474924 Entropy: 4.375885 
[2022-10-02 20:03:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.002111	 E_lr_2:0.002111
[2022-10-02 20:03:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000021	 C_lr_2:0.000021	 C_lr_3:0.000021	 C_lr_4:0.000021
[2022-10-02 20:03:53 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.007711	Loss2: 0.006098	 Dis: 5.412884 Entropy: 4.803480 
[2022-10-02 20:03:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.002034	 E_lr_2:0.002034
[2022-10-02 20:03:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 20:03:59 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.027564	Loss2: 0.037556	 Dis: 2.289366 Entropy: 5.683009 
[2022-10-02 20:03:59 demo] (houston_program2.py 512): INFO E_lr_1: 0.001957	 E_lr_2:0.001957
[2022-10-02 20:03:59 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 20:04:06 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.026411	Loss2: 0.021408	 Dis: 4.007790 Entropy: 5.377902 
[2022-10-02 20:04:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.001880	 E_lr_2:0.001880
[2022-10-02 20:04:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000019	 C_lr_2:0.000019	 C_lr_3:0.000019	 C_lr_4:0.000019
[2022-10-02 20:04:13 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.027408	Loss2: 0.031255	 Dis: 2.163862 Entropy: 5.463032 
[2022-10-02 20:04:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.001804	 E_lr_2:0.001804
[2022-10-02 20:04:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000018	 C_lr_2:0.000018	 C_lr_3:0.000018	 C_lr_4:0.000018
[2022-10-02 20:04:19 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.037994	Loss2: 0.041434	 Dis: 3.722328 Entropy: 5.844001 
[2022-10-02 20:04:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.001729	 E_lr_2:0.001729
[2022-10-02 20:04:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 20:04:26 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.032454	Loss2: 0.032939	 Dis: 4.395422 Entropy: 4.376204 
[2022-10-02 20:04:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.001655	 E_lr_2:0.001655
[2022-10-02 20:04:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 20:04:33 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.043116	Loss2: 0.046667	 Dis: 1.775805 Entropy: 4.903112 
[2022-10-02 20:04:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.001582	 E_lr_2:0.001582
[2022-10-02 20:04:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000016	 C_lr_2:0.000016	 C_lr_3:0.000016	 C_lr_4:0.000016
[2022-10-02 20:04:39 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.024286	Loss2: 0.033396	 Dis: 4.432730 Entropy: 4.719852 
[2022-10-02 20:04:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.001509	 E_lr_2:0.001509
[2022-10-02 20:04:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 20:04:46 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.004652	Loss2: 0.006547	 Dis: 3.099657 Entropy: 5.978742 
[2022-10-02 20:04:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.001437	 E_lr_2:0.001437
[2022-10-02 20:04:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 20:04:52 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.017025	Loss2: 0.014256	 Dis: 1.831076 Entropy: 5.429110 
[2022-10-02 20:04:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.001367	 E_lr_2:0.001367
[2022-10-02 20:04:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 20:04:59 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.036723	Loss2: 0.041307	 Dis: 2.159887 Entropy: 6.217333 
[2022-10-02 20:04:59 demo] (houston_program2.py 512): INFO E_lr_1: 0.001297	 E_lr_2:0.001297
[2022-10-02 20:04:59 demo] (houston_program2.py 513): INFO C_lr_1: 0.000013	 C_lr_2:0.000013	 C_lr_3:0.000013	 C_lr_4:0.000013
[2022-10-02 20:05:06 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.018835	Loss2: 0.023070	 Dis: 3.515587 Entropy: 4.230053 
[2022-10-02 20:05:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.001229	 E_lr_2:0.001229
[2022-10-02 20:05:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 20:05:13 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.029915	Loss2: 0.042797	 Dis: 1.760494 Entropy: 4.531446 
[2022-10-02 20:05:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.001162	 E_lr_2:0.001162
[2022-10-02 20:05:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 20:05:20 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.008769	Loss2: 0.009558	 Dis: 1.138378 Entropy: 6.619399 
[2022-10-02 20:05:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.001096	 E_lr_2:0.001096
[2022-10-02 20:05:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000011	 C_lr_2:0.000011	 C_lr_3:0.000011	 C_lr_4:0.000011
[2022-10-02 20:05:24 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.011322	Loss2: 0.013673	 Dis: 2.178616 Entropy: 5.423682 
[2022-10-02 20:05:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.001032	 E_lr_2:0.001032
[2022-10-02 20:05:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 20:05:30 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.006612	Loss2: 0.006290	 Dis: 3.140095 Entropy: 4.907289 
[2022-10-02 20:05:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.000969	 E_lr_2:0.000969
[2022-10-02 20:05:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 20:05:37 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.014216	Loss2: 0.018192	 Dis: 3.468662 Entropy: 4.226908 
[2022-10-02 20:05:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.000908	 E_lr_2:0.000908
[2022-10-02 20:05:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000009	 C_lr_2:0.000009	 C_lr_3:0.000009	 C_lr_4:0.000009
[2022-10-02 20:05:44 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.010753	Loss2: 0.011524	 Dis: 2.816557 Entropy: 5.311781 
[2022-10-02 20:05:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.000848	 E_lr_2:0.000848
[2022-10-02 20:05:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 20:05:50 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.009160	Loss2: 0.019808	 Dis: 1.545601 Entropy: 5.491110 
[2022-10-02 20:05:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.000790	 E_lr_2:0.000790
[2022-10-02 20:05:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 20:05:57 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.015079	Loss2: 0.024334	 Dis: 1.197748 Entropy: 6.442367 
[2022-10-02 20:05:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.000734	 E_lr_2:0.000734
[2022-10-02 20:05:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 20:06:04 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.003834	Loss2: 0.003404	 Dis: 1.562574 Entropy: 4.408396 
[2022-10-02 20:06:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.000679	 E_lr_2:0.000679
[2022-10-02 20:06:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 20:06:11 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.029182	Loss2: 0.025582	 Dis: 2.033621 Entropy: 4.803500 
[2022-10-02 20:06:11 demo] (houston_program2.py 512): INFO E_lr_1: 0.000626	 E_lr_2:0.000626
[2022-10-02 20:06:11 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 20:06:18 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.005731	Loss2: 0.006221	 Dis: 3.440845 Entropy: 4.257490 
[2022-10-02 20:06:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.000575	 E_lr_2:0.000575
[2022-10-02 20:06:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 20:06:25 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.005104	Loss2: 0.005131	 Dis: 2.406487 Entropy: 4.821132 
[2022-10-02 20:06:25 demo] (houston_program2.py 512): INFO E_lr_1: 0.000526	 E_lr_2:0.000526
[2022-10-02 20:06:25 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 20:06:32 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.040634	Loss2: 0.051492	 Dis: 1.898619 Entropy: 4.453043 
[2022-10-02 20:06:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.000479	 E_lr_2:0.000479
[2022-10-02 20:06:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 20:06:39 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.004964	Loss2: 0.004109	 Dis: 2.936974 Entropy: 5.742774 
[2022-10-02 20:06:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.000433	 E_lr_2:0.000433
[2022-10-02 20:06:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 20:06:46 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.007501	Loss2: 0.005556	 Dis: 2.692797 Entropy: 4.899589 
[2022-10-02 20:06:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.000390	 E_lr_2:0.000390
[2022-10-02 20:06:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 20:06:53 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.142484	Loss2: 0.130884	 Dis: 2.692137 Entropy: 4.461718 
[2022-10-02 20:06:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.000349	 E_lr_2:0.000349
[2022-10-02 20:06:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 20:07:00 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.005435	Loss2: 0.004808	 Dis: 3.738258 Entropy: 5.797390 
[2022-10-02 20:07:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.000310	 E_lr_2:0.000310
[2022-10-02 20:07:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 20:07:07 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.028447	Loss2: 0.030194	 Dis: 0.804310 Entropy: 6.682202 
[2022-10-02 20:07:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.000273	 E_lr_2:0.000273
[2022-10-02 20:07:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 20:07:14 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.023965	Loss2: 0.023491	 Dis: 3.136425 Entropy: 5.160002 
[2022-10-02 20:07:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.000239	 E_lr_2:0.000239
[2022-10-02 20:07:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 20:07:20 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.027495	Loss2: 0.025682	 Dis: 1.803583 Entropy: 4.310338 
[2022-10-02 20:07:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.000206	 E_lr_2:0.000206
[2022-10-02 20:07:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 20:07:27 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.017811	Loss2: 0.022407	 Dis: 1.672178 Entropy: 4.658834 
[2022-10-02 20:07:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.000176	 E_lr_2:0.000176
[2022-10-02 20:07:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 20:07:34 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.011757	Loss2: 0.009414	 Dis: 0.894203 Entropy: 4.967941 
[2022-10-02 20:07:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.000148	 E_lr_2:0.000148
[2022-10-02 20:07:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 20:07:41 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.009956	Loss2: 0.015188	 Dis: 2.550051 Entropy: 6.389339 
[2022-10-02 20:07:41 demo] (houston_program2.py 512): INFO E_lr_1: 0.000123	 E_lr_2:0.000123
[2022-10-02 20:07:41 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 20:07:47 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.014101	Loss2: 0.014302	 Dis: 3.184071 Entropy: 5.738235 
[2022-10-02 20:07:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.000100	 E_lr_2:0.000100
[2022-10-02 20:07:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 20:07:54 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.014736	Loss2: 0.011049	 Dis: 2.993866 Entropy: 4.317158 
[2022-10-02 20:07:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.000079	 E_lr_2:0.000079
[2022-10-02 20:07:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 20:08:01 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.006662	Loss2: 0.007575	 Dis: 2.827045 Entropy: 4.799213 
[2022-10-02 20:08:01 demo] (houston_program2.py 512): INFO E_lr_1: 0.000061	 E_lr_2:0.000061
[2022-10-02 20:08:01 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 20:08:08 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.006864	Loss2: 0.004462	 Dis: 2.794073 Entropy: 4.430069 
[2022-10-02 20:08:08 demo] (houston_program2.py 512): INFO E_lr_1: 0.000045	 E_lr_2:0.000045
[2022-10-02 20:08:08 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 20:08:14 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.006160	Loss2: 0.004531	 Dis: 1.976692 Entropy: 4.825494 
[2022-10-02 20:08:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.000031	 E_lr_2:0.000031
[2022-10-02 20:08:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 20:08:21 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.004036	Loss2: 0.003621	 Dis: 1.835260 Entropy: 5.081827 
[2022-10-02 20:08:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.000020	 E_lr_2:0.000020
[2022-10-02 20:08:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 20:08:27 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.006056	Loss2: 0.005472	 Dis: 1.472458 Entropy: 4.504972 
[2022-10-02 20:08:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.000011	 E_lr_2:0.000011
[2022-10-02 20:08:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 20:08:31 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.007872	Loss2: 0.006091	 Dis: 3.044508 Entropy: 5.462790 
[2022-10-02 20:08:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.000005	 E_lr_2:0.000005
[2022-10-02 20:08:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 20:08:37 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.001370	Loss2: 0.001548	 Dis: 2.182083 Entropy: 6.240520 
[2022-10-02 20:08:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.000001	 E_lr_2:0.000001
[2022-10-02 20:08:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 20:08:44 demo] (houston_program2.py 504): INFO Train Ep: 90 	Loss1: 0.002508	Loss2: 0.001854	 Dis: 0.988838 Entropy: 4.587477 
[2022-10-02 20:08:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.000000	 E_lr_2:0.000000
[2022-10-02 20:08:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 20:08:44 demo] (houston_program2.py 515): INFO time_90_epoch:655.5878126621246
[2022-10-02 20:08:51 demo] (houston_program2.py 673): INFO 	val_Accuracy: 29045/53200 (54.60%)	
[2022-10-02 20:08:51 demo] (utils.py 47): INFO outputs/demo/finetune_100/ckpt_epoch_90.pth saving......
[2022-10-02 20:08:51 demo] (utils.py 49): INFO outputs/demo/finetune_100/ckpt_epoch_90.pth saved !!!
[2022-10-02 20:08:52 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0116 (0.0116)	loss 0.0061 (0.0061)	grad_norm 0.0439 (0.0439)	mem 460MB
[2022-10-02 20:08:52 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:52 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0216 (0.0216)	loss 0.0116 (0.0116)	grad_norm 0.0339 (0.0339)	mem 460MB
[2022-10-02 20:08:52 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:52 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0215 (0.0215)	loss 0.0081 (0.0081)	grad_norm 0.0278 (0.0278)	mem 460MB
[2022-10-02 20:08:52 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:52 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0204 (0.0204)	loss 0.0098 (0.0098)	grad_norm 0.0438 (0.0438)	mem 460MB
[2022-10-02 20:08:52 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:52 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0217 (0.0217)	loss 0.0106 (0.0106)	grad_norm 0.0530 (0.0530)	mem 460MB
[2022-10-02 20:08:53 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:53 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0216 (0.0216)	loss 0.0104 (0.0104)	grad_norm 0.0411 (0.0411)	mem 460MB
[2022-10-02 20:08:53 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:53 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0207 (0.0207)	loss 0.0099 (0.0099)	grad_norm 0.0373 (0.0373)	mem 460MB
[2022-10-02 20:08:53 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:53 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0210 (0.0210)	loss 0.0084 (0.0084)	grad_norm 0.0390 (0.0390)	mem 460MB
[2022-10-02 20:08:53 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:53 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0215 (0.0215)	loss 0.0073 (0.0073)	grad_norm 0.0300 (0.0300)	mem 460MB
[2022-10-02 20:08:53 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:53 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0218 (0.0218)	loss 0.0086 (0.0086)	grad_norm 0.0284 (0.0284)	mem 460MB
[2022-10-02 20:08:53 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:54 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0202 (0.0202)	loss 0.0092 (0.0092)	grad_norm 0.0386 (0.0386)	mem 460MB
[2022-10-02 20:08:54 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:54 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0219 (0.0219)	loss 0.0078 (0.0078)	grad_norm 0.0242 (0.0242)	mem 460MB
[2022-10-02 20:08:54 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:54 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0209 (0.0209)	loss 0.0109 (0.0109)	grad_norm 0.0548 (0.0548)	mem 460MB
[2022-10-02 20:08:54 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:54 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0207 (0.0207)	loss 0.0080 (0.0080)	grad_norm 0.0458 (0.0458)	mem 460MB
[2022-10-02 20:08:54 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:54 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0216 (0.0216)	loss 0.0075 (0.0075)	grad_norm 0.0313 (0.0313)	mem 460MB
[2022-10-02 20:08:54 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:55 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0204 (0.0204)	loss 0.0077 (0.0077)	grad_norm 0.0413 (0.0413)	mem 460MB
[2022-10-02 20:08:55 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:55 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0214 (0.0214)	loss 0.0070 (0.0070)	grad_norm 0.0455 (0.0455)	mem 460MB
[2022-10-02 20:08:55 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:55 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0211 (0.0211)	loss 0.0082 (0.0082)	grad_norm 0.0724 (0.0724)	mem 460MB
[2022-10-02 20:08:55 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:55 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0219 (0.0219)	loss 0.0100 (0.0100)	grad_norm 0.0419 (0.0419)	mem 460MB
[2022-10-02 20:08:55 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:55 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0181 (0.0181)	loss 0.0083 (0.0083)	grad_norm 0.0447 (0.0447)	mem 460MB
[2022-10-02 20:08:55 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:56 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0203 (0.0203)	loss 0.0090 (0.0090)	grad_norm 0.0381 (0.0381)	mem 460MB
[2022-10-02 20:08:56 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:56 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0209 (0.0209)	loss 0.0077 (0.0077)	grad_norm 0.0443 (0.0443)	mem 460MB
[2022-10-02 20:08:56 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:56 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0218 (0.0218)	loss 0.0090 (0.0090)	grad_norm 0.0488 (0.0488)	mem 460MB
[2022-10-02 20:08:56 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:56 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0212 (0.0212)	loss 0.0060 (0.0060)	grad_norm 0.0379 (0.0379)	mem 460MB
[2022-10-02 20:08:56 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:56 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0217 (0.0217)	loss 0.0071 (0.0071)	grad_norm 0.0337 (0.0337)	mem 460MB
[2022-10-02 20:08:56 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:57 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0219 (0.0219)	loss 0.0103 (0.0103)	grad_norm 0.0468 (0.0468)	mem 460MB
[2022-10-02 20:08:57 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:57 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0216 (0.0216)	loss 0.0097 (0.0097)	grad_norm 0.0345 (0.0345)	mem 460MB
[2022-10-02 20:08:57 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:57 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0219 (0.0219)	loss 0.0079 (0.0079)	grad_norm 0.0522 (0.0522)	mem 460MB
[2022-10-02 20:08:57 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:57 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0217 (0.0217)	loss 0.0099 (0.0099)	grad_norm 0.0387 (0.0387)	mem 460MB
[2022-10-02 20:08:57 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:57 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0207 (0.0207)	loss 0.0076 (0.0076)	grad_norm 0.0582 (0.0582)	mem 460MB
[2022-10-02 20:08:57 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:57 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0207 (0.0207)	loss 0.0097 (0.0097)	grad_norm 0.0560 (0.0560)	mem 460MB
[2022-10-02 20:08:58 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:58 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0219 (0.0219)	loss 0.0079 (0.0079)	grad_norm 0.0344 (0.0344)	mem 460MB
[2022-10-02 20:08:58 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:58 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0202 (0.0202)	loss 0.0086 (0.0086)	grad_norm 0.0297 (0.0297)	mem 460MB
[2022-10-02 20:08:58 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:58 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0205 (0.0205)	loss 0.0086 (0.0086)	grad_norm 0.0363 (0.0363)	mem 460MB
[2022-10-02 20:08:58 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:58 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0199 (0.0199)	loss 0.0094 (0.0094)	grad_norm 0.0290 (0.0290)	mem 460MB
[2022-10-02 20:08:58 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:58 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0210 (0.0210)	loss 0.0085 (0.0085)	grad_norm 0.0495 (0.0495)	mem 460MB
[2022-10-02 20:08:58 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:59 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0205 (0.0205)	loss 0.0081 (0.0081)	grad_norm 0.0368 (0.0368)	mem 460MB
[2022-10-02 20:08:59 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:59 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0211 (0.0211)	loss 0.0059 (0.0059)	grad_norm 0.0319 (0.0319)	mem 460MB
[2022-10-02 20:08:59 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:59 demo] (houston_program2.py 243): INFO Train: [91/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0201 (0.0201)	loss 0.0095 (0.0095)	grad_norm 0.0394 (0.0394)	mem 460MB
[2022-10-02 20:08:59 demo] (houston_program2.py 252): INFO EPOCH 91 training takes 0:00:00
[2022-10-02 20:08:59 demo] (houston_program2.py 333): INFO Train Ep: 91 	Loss1: 0.060749	Loss2: 0.071660	 Dis: 2.688110 Entropy: 5.674349 
[2022-10-02 20:08:59 demo] (houston_program2.py 335): INFO time_91_epoch:7.88568377494812
[2022-10-02 20:09:00 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0111 (0.0111)	loss 0.0109 (0.0109)	grad_norm 0.0399 (0.0399)	mem 460MB
[2022-10-02 20:09:00 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:00 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0203 (0.0203)	loss 0.0097 (0.0097)	grad_norm 0.0364 (0.0364)	mem 460MB
[2022-10-02 20:09:00 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:00 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0213 (0.0213)	loss 0.0097 (0.0097)	grad_norm 0.0452 (0.0452)	mem 460MB
[2022-10-02 20:09:00 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:00 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0195 (0.0195)	loss 0.0082 (0.0082)	grad_norm 0.0341 (0.0341)	mem 460MB
[2022-10-02 20:09:00 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:00 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0216 (0.0216)	loss 0.0099 (0.0099)	grad_norm 0.0422 (0.0422)	mem 460MB
[2022-10-02 20:09:00 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:01 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0207 (0.0207)	loss 0.0090 (0.0090)	grad_norm 0.0482 (0.0482)	mem 460MB
[2022-10-02 20:09:01 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:01 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0215 (0.0215)	loss 0.0110 (0.0110)	grad_norm 0.0522 (0.0522)	mem 460MB
[2022-10-02 20:09:01 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:01 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0215 (0.0215)	loss 0.0074 (0.0074)	grad_norm 0.0387 (0.0387)	mem 460MB
[2022-10-02 20:09:01 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:01 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0212 (0.0212)	loss 0.0075 (0.0075)	grad_norm 0.0430 (0.0430)	mem 460MB
[2022-10-02 20:09:01 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:01 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0212 (0.0212)	loss 0.0074 (0.0074)	grad_norm 0.0343 (0.0343)	mem 460MB
[2022-10-02 20:09:01 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:02 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0205 (0.0205)	loss 0.0084 (0.0084)	grad_norm 0.0588 (0.0588)	mem 460MB
[2022-10-02 20:09:02 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:02 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0219 (0.0219)	loss 0.0085 (0.0085)	grad_norm 0.0702 (0.0702)	mem 460MB
[2022-10-02 20:09:02 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:02 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0217 (0.0217)	loss 0.0078 (0.0078)	grad_norm 0.0381 (0.0381)	mem 460MB
[2022-10-02 20:09:02 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:02 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0210 (0.0210)	loss 0.0067 (0.0067)	grad_norm 0.0455 (0.0455)	mem 460MB
[2022-10-02 20:09:02 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:02 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0211 (0.0211)	loss 0.0087 (0.0087)	grad_norm 0.0357 (0.0357)	mem 460MB
[2022-10-02 20:09:02 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:03 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0203 (0.0203)	loss 0.0098 (0.0098)	grad_norm 0.0370 (0.0370)	mem 460MB
[2022-10-02 20:09:03 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:03 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0218 (0.0218)	loss 0.0076 (0.0076)	grad_norm 0.0509 (0.0509)	mem 460MB
[2022-10-02 20:09:03 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:03 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0218 (0.0218)	loss 0.0078 (0.0078)	grad_norm 0.0490 (0.0490)	mem 460MB
[2022-10-02 20:09:03 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:03 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0220 (0.0220)	loss 0.0087 (0.0087)	grad_norm 0.0355 (0.0355)	mem 460MB
[2022-10-02 20:09:03 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:03 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0219 (0.0219)	loss 0.0142 (0.0142)	grad_norm 0.0411 (0.0411)	mem 460MB
[2022-10-02 20:09:03 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:03 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0202 (0.0202)	loss 0.0068 (0.0068)	grad_norm 0.0311 (0.0311)	mem 460MB
[2022-10-02 20:09:04 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:04 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0210 (0.0210)	loss 0.0113 (0.0113)	grad_norm 0.0442 (0.0442)	mem 460MB
[2022-10-02 20:09:04 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:04 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0207 (0.0207)	loss 0.0080 (0.0080)	grad_norm 0.0320 (0.0320)	mem 460MB
[2022-10-02 20:09:04 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:04 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0219 (0.0219)	loss 0.0108 (0.0108)	grad_norm 0.0495 (0.0495)	mem 460MB
[2022-10-02 20:09:04 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:04 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0217 (0.0217)	loss 0.0072 (0.0072)	grad_norm 0.0265 (0.0265)	mem 460MB
[2022-10-02 20:09:04 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:04 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0218 (0.0218)	loss 0.0092 (0.0092)	grad_norm 0.0445 (0.0445)	mem 460MB
[2022-10-02 20:09:04 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:05 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0219 (0.0219)	loss 0.0111 (0.0111)	grad_norm 0.0631 (0.0631)	mem 460MB
[2022-10-02 20:09:05 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:05 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0209 (0.0209)	loss 0.0089 (0.0089)	grad_norm 0.0421 (0.0421)	mem 460MB
[2022-10-02 20:09:05 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:05 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0203 (0.0203)	loss 0.0064 (0.0064)	grad_norm 0.0285 (0.0285)	mem 460MB
[2022-10-02 20:09:05 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:05 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0201 (0.0201)	loss 0.0070 (0.0070)	grad_norm 0.0281 (0.0281)	mem 460MB
[2022-10-02 20:09:05 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:05 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0218 (0.0218)	loss 0.0082 (0.0082)	grad_norm 0.0332 (0.0332)	mem 460MB
[2022-10-02 20:09:05 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:06 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0219 (0.0219)	loss 0.0086 (0.0086)	grad_norm 0.0366 (0.0366)	mem 460MB
[2022-10-02 20:09:06 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:06 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0221 (0.0221)	loss 0.0108 (0.0108)	grad_norm 0.0413 (0.0413)	mem 460MB
[2022-10-02 20:09:06 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:06 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0212 (0.0212)	loss 0.0076 (0.0076)	grad_norm 0.0355 (0.0355)	mem 460MB
[2022-10-02 20:09:06 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:06 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0209 (0.0209)	loss 0.0076 (0.0076)	grad_norm 0.0349 (0.0349)	mem 460MB
[2022-10-02 20:09:06 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:06 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0218 (0.0218)	loss 0.0095 (0.0095)	grad_norm 0.0322 (0.0322)	mem 460MB
[2022-10-02 20:09:06 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:07 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0205 (0.0205)	loss 0.0126 (0.0126)	grad_norm 0.0630 (0.0630)	mem 460MB
[2022-10-02 20:09:07 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:07 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0219 (0.0219)	loss 0.0071 (0.0071)	grad_norm 0.0342 (0.0342)	mem 460MB
[2022-10-02 20:09:07 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:07 demo] (houston_program2.py 243): INFO Train: [92/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0217 (0.0217)	loss 0.0113 (0.0113)	grad_norm 0.0505 (0.0505)	mem 460MB
[2022-10-02 20:09:07 demo] (houston_program2.py 252): INFO EPOCH 92 training takes 0:00:00
[2022-10-02 20:09:07 demo] (houston_program2.py 333): INFO Train Ep: 92 	Loss1: 0.049035	Loss2: 0.044286	 Dis: 4.138012 Entropy: 5.099671 
[2022-10-02 20:09:07 demo] (houston_program2.py 335): INFO time_92_epoch:7.966040849685669
[2022-10-02 20:09:08 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0154 (0.0154)	loss 0.0087 (0.0087)	grad_norm 0.0344 (0.0344)	mem 460MB
[2022-10-02 20:09:08 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:08 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0216 (0.0216)	loss 0.0105 (0.0105)	grad_norm 0.0421 (0.0421)	mem 460MB
[2022-10-02 20:09:08 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:08 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0220 (0.0220)	loss 0.0097 (0.0097)	grad_norm 0.0425 (0.0425)	mem 460MB
[2022-10-02 20:09:08 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:08 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0192 (0.0192)	loss 0.0064 (0.0064)	grad_norm 0.0327 (0.0327)	mem 460MB
[2022-10-02 20:09:08 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:08 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0218 (0.0218)	loss 0.0089 (0.0089)	grad_norm 0.0369 (0.0369)	mem 460MB
[2022-10-02 20:09:08 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:09 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0221 (0.0221)	loss 0.0094 (0.0094)	grad_norm 0.0320 (0.0320)	mem 460MB
[2022-10-02 20:09:09 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:09 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0219 (0.0219)	loss 0.0099 (0.0099)	grad_norm 0.0383 (0.0383)	mem 460MB
[2022-10-02 20:09:09 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:09 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0217 (0.0217)	loss 0.0116 (0.0116)	grad_norm 0.0507 (0.0507)	mem 460MB
[2022-10-02 20:09:09 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:09 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0220 (0.0220)	loss 0.0084 (0.0084)	grad_norm 0.0420 (0.0420)	mem 460MB
[2022-10-02 20:09:09 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:09 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0204 (0.0204)	loss 0.0091 (0.0091)	grad_norm 0.0389 (0.0389)	mem 460MB
[2022-10-02 20:09:09 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:10 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0219 (0.0219)	loss 0.0078 (0.0078)	grad_norm 0.0509 (0.0509)	mem 460MB
[2022-10-02 20:09:10 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:10 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0221 (0.0221)	loss 0.0117 (0.0117)	grad_norm 0.0440 (0.0440)	mem 460MB
[2022-10-02 20:09:10 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:10 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0205 (0.0205)	loss 0.0070 (0.0070)	grad_norm 0.0299 (0.0299)	mem 460MB
[2022-10-02 20:09:10 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:10 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0210 (0.0210)	loss 0.0064 (0.0064)	grad_norm 0.0441 (0.0441)	mem 460MB
[2022-10-02 20:09:10 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:10 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0211 (0.0211)	loss 0.0074 (0.0074)	grad_norm 0.0397 (0.0397)	mem 460MB
[2022-10-02 20:09:10 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:11 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0207 (0.0207)	loss 0.0093 (0.0093)	grad_norm 0.0389 (0.0389)	mem 460MB
[2022-10-02 20:09:11 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:11 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0222 (0.0222)	loss 0.0123 (0.0123)	grad_norm 0.0467 (0.0467)	mem 460MB
[2022-10-02 20:09:11 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:11 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0207 (0.0207)	loss 0.0080 (0.0080)	grad_norm 0.0323 (0.0323)	mem 460MB
[2022-10-02 20:09:11 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:11 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0219 (0.0219)	loss 0.0082 (0.0082)	grad_norm 0.0292 (0.0292)	mem 460MB
[2022-10-02 20:09:11 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:11 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0219 (0.0219)	loss 0.0111 (0.0111)	grad_norm 0.0459 (0.0459)	mem 460MB
[2022-10-02 20:09:11 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:12 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0222 (0.0222)	loss 0.0081 (0.0081)	grad_norm 0.0470 (0.0470)	mem 460MB
[2022-10-02 20:09:12 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:12 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0192 (0.0192)	loss 0.0100 (0.0100)	grad_norm 0.0402 (0.0402)	mem 460MB
[2022-10-02 20:09:12 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:12 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0215 (0.0215)	loss 0.0100 (0.0100)	grad_norm 0.0568 (0.0568)	mem 460MB
[2022-10-02 20:09:12 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:12 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0202 (0.0202)	loss 0.0121 (0.0121)	grad_norm 0.0488 (0.0488)	mem 460MB
[2022-10-02 20:09:12 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:12 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0200 (0.0200)	loss 0.0084 (0.0084)	grad_norm 0.0451 (0.0451)	mem 460MB
[2022-10-02 20:09:12 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:13 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0215 (0.0215)	loss 0.0116 (0.0116)	grad_norm 0.0317 (0.0317)	mem 460MB
[2022-10-02 20:09:13 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:13 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0200 (0.0200)	loss 0.0076 (0.0076)	grad_norm 0.0321 (0.0321)	mem 460MB
[2022-10-02 20:09:13 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:13 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0220 (0.0220)	loss 0.0075 (0.0075)	grad_norm 0.0363 (0.0363)	mem 460MB
[2022-10-02 20:09:13 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:13 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000001	time 0.0197 (0.0197)	loss 0.0074 (0.0074)	grad_norm 0.0390 (0.0390)	mem 460MB
[2022-10-02 20:09:13 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:13 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0220 (0.0220)	loss 0.0103 (0.0103)	grad_norm 0.0333 (0.0333)	mem 460MB
[2022-10-02 20:09:13 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:13 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0221 (0.0221)	loss 0.0097 (0.0097)	grad_norm 0.0295 (0.0295)	mem 460MB
[2022-10-02 20:09:13 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:14 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0206 (0.0206)	loss 0.0094 (0.0094)	grad_norm 0.0318 (0.0318)	mem 460MB
[2022-10-02 20:09:14 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:14 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0203 (0.0203)	loss 0.0101 (0.0101)	grad_norm 0.0370 (0.0370)	mem 460MB
[2022-10-02 20:09:14 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:14 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0217 (0.0217)	loss 0.0091 (0.0091)	grad_norm 0.0303 (0.0303)	mem 460MB
[2022-10-02 20:09:14 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:14 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0220 (0.0220)	loss 0.0063 (0.0063)	grad_norm 0.0256 (0.0256)	mem 460MB
[2022-10-02 20:09:14 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:14 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0198 (0.0198)	loss 0.0093 (0.0093)	grad_norm 0.0331 (0.0331)	mem 460MB
[2022-10-02 20:09:14 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:15 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0221 (0.0221)	loss 0.0074 (0.0074)	grad_norm 0.0330 (0.0330)	mem 460MB
[2022-10-02 20:09:15 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:15 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0210 (0.0210)	loss 0.0063 (0.0063)	grad_norm 0.0363 (0.0363)	mem 460MB
[2022-10-02 20:09:15 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:15 demo] (houston_program2.py 243): INFO Train: [93/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0220 (0.0220)	loss 0.0083 (0.0083)	grad_norm 0.0393 (0.0393)	mem 460MB
[2022-10-02 20:09:15 demo] (houston_program2.py 252): INFO EPOCH 93 training takes 0:00:00
[2022-10-02 20:09:15 demo] (houston_program2.py 333): INFO Train Ep: 93 	Loss1: 0.011336	Loss2: 0.013775	 Dis: 3.534050 Entropy: 4.280720 
[2022-10-02 20:09:15 demo] (houston_program2.py 335): INFO time_93_epoch:8.074487686157227
[2022-10-02 20:09:16 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0153 (0.0153)	loss 0.0081 (0.0081)	grad_norm 0.0390 (0.0390)	mem 460MB
[2022-10-02 20:09:16 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:16 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0234 (0.0234)	loss 0.0102 (0.0102)	grad_norm 0.0327 (0.0327)	mem 460MB
[2022-10-02 20:09:16 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:16 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0208 (0.0208)	loss 0.0075 (0.0075)	grad_norm 0.0439 (0.0439)	mem 460MB
[2022-10-02 20:09:16 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:16 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0208 (0.0208)	loss 0.0091 (0.0091)	grad_norm 0.0317 (0.0317)	mem 460MB
[2022-10-02 20:09:16 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:17 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0233 (0.0233)	loss 0.0106 (0.0106)	grad_norm 0.0329 (0.0329)	mem 460MB
[2022-10-02 20:09:17 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:17 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0234 (0.0234)	loss 0.0094 (0.0094)	grad_norm 0.0380 (0.0380)	mem 460MB
[2022-10-02 20:09:17 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:17 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0233 (0.0233)	loss 0.0076 (0.0076)	grad_norm 0.0362 (0.0362)	mem 460MB
[2022-10-02 20:09:17 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:17 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0220 (0.0220)	loss 0.0084 (0.0084)	grad_norm 0.0381 (0.0381)	mem 460MB
[2022-10-02 20:09:17 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:17 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0233 (0.0233)	loss 0.0076 (0.0076)	grad_norm 0.0276 (0.0276)	mem 460MB
[2022-10-02 20:09:17 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:18 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0215 (0.0215)	loss 0.0139 (0.0139)	grad_norm 0.0601 (0.0601)	mem 460MB
[2022-10-02 20:09:18 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:18 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0227 (0.0227)	loss 0.0076 (0.0076)	grad_norm 0.0326 (0.0326)	mem 460MB
[2022-10-02 20:09:18 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:18 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0229 (0.0229)	loss 0.0091 (0.0091)	grad_norm 0.0289 (0.0289)	mem 460MB
[2022-10-02 20:09:18 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:18 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0235 (0.0235)	loss 0.0065 (0.0065)	grad_norm 0.0321 (0.0321)	mem 460MB
[2022-10-02 20:09:18 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:18 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0213 (0.0213)	loss 0.0125 (0.0125)	grad_norm 0.0405 (0.0405)	mem 460MB
[2022-10-02 20:09:18 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:19 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0209 (0.0209)	loss 0.0081 (0.0081)	grad_norm 0.0284 (0.0284)	mem 460MB
[2022-10-02 20:09:19 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:19 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0227 (0.0227)	loss 0.0090 (0.0090)	grad_norm 0.0705 (0.0705)	mem 460MB
[2022-10-02 20:09:19 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:19 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0215 (0.0215)	loss 0.0115 (0.0115)	grad_norm 0.0503 (0.0503)	mem 460MB
[2022-10-02 20:09:19 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:19 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0208 (0.0208)	loss 0.0059 (0.0059)	grad_norm 0.0227 (0.0227)	mem 460MB
[2022-10-02 20:09:19 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:19 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0205 (0.0205)	loss 0.0076 (0.0076)	grad_norm 0.0414 (0.0414)	mem 460MB
[2022-10-02 20:09:19 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:20 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0228 (0.0228)	loss 0.0063 (0.0063)	grad_norm 0.0408 (0.0408)	mem 460MB
[2022-10-02 20:09:20 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:20 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0232 (0.0232)	loss 0.0076 (0.0076)	grad_norm 0.0330 (0.0330)	mem 460MB
[2022-10-02 20:09:20 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:20 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0196 (0.0196)	loss 0.0089 (0.0089)	grad_norm 0.0422 (0.0422)	mem 460MB
[2022-10-02 20:09:20 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:20 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0206 (0.0206)	loss 0.0111 (0.0111)	grad_norm 0.0363 (0.0363)	mem 460MB
[2022-10-02 20:09:20 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:21 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0213 (0.0213)	loss 0.0069 (0.0069)	grad_norm 0.0265 (0.0265)	mem 460MB
[2022-10-02 20:09:21 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:21 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0229 (0.0229)	loss 0.0075 (0.0075)	grad_norm 0.0268 (0.0268)	mem 460MB
[2022-10-02 20:09:21 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:21 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0214 (0.0214)	loss 0.0067 (0.0067)	grad_norm 0.0317 (0.0317)	mem 460MB
[2022-10-02 20:09:21 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:21 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0209 (0.0209)	loss 0.0062 (0.0062)	grad_norm 0.0291 (0.0291)	mem 460MB
[2022-10-02 20:09:21 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:21 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0217 (0.0217)	loss 0.0078 (0.0078)	grad_norm 0.0259 (0.0259)	mem 460MB
[2022-10-02 20:09:21 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:22 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0210 (0.0210)	loss 0.0098 (0.0098)	grad_norm 0.0364 (0.0364)	mem 460MB
[2022-10-02 20:09:22 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:22 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0216 (0.0216)	loss 0.0098 (0.0098)	grad_norm 0.0387 (0.0387)	mem 460MB
[2022-10-02 20:09:22 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:22 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0210 (0.0210)	loss 0.0093 (0.0093)	grad_norm 0.0499 (0.0499)	mem 460MB
[2022-10-02 20:09:22 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:22 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0227 (0.0227)	loss 0.0087 (0.0087)	grad_norm 0.0358 (0.0358)	mem 460MB
[2022-10-02 20:09:22 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:22 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0228 (0.0228)	loss 0.0080 (0.0080)	grad_norm 0.0322 (0.0322)	mem 460MB
[2022-10-02 20:09:22 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:23 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0213 (0.0213)	loss 0.0120 (0.0120)	grad_norm 0.0493 (0.0493)	mem 460MB
[2022-10-02 20:09:23 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:23 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0234 (0.0234)	loss 0.0082 (0.0082)	grad_norm 0.0360 (0.0360)	mem 460MB
[2022-10-02 20:09:23 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:23 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0221 (0.0221)	loss 0.0097 (0.0097)	grad_norm 0.0489 (0.0489)	mem 460MB
[2022-10-02 20:09:23 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:23 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0219 (0.0219)	loss 0.0088 (0.0088)	grad_norm 0.0389 (0.0389)	mem 460MB
[2022-10-02 20:09:23 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:23 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0215 (0.0215)	loss 0.0059 (0.0059)	grad_norm 0.0258 (0.0258)	mem 460MB
[2022-10-02 20:09:23 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:24 demo] (houston_program2.py 243): INFO Train: [94/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0204 (0.0204)	loss 0.0145 (0.0145)	grad_norm 0.0652 (0.0652)	mem 460MB
[2022-10-02 20:09:24 demo] (houston_program2.py 252): INFO EPOCH 94 training takes 0:00:00
[2022-10-02 20:09:24 demo] (houston_program2.py 333): INFO Train Ep: 94 	Loss1: 0.074154	Loss2: 0.056723	 Dis: 2.498634 Entropy: 5.910601 
[2022-10-02 20:09:24 demo] (houston_program2.py 335): INFO time_94_epoch:8.612028360366821
[2022-10-02 20:09:24 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0148 (0.0148)	loss 0.0115 (0.0115)	grad_norm 0.0512 (0.0512)	mem 460MB
[2022-10-02 20:09:24 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:25 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0202 (0.0202)	loss 0.0091 (0.0091)	grad_norm 0.0325 (0.0325)	mem 460MB
[2022-10-02 20:09:25 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:25 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0225 (0.0225)	loss 0.0076 (0.0076)	grad_norm 0.0378 (0.0378)	mem 460MB
[2022-10-02 20:09:25 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:25 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0197 (0.0197)	loss 0.0070 (0.0070)	grad_norm 0.0364 (0.0364)	mem 460MB
[2022-10-02 20:09:25 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:25 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0244 (0.0244)	loss 0.0088 (0.0088)	grad_norm 0.0290 (0.0290)	mem 460MB
[2022-10-02 20:09:25 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:25 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0246 (0.0246)	loss 0.0116 (0.0116)	grad_norm 0.0591 (0.0591)	mem 460MB
[2022-10-02 20:09:25 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:26 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0236 (0.0236)	loss 0.0106 (0.0106)	grad_norm 0.0445 (0.0445)	mem 460MB
[2022-10-02 20:09:26 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:26 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0236 (0.0236)	loss 0.0071 (0.0071)	grad_norm 0.0348 (0.0348)	mem 460MB
[2022-10-02 20:09:26 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:26 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0218 (0.0218)	loss 0.0082 (0.0082)	grad_norm 0.0370 (0.0370)	mem 460MB
[2022-10-02 20:09:26 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:26 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0221 (0.0221)	loss 0.0092 (0.0092)	grad_norm 0.0380 (0.0380)	mem 460MB
[2022-10-02 20:09:26 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:26 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0200 (0.0200)	loss 0.0077 (0.0077)	grad_norm 0.0296 (0.0296)	mem 460MB
[2022-10-02 20:09:26 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:27 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0208 (0.0208)	loss 0.0105 (0.0105)	grad_norm 0.0302 (0.0302)	mem 460MB
[2022-10-02 20:09:27 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:27 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0218 (0.0218)	loss 0.0099 (0.0099)	grad_norm 0.0452 (0.0452)	mem 460MB
[2022-10-02 20:09:27 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:27 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0208 (0.0208)	loss 0.0092 (0.0092)	grad_norm 0.0549 (0.0549)	mem 460MB
[2022-10-02 20:09:27 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:27 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0202 (0.0202)	loss 0.0091 (0.0091)	grad_norm 0.0356 (0.0356)	mem 460MB
[2022-10-02 20:09:27 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:27 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0202 (0.0202)	loss 0.0090 (0.0090)	grad_norm 0.0335 (0.0335)	mem 460MB
[2022-10-02 20:09:27 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:28 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0212 (0.0212)	loss 0.0069 (0.0069)	grad_norm 0.0391 (0.0391)	mem 460MB
[2022-10-02 20:09:28 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:28 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0204 (0.0204)	loss 0.0080 (0.0080)	grad_norm 0.0456 (0.0456)	mem 460MB
[2022-10-02 20:09:28 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:28 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0220 (0.0220)	loss 0.0094 (0.0094)	grad_norm 0.0332 (0.0332)	mem 460MB
[2022-10-02 20:09:28 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:28 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0219 (0.0219)	loss 0.0099 (0.0099)	grad_norm 0.0374 (0.0374)	mem 460MB
[2022-10-02 20:09:28 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:28 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0197 (0.0197)	loss 0.0076 (0.0076)	grad_norm 0.0292 (0.0292)	mem 460MB
[2022-10-02 20:09:28 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:28 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0201 (0.0201)	loss 0.0074 (0.0074)	grad_norm 0.0346 (0.0346)	mem 460MB
[2022-10-02 20:09:29 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:29 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0221 (0.0221)	loss 0.0078 (0.0078)	grad_norm 0.0290 (0.0290)	mem 460MB
[2022-10-02 20:09:29 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:29 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0205 (0.0205)	loss 0.0118 (0.0118)	grad_norm 0.0794 (0.0794)	mem 460MB
[2022-10-02 20:09:29 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:29 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0215 (0.0215)	loss 0.0113 (0.0113)	grad_norm 0.0435 (0.0435)	mem 460MB
[2022-10-02 20:09:29 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:29 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0207 (0.0207)	loss 0.0076 (0.0076)	grad_norm 0.0355 (0.0355)	mem 460MB
[2022-10-02 20:09:29 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:29 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0202 (0.0202)	loss 0.0098 (0.0098)	grad_norm 0.0353 (0.0353)	mem 460MB
[2022-10-02 20:09:30 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:30 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0207 (0.0207)	loss 0.0075 (0.0075)	grad_norm 0.0353 (0.0353)	mem 460MB
[2022-10-02 20:09:30 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:30 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0201 (0.0201)	loss 0.0073 (0.0073)	grad_norm 0.0313 (0.0313)	mem 460MB
[2022-10-02 20:09:30 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:30 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0210 (0.0210)	loss 0.0081 (0.0081)	grad_norm 0.0346 (0.0346)	mem 460MB
[2022-10-02 20:09:30 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:30 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0221 (0.0221)	loss 0.0075 (0.0075)	grad_norm 0.0394 (0.0394)	mem 460MB
[2022-10-02 20:09:30 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:30 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0205 (0.0205)	loss 0.0075 (0.0075)	grad_norm 0.0331 (0.0331)	mem 460MB
[2022-10-02 20:09:30 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:31 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0218 (0.0218)	loss 0.0065 (0.0065)	grad_norm 0.0294 (0.0294)	mem 460MB
[2022-10-02 20:09:31 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:31 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0211 (0.0211)	loss 0.0060 (0.0060)	grad_norm 0.0305 (0.0305)	mem 460MB
[2022-10-02 20:09:31 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:31 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0204 (0.0204)	loss 0.0078 (0.0078)	grad_norm 0.0391 (0.0391)	mem 460MB
[2022-10-02 20:09:31 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:31 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0219 (0.0219)	loss 0.0083 (0.0083)	grad_norm 0.0311 (0.0311)	mem 460MB
[2022-10-02 20:09:31 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:31 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0198 (0.0198)	loss 0.0094 (0.0094)	grad_norm 0.0355 (0.0355)	mem 460MB
[2022-10-02 20:09:31 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:32 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0222 (0.0222)	loss 0.0098 (0.0098)	grad_norm 0.0566 (0.0566)	mem 460MB
[2022-10-02 20:09:32 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:32 demo] (houston_program2.py 243): INFO Train: [95/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0201 (0.0201)	loss 0.0100 (0.0100)	grad_norm 0.0309 (0.0309)	mem 460MB
[2022-10-02 20:09:32 demo] (houston_program2.py 252): INFO EPOCH 95 training takes 0:00:00
[2022-10-02 20:09:32 demo] (houston_program2.py 333): INFO Train Ep: 95 	Loss1: 0.026064	Loss2: 0.020200	 Dis: 3.363506 Entropy: 5.072690 
[2022-10-02 20:09:32 demo] (houston_program2.py 335): INFO time_95_epoch:8.114660263061523
[2022-10-02 20:09:32 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 20:09:32 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 20:09:32 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 20:09:32 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 20:09:32 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 20:09:32 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 20:09:32 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 20:09:32 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 20:09:32 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 20:09:32 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 20:09:39 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.031840	Loss2: 0.036432	 Dis: 2.715063 Entropy: 5.450100 
[2022-10-02 20:09:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.000487	 E_lr_2:0.000487
[2022-10-02 20:09:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 20:09:46 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.016174	Loss2: 0.014616	 Dis: 2.652433 Entropy: 4.808790 
[2022-10-02 20:09:46 demo] (houston_program2.py 512): INFO E_lr_1: 0.000987	 E_lr_2:0.000987
[2022-10-02 20:09:46 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 20:09:53 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.023359	Loss2: 0.025700	 Dis: 5.330112 Entropy: 5.187362 
[2022-10-02 20:09:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.001487	 E_lr_2:0.001487
[2022-10-02 20:09:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 20:10:00 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.171606	Loss2: 0.158027	 Dis: 2.843758 Entropy: 4.584986 
[2022-10-02 20:10:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.001987	 E_lr_2:0.001987
[2022-10-02 20:10:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 20:10:06 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.053883	Loss2: 0.056047	 Dis: 2.735966 Entropy: 5.318962 
[2022-10-02 20:10:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.002487	 E_lr_2:0.002487
[2022-10-02 20:10:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 20:10:13 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.111409	Loss2: 0.143369	 Dis: 3.721500 Entropy: 4.480876 
[2022-10-02 20:10:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.002987	 E_lr_2:0.002987
[2022-10-02 20:10:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 20:10:20 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.035554	Loss2: 0.031281	 Dis: 4.198591 Entropy: 6.170787 
[2022-10-02 20:10:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.003487	 E_lr_2:0.003487
[2022-10-02 20:10:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 20:10:27 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.303228	Loss2: 0.325731	 Dis: 8.258108 Entropy: 5.311219 
[2022-10-02 20:10:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.003987	 E_lr_2:0.003987
[2022-10-02 20:10:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 20:10:34 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.395346	Loss2: 0.330893	 Dis: 4.011541 Entropy: 6.118054 
[2022-10-02 20:10:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.004487	 E_lr_2:0.004487
[2022-10-02 20:10:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 20:10:41 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.110788	Loss2: 0.091187	 Dis: 5.201828 Entropy: 4.530200 
[2022-10-02 20:10:41 demo] (houston_program2.py 512): INFO E_lr_1: 0.004987	 E_lr_2:0.004987
[2022-10-02 20:10:41 demo] (houston_program2.py 513): INFO C_lr_1: 0.000050	 C_lr_2:0.000050	 C_lr_3:0.000050	 C_lr_4:0.000050
[2022-10-02 20:10:47 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.087546	Loss2: 0.107283	 Dis: 5.227850 Entropy: 4.492670 
[2022-10-02 20:10:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.004853	 E_lr_2:0.004853
[2022-10-02 20:10:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000049	 C_lr_2:0.000049	 C_lr_3:0.000049	 C_lr_4:0.000049
[2022-10-02 20:10:54 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.121613	Loss2: 0.123185	 Dis: 4.278084 Entropy: 5.216713 
[2022-10-02 20:10:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.004825	 E_lr_2:0.004825
[2022-10-02 20:10:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 20:11:00 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.015001	Loss2: 0.018767	 Dis: 2.900265 Entropy: 5.651209 
[2022-10-02 20:11:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.004795	 E_lr_2:0.004795
[2022-10-02 20:11:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 20:11:07 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.078408	Loss2: 0.105565	 Dis: 5.059692 Entropy: 5.111940 
[2022-10-02 20:11:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.004763	 E_lr_2:0.004763
[2022-10-02 20:11:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 20:11:14 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.041710	Loss2: 0.055947	 Dis: 3.802612 Entropy: 5.994611 
[2022-10-02 20:11:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.004728	 E_lr_2:0.004728
[2022-10-02 20:11:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 20:11:21 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.230605	Loss2: 0.227253	 Dis: 4.480572 Entropy: 4.857802 
[2022-10-02 20:11:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.004692	 E_lr_2:0.004692
[2022-10-02 20:11:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 20:11:27 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.217719	Loss2: 0.214714	 Dis: 2.439552 Entropy: 6.083594 
[2022-10-02 20:11:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.004653	 E_lr_2:0.004653
[2022-10-02 20:11:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 20:11:34 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.232637	Loss2: 0.191533	 Dis: 4.981863 Entropy: 5.499712 
[2022-10-02 20:11:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.004612	 E_lr_2:0.004612
[2022-10-02 20:11:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 20:11:40 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.123686	Loss2: 0.120249	 Dis: 3.783054 Entropy: 6.570794 
[2022-10-02 20:11:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.004569	 E_lr_2:0.004569
[2022-10-02 20:11:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 20:11:47 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.050847	Loss2: 0.046429	 Dis: 4.039516 Entropy: 5.687706 
[2022-10-02 20:11:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.004524	 E_lr_2:0.004524
[2022-10-02 20:11:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 20:11:53 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.053762	Loss2: 0.075951	 Dis: 4.934042 Entropy: 5.645103 
[2022-10-02 20:11:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.004477	 E_lr_2:0.004477
[2022-10-02 20:11:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 20:12:00 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.072339	Loss2: 0.033052	 Dis: 2.842979 Entropy: 5.010043 
[2022-10-02 20:12:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.004428	 E_lr_2:0.004428
[2022-10-02 20:12:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 20:12:06 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.071703	Loss2: 0.066185	 Dis: 3.092949 Entropy: 4.398602 
[2022-10-02 20:12:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.004377	 E_lr_2:0.004377
[2022-10-02 20:12:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 20:12:13 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.047662	Loss2: 0.048571	 Dis: 2.969215 Entropy: 4.325801 
[2022-10-02 20:12:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.004324	 E_lr_2:0.004324
[2022-10-02 20:12:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 20:12:16 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.040896	Loss2: 0.049090	 Dis: 3.748775 Entropy: 4.833689 
[2022-10-02 20:12:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.004269	 E_lr_2:0.004269
[2022-10-02 20:12:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 20:12:22 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.099041	Loss2: 0.065306	 Dis: 2.442274 Entropy: 4.859220 
[2022-10-02 20:12:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.004213	 E_lr_2:0.004213
[2022-10-02 20:12:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 20:12:29 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.017423	Loss2: 0.015770	 Dis: 4.265762 Entropy: 4.736577 
[2022-10-02 20:12:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.004155	 E_lr_2:0.004155
[2022-10-02 20:12:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 20:12:35 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.170031	Loss2: 0.181751	 Dis: 3.615486 Entropy: 5.484485 
[2022-10-02 20:12:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.004095	 E_lr_2:0.004095
[2022-10-02 20:12:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000041	 C_lr_2:0.000041	 C_lr_3:0.000041	 C_lr_4:0.000041
[2022-10-02 20:12:42 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.067041	Loss2: 0.059147	 Dis: 4.173054 Entropy: 4.889896 
[2022-10-02 20:12:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.004034	 E_lr_2:0.004034
[2022-10-02 20:12:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 20:12:48 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.037712	Loss2: 0.031803	 Dis: 3.442591 Entropy: 5.799371 
[2022-10-02 20:12:48 demo] (houston_program2.py 512): INFO E_lr_1: 0.003971	 E_lr_2:0.003971
[2022-10-02 20:12:48 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 20:12:55 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.203528	Loss2: 0.165691	 Dis: 2.445389 Entropy: 6.286729 
[2022-10-02 20:12:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.003907	 E_lr_2:0.003907
[2022-10-02 20:12:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000039	 C_lr_2:0.000039	 C_lr_3:0.000039	 C_lr_4:0.000039
[2022-10-02 20:13:00 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.108905	Loss2: 0.169624	 Dis: 3.640247 Entropy: 4.841685 
[2022-10-02 20:13:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.003841	 E_lr_2:0.003841
[2022-10-02 20:13:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 20:13:06 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.131400	Loss2: 0.117134	 Dis: 3.124180 Entropy: 4.387397 
[2022-10-02 20:13:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.003774	 E_lr_2:0.003774
[2022-10-02 20:13:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 20:13:13 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.007372	Loss2: 0.006522	 Dis: 3.178471 Entropy: 6.431426 
[2022-10-02 20:13:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.003706	 E_lr_2:0.003706
[2022-10-02 20:13:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000037	 C_lr_2:0.000037	 C_lr_3:0.000037	 C_lr_4:0.000037
[2022-10-02 20:13:19 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.145045	Loss2: 0.136769	 Dis: 5.380688 Entropy: 5.192543 
[2022-10-02 20:13:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.003637	 E_lr_2:0.003637
[2022-10-02 20:13:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 20:13:26 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.142597	Loss2: 0.105769	 Dis: 3.415537 Entropy: 5.352368 
[2022-10-02 20:13:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.003566	 E_lr_2:0.003566
[2022-10-02 20:13:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 20:13:32 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.209406	Loss2: 0.210184	 Dis: 3.438059 Entropy: 5.571927 
[2022-10-02 20:13:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.003495	 E_lr_2:0.003495
[2022-10-02 20:13:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 20:13:39 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.395800	Loss2: 0.389895	 Dis: 10.556021 Entropy: 4.201306 
[2022-10-02 20:13:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.003422	 E_lr_2:0.003422
[2022-10-02 20:13:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000034	 C_lr_2:0.000034	 C_lr_3:0.000034	 C_lr_4:0.000034
[2022-10-02 20:13:45 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.298191	Loss2: 0.252593	 Dis: 9.342016 Entropy: 4.478160 
[2022-10-02 20:13:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.003349	 E_lr_2:0.003349
[2022-10-02 20:13:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 20:13:52 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.199269	Loss2: 0.228149	 Dis: 7.635960 Entropy: 4.516233 
[2022-10-02 20:13:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.003274	 E_lr_2:0.003274
[2022-10-02 20:13:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 20:13:59 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.423374	Loss2: 0.408350	 Dis: 8.385334 Entropy: 4.564658 
[2022-10-02 20:13:59 demo] (houston_program2.py 512): INFO E_lr_1: 0.003199	 E_lr_2:0.003199
[2022-10-02 20:13:59 demo] (houston_program2.py 513): INFO C_lr_1: 0.000032	 C_lr_2:0.000032	 C_lr_3:0.000032	 C_lr_4:0.000032
[2022-10-02 20:14:06 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.183740	Loss2: 0.156006	 Dis: 7.043385 Entropy: 5.259604 
[2022-10-02 20:14:06 demo] (houston_program2.py 512): INFO E_lr_1: 0.003124	 E_lr_2:0.003124
[2022-10-02 20:14:06 demo] (houston_program2.py 513): INFO C_lr_1: 0.000031	 C_lr_2:0.000031	 C_lr_3:0.000031	 C_lr_4:0.000031
[2022-10-02 20:14:12 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.207344	Loss2: 0.235350	 Dis: 7.269104 Entropy: 5.022333 
[2022-10-02 20:14:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.003047	 E_lr_2:0.003047
[2022-10-02 20:14:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 20:14:19 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.266801	Loss2: 0.232706	 Dis: 5.520477 Entropy: 5.377691 
[2022-10-02 20:14:19 demo] (houston_program2.py 512): INFO E_lr_1: 0.002970	 E_lr_2:0.002970
[2022-10-02 20:14:19 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 20:14:26 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.120302	Loss2: 0.132623	 Dis: 5.490990 Entropy: 4.787978 
[2022-10-02 20:14:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.002893	 E_lr_2:0.002893
[2022-10-02 20:14:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000029	 C_lr_2:0.000029	 C_lr_3:0.000029	 C_lr_4:0.000029
[2022-10-02 20:14:33 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.246781	Loss2: 0.256851	 Dis: 5.048903 Entropy: 4.553809 
[2022-10-02 20:14:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.002815	 E_lr_2:0.002815
[2022-10-02 20:14:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000028	 C_lr_2:0.000028	 C_lr_3:0.000028	 C_lr_4:0.000028
[2022-10-02 20:14:40 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.311823	Loss2: 0.327339	 Dis: 5.967079 Entropy: 4.190753 
[2022-10-02 20:14:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.002737	 E_lr_2:0.002737
[2022-10-02 20:14:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 20:14:47 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.051487	Loss2: 0.053425	 Dis: 4.501022 Entropy: 4.883885 
[2022-10-02 20:14:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.002659	 E_lr_2:0.002659
[2022-10-02 20:14:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 20:14:53 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.124146	Loss2: 0.128000	 Dis: 5.212566 Entropy: 6.449039 
[2022-10-02 20:14:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.002581	 E_lr_2:0.002581
[2022-10-02 20:14:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000026	 C_lr_2:0.000026	 C_lr_3:0.000026	 C_lr_4:0.000026
[2022-10-02 20:15:00 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.138827	Loss2: 0.138587	 Dis: 5.542309 Entropy: 5.017023 
[2022-10-02 20:15:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.002502	 E_lr_2:0.002502
[2022-10-02 20:15:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 20:15:07 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.034807	Loss2: 0.033610	 Dis: 5.779009 Entropy: 5.397770 
[2022-10-02 20:15:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.002423	 E_lr_2:0.002423
[2022-10-02 20:15:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000024	 C_lr_2:0.000024	 C_lr_3:0.000024	 C_lr_4:0.000024
[2022-10-02 20:15:14 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.054238	Loss2: 0.050472	 Dis: 3.373358 Entropy: 5.213977 
[2022-10-02 20:15:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.002345	 E_lr_2:0.002345
[2022-10-02 20:15:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 20:15:21 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.090651	Loss2: 0.084771	 Dis: 4.944687 Entropy: 4.427225 
[2022-10-02 20:15:21 demo] (houston_program2.py 512): INFO E_lr_1: 0.002267	 E_lr_2:0.002267
[2022-10-02 20:15:21 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 20:15:27 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.048265	Loss2: 0.066647	 Dis: 5.169672 Entropy: 5.810538 
[2022-10-02 20:15:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.002189	 E_lr_2:0.002189
[2022-10-02 20:15:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000022	 C_lr_2:0.000022	 C_lr_3:0.000022	 C_lr_4:0.000022
[2022-10-02 20:15:34 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.056604	Loss2: 0.048005	 Dis: 3.163361 Entropy: 5.096345 
[2022-10-02 20:15:34 demo] (houston_program2.py 512): INFO E_lr_1: 0.002111	 E_lr_2:0.002111
[2022-10-02 20:15:34 demo] (houston_program2.py 513): INFO C_lr_1: 0.000021	 C_lr_2:0.000021	 C_lr_3:0.000021	 C_lr_4:0.000021
[2022-10-02 20:15:40 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.036479	Loss2: 0.047960	 Dis: 3.415188 Entropy: 4.556280 
[2022-10-02 20:15:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.002034	 E_lr_2:0.002034
[2022-10-02 20:15:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 20:15:47 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.029732	Loss2: 0.029306	 Dis: 2.194538 Entropy: 4.880172 
[2022-10-02 20:15:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.001957	 E_lr_2:0.001957
[2022-10-02 20:15:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 20:15:53 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.029139	Loss2: 0.020261	 Dis: 2.120605 Entropy: 5.818880 
[2022-10-02 20:15:53 demo] (houston_program2.py 512): INFO E_lr_1: 0.001880	 E_lr_2:0.001880
[2022-10-02 20:15:53 demo] (houston_program2.py 513): INFO C_lr_1: 0.000019	 C_lr_2:0.000019	 C_lr_3:0.000019	 C_lr_4:0.000019
[2022-10-02 20:16:00 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.014182	Loss2: 0.012800	 Dis: 3.596109 Entropy: 5.436031 
[2022-10-02 20:16:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.001804	 E_lr_2:0.001804
[2022-10-02 20:16:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000018	 C_lr_2:0.000018	 C_lr_3:0.000018	 C_lr_4:0.000018
[2022-10-02 20:16:07 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.230357	Loss2: 0.223537	 Dis: 2.901405 Entropy: 6.193563 
[2022-10-02 20:16:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.001729	 E_lr_2:0.001729
[2022-10-02 20:16:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 20:16:14 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.028248	Loss2: 0.033917	 Dis: 3.854689 Entropy: 5.204945 
[2022-10-02 20:16:14 demo] (houston_program2.py 512): INFO E_lr_1: 0.001655	 E_lr_2:0.001655
[2022-10-02 20:16:14 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 20:16:20 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.048576	Loss2: 0.048229	 Dis: 3.339504 Entropy: 4.540863 
[2022-10-02 20:16:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.001582	 E_lr_2:0.001582
[2022-10-02 20:16:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000016	 C_lr_2:0.000016	 C_lr_3:0.000016	 C_lr_4:0.000016
[2022-10-02 20:16:27 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.025300	Loss2: 0.025339	 Dis: 5.344181 Entropy: 5.027117 
[2022-10-02 20:16:27 demo] (houston_program2.py 512): INFO E_lr_1: 0.001509	 E_lr_2:0.001509
[2022-10-02 20:16:27 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 20:16:33 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.040067	Loss2: 0.042344	 Dis: 3.212744 Entropy: 4.432292 
[2022-10-02 20:16:33 demo] (houston_program2.py 512): INFO E_lr_1: 0.001437	 E_lr_2:0.001437
[2022-10-02 20:16:33 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 20:16:40 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.180316	Loss2: 0.181283	 Dis: 2.095232 Entropy: 5.733697 
[2022-10-02 20:16:40 demo] (houston_program2.py 512): INFO E_lr_1: 0.001367	 E_lr_2:0.001367
[2022-10-02 20:16:40 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 20:16:47 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.017877	Loss2: 0.023454	 Dis: 3.268957 Entropy: 5.568975 
[2022-10-02 20:16:47 demo] (houston_program2.py 512): INFO E_lr_1: 0.001297	 E_lr_2:0.001297
[2022-10-02 20:16:47 demo] (houston_program2.py 513): INFO C_lr_1: 0.000013	 C_lr_2:0.000013	 C_lr_3:0.000013	 C_lr_4:0.000013
[2022-10-02 20:16:53 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.022228	Loss2: 0.029715	 Dis: 2.665569 Entropy: 5.249093 
[2022-10-02 20:16:54 demo] (houston_program2.py 512): INFO E_lr_1: 0.001229	 E_lr_2:0.001229
[2022-10-02 20:16:54 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 20:17:00 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.014187	Loss2: 0.014710	 Dis: 2.747406 Entropy: 4.264230 
[2022-10-02 20:17:00 demo] (houston_program2.py 512): INFO E_lr_1: 0.001162	 E_lr_2:0.001162
[2022-10-02 20:17:00 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 20:17:07 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.031726	Loss2: 0.033042	 Dis: 2.429396 Entropy: 4.414902 
[2022-10-02 20:17:07 demo] (houston_program2.py 512): INFO E_lr_1: 0.001096	 E_lr_2:0.001096
[2022-10-02 20:17:07 demo] (houston_program2.py 513): INFO C_lr_1: 0.000011	 C_lr_2:0.000011	 C_lr_3:0.000011	 C_lr_4:0.000011
[2022-10-02 20:17:13 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.110890	Loss2: 0.126679	 Dis: 2.572947 Entropy: 6.062717 
[2022-10-02 20:17:13 demo] (houston_program2.py 512): INFO E_lr_1: 0.001032	 E_lr_2:0.001032
[2022-10-02 20:17:13 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 20:17:20 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.022916	Loss2: 0.020307	 Dis: 3.393616 Entropy: 4.227053 
[2022-10-02 20:17:20 demo] (houston_program2.py 512): INFO E_lr_1: 0.000969	 E_lr_2:0.000969
[2022-10-02 20:17:20 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 20:17:26 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.039883	Loss2: 0.033461	 Dis: 3.496880 Entropy: 4.333384 
[2022-10-02 20:17:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.000908	 E_lr_2:0.000908
[2022-10-02 20:17:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000009	 C_lr_2:0.000009	 C_lr_3:0.000009	 C_lr_4:0.000009
[2022-10-02 20:17:29 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.019034	Loss2: 0.017722	 Dis: 1.670393 Entropy: 4.499730 
[2022-10-02 20:17:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.000848	 E_lr_2:0.000848
[2022-10-02 20:17:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 20:17:35 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.008675	Loss2: 0.006207	 Dis: 2.834703 Entropy: 5.100754 
[2022-10-02 20:17:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.000790	 E_lr_2:0.000790
[2022-10-02 20:17:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 20:17:42 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.084261	Loss2: 0.086926	 Dis: 2.619795 Entropy: 4.128643 
[2022-10-02 20:17:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.000734	 E_lr_2:0.000734
[2022-10-02 20:17:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 20:17:49 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.191341	Loss2: 0.171054	 Dis: 2.759254 Entropy: 4.722347 
[2022-10-02 20:17:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.000679	 E_lr_2:0.000679
[2022-10-02 20:17:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 20:17:55 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.016520	Loss2: 0.022677	 Dis: 2.012894 Entropy: 4.508202 
[2022-10-02 20:17:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.000626	 E_lr_2:0.000626
[2022-10-02 20:17:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 20:18:02 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.023445	Loss2: 0.021035	 Dis: 3.365074 Entropy: 4.665326 
[2022-10-02 20:18:02 demo] (houston_program2.py 512): INFO E_lr_1: 0.000575	 E_lr_2:0.000575
[2022-10-02 20:18:02 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 20:18:08 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.025145	Loss2: 0.033672	 Dis: 3.666870 Entropy: 4.738047 
[2022-10-02 20:18:08 demo] (houston_program2.py 512): INFO E_lr_1: 0.000526	 E_lr_2:0.000526
[2022-10-02 20:18:08 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 20:18:15 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.026823	Loss2: 0.046766	 Dis: 1.656931 Entropy: 5.800478 
[2022-10-02 20:18:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.000479	 E_lr_2:0.000479
[2022-10-02 20:18:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 20:18:22 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.085330	Loss2: 0.100588	 Dis: 2.319746 Entropy: 4.719510 
[2022-10-02 20:18:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.000433	 E_lr_2:0.000433
[2022-10-02 20:18:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 20:18:28 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.006817	Loss2: 0.006902	 Dis: 2.733530 Entropy: 4.448521 
[2022-10-02 20:18:28 demo] (houston_program2.py 512): INFO E_lr_1: 0.000390	 E_lr_2:0.000390
[2022-10-02 20:18:28 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 20:18:35 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.018687	Loss2: 0.017088	 Dis: 2.220516 Entropy: 5.902872 
[2022-10-02 20:18:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.000349	 E_lr_2:0.000349
[2022-10-02 20:18:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 20:18:42 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.014079	Loss2: 0.017465	 Dis: 0.858547 Entropy: 5.315216 
[2022-10-02 20:18:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.000310	 E_lr_2:0.000310
[2022-10-02 20:18:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 20:18:49 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.008035	Loss2: 0.007522	 Dis: 1.800131 Entropy: 5.142865 
[2022-10-02 20:18:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.000273	 E_lr_2:0.000273
[2022-10-02 20:18:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 20:18:55 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.018259	Loss2: 0.010810	 Dis: 2.946016 Entropy: 5.254405 
[2022-10-02 20:18:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.000239	 E_lr_2:0.000239
[2022-10-02 20:18:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 20:19:02 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.004389	Loss2: 0.003604	 Dis: 1.285261 Entropy: 4.407196 
[2022-10-02 20:19:02 demo] (houston_program2.py 512): INFO E_lr_1: 0.000206	 E_lr_2:0.000206
[2022-10-02 20:19:02 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 20:19:08 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.034014	Loss2: 0.031790	 Dis: 3.034340 Entropy: 5.974244 
[2022-10-02 20:19:08 demo] (houston_program2.py 512): INFO E_lr_1: 0.000176	 E_lr_2:0.000176
[2022-10-02 20:19:08 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 20:19:15 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.010030	Loss2: 0.011243	 Dis: 2.280893 Entropy: 4.625941 
[2022-10-02 20:19:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.000148	 E_lr_2:0.000148
[2022-10-02 20:19:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 20:19:22 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.002124	Loss2: 0.002080	 Dis: 2.800129 Entropy: 5.207223 
[2022-10-02 20:19:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.000123	 E_lr_2:0.000123
[2022-10-02 20:19:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 20:19:29 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.014925	Loss2: 0.019474	 Dis: 3.060112 Entropy: 4.385925 
[2022-10-02 20:19:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.000100	 E_lr_2:0.000100
[2022-10-02 20:19:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 20:19:35 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.064081	Loss2: 0.046650	 Dis: 1.705494 Entropy: 4.833393 
[2022-10-02 20:19:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.000079	 E_lr_2:0.000079
[2022-10-02 20:19:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 20:19:42 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.003456	Loss2: 0.002649	 Dis: 1.496323 Entropy: 5.819523 
[2022-10-02 20:19:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.000061	 E_lr_2:0.000061
[2022-10-02 20:19:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 20:19:49 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.040816	Loss2: 0.031988	 Dis: 4.509041 Entropy: 4.657089 
[2022-10-02 20:19:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.000045	 E_lr_2:0.000045
[2022-10-02 20:19:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 20:19:56 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.005083	Loss2: 0.005399	 Dis: 2.357649 Entropy: 4.797688 
[2022-10-02 20:19:56 demo] (houston_program2.py 512): INFO E_lr_1: 0.000031	 E_lr_2:0.000031
[2022-10-02 20:19:56 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 20:20:03 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.016762	Loss2: 0.015365	 Dis: 3.083851 Entropy: 4.633341 
[2022-10-02 20:20:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.000020	 E_lr_2:0.000020
[2022-10-02 20:20:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 20:20:09 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.007509	Loss2: 0.009233	 Dis: 2.645237 Entropy: 4.897573 
[2022-10-02 20:20:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.000011	 E_lr_2:0.000011
[2022-10-02 20:20:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 20:20:16 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.022425	Loss2: 0.016422	 Dis: 2.106150 Entropy: 5.247249 
[2022-10-02 20:20:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.000005	 E_lr_2:0.000005
[2022-10-02 20:20:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 20:20:23 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.008774	Loss2: 0.012497	 Dis: 2.578274 Entropy: 4.543901 
[2022-10-02 20:20:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.000001	 E_lr_2:0.000001
[2022-10-02 20:20:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 20:20:30 demo] (houston_program2.py 504): INFO Train Ep: 95 	Loss1: 0.005483	Loss2: 0.004304	 Dis: 0.641033 Entropy: 4.743756 
[2022-10-02 20:20:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.000000	 E_lr_2:0.000000
[2022-10-02 20:20:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 20:20:30 demo] (houston_program2.py 515): INFO time_95_epoch:657.5325193405151
[2022-10-02 20:20:37 demo] (houston_program2.py 673): INFO 	val_Accuracy: 32050/53200 (60.24%)	
[2022-10-02 20:20:37 demo] (utils.py 47): INFO outputs/demo/finetune_100/ckpt_epoch_95.pth saving......
[2022-10-02 20:20:37 demo] (utils.py 49): INFO outputs/demo/finetune_100/ckpt_epoch_95.pth saved !!!
[2022-10-02 20:20:38 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0114 (0.0114)	loss 0.0089 (0.0089)	grad_norm 0.0324 (0.0324)	mem 460MB
[2022-10-02 20:20:38 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:38 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0085 (0.0085)	loss 0.0083 (0.0083)	grad_norm 0.0409 (0.0409)	mem 460MB
[2022-10-02 20:20:38 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:38 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0086 (0.0086)	loss 0.0133 (0.0133)	grad_norm 0.0329 (0.0329)	mem 460MB
[2022-10-02 20:20:38 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:38 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0095 (0.0095)	loss 0.0098 (0.0098)	grad_norm 0.0389 (0.0389)	mem 460MB
[2022-10-02 20:20:38 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:38 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0085 (0.0085)	loss 0.0074 (0.0074)	grad_norm 0.0280 (0.0280)	mem 460MB
[2022-10-02 20:20:38 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:38 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0080 (0.0080)	loss 0.0091 (0.0091)	grad_norm 0.0631 (0.0631)	mem 460MB
[2022-10-02 20:20:38 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:38 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0081 (0.0081)	loss 0.0069 (0.0069)	grad_norm 0.0313 (0.0313)	mem 460MB
[2022-10-02 20:20:38 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:38 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0084 (0.0084)	loss 0.0118 (0.0118)	grad_norm 0.0501 (0.0501)	mem 460MB
[2022-10-02 20:20:38 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:38 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0084 (0.0084)	loss 0.0073 (0.0073)	grad_norm 0.0335 (0.0335)	mem 460MB
[2022-10-02 20:20:38 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:38 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0084 (0.0084)	loss 0.0086 (0.0086)	grad_norm 0.0415 (0.0415)	mem 460MB
[2022-10-02 20:20:39 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:39 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0084 (0.0084)	loss 0.0078 (0.0078)	grad_norm 0.0358 (0.0358)	mem 460MB
[2022-10-02 20:20:39 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:39 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0096 (0.0096)	loss 0.0122 (0.0122)	grad_norm 0.0481 (0.0481)	mem 460MB
[2022-10-02 20:20:39 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:39 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0083 (0.0083)	loss 0.0097 (0.0097)	grad_norm 0.0551 (0.0551)	mem 460MB
[2022-10-02 20:20:39 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:39 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0090 (0.0090)	loss 0.0098 (0.0098)	grad_norm 0.0470 (0.0470)	mem 460MB
[2022-10-02 20:20:39 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:39 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0084 (0.0084)	loss 0.0080 (0.0080)	grad_norm 0.0343 (0.0343)	mem 460MB
[2022-10-02 20:20:39 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:39 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0085 (0.0085)	loss 0.0081 (0.0081)	grad_norm 0.0253 (0.0253)	mem 460MB
[2022-10-02 20:20:39 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:39 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0084 (0.0084)	loss 0.0094 (0.0094)	grad_norm 0.0308 (0.0308)	mem 460MB
[2022-10-02 20:20:39 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:39 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0087 (0.0087)	loss 0.0101 (0.0101)	grad_norm 0.0395 (0.0395)	mem 460MB
[2022-10-02 20:20:39 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:39 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0167 (0.0167)	loss 0.0090 (0.0090)	grad_norm 0.0370 (0.0370)	mem 460MB
[2022-10-02 20:20:39 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:39 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0221 (0.0221)	loss 0.0097 (0.0097)	grad_norm 0.0341 (0.0341)	mem 460MB
[2022-10-02 20:20:39 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:40 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0205 (0.0205)	loss 0.0090 (0.0090)	grad_norm 0.0653 (0.0653)	mem 460MB
[2022-10-02 20:20:40 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:40 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0224 (0.0224)	loss 0.0085 (0.0085)	grad_norm 0.0398 (0.0398)	mem 460MB
[2022-10-02 20:20:40 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:40 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0213 (0.0213)	loss 0.0093 (0.0093)	grad_norm 0.0417 (0.0417)	mem 460MB
[2022-10-02 20:20:40 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:40 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0213 (0.0213)	loss 0.0076 (0.0076)	grad_norm 0.0274 (0.0274)	mem 460MB
[2022-10-02 20:20:40 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:40 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0201 (0.0201)	loss 0.0084 (0.0084)	grad_norm 0.0445 (0.0445)	mem 460MB
[2022-10-02 20:20:40 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:41 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0218 (0.0218)	loss 0.0071 (0.0071)	grad_norm 0.0253 (0.0253)	mem 460MB
[2022-10-02 20:20:41 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:41 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0205 (0.0205)	loss 0.0084 (0.0084)	grad_norm 0.0317 (0.0317)	mem 460MB
[2022-10-02 20:20:41 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:41 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0216 (0.0216)	loss 0.0072 (0.0072)	grad_norm 0.0294 (0.0294)	mem 460MB
[2022-10-02 20:20:41 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:41 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0205 (0.0205)	loss 0.0109 (0.0109)	grad_norm 0.0397 (0.0397)	mem 460MB
[2022-10-02 20:20:41 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:41 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0188 (0.0188)	loss 0.0096 (0.0096)	grad_norm 0.0596 (0.0596)	mem 460MB
[2022-10-02 20:20:41 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:42 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0218 (0.0218)	loss 0.0077 (0.0077)	grad_norm 0.0380 (0.0380)	mem 460MB
[2022-10-02 20:20:42 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:42 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0207 (0.0207)	loss 0.0060 (0.0060)	grad_norm 0.0290 (0.0290)	mem 460MB
[2022-10-02 20:20:42 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:42 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0225 (0.0225)	loss 0.0080 (0.0080)	grad_norm 0.0383 (0.0383)	mem 460MB
[2022-10-02 20:20:42 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:42 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0210 (0.0210)	loss 0.0095 (0.0095)	grad_norm 0.0337 (0.0337)	mem 460MB
[2022-10-02 20:20:42 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:42 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0217 (0.0217)	loss 0.0077 (0.0077)	grad_norm 0.0532 (0.0532)	mem 460MB
[2022-10-02 20:20:42 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:42 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0209 (0.0209)	loss 0.0135 (0.0135)	grad_norm 0.0391 (0.0391)	mem 460MB
[2022-10-02 20:20:43 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:43 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0204 (0.0204)	loss 0.0111 (0.0111)	grad_norm 0.0335 (0.0335)	mem 460MB
[2022-10-02 20:20:43 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:43 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0211 (0.0211)	loss 0.0070 (0.0070)	grad_norm 0.0258 (0.0258)	mem 460MB
[2022-10-02 20:20:43 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:43 demo] (houston_program2.py 243): INFO Train: [96/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0217 (0.0217)	loss 0.0091 (0.0091)	grad_norm 0.0344 (0.0344)	mem 460MB
[2022-10-02 20:20:43 demo] (houston_program2.py 252): INFO EPOCH 96 training takes 0:00:00
[2022-10-02 20:20:43 demo] (houston_program2.py 333): INFO Train Ep: 96 	Loss1: 0.071836	Loss2: 0.084599	 Dis: 2.827953 Entropy: 5.665277 
[2022-10-02 20:20:43 demo] (houston_program2.py 335): INFO time_96_epoch:5.8949761390686035
[2022-10-02 20:20:44 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0120 (0.0120)	loss 0.0099 (0.0099)	grad_norm 0.0325 (0.0325)	mem 460MB
[2022-10-02 20:20:44 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:44 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0205 (0.0205)	loss 0.0077 (0.0077)	grad_norm 0.0405 (0.0405)	mem 460MB
[2022-10-02 20:20:44 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:44 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0207 (0.0207)	loss 0.0077 (0.0077)	grad_norm 0.0277 (0.0277)	mem 460MB
[2022-10-02 20:20:44 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:44 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0212 (0.0212)	loss 0.0070 (0.0070)	grad_norm 0.0494 (0.0494)	mem 460MB
[2022-10-02 20:20:44 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:44 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0214 (0.0214)	loss 0.0079 (0.0079)	grad_norm 0.0781 (0.0781)	mem 460MB
[2022-10-02 20:20:44 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:45 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0224 (0.0224)	loss 0.0128 (0.0128)	grad_norm 0.0442 (0.0442)	mem 460MB
[2022-10-02 20:20:45 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:45 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0216 (0.0216)	loss 0.0091 (0.0091)	grad_norm 0.0372 (0.0372)	mem 460MB
[2022-10-02 20:20:45 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:45 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0222 (0.0222)	loss 0.0078 (0.0078)	grad_norm 0.0292 (0.0292)	mem 460MB
[2022-10-02 20:20:45 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:45 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0216 (0.0216)	loss 0.0090 (0.0090)	grad_norm 0.0311 (0.0311)	mem 460MB
[2022-10-02 20:20:45 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:45 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0207 (0.0207)	loss 0.0103 (0.0103)	grad_norm 0.0578 (0.0578)	mem 460MB
[2022-10-02 20:20:45 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:46 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0190 (0.0190)	loss 0.0099 (0.0099)	grad_norm 0.0395 (0.0395)	mem 460MB
[2022-10-02 20:20:46 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:46 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0218 (0.0218)	loss 0.0078 (0.0078)	grad_norm 0.0378 (0.0378)	mem 460MB
[2022-10-02 20:20:46 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:46 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0207 (0.0207)	loss 0.0090 (0.0090)	grad_norm 0.0357 (0.0357)	mem 460MB
[2022-10-02 20:20:46 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:46 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0211 (0.0211)	loss 0.0094 (0.0094)	grad_norm 0.0617 (0.0617)	mem 460MB
[2022-10-02 20:20:46 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:46 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0221 (0.0221)	loss 0.0084 (0.0084)	grad_norm 0.0392 (0.0392)	mem 460MB
[2022-10-02 20:20:46 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:47 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0209 (0.0209)	loss 0.0063 (0.0063)	grad_norm 0.0310 (0.0310)	mem 460MB
[2022-10-02 20:20:47 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:47 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0220 (0.0220)	loss 0.0096 (0.0096)	grad_norm 0.0372 (0.0372)	mem 460MB
[2022-10-02 20:20:47 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:47 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0205 (0.0205)	loss 0.0086 (0.0086)	grad_norm 0.0349 (0.0349)	mem 460MB
[2022-10-02 20:20:47 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:47 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0219 (0.0219)	loss 0.0088 (0.0088)	grad_norm 0.0351 (0.0351)	mem 460MB
[2022-10-02 20:20:47 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:47 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0224 (0.0224)	loss 0.0076 (0.0076)	grad_norm 0.0367 (0.0367)	mem 460MB
[2022-10-02 20:20:47 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:48 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0202 (0.0202)	loss 0.0082 (0.0082)	grad_norm 0.0363 (0.0363)	mem 460MB
[2022-10-02 20:20:48 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:48 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0219 (0.0219)	loss 0.0060 (0.0060)	grad_norm 0.0326 (0.0326)	mem 460MB
[2022-10-02 20:20:48 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:48 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0200 (0.0200)	loss 0.0096 (0.0096)	grad_norm 0.0553 (0.0553)	mem 460MB
[2022-10-02 20:20:48 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:48 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0208 (0.0208)	loss 0.0077 (0.0077)	grad_norm 0.0371 (0.0371)	mem 460MB
[2022-10-02 20:20:48 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:48 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0218 (0.0218)	loss 0.0069 (0.0069)	grad_norm 0.0378 (0.0378)	mem 460MB
[2022-10-02 20:20:48 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:49 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0209 (0.0209)	loss 0.0072 (0.0072)	grad_norm 0.0331 (0.0331)	mem 460MB
[2022-10-02 20:20:49 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:49 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0204 (0.0204)	loss 0.0087 (0.0087)	grad_norm 0.0310 (0.0310)	mem 460MB
[2022-10-02 20:20:49 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:49 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0205 (0.0205)	loss 0.0091 (0.0091)	grad_norm 0.0390 (0.0390)	mem 460MB
[2022-10-02 20:20:49 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:49 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0201 (0.0201)	loss 0.0127 (0.0127)	grad_norm 0.0344 (0.0344)	mem 460MB
[2022-10-02 20:20:49 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:49 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0190 (0.0190)	loss 0.0070 (0.0070)	grad_norm 0.0309 (0.0309)	mem 460MB
[2022-10-02 20:20:49 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:49 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0220 (0.0220)	loss 0.0088 (0.0088)	grad_norm 0.0374 (0.0374)	mem 460MB
[2022-10-02 20:20:50 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:50 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0223 (0.0223)	loss 0.0083 (0.0083)	grad_norm 0.0447 (0.0447)	mem 460MB
[2022-10-02 20:20:50 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:50 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0205 (0.0205)	loss 0.0076 (0.0076)	grad_norm 0.0219 (0.0219)	mem 460MB
[2022-10-02 20:20:50 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:50 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0218 (0.0218)	loss 0.0072 (0.0072)	grad_norm 0.0339 (0.0339)	mem 460MB
[2022-10-02 20:20:50 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:50 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0207 (0.0207)	loss 0.0075 (0.0075)	grad_norm 0.0443 (0.0443)	mem 460MB
[2022-10-02 20:20:50 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:50 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0216 (0.0216)	loss 0.0073 (0.0073)	grad_norm 0.0362 (0.0362)	mem 460MB
[2022-10-02 20:20:50 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:51 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0219 (0.0219)	loss 0.0082 (0.0082)	grad_norm 0.0370 (0.0370)	mem 460MB
[2022-10-02 20:20:51 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:51 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0205 (0.0205)	loss 0.0111 (0.0111)	grad_norm 0.0553 (0.0553)	mem 460MB
[2022-10-02 20:20:51 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:51 demo] (houston_program2.py 243): INFO Train: [97/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0207 (0.0207)	loss 0.0134 (0.0134)	grad_norm 0.0546 (0.0546)	mem 460MB
[2022-10-02 20:20:51 demo] (houston_program2.py 252): INFO EPOCH 97 training takes 0:00:00
[2022-10-02 20:20:51 demo] (houston_program2.py 333): INFO Train Ep: 97 	Loss1: 0.034707	Loss2: 0.035642	 Dis: 4.012482 Entropy: 4.623904 
[2022-10-02 20:20:51 demo] (houston_program2.py 335): INFO time_97_epoch:7.990654945373535
[2022-10-02 20:20:52 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0130 (0.0130)	loss 0.0075 (0.0075)	grad_norm 0.0304 (0.0304)	mem 460MB
[2022-10-02 20:20:52 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:52 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0215 (0.0215)	loss 0.0106 (0.0106)	grad_norm 0.0510 (0.0510)	mem 460MB
[2022-10-02 20:20:52 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:52 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0198 (0.0198)	loss 0.0112 (0.0112)	grad_norm 0.0700 (0.0700)	mem 460MB
[2022-10-02 20:20:52 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:52 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0218 (0.0218)	loss 0.0115 (0.0115)	grad_norm 0.0421 (0.0421)	mem 460MB
[2022-10-02 20:20:52 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:52 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0207 (0.0207)	loss 0.0081 (0.0081)	grad_norm 0.0269 (0.0269)	mem 460MB
[2022-10-02 20:20:52 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:53 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0217 (0.0217)	loss 0.0069 (0.0069)	grad_norm 0.0366 (0.0366)	mem 460MB
[2022-10-02 20:20:53 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:53 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0216 (0.0216)	loss 0.0102 (0.0102)	grad_norm 0.0343 (0.0343)	mem 460MB
[2022-10-02 20:20:53 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:53 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0207 (0.0207)	loss 0.0106 (0.0106)	grad_norm 0.0615 (0.0615)	mem 460MB
[2022-10-02 20:20:53 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:53 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0222 (0.0222)	loss 0.0104 (0.0104)	grad_norm 0.0419 (0.0419)	mem 460MB
[2022-10-02 20:20:53 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:53 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0197 (0.0197)	loss 0.0098 (0.0098)	grad_norm 0.0311 (0.0311)	mem 460MB
[2022-10-02 20:20:53 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:54 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0182 (0.0182)	loss 0.0073 (0.0073)	grad_norm 0.0356 (0.0356)	mem 460MB
[2022-10-02 20:20:54 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:54 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0218 (0.0218)	loss 0.0068 (0.0068)	grad_norm 0.0265 (0.0265)	mem 460MB
[2022-10-02 20:20:54 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:54 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0201 (0.0201)	loss 0.0093 (0.0093)	grad_norm 0.0348 (0.0348)	mem 460MB
[2022-10-02 20:20:54 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:54 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0206 (0.0206)	loss 0.0092 (0.0092)	grad_norm 0.0343 (0.0343)	mem 460MB
[2022-10-02 20:20:54 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:54 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0205 (0.0205)	loss 0.0080 (0.0080)	grad_norm 0.0326 (0.0326)	mem 460MB
[2022-10-02 20:20:54 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:55 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0220 (0.0220)	loss 0.0092 (0.0092)	grad_norm 0.0386 (0.0386)	mem 460MB
[2022-10-02 20:20:55 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:55 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0206 (0.0206)	loss 0.0067 (0.0067)	grad_norm 0.0317 (0.0317)	mem 460MB
[2022-10-02 20:20:55 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:55 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0221 (0.0221)	loss 0.0093 (0.0093)	grad_norm 0.0405 (0.0405)	mem 460MB
[2022-10-02 20:20:55 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:55 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0202 (0.0202)	loss 0.0078 (0.0078)	grad_norm 0.0290 (0.0290)	mem 460MB
[2022-10-02 20:20:55 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:55 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0212 (0.0212)	loss 0.0072 (0.0072)	grad_norm 0.0304 (0.0304)	mem 460MB
[2022-10-02 20:20:55 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:56 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0204 (0.0204)	loss 0.0105 (0.0105)	grad_norm 0.0293 (0.0293)	mem 460MB
[2022-10-02 20:20:56 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:56 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0220 (0.0220)	loss 0.0114 (0.0114)	grad_norm 0.0428 (0.0428)	mem 460MB
[2022-10-02 20:20:56 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:56 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0217 (0.0217)	loss 0.0071 (0.0071)	grad_norm 0.0266 (0.0266)	mem 460MB
[2022-10-02 20:20:56 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:56 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0203 (0.0203)	loss 0.0121 (0.0121)	grad_norm 0.0327 (0.0327)	mem 460MB
[2022-10-02 20:20:56 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:56 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0204 (0.0204)	loss 0.0137 (0.0137)	grad_norm 0.0663 (0.0663)	mem 460MB
[2022-10-02 20:20:56 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:57 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0205 (0.0205)	loss 0.0062 (0.0062)	grad_norm 0.0324 (0.0324)	mem 460MB
[2022-10-02 20:20:57 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:57 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0217 (0.0217)	loss 0.0083 (0.0083)	grad_norm 0.0410 (0.0410)	mem 460MB
[2022-10-02 20:20:57 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:57 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0219 (0.0219)	loss 0.0080 (0.0080)	grad_norm 0.0417 (0.0417)	mem 460MB
[2022-10-02 20:20:57 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:57 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0219 (0.0219)	loss 0.0060 (0.0060)	grad_norm 0.0294 (0.0294)	mem 460MB
[2022-10-02 20:20:57 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:57 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0185 (0.0185)	loss 0.0101 (0.0101)	grad_norm 0.0402 (0.0402)	mem 460MB
[2022-10-02 20:20:57 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:57 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0203 (0.0203)	loss 0.0077 (0.0077)	grad_norm 0.0278 (0.0278)	mem 460MB
[2022-10-02 20:20:57 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:58 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0216 (0.0216)	loss 0.0078 (0.0078)	grad_norm 0.0336 (0.0336)	mem 460MB
[2022-10-02 20:20:58 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:58 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0223 (0.0223)	loss 0.0084 (0.0084)	grad_norm 0.0316 (0.0316)	mem 460MB
[2022-10-02 20:20:58 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:58 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0219 (0.0219)	loss 0.0084 (0.0084)	grad_norm 0.0283 (0.0283)	mem 460MB
[2022-10-02 20:20:58 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:58 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0204 (0.0204)	loss 0.0089 (0.0089)	grad_norm 0.0402 (0.0402)	mem 460MB
[2022-10-02 20:20:58 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:58 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0218 (0.0218)	loss 0.0103 (0.0103)	grad_norm 0.0463 (0.0463)	mem 460MB
[2022-10-02 20:20:58 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:59 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0222 (0.0222)	loss 0.0113 (0.0113)	grad_norm 0.0643 (0.0643)	mem 460MB
[2022-10-02 20:20:59 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:59 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0208 (0.0208)	loss 0.0075 (0.0075)	grad_norm 0.0749 (0.0749)	mem 460MB
[2022-10-02 20:20:59 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:59 demo] (houston_program2.py 243): INFO Train: [98/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0209 (0.0209)	loss 0.0081 (0.0081)	grad_norm 0.0372 (0.0372)	mem 460MB
[2022-10-02 20:20:59 demo] (houston_program2.py 252): INFO EPOCH 98 training takes 0:00:00
[2022-10-02 20:20:59 demo] (houston_program2.py 333): INFO Train Ep: 98 	Loss1: 0.029938	Loss2: 0.049656	 Dis: 3.478849 Entropy: 5.681632 
[2022-10-02 20:20:59 demo] (houston_program2.py 335): INFO time_98_epoch:7.9528648853302
[2022-10-02 20:21:00 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0109 (0.0109)	loss 0.0070 (0.0070)	grad_norm 0.0540 (0.0540)	mem 460MB
[2022-10-02 20:21:00 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:00 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0080 (0.0080)	loss 0.0081 (0.0081)	grad_norm 0.0303 (0.0303)	mem 460MB
[2022-10-02 20:21:00 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:00 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0097 (0.0097)	loss 0.0078 (0.0078)	grad_norm 0.0349 (0.0349)	mem 460MB
[2022-10-02 20:21:00 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:00 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0096 (0.0096)	loss 0.0079 (0.0079)	grad_norm 0.0349 (0.0349)	mem 460MB
[2022-10-02 20:21:00 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:00 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0091 (0.0091)	loss 0.0090 (0.0090)	grad_norm 0.0790 (0.0790)	mem 460MB
[2022-10-02 20:21:00 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:00 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0080 (0.0080)	loss 0.0070 (0.0070)	grad_norm 0.0367 (0.0367)	mem 460MB
[2022-10-02 20:21:00 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:00 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0079 (0.0079)	loss 0.0129 (0.0129)	grad_norm 0.0475 (0.0475)	mem 460MB
[2022-10-02 20:21:00 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:00 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0084 (0.0084)	loss 0.0118 (0.0118)	grad_norm 0.0449 (0.0449)	mem 460MB
[2022-10-02 20:21:00 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:00 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0088 (0.0088)	loss 0.0083 (0.0083)	grad_norm 0.0449 (0.0449)	mem 460MB
[2022-10-02 20:21:00 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:00 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0085 (0.0085)	loss 0.0082 (0.0082)	grad_norm 0.0350 (0.0350)	mem 460MB
[2022-10-02 20:21:00 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:00 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0084 (0.0084)	loss 0.0072 (0.0072)	grad_norm 0.0368 (0.0368)	mem 460MB
[2022-10-02 20:21:00 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:01 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0086 (0.0086)	loss 0.0096 (0.0096)	grad_norm 0.0417 (0.0417)	mem 460MB
[2022-10-02 20:21:01 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:01 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0085 (0.0085)	loss 0.0076 (0.0076)	grad_norm 0.0418 (0.0418)	mem 460MB
[2022-10-02 20:21:01 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:01 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0094 (0.0094)	loss 0.0085 (0.0085)	grad_norm 0.0439 (0.0439)	mem 460MB
[2022-10-02 20:21:01 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:01 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0085 (0.0085)	loss 0.0102 (0.0102)	grad_norm 0.0363 (0.0363)	mem 460MB
[2022-10-02 20:21:01 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:01 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0083 (0.0083)	loss 0.0078 (0.0078)	grad_norm 0.0322 (0.0322)	mem 460MB
[2022-10-02 20:21:01 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:01 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0083 (0.0083)	loss 0.0070 (0.0070)	grad_norm 0.0429 (0.0429)	mem 460MB
[2022-10-02 20:21:01 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:01 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0085 (0.0085)	loss 0.0118 (0.0118)	grad_norm 0.0355 (0.0355)	mem 460MB
[2022-10-02 20:21:01 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:01 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0084 (0.0084)	loss 0.0099 (0.0099)	grad_norm 0.0416 (0.0416)	mem 460MB
[2022-10-02 20:21:01 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:01 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0085 (0.0085)	loss 0.0074 (0.0074)	grad_norm 0.0303 (0.0303)	mem 460MB
[2022-10-02 20:21:01 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:01 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0083 (0.0083)	loss 0.0068 (0.0068)	grad_norm 0.0387 (0.0387)	mem 460MB
[2022-10-02 20:21:01 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:01 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0083 (0.0083)	loss 0.0104 (0.0104)	grad_norm 0.0391 (0.0391)	mem 460MB
[2022-10-02 20:21:01 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:01 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0092 (0.0092)	loss 0.0097 (0.0097)	grad_norm 0.0445 (0.0445)	mem 460MB
[2022-10-02 20:21:01 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:01 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0088 (0.0088)	loss 0.0091 (0.0091)	grad_norm 0.0540 (0.0540)	mem 460MB
[2022-10-02 20:21:01 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:02 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0087 (0.0087)	loss 0.0123 (0.0123)	grad_norm 0.0473 (0.0473)	mem 460MB
[2022-10-02 20:21:02 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:02 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0083 (0.0083)	loss 0.0124 (0.0124)	grad_norm 0.0338 (0.0338)	mem 460MB
[2022-10-02 20:21:02 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:02 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0084 (0.0084)	loss 0.0093 (0.0093)	grad_norm 0.0413 (0.0413)	mem 460MB
[2022-10-02 20:21:02 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:02 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0084 (0.0084)	loss 0.0089 (0.0089)	grad_norm 0.0610 (0.0610)	mem 460MB
[2022-10-02 20:21:02 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:02 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0082 (0.0082)	loss 0.0066 (0.0066)	grad_norm 0.0299 (0.0299)	mem 460MB
[2022-10-02 20:21:02 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:02 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0083 (0.0083)	loss 0.0096 (0.0096)	grad_norm 0.0338 (0.0338)	mem 460MB
[2022-10-02 20:21:02 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:02 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0084 (0.0084)	loss 0.0075 (0.0075)	grad_norm 0.0460 (0.0460)	mem 460MB
[2022-10-02 20:21:02 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:02 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0084 (0.0084)	loss 0.0123 (0.0123)	grad_norm 0.0302 (0.0302)	mem 460MB
[2022-10-02 20:21:02 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:02 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0081 (0.0081)	loss 0.0091 (0.0091)	grad_norm 0.0287 (0.0287)	mem 460MB
[2022-10-02 20:21:02 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:02 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0094 (0.0094)	loss 0.0077 (0.0077)	grad_norm 0.0308 (0.0308)	mem 460MB
[2022-10-02 20:21:02 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:02 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0087 (0.0087)	loss 0.0071 (0.0071)	grad_norm 0.0325 (0.0325)	mem 460MB
[2022-10-02 20:21:02 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:02 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0088 (0.0088)	loss 0.0078 (0.0078)	grad_norm 0.0325 (0.0325)	mem 460MB
[2022-10-02 20:21:02 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:02 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0084 (0.0084)	loss 0.0099 (0.0099)	grad_norm 0.0385 (0.0385)	mem 460MB
[2022-10-02 20:21:02 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:02 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0091 (0.0091)	loss 0.0083 (0.0083)	grad_norm 0.0294 (0.0294)	mem 460MB
[2022-10-02 20:21:02 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:03 demo] (houston_program2.py 243): INFO Train: [99/100][0/2]	eta 0:00:00 lr 0.000000	time 0.0083 (0.0083)	loss 0.0109 (0.0109)	grad_norm 0.0572 (0.0572)	mem 460MB
[2022-10-02 20:21:03 demo] (houston_program2.py 252): INFO EPOCH 99 training takes 0:00:00
[2022-10-02 20:21:03 demo] (houston_program2.py 333): INFO Train Ep: 99 	Loss1: 0.036464	Loss2: 0.026058	 Dis: 2.794489 Entropy: 6.328987 
[2022-10-02 20:21:03 demo] (houston_program2.py 335): INFO time_99_epoch:3.438079833984375
[2022-10-02 20:21:03 demo] (optimizer.py 109): INFO >>>>>>>>>> Build Optimizer for Fine-tuning Stage
[2022-10-02 20:21:03 demo] (optimizer.py 63): INFO No decay params: ['attn_layers.layers.0.0.weight', 'attn_layers.layers.0.0.bias', 'attn_layers.layers.0.1.to_out.bias', 'attn_layers.layers.1.0.weight', 'attn_layers.layers.1.0.bias', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.2.bias', 'mlp.bias']
[2022-10-02 20:21:03 demo] (optimizer.py 64): INFO Has decay params: ['attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.2.weight', 'mlp.weight']
[2022-10-02 20:21:03 demo] (optimizer.py 166): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 20:21:03 demo] (optimizer.py 260): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-10-02 20:21:03 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 20:21:03 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 20:21:03 demo] (optimizer.py 63): INFO No decay params: ['fc11.bias', 'fc12.bias', 'fc13.bias', 'classifier1.2.weight', 'classifier1.2.bias', 'classifier1.5.weight', 'classifier1.5.bias']
[2022-10-02 20:21:03 demo] (optimizer.py 64): INFO Has decay params: ['fc11.weight', 'fc12.weight', 'fc13.weight']
[2022-10-02 20:21:03 demo] (optimizer.py 276): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 3
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-10-02 20:21:09 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.021706	Loss2: 0.038824	 Dis: 2.699909 Entropy: 4.888571 
[2022-10-02 20:21:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.000487	 E_lr_2:0.000487
[2022-10-02 20:21:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 20:21:16 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.019978	Loss2: 0.018760	 Dis: 3.235886 Entropy: 4.521930 
[2022-10-02 20:21:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.000987	 E_lr_2:0.000987
[2022-10-02 20:21:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 20:21:22 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.049279	Loss2: 0.052327	 Dis: 3.867413 Entropy: 4.926423 
[2022-10-02 20:21:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.001487	 E_lr_2:0.001487
[2022-10-02 20:21:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 20:21:29 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.032742	Loss2: 0.050109	 Dis: 3.046314 Entropy: 5.338285 
[2022-10-02 20:21:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.001987	 E_lr_2:0.001987
[2022-10-02 20:21:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 20:21:36 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.054056	Loss2: 0.050858	 Dis: 3.429007 Entropy: 5.411511 
[2022-10-02 20:21:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.002487	 E_lr_2:0.002487
[2022-10-02 20:21:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 20:21:43 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.046561	Loss2: 0.045517	 Dis: 3.275642 Entropy: 4.649362 
[2022-10-02 20:21:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.002987	 E_lr_2:0.002987
[2022-10-02 20:21:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 20:21:50 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.086543	Loss2: 0.109214	 Dis: 3.243490 Entropy: 5.104186 
[2022-10-02 20:21:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.003487	 E_lr_2:0.003487
[2022-10-02 20:21:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 20:21:56 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.124030	Loss2: 0.134920	 Dis: 3.944328 Entropy: 5.432369 
[2022-10-02 20:21:56 demo] (houston_program2.py 512): INFO E_lr_1: 0.003987	 E_lr_2:0.003987
[2022-10-02 20:21:56 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 20:22:03 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.109508	Loss2: 0.112809	 Dis: 3.914892 Entropy: 4.475564 
[2022-10-02 20:22:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.004487	 E_lr_2:0.004487
[2022-10-02 20:22:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 20:22:09 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.104674	Loss2: 0.109782	 Dis: 3.933977 Entropy: 5.343410 
[2022-10-02 20:22:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.004987	 E_lr_2:0.004987
[2022-10-02 20:22:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000050	 C_lr_2:0.000050	 C_lr_3:0.000050	 C_lr_4:0.000050
[2022-10-02 20:22:16 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.049185	Loss2: 0.058678	 Dis: 3.082445 Entropy: 4.697091 
[2022-10-02 20:22:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.004853	 E_lr_2:0.004853
[2022-10-02 20:22:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000049	 C_lr_2:0.000049	 C_lr_3:0.000049	 C_lr_4:0.000049
[2022-10-02 20:22:22 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.177774	Loss2: 0.239047	 Dis: 5.568621 Entropy: 4.349409 
[2022-10-02 20:22:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.004825	 E_lr_2:0.004825
[2022-10-02 20:22:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 20:22:29 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.084098	Loss2: 0.065832	 Dis: 5.110302 Entropy: 4.521338 
[2022-10-02 20:22:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.004795	 E_lr_2:0.004795
[2022-10-02 20:22:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 20:22:35 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.170238	Loss2: 0.192254	 Dis: 3.267450 Entropy: 4.481665 
[2022-10-02 20:22:35 demo] (houston_program2.py 512): INFO E_lr_1: 0.004763	 E_lr_2:0.004763
[2022-10-02 20:22:35 demo] (houston_program2.py 513): INFO C_lr_1: 0.000048	 C_lr_2:0.000048	 C_lr_3:0.000048	 C_lr_4:0.000048
[2022-10-02 20:22:42 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.169010	Loss2: 0.173864	 Dis: 5.303608 Entropy: 5.735951 
[2022-10-02 20:22:42 demo] (houston_program2.py 512): INFO E_lr_1: 0.004728	 E_lr_2:0.004728
[2022-10-02 20:22:42 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 20:22:49 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.196165	Loss2: 0.174398	 Dis: 4.996576 Entropy: 4.609914 
[2022-10-02 20:22:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.004692	 E_lr_2:0.004692
[2022-10-02 20:22:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 20:22:55 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.129198	Loss2: 0.134891	 Dis: 2.933369 Entropy: 4.724794 
[2022-10-02 20:22:55 demo] (houston_program2.py 512): INFO E_lr_1: 0.004653	 E_lr_2:0.004653
[2022-10-02 20:22:55 demo] (houston_program2.py 513): INFO C_lr_1: 0.000047	 C_lr_2:0.000047	 C_lr_3:0.000047	 C_lr_4:0.000047
[2022-10-02 20:23:02 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.063255	Loss2: 0.070615	 Dis: 3.179155 Entropy: 5.279999 
[2022-10-02 20:23:02 demo] (houston_program2.py 512): INFO E_lr_1: 0.004612	 E_lr_2:0.004612
[2022-10-02 20:23:02 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 20:23:08 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.050088	Loss2: 0.059009	 Dis: 4.548519 Entropy: 4.630260 
[2022-10-02 20:23:08 demo] (houston_program2.py 512): INFO E_lr_1: 0.004569	 E_lr_2:0.004569
[2022-10-02 20:23:08 demo] (houston_program2.py 513): INFO C_lr_1: 0.000046	 C_lr_2:0.000046	 C_lr_3:0.000046	 C_lr_4:0.000046
[2022-10-02 20:23:15 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.191797	Loss2: 0.209025	 Dis: 5.564692 Entropy: 4.571196 
[2022-10-02 20:23:15 demo] (houston_program2.py 512): INFO E_lr_1: 0.004524	 E_lr_2:0.004524
[2022-10-02 20:23:15 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 20:23:22 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.258716	Loss2: 0.271069	 Dis: 4.598509 Entropy: 4.518333 
[2022-10-02 20:23:22 demo] (houston_program2.py 512): INFO E_lr_1: 0.004477	 E_lr_2:0.004477
[2022-10-02 20:23:22 demo] (houston_program2.py 513): INFO C_lr_1: 0.000045	 C_lr_2:0.000045	 C_lr_3:0.000045	 C_lr_4:0.000045
[2022-10-02 20:23:29 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.038308	Loss2: 0.054643	 Dis: 5.381723 Entropy: 4.902368 
[2022-10-02 20:23:29 demo] (houston_program2.py 512): INFO E_lr_1: 0.004428	 E_lr_2:0.004428
[2022-10-02 20:23:29 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 20:23:36 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.015521	Loss2: 0.050892	 Dis: 3.426685 Entropy: 4.679548 
[2022-10-02 20:23:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.004377	 E_lr_2:0.004377
[2022-10-02 20:23:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000044	 C_lr_2:0.000044	 C_lr_3:0.000044	 C_lr_4:0.000044
[2022-10-02 20:23:43 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.100999	Loss2: 0.069937	 Dis: 5.756912 Entropy: 4.392412 
[2022-10-02 20:23:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.004324	 E_lr_2:0.004324
[2022-10-02 20:23:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 20:23:45 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.038729	Loss2: 0.028946	 Dis: 4.573162 Entropy: 5.451942 
[2022-10-02 20:23:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.004269	 E_lr_2:0.004269
[2022-10-02 20:23:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000043	 C_lr_2:0.000043	 C_lr_3:0.000043	 C_lr_4:0.000043
[2022-10-02 20:23:52 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.210052	Loss2: 0.207739	 Dis: 2.370419 Entropy: 5.125420 
[2022-10-02 20:23:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.004213	 E_lr_2:0.004213
[2022-10-02 20:23:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 20:23:59 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.143560	Loss2: 0.129832	 Dis: 3.985552 Entropy: 6.563510 
[2022-10-02 20:23:59 demo] (houston_program2.py 512): INFO E_lr_1: 0.004155	 E_lr_2:0.004155
[2022-10-02 20:23:59 demo] (houston_program2.py 513): INFO C_lr_1: 0.000042	 C_lr_2:0.000042	 C_lr_3:0.000042	 C_lr_4:0.000042
[2022-10-02 20:24:05 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.170732	Loss2: 0.184663	 Dis: 4.733444 Entropy: 5.112874 
[2022-10-02 20:24:05 demo] (houston_program2.py 512): INFO E_lr_1: 0.004095	 E_lr_2:0.004095
[2022-10-02 20:24:05 demo] (houston_program2.py 513): INFO C_lr_1: 0.000041	 C_lr_2:0.000041	 C_lr_3:0.000041	 C_lr_4:0.000041
[2022-10-02 20:24:12 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.065484	Loss2: 0.053709	 Dis: 3.671640 Entropy: 5.593933 
[2022-10-02 20:24:12 demo] (houston_program2.py 512): INFO E_lr_1: 0.004034	 E_lr_2:0.004034
[2022-10-02 20:24:12 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 20:24:17 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.124535	Loss2: 0.122149	 Dis: 3.283260 Entropy: 4.917226 
[2022-10-02 20:24:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.003971	 E_lr_2:0.003971
[2022-10-02 20:24:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000040	 C_lr_2:0.000040	 C_lr_3:0.000040	 C_lr_4:0.000040
[2022-10-02 20:24:23 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.101166	Loss2: 0.092837	 Dis: 3.855059 Entropy: 5.411906 
[2022-10-02 20:24:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.003907	 E_lr_2:0.003907
[2022-10-02 20:24:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000039	 C_lr_2:0.000039	 C_lr_3:0.000039	 C_lr_4:0.000039
[2022-10-02 20:24:30 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.062070	Loss2: 0.063980	 Dis: 4.164124 Entropy: 4.469974 
[2022-10-02 20:24:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.003841	 E_lr_2:0.003841
[2022-10-02 20:24:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 20:24:37 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.543784	Loss2: 0.526388	 Dis: 5.525047 Entropy: 5.742017 
[2022-10-02 20:24:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.003774	 E_lr_2:0.003774
[2022-10-02 20:24:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000038	 C_lr_2:0.000038	 C_lr_3:0.000038	 C_lr_4:0.000038
[2022-10-02 20:24:44 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.043475	Loss2: 0.064985	 Dis: 3.933622 Entropy: 4.325721 
[2022-10-02 20:24:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.003706	 E_lr_2:0.003706
[2022-10-02 20:24:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000037	 C_lr_2:0.000037	 C_lr_3:0.000037	 C_lr_4:0.000037
[2022-10-02 20:24:50 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.059285	Loss2: 0.087970	 Dis: 3.902277 Entropy: 5.129066 
[2022-10-02 20:24:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.003637	 E_lr_2:0.003637
[2022-10-02 20:24:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 20:24:57 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.059962	Loss2: 0.067433	 Dis: 3.491028 Entropy: 6.156459 
[2022-10-02 20:24:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.003566	 E_lr_2:0.003566
[2022-10-02 20:24:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000036	 C_lr_2:0.000036	 C_lr_3:0.000036	 C_lr_4:0.000036
[2022-10-02 20:25:03 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.048490	Loss2: 0.041691	 Dis: 2.840694 Entropy: 4.639805 
[2022-10-02 20:25:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.003495	 E_lr_2:0.003495
[2022-10-02 20:25:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000035	 C_lr_2:0.000035	 C_lr_3:0.000035	 C_lr_4:0.000035
[2022-10-02 20:25:10 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.026813	Loss2: 0.026402	 Dis: 2.825909 Entropy: 5.169217 
[2022-10-02 20:25:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.003422	 E_lr_2:0.003422
[2022-10-02 20:25:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000034	 C_lr_2:0.000034	 C_lr_3:0.000034	 C_lr_4:0.000034
[2022-10-02 20:25:17 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.058312	Loss2: 0.059069	 Dis: 2.701385 Entropy: 4.906704 
[2022-10-02 20:25:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.003349	 E_lr_2:0.003349
[2022-10-02 20:25:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 20:25:24 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.029052	Loss2: 0.025761	 Dis: 4.579458 Entropy: 5.213431 
[2022-10-02 20:25:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.003274	 E_lr_2:0.003274
[2022-10-02 20:25:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000033	 C_lr_2:0.000033	 C_lr_3:0.000033	 C_lr_4:0.000033
[2022-10-02 20:25:31 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.183428	Loss2: 0.163617	 Dis: 3.326910 Entropy: 6.099031 
[2022-10-02 20:25:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.003199	 E_lr_2:0.003199
[2022-10-02 20:25:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000032	 C_lr_2:0.000032	 C_lr_3:0.000032	 C_lr_4:0.000032
[2022-10-02 20:25:37 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.013332	Loss2: 0.019339	 Dis: 3.111967 Entropy: 4.671815 
[2022-10-02 20:25:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.003124	 E_lr_2:0.003124
[2022-10-02 20:25:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000031	 C_lr_2:0.000031	 C_lr_3:0.000031	 C_lr_4:0.000031
[2022-10-02 20:25:44 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.030835	Loss2: 0.039546	 Dis: 2.703550 Entropy: 4.780688 
[2022-10-02 20:25:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.003047	 E_lr_2:0.003047
[2022-10-02 20:25:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 20:25:50 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.041004	Loss2: 0.036656	 Dis: 2.328924 Entropy: 4.510335 
[2022-10-02 20:25:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.002970	 E_lr_2:0.002970
[2022-10-02 20:25:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000030	 C_lr_2:0.000030	 C_lr_3:0.000030	 C_lr_4:0.000030
[2022-10-02 20:25:57 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.051236	Loss2: 0.040916	 Dis: 4.276976 Entropy: 6.306847 
[2022-10-02 20:25:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.002893	 E_lr_2:0.002893
[2022-10-02 20:25:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000029	 C_lr_2:0.000029	 C_lr_3:0.000029	 C_lr_4:0.000029
[2022-10-02 20:26:03 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.037422	Loss2: 0.041236	 Dis: 4.594906 Entropy: 5.355546 
[2022-10-02 20:26:03 demo] (houston_program2.py 512): INFO E_lr_1: 0.002815	 E_lr_2:0.002815
[2022-10-02 20:26:03 demo] (houston_program2.py 513): INFO C_lr_1: 0.000028	 C_lr_2:0.000028	 C_lr_3:0.000028	 C_lr_4:0.000028
[2022-10-02 20:26:10 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.061726	Loss2: 0.080667	 Dis: 3.775114 Entropy: 5.316082 
[2022-10-02 20:26:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.002737	 E_lr_2:0.002737
[2022-10-02 20:26:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 20:26:17 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.012937	Loss2: 0.013125	 Dis: 4.853840 Entropy: 4.519295 
[2022-10-02 20:26:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.002659	 E_lr_2:0.002659
[2022-10-02 20:26:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000027	 C_lr_2:0.000027	 C_lr_3:0.000027	 C_lr_4:0.000027
[2022-10-02 20:26:23 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.137120	Loss2: 0.146240	 Dis: 4.022394 Entropy: 5.026592 
[2022-10-02 20:26:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.002581	 E_lr_2:0.002581
[2022-10-02 20:26:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000026	 C_lr_2:0.000026	 C_lr_3:0.000026	 C_lr_4:0.000026
[2022-10-02 20:26:30 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.092632	Loss2: 0.100969	 Dis: 3.688559 Entropy: 4.676083 
[2022-10-02 20:26:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.002502	 E_lr_2:0.002502
[2022-10-02 20:26:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000025	 C_lr_2:0.000025	 C_lr_3:0.000025	 C_lr_4:0.000025
[2022-10-02 20:26:37 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.032455	Loss2: 0.036488	 Dis: 4.533871 Entropy: 4.206676 
[2022-10-02 20:26:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.002423	 E_lr_2:0.002423
[2022-10-02 20:26:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000024	 C_lr_2:0.000024	 C_lr_3:0.000024	 C_lr_4:0.000024
[2022-10-02 20:26:44 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.286039	Loss2: 0.279159	 Dis: 2.696611 Entropy: 5.694929 
[2022-10-02 20:26:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.002345	 E_lr_2:0.002345
[2022-10-02 20:26:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 20:26:51 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.040916	Loss2: 0.030267	 Dis: 4.364002 Entropy: 4.366670 
[2022-10-02 20:26:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.002267	 E_lr_2:0.002267
[2022-10-02 20:26:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000023	 C_lr_2:0.000023	 C_lr_3:0.000023	 C_lr_4:0.000023
[2022-10-02 20:26:58 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.028295	Loss2: 0.026322	 Dis: 1.638947 Entropy: 4.770021 
[2022-10-02 20:26:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.002189	 E_lr_2:0.002189
[2022-10-02 20:26:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000022	 C_lr_2:0.000022	 C_lr_3:0.000022	 C_lr_4:0.000022
[2022-10-02 20:27:04 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.108352	Loss2: 0.102217	 Dis: 2.292030 Entropy: 4.411650 
[2022-10-02 20:27:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.002111	 E_lr_2:0.002111
[2022-10-02 20:27:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000021	 C_lr_2:0.000021	 C_lr_3:0.000021	 C_lr_4:0.000021
[2022-10-02 20:27:11 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.039388	Loss2: 0.041797	 Dis: 4.162296 Entropy: 4.928086 
[2022-10-02 20:27:11 demo] (houston_program2.py 512): INFO E_lr_1: 0.002034	 E_lr_2:0.002034
[2022-10-02 20:27:11 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 20:27:17 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.030047	Loss2: 0.023706	 Dis: 2.554253 Entropy: 5.473596 
[2022-10-02 20:27:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.001957	 E_lr_2:0.001957
[2022-10-02 20:27:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000020	 C_lr_2:0.000020	 C_lr_3:0.000020	 C_lr_4:0.000020
[2022-10-02 20:27:24 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.011067	Loss2: 0.011866	 Dis: 2.720249 Entropy: 5.445895 
[2022-10-02 20:27:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.001880	 E_lr_2:0.001880
[2022-10-02 20:27:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000019	 C_lr_2:0.000019	 C_lr_3:0.000019	 C_lr_4:0.000019
[2022-10-02 20:27:31 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.033697	Loss2: 0.029092	 Dis: 2.312294 Entropy: 5.532157 
[2022-10-02 20:27:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.001804	 E_lr_2:0.001804
[2022-10-02 20:27:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000018	 C_lr_2:0.000018	 C_lr_3:0.000018	 C_lr_4:0.000018
[2022-10-02 20:27:38 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.010639	Loss2: 0.012685	 Dis: 2.431362 Entropy: 5.369995 
[2022-10-02 20:27:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.001729	 E_lr_2:0.001729
[2022-10-02 20:27:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 20:27:44 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.010543	Loss2: 0.012641	 Dis: 2.239784 Entropy: 5.406271 
[2022-10-02 20:27:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.001655	 E_lr_2:0.001655
[2022-10-02 20:27:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000017	 C_lr_2:0.000017	 C_lr_3:0.000017	 C_lr_4:0.000017
[2022-10-02 20:27:51 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.008139	Loss2: 0.008995	 Dis: 2.679205 Entropy: 4.464813 
[2022-10-02 20:27:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.001582	 E_lr_2:0.001582
[2022-10-02 20:27:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000016	 C_lr_2:0.000016	 C_lr_3:0.000016	 C_lr_4:0.000016
[2022-10-02 20:27:58 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.028057	Loss2: 0.041646	 Dis: 3.218332 Entropy: 5.129775 
[2022-10-02 20:27:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.001509	 E_lr_2:0.001509
[2022-10-02 20:27:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000015	 C_lr_2:0.000015	 C_lr_3:0.000015	 C_lr_4:0.000015
[2022-10-02 20:28:04 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.047928	Loss2: 0.043068	 Dis: 5.770945 Entropy: 4.426037 
[2022-10-02 20:28:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.001437	 E_lr_2:0.001437
[2022-10-02 20:28:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 20:28:11 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.023990	Loss2: 0.030153	 Dis: 3.642334 Entropy: 5.564016 
[2022-10-02 20:28:11 demo] (houston_program2.py 512): INFO E_lr_1: 0.001367	 E_lr_2:0.001367
[2022-10-02 20:28:11 demo] (houston_program2.py 513): INFO C_lr_1: 0.000014	 C_lr_2:0.000014	 C_lr_3:0.000014	 C_lr_4:0.000014
[2022-10-02 20:28:18 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.008198	Loss2: 0.007591	 Dis: 2.826765 Entropy: 5.397517 
[2022-10-02 20:28:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.001297	 E_lr_2:0.001297
[2022-10-02 20:28:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000013	 C_lr_2:0.000013	 C_lr_3:0.000013	 C_lr_4:0.000013
[2022-10-02 20:28:24 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.009626	Loss2: 0.007933	 Dis: 1.982773 Entropy: 5.448468 
[2022-10-02 20:28:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.001229	 E_lr_2:0.001229
[2022-10-02 20:28:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 20:28:31 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.006700	Loss2: 0.006694	 Dis: 2.934641 Entropy: 5.285131 
[2022-10-02 20:28:31 demo] (houston_program2.py 512): INFO E_lr_1: 0.001162	 E_lr_2:0.001162
[2022-10-02 20:28:31 demo] (houston_program2.py 513): INFO C_lr_1: 0.000012	 C_lr_2:0.000012	 C_lr_3:0.000012	 C_lr_4:0.000012
[2022-10-02 20:28:37 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.010175	Loss2: 0.008417	 Dis: 1.334541 Entropy: 5.706029 
[2022-10-02 20:28:37 demo] (houston_program2.py 512): INFO E_lr_1: 0.001096	 E_lr_2:0.001096
[2022-10-02 20:28:37 demo] (houston_program2.py 513): INFO C_lr_1: 0.000011	 C_lr_2:0.000011	 C_lr_3:0.000011	 C_lr_4:0.000011
[2022-10-02 20:28:44 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.005409	Loss2: 0.008062	 Dis: 1.499359 Entropy: 4.550047 
[2022-10-02 20:28:44 demo] (houston_program2.py 512): INFO E_lr_1: 0.001032	 E_lr_2:0.001032
[2022-10-02 20:28:44 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 20:28:50 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.017271	Loss2: 0.011841	 Dis: 3.325171 Entropy: 4.456415 
[2022-10-02 20:28:50 demo] (houston_program2.py 512): INFO E_lr_1: 0.000969	 E_lr_2:0.000969
[2022-10-02 20:28:50 demo] (houston_program2.py 513): INFO C_lr_1: 0.000010	 C_lr_2:0.000010	 C_lr_3:0.000010	 C_lr_4:0.000010
[2022-10-02 20:28:57 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.006700	Loss2: 0.007853	 Dis: 1.128323 Entropy: 5.807400 
[2022-10-02 20:28:57 demo] (houston_program2.py 512): INFO E_lr_1: 0.000908	 E_lr_2:0.000908
[2022-10-02 20:28:57 demo] (houston_program2.py 513): INFO C_lr_1: 0.000009	 C_lr_2:0.000009	 C_lr_3:0.000009	 C_lr_4:0.000009
[2022-10-02 20:29:04 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.021120	Loss2: 0.023889	 Dis: 3.483421 Entropy: 4.542531 
[2022-10-02 20:29:04 demo] (houston_program2.py 512): INFO E_lr_1: 0.000848	 E_lr_2:0.000848
[2022-10-02 20:29:04 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 20:29:10 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.008788	Loss2: 0.003991	 Dis: 2.108585 Entropy: 4.897155 
[2022-10-02 20:29:10 demo] (houston_program2.py 512): INFO E_lr_1: 0.000790	 E_lr_2:0.000790
[2022-10-02 20:29:10 demo] (houston_program2.py 513): INFO C_lr_1: 0.000008	 C_lr_2:0.000008	 C_lr_3:0.000008	 C_lr_4:0.000008
[2022-10-02 20:29:17 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.006159	Loss2: 0.007427	 Dis: 0.320059 Entropy: 6.868952 
[2022-10-02 20:29:17 demo] (houston_program2.py 512): INFO E_lr_1: 0.000734	 E_lr_2:0.000734
[2022-10-02 20:29:17 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 20:29:23 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.063587	Loss2: 0.009529	 Dis: 1.218895 Entropy: 4.897343 
[2022-10-02 20:29:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.000679	 E_lr_2:0.000679
[2022-10-02 20:29:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000007	 C_lr_2:0.000007	 C_lr_3:0.000007	 C_lr_4:0.000007
[2022-10-02 20:29:30 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.004230	Loss2: 0.006113	 Dis: 1.856716 Entropy: 6.090333 
[2022-10-02 20:29:30 demo] (houston_program2.py 512): INFO E_lr_1: 0.000626	 E_lr_2:0.000626
[2022-10-02 20:29:30 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 20:29:36 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.002479	Loss2: 0.002619	 Dis: 2.788450 Entropy: 4.783778 
[2022-10-02 20:29:36 demo] (houston_program2.py 512): INFO E_lr_1: 0.000575	 E_lr_2:0.000575
[2022-10-02 20:29:36 demo] (houston_program2.py 513): INFO C_lr_1: 0.000006	 C_lr_2:0.000006	 C_lr_3:0.000006	 C_lr_4:0.000006
[2022-10-02 20:29:43 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.004698	Loss2: 0.004857	 Dis: 2.682379 Entropy: 6.090097 
[2022-10-02 20:29:43 demo] (houston_program2.py 512): INFO E_lr_1: 0.000526	 E_lr_2:0.000526
[2022-10-02 20:29:43 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 20:29:49 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.003534	Loss2: 0.004693	 Dis: 2.478720 Entropy: 4.902898 
[2022-10-02 20:29:49 demo] (houston_program2.py 512): INFO E_lr_1: 0.000479	 E_lr_2:0.000479
[2022-10-02 20:29:49 demo] (houston_program2.py 513): INFO C_lr_1: 0.000005	 C_lr_2:0.000005	 C_lr_3:0.000005	 C_lr_4:0.000005
[2022-10-02 20:29:56 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.020468	Loss2: 0.027069	 Dis: 1.244225 Entropy: 5.743499 
[2022-10-02 20:29:56 demo] (houston_program2.py 512): INFO E_lr_1: 0.000433	 E_lr_2:0.000433
[2022-10-02 20:29:56 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 20:30:02 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.029497	Loss2: 0.041188	 Dis: 1.937840 Entropy: 4.653909 
[2022-10-02 20:30:02 demo] (houston_program2.py 512): INFO E_lr_1: 0.000390	 E_lr_2:0.000390
[2022-10-02 20:30:02 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 20:30:09 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.014244	Loss2: 0.015416	 Dis: 3.589083 Entropy: 5.772223 
[2022-10-02 20:30:09 demo] (houston_program2.py 512): INFO E_lr_1: 0.000349	 E_lr_2:0.000349
[2022-10-02 20:30:09 demo] (houston_program2.py 513): INFO C_lr_1: 0.000004	 C_lr_2:0.000004	 C_lr_3:0.000004	 C_lr_4:0.000004
[2022-10-02 20:30:16 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.001885	Loss2: 0.001903	 Dis: 1.639277 Entropy: 4.488286 
[2022-10-02 20:30:16 demo] (houston_program2.py 512): INFO E_lr_1: 0.000310	 E_lr_2:0.000310
[2022-10-02 20:30:16 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 20:30:23 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.001533	Loss2: 0.001835	 Dis: 3.227884 Entropy: 5.624377 
[2022-10-02 20:30:23 demo] (houston_program2.py 512): INFO E_lr_1: 0.000273	 E_lr_2:0.000273
[2022-10-02 20:30:23 demo] (houston_program2.py 513): INFO C_lr_1: 0.000003	 C_lr_2:0.000003	 C_lr_3:0.000003	 C_lr_4:0.000003
[2022-10-02 20:30:26 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.003219	Loss2: 0.002713	 Dis: 0.985357 Entropy: 4.512755 
[2022-10-02 20:30:26 demo] (houston_program2.py 512): INFO E_lr_1: 0.000239	 E_lr_2:0.000239
[2022-10-02 20:30:26 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 20:30:32 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.001759	Loss2: 0.002316	 Dis: 1.713936 Entropy: 5.106291 
[2022-10-02 20:30:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.000206	 E_lr_2:0.000206
[2022-10-02 20:30:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 20:30:39 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.004420	Loss2: 0.003886	 Dis: 3.571836 Entropy: 4.771358 
[2022-10-02 20:30:39 demo] (houston_program2.py 512): INFO E_lr_1: 0.000176	 E_lr_2:0.000176
[2022-10-02 20:30:39 demo] (houston_program2.py 513): INFO C_lr_1: 0.000002	 C_lr_2:0.000002	 C_lr_3:0.000002	 C_lr_4:0.000002
[2022-10-02 20:30:45 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.004873	Loss2: 0.004513	 Dis: 1.539040 Entropy: 4.582961 
[2022-10-02 20:30:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.000148	 E_lr_2:0.000148
[2022-10-02 20:30:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 20:30:52 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.004842	Loss2: 0.009865	 Dis: 1.792263 Entropy: 5.811980 
[2022-10-02 20:30:52 demo] (houston_program2.py 512): INFO E_lr_1: 0.000123	 E_lr_2:0.000123
[2022-10-02 20:30:52 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 20:30:58 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.005500	Loss2: 0.003741	 Dis: 2.208935 Entropy: 4.320720 
[2022-10-02 20:30:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.000100	 E_lr_2:0.000100
[2022-10-02 20:30:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 20:31:05 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.005034	Loss2: 0.005727	 Dis: 1.414967 Entropy: 6.825263 
[2022-10-02 20:31:05 demo] (houston_program2.py 512): INFO E_lr_1: 0.000079	 E_lr_2:0.000079
[2022-10-02 20:31:05 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 20:31:11 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.032844	Loss2: 0.019071	 Dis: 0.852367 Entropy: 4.419856 
[2022-10-02 20:31:11 demo] (houston_program2.py 512): INFO E_lr_1: 0.000061	 E_lr_2:0.000061
[2022-10-02 20:31:11 demo] (houston_program2.py 513): INFO C_lr_1: 0.000001	 C_lr_2:0.000001	 C_lr_3:0.000001	 C_lr_4:0.000001
[2022-10-02 20:31:18 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.001653	Loss2: 0.001588	 Dis: 1.656906 Entropy: 4.574986 
[2022-10-02 20:31:18 demo] (houston_program2.py 512): INFO E_lr_1: 0.000045	 E_lr_2:0.000045
[2022-10-02 20:31:18 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 20:31:24 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.002924	Loss2: 0.002392	 Dis: 0.968964 Entropy: 4.863244 
[2022-10-02 20:31:24 demo] (houston_program2.py 512): INFO E_lr_1: 0.000031	 E_lr_2:0.000031
[2022-10-02 20:31:24 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 20:31:32 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.039630	Loss2: 0.012763	 Dis: 1.716345 Entropy: 5.634710 
[2022-10-02 20:31:32 demo] (houston_program2.py 512): INFO E_lr_1: 0.000020	 E_lr_2:0.000020
[2022-10-02 20:31:32 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 20:31:38 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.002802	Loss2: 0.002909	 Dis: 3.288887 Entropy: 4.898602 
[2022-10-02 20:31:38 demo] (houston_program2.py 512): INFO E_lr_1: 0.000011	 E_lr_2:0.000011
[2022-10-02 20:31:38 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 20:31:45 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.003969	Loss2: 0.002731	 Dis: 1.903431 Entropy: 4.581710 
[2022-10-02 20:31:45 demo] (houston_program2.py 512): INFO E_lr_1: 0.000005	 E_lr_2:0.000005
[2022-10-02 20:31:45 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 20:31:51 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.002447	Loss2: 0.002425	 Dis: 2.401142 Entropy: 5.027815 
[2022-10-02 20:31:51 demo] (houston_program2.py 512): INFO E_lr_1: 0.000001	 E_lr_2:0.000001
[2022-10-02 20:31:51 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 20:31:58 demo] (houston_program2.py 504): INFO Train Ep: 99 	Loss1: 0.005319	Loss2: 0.003629	 Dis: 1.511223 Entropy: 4.620220 
[2022-10-02 20:31:58 demo] (houston_program2.py 512): INFO E_lr_1: 0.000000	 E_lr_2:0.000000
[2022-10-02 20:31:58 demo] (houston_program2.py 513): INFO C_lr_1: 0.000000	 C_lr_2:0.000000	 C_lr_3:0.000000	 C_lr_4:0.000000
[2022-10-02 20:31:58 demo] (houston_program2.py 515): INFO time_99_epoch:654.8313946723938
[2022-10-02 20:32:05 demo] (houston_program2.py 673): INFO 	val_Accuracy: 34225/53200 (64.33%)	
[2022-10-02 20:32:05 demo] (utils.py 47): INFO outputs/demo/finetune_100/ckpt_epoch_99.pth saving......
[2022-10-02 20:32:05 demo] (utils.py 49): INFO outputs/demo/finetune_100/ckpt_epoch_99.pth saved !!!
